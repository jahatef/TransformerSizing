num_attention_heads: 40, hidden_size: 40, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x1x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x1x2048): 1.023
Elapsed time for attention_prob_times_values (160x2048x2048x1): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x1): 0.212

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 0.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x2x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x2x2048): 1.892
Elapsed time for attention_prob_times_values (160x2048x2048x2): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x2): 1.529

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 1.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x3x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x3x2048): 2.636
Elapsed time for attention_prob_times_values (160x2048x2048x3): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x3): 2.259

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 2.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x4x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x4x2048): 3.381
Elapsed time for attention_prob_times_values (160x2048x2048x4): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x4): 3.018

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 3.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x5x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x5x2048): 3.972
Elapsed time for attention_prob_times_values (160x2048x2048x5): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x5): 3.188

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 4.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x6x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x6x2048): 4.963
Elapsed time for attention_prob_times_values (160x2048x2048x6): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x6): 3.940

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 5.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x7x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x7x2048): 5.836
Elapsed time for attention_prob_times_values (160x2048x2048x7): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x7): 4.609

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 6.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x8x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x8x2048): 6.571
Elapsed time for attention_prob_times_values (160x2048x2048x8): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x8): 7.041

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 8.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x9x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x9x2048): 7.131
Elapsed time for attention_prob_times_values (160x2048x2048x9): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x9): 7.604

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 9.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x10x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x10x2048): 7.893
Elapsed time for attention_prob_times_values (160x2048x2048x10): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x10): 8.720

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 11.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x11x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x11x2048): 7.885
Elapsed time for attention_prob_times_values (160x2048x2048x11): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x11): 9.247

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 12.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x12x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x12x2048): 8.819
Elapsed time for attention_prob_times_values (160x2048x2048x12): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x12): 10.457

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 14.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x13x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x13x2048): 10.159
Elapsed time for attention_prob_times_values (160x2048x2048x13): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x13): 10.920

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 15.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x14x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x14x2048): 10.952
Elapsed time for attention_prob_times_values (160x2048x2048x14): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x14): 12.125

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 17.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x15x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x15x2048): 11.709
Elapsed time for attention_prob_times_values (160x2048x2048x15): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x15): 12.488

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 19.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x16x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x16x2048): 12.555
Elapsed time for attention_prob_times_values (160x2048x2048x16): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x16): 13.911

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 21.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x17x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x17x2048): 12.832
Elapsed time for attention_prob_times_values (160x2048x2048x17): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x17): 13.058

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 21.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x18x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x18x2048): 13.636
Elapsed time for attention_prob_times_values (160x2048x2048x18): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x18): 15.261

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 24.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x19x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x19x2048): 11.568
Elapsed time for attention_prob_times_values (160x2048x2048x19): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x19): 15.665

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 23.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x20x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x20x2048): 14.917
Elapsed time for attention_prob_times_values (160x2048x2048x20): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x20): 15.481

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 27.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x21x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x21x2048): 15.792
Elapsed time for attention_prob_times_values (160x2048x2048x21): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x21): 17.133

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 29.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x22x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x22x2048): 16.564
Elapsed time for attention_prob_times_values (160x2048x2048x22): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x22): 18.405

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 32.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x23x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x23x2048): 14.821
Elapsed time for attention_prob_times_values (160x2048x2048x23): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x23): 16.672

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 29.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x24x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x24x2048): 18.010
Elapsed time for attention_prob_times_values (160x2048x2048x24): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x24): 18.812

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 35.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x25x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x25x2048): 18.278
Elapsed time for attention_prob_times_values (160x2048x2048x25): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x25): 19.303

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 37.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x26x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x26x2048): 18.959
Elapsed time for attention_prob_times_values (160x2048x2048x26): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x26): 21.315

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 40.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x27x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x27x2048): 19.454
Elapsed time for attention_prob_times_values (160x2048x2048x27): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x27): 21.667

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 42.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x28x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x28x2048): 19.379
Elapsed time for attention_prob_times_values (160x2048x2048x28): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x28): 22.951

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 43.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x29x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x29x2048): 20.901
Elapsed time for attention_prob_times_values (160x2048x2048x29): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x29): 23.196

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 46.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x30x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x30x2048): 21.621
Elapsed time for attention_prob_times_values (160x2048x2048x30): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x30): 24.448

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 49.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x31x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x31x2048): 22.279
Elapsed time for attention_prob_times_values (160x2048x2048x31): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x31): 24.663

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 51.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x32x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x32x2048): 39.445
Elapsed time for attention_prob_times_values (160x2048x2048x32): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x32): 26.341

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 71.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x33x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x33x2048): 26.388
Elapsed time for attention_prob_times_values (160x2048x2048x33): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x33): 26.059

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 60.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x34x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x34x2048): 26.665
Elapsed time for attention_prob_times_values (160x2048x2048x34): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x34): 27.038

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 62.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x35x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x35x2048): 23.417
Elapsed time for attention_prob_times_values (160x2048x2048x35): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x35): 27.453

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 59.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x36x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x36x2048): 23.767
Elapsed time for attention_prob_times_values (160x2048x2048x36): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x36): 28.791

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 62.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x37x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x37x2048): 24.936
Elapsed time for attention_prob_times_values (160x2048x2048x37): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x37): 29.030

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 65.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x38x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x38x2048): 25.858
Elapsed time for attention_prob_times_values (160x2048x2048x38): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x38): 30.142

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 69.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x39x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x39x2048): 26.246
Elapsed time for attention_prob_times_values (160x2048x2048x39): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x39): 30.304

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 70.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x40x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x40x2048): 28.007
Elapsed time for attention_prob_times_values (160x2048x2048x40): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x40): 29.435

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 73.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x41x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x41x2048): 26.995
Elapsed time for attention_prob_times_values (160x2048x2048x41): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x41): 31.536

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 75.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x42x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x42x2048): 28.487
Elapsed time for attention_prob_times_values (160x2048x2048x42): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x42): 30.839

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 78.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x43x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x43x2048): 28.684
Elapsed time for attention_prob_times_values (160x2048x2048x43): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x43): 33.002

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 82.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x44x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x44x2048): 29.707
Elapsed time for attention_prob_times_values (160x2048x2048x44): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x44): 34.455

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 86.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x45x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x45x2048): 29.300
Elapsed time for attention_prob_times_values (160x2048x2048x45): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x45): 34.100

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 86.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x46x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x46x2048): 31.031
Elapsed time for attention_prob_times_values (160x2048x2048x46): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x46): 32.528

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 88.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x47x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x47x2048): 31.581
Elapsed time for attention_prob_times_values (160x2048x2048x47): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x47): 35.784

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 95.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x48x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x48x2048): 31.056
Elapsed time for attention_prob_times_values (160x2048x2048x48): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x48): 38.421

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 98.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 1960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x49x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x49x2048): 30.714
Elapsed time for attention_prob_times_values (160x2048x2048x49): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x49): 37.181

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 98.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x50x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x50x2048): 33.774
Elapsed time for attention_prob_times_values (160x2048x2048x50): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x50): 38.073

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 105.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x51x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x51x2048): 33.080
Elapsed time for attention_prob_times_values (160x2048x2048x51): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x51): 37.717

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 105.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x52x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x52x2048): 35.189
Elapsed time for attention_prob_times_values (160x2048x2048x52): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x52): 39.441

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 112.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x53x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x53x2048): 35.456
Elapsed time for attention_prob_times_values (160x2048x2048x53): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x53): 39.503

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 114.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x54x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x54x2048): 36.039
Elapsed time for attention_prob_times_values (160x2048x2048x54): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x54): 41.136

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 119.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x55x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x55x2048): 36.961
Elapsed time for attention_prob_times_values (160x2048x2048x55): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x55): 40.837

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 122.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x56x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x56x2048): 37.936
Elapsed time for attention_prob_times_values (160x2048x2048x56): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x56): 42.763

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 128.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x57x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x57x2048): 37.247
Elapsed time for attention_prob_times_values (160x2048x2048x57): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x57): 42.149

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 127.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x58x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x58x2048): 37.082
Elapsed time for attention_prob_times_values (160x2048x2048x58): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x58): 43.839

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 131.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x59x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x59x2048): 35.496
Elapsed time for attention_prob_times_values (160x2048x2048x59): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x59): 43.276

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 128.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x60x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x60x2048): 36.862
Elapsed time for attention_prob_times_values (160x2048x2048x60): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x60): 45.601

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 136.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x61x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x61x2048): 39.439
Elapsed time for attention_prob_times_values (160x2048x2048x61): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x61): 44.650

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 141.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x62x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x62x2048): 38.601
Elapsed time for attention_prob_times_values (160x2048x2048x62): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x62): 46.894

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 144.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x63x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x63x2048): 40.569
Elapsed time for attention_prob_times_values (160x2048x2048x63): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x63): 46.222

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 149.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x64x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x64x2048): 49.436
Elapsed time for attention_prob_times_values (160x2048x2048x64): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x64): 50.341

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 174.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x65x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x65x2048): 42.648
Elapsed time for attention_prob_times_values (160x2048x2048x65): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x65): 32.971

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 131.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x66x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x66x2048): 43.461
Elapsed time for attention_prob_times_values (160x2048x2048x66): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x66): 34.231

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 137.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x67x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x67x2048): 43.071
Elapsed time for attention_prob_times_values (160x2048x2048x67): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x67): 33.821

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 137.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x68x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x68x2048): 44.097
Elapsed time for attention_prob_times_values (160x2048x2048x68): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x68): 31.752

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 134.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x69x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x69x2048): 43.792
Elapsed time for attention_prob_times_values (160x2048x2048x69): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x69): 33.936

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 141.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x70x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x70x2048): 44.846
Elapsed time for attention_prob_times_values (160x2048x2048x70): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x70): 36.149

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 149.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x71x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x71x2048): 43.754
Elapsed time for attention_prob_times_values (160x2048x2048x71): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x71): 35.113

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 147.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x72x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x72x2048): 42.992
Elapsed time for attention_prob_times_values (160x2048x2048x72): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x72): 35.567

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 148.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x73x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x73x2048): 40.290
Elapsed time for attention_prob_times_values (160x2048x2048x73): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x73): 35.970

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 146.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x74x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x74x2048): 45.444
Elapsed time for attention_prob_times_values (160x2048x2048x74): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x74): 36.784

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 158.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x75x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x75x2048): 45.375
Elapsed time for attention_prob_times_values (160x2048x2048x75): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x75): 36.903

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 159.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x76x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x76x2048): 46.153
Elapsed time for attention_prob_times_values (160x2048x2048x76): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x76): 34.977

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 157.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x77x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x77x2048): 46.124
Elapsed time for attention_prob_times_values (160x2048x2048x77): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x77): 36.857

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 164.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x78x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x78x2048): 47.230
Elapsed time for attention_prob_times_values (160x2048x2048x78): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x78): 39.705

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 174.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x79x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x79x2048): 41.712
Elapsed time for attention_prob_times_values (160x2048x2048x79): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x79): 36.492

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 159.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x80x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x80x2048): 48.867
Elapsed time for attention_prob_times_values (160x2048x2048x80): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x80): 39.165

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 179.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x81x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x81x2048): 46.293
Elapsed time for attention_prob_times_values (160x2048x2048x81): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x81): 39.745

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 178.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x82x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x82x2048): 47.268
Elapsed time for attention_prob_times_values (160x2048x2048x82): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x82): 38.369

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 178.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x83x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x83x2048): 47.543
Elapsed time for attention_prob_times_values (160x2048x2048x83): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x83): 39.806

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 183.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x84x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x84x2048): 46.274
Elapsed time for attention_prob_times_values (160x2048x2048x84): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x84): 42.403

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 189.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x85x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x85x2048): 48.111
Elapsed time for attention_prob_times_values (160x2048x2048x85): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x85): 39.737

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 188.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x86x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x86x2048): 46.276
Elapsed time for attention_prob_times_values (160x2048x2048x86): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x86): 41.819

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 191.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x87x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x87x2048): 47.216
Elapsed time for attention_prob_times_values (160x2048x2048x87): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x87): 38.643

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 186.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x88x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x88x2048): 48.270
Elapsed time for attention_prob_times_values (160x2048x2048x88): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x88): 42.323

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 200.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x89x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x89x2048): 48.334
Elapsed time for attention_prob_times_values (160x2048x2048x89): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x89): 42.940

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 203.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x90x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x90x2048): 44.482
Elapsed time for attention_prob_times_values (160x2048x2048x90): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x90): 44.845

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 201.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x91x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x91x2048): 49.013
Elapsed time for attention_prob_times_values (160x2048x2048x91): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x91): 43.830

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 210.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x92x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x92x2048): 49.934
Elapsed time for attention_prob_times_values (160x2048x2048x92): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x92): 45.910

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 219.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x93x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x93x2048): 49.838
Elapsed time for attention_prob_times_values (160x2048x2048x93): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x93): 43.712

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 215.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x94x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x94x2048): 50.537
Elapsed time for attention_prob_times_values (160x2048x2048x94): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x94): 46.607

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 226.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x95x2048): 0.0201
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x95x2048): 6.331
Elapsed time for attention_prob_times_values (160x2048x2048x95): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x95): 45.418

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 52.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x96x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x96x2048): 65.341
Elapsed time for attention_prob_times_values (160x2048x2048x96): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x96): 43.207

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 247.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x97x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x97x2048): 51.168
Elapsed time for attention_prob_times_values (160x2048x2048x97): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x97): 46.426

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 233.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x98x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x98x2048): 52.293
Elapsed time for attention_prob_times_values (160x2048x2048x98): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x98): 46.698

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 238.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x99x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x99x2048): 50.661
Elapsed time for attention_prob_times_values (160x2048x2048x99): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x99): 47.267

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 238.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x100x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x100x2048): 53.113
Elapsed time for attention_prob_times_values (160x2048x2048x100): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x100): 49.037

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 250.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x101x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x101x2048): 52.101
Elapsed time for attention_prob_times_values (160x2048x2048x101): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x101): 48.160

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 247.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x102x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x102x2048): 52.111
Elapsed time for attention_prob_times_values (160x2048x2048x102): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x102): 44.909

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 240.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x103x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x103x2048): 52.973
Elapsed time for attention_prob_times_values (160x2048x2048x103): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x103): 48.328

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 253.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x104x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x104x2048): 54.487
Elapsed time for attention_prob_times_values (160x2048x2048x104): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x104): 48.056

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 258.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x105x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x105x2048): 52.631
Elapsed time for attention_prob_times_values (160x2048x2048x105): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x105): 48.795

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 258.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x106x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x106x2048): 53.745
Elapsed time for attention_prob_times_values (160x2048x2048x106): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x106): 51.349

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 269.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x107x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x107x2048): 53.339
Elapsed time for attention_prob_times_values (160x2048x2048x107): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x107): 48.235

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 262.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x108x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x108x2048): 52.896
Elapsed time for attention_prob_times_values (160x2048x2048x108): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x108): 52.165

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 274.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x109x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x109x2048): 51.173
Elapsed time for attention_prob_times_values (160x2048x2048x109): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x109): 50.524

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 267.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x110x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x110x2048): 52.226
Elapsed time for attention_prob_times_values (160x2048x2048x110): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x110): 52.765

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 278.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x111x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x111x2048): 54.886
Elapsed time for attention_prob_times_values (160x2048x2048x111): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x111): 51.144

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 282.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x112x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x112x2048): 56.751
Elapsed time for attention_prob_times_values (160x2048x2048x112): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x112): 53.437

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 295.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x113x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x113x2048): 53.184
Elapsed time for attention_prob_times_values (160x2048x2048x113): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x113): 52.155

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 285.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x114x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x114x2048): 54.880
Elapsed time for attention_prob_times_values (160x2048x2048x114): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x114): 54.322

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 297.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x115x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x115x2048): 54.189
Elapsed time for attention_prob_times_values (160x2048x2048x115): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x115): 51.327

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 289.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x116x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x116x2048): 55.804
Elapsed time for attention_prob_times_values (160x2048x2048x116): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x116): 55.010

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 306.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x117x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x117x2048): 55.941
Elapsed time for attention_prob_times_values (160x2048x2048x117): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x117): 52.051

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 300.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x118x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x118x2048): 55.906
Elapsed time for attention_prob_times_values (160x2048x2048x118): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x118): 55.756

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 313.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x119x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x119x2048): 54.983
Elapsed time for attention_prob_times_values (160x2048x2048x119): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x119): 53.882

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 307.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x120x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x120x2048): 56.010
Elapsed time for attention_prob_times_values (160x2048x2048x120): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x120): 55.761

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 317.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x121x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x121x2048): 52.021
Elapsed time for attention_prob_times_values (160x2048x2048x121): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x121): 42.919

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 269.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x122x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x122x2048): 55.173
Elapsed time for attention_prob_times_values (160x2048x2048x122): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x122): 56.746

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 322.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x123x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x123x2048): 55.864
Elapsed time for attention_prob_times_values (160x2048x2048x123): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x123): 43.615

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 284.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x124x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x124x2048): 54.849
Elapsed time for attention_prob_times_values (160x2048x2048x124): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x124): 57.543

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 328.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x125x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x125x2048): 52.703
Elapsed time for attention_prob_times_values (160x2048x2048x125): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x125): 43.360

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 279.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x126x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x126x2048): 57.351
Elapsed time for attention_prob_times_values (160x2048x2048x126): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x126): 52.753

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 325.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x127x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x127x2048): 54.476
Elapsed time for attention_prob_times_values (160x2048x2048x127): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x127): 43.291

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 287.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x128x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x128x2048): 62.812
Elapsed time for attention_prob_times_values (160x2048x2048x128): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x128): 61.783

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 373.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x129x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x129x2048): 56.615
Elapsed time for attention_prob_times_values (160x2048x2048x129): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x129): 41.182

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 287.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x130x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x130x2048): 55.581
Elapsed time for attention_prob_times_values (160x2048x2048x130): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x130): 46.537

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 307.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x131x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x131x2048): 55.237
Elapsed time for attention_prob_times_values (160x2048x2048x131): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x131): 44.989

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 303.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x132x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x132x2048): 58.567
Elapsed time for attention_prob_times_values (160x2048x2048x132): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x132): 47.316

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 322.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x133x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x133x2048): 57.741
Elapsed time for attention_prob_times_values (160x2048x2048x133): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x133): 43.142

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 305.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x134x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x134x2048): 59.132
Elapsed time for attention_prob_times_values (160x2048x2048x134): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x134): 46.373

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 324.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x135x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x135x2048): 58.501
Elapsed time for attention_prob_times_values (160x2048x2048x135): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x135): 45.202

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 319.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x136x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x136x2048): 54.737
Elapsed time for attention_prob_times_values (160x2048x2048x136): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x136): 45.388

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 313.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x137x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x137x2048): 56.885
Elapsed time for attention_prob_times_values (160x2048x2048x137): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x137): 46.530

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 325.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x138x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x138x2048): 58.539
Elapsed time for attention_prob_times_values (160x2048x2048x138): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x138): 48.906

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 340.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x139x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x139x2048): 56.963
Elapsed time for attention_prob_times_values (160x2048x2048x139): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x139): 47.144

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 331.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x140x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x140x2048): 58.156
Elapsed time for attention_prob_times_values (160x2048x2048x140): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x140): 49.286

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 345.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x141x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x141x2048): 58.615
Elapsed time for attention_prob_times_values (160x2048x2048x141): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x141): 47.437

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 341.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x142x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x142x2048): 59.009
Elapsed time for attention_prob_times_values (160x2048x2048x142): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x142): 50.140

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 354.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x143x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x143x2048): 60.184
Elapsed time for attention_prob_times_values (160x2048x2048x143): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x143): 46.536

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 345.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x144x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x144x2048): 62.516
Elapsed time for attention_prob_times_values (160x2048x2048x144): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x144): 48.920

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 363.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x145x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x145x2048): 54.994
Elapsed time for attention_prob_times_values (160x2048x2048x145): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x145): 45.635

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 332.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x146x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x146x2048): 61.036
Elapsed time for attention_prob_times_values (160x2048x2048x146): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x146): 51.225

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 373.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x147x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x147x2048): 60.395
Elapsed time for attention_prob_times_values (160x2048x2048x147): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x147): 49.506

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 366.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x148x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x148x2048): 60.126
Elapsed time for attention_prob_times_values (160x2048x2048x148): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x148): 52.220

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 379.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 5960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x149x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x149x2048): 60.655
Elapsed time for attention_prob_times_values (160x2048x2048x149): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x149): 50.065

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 374.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x150x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x150x2048): 60.851
Elapsed time for attention_prob_times_values (160x2048x2048x150): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x150): 52.566

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 386.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x151x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x151x2048): 61.589
Elapsed time for attention_prob_times_values (160x2048x2048x151): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x151): 49.592

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 379.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x152x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x152x2048): 61.055
Elapsed time for attention_prob_times_values (160x2048x2048x152): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x152): 49.093

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 377.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x153x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x153x2048): 58.418
Elapsed time for attention_prob_times_values (160x2048x2048x153): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x153): 51.171

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 380.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x154x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x154x2048): 62.182
Elapsed time for attention_prob_times_values (160x2048x2048x154): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x154): 52.019

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 397.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x155x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x155x2048): 61.771
Elapsed time for attention_prob_times_values (160x2048x2048x155): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x155): 47.988

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 381.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x156x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x156x2048): 63.175
Elapsed time for attention_prob_times_values (160x2048x2048x156): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x156): 54.455

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 414.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x157x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x157x2048): 59.549
Elapsed time for attention_prob_times_values (160x2048x2048x157): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x157): 48.517

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 381.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x158x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x158x2048): 63.528
Elapsed time for attention_prob_times_values (160x2048x2048x158): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x158): 54.946

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 422.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x159x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x159x2048): 60.267
Elapsed time for attention_prob_times_values (160x2048x2048x159): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x159): 52.235

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 403.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x160x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x160x2048): 76.274
Elapsed time for attention_prob_times_values (160x2048x2048x160): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x160): 55.117

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 463.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x161x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x161x2048): 61.842
Elapsed time for attention_prob_times_values (160x2048x2048x161): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x161): 52.536

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 414.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x162x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x162x2048): 63.143
Elapsed time for attention_prob_times_values (160x2048x2048x162): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x162): 56.080

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 435.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x163x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x163x2048): 62.520
Elapsed time for attention_prob_times_values (160x2048x2048x163): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x163): 54.503

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 429.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x164x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x164x2048): 63.632
Elapsed time for attention_prob_times_values (160x2048x2048x164): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x164): 55.383

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 438.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x165x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x165x2048): 60.402
Elapsed time for attention_prob_times_values (160x2048x2048x165): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x165): 53.751

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 423.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x166x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x166x2048): 58.892
Elapsed time for attention_prob_times_values (160x2048x2048x166): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x166): 56.988

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 433.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x167x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x167x2048): 63.219
Elapsed time for attention_prob_times_values (160x2048x2048x167): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x167): 52.980

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 433.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x168x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x168x2048): 65.536
Elapsed time for attention_prob_times_values (160x2048x2048x168): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x168): 53.321

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 444.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x169x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x169x2048): 62.571
Elapsed time for attention_prob_times_values (160x2048x2048x169): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x169): 55.957

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 449.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x170x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x170x2048): 60.991
Elapsed time for attention_prob_times_values (160x2048x2048x170): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x170): 58.426

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 456.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x171x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x171x2048): 60.540
Elapsed time for attention_prob_times_values (160x2048x2048x171): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x171): 56.440

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 448.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x172x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x172x2048): 63.207
Elapsed time for attention_prob_times_values (160x2048x2048x172): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x172): 59.005

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 471.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x173x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x173x2048): 64.288
Elapsed time for attention_prob_times_values (160x2048x2048x173): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x173): 55.846

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 463.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x174x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x174x2048): 65.477
Elapsed time for attention_prob_times_values (160x2048x2048x174): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x174): 59.655

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 486.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x175x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x175x2048): 63.151
Elapsed time for attention_prob_times_values (160x2048x2048x175): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x175): 57.896

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 473.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x176x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x176x2048): 67.918
Elapsed time for attention_prob_times_values (160x2048x2048x176): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x176): 58.948

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 497.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x177x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x177x2048): 64.506
Elapsed time for attention_prob_times_values (160x2048x2048x177): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x177): 56.770

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 477.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x178x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x178x2048): 65.644
Elapsed time for attention_prob_times_values (160x2048x2048x178): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x178): 58.709

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 492.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x179x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x179x2048): 65.058
Elapsed time for attention_prob_times_values (160x2048x2048x179): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x179): 58.980

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 494.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x180x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x180x2048): 66.582
Elapsed time for attention_prob_times_values (160x2048x2048x180): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x180): 55.500

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 486.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x181x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x181x2048): 61.139
Elapsed time for attention_prob_times_values (160x2048x2048x181): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x181): 59.441

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 486.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x182x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x182x2048): 64.293
Elapsed time for attention_prob_times_values (160x2048x2048x182): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x182): 60.733

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 506.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x183x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x183x2048): 65.992
Elapsed time for attention_prob_times_values (160x2048x2048x183): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x183): 59.367

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 509.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x184x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x184x2048): 68.057
Elapsed time for attention_prob_times_values (160x2048x2048x184): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x184): 60.293

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 523.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x185x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x185x2048): 65.463
Elapsed time for attention_prob_times_values (160x2048x2048x185): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x185): 60.536

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 517.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x186x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x186x2048): 66.954
Elapsed time for attention_prob_times_values (160x2048x2048x186): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x186): 62.068

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 532.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x187x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x187x2048): 66.084
Elapsed time for attention_prob_times_values (160x2048x2048x187): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x187): 61.041

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 527.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x188x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x188x2048): 67.630
Elapsed time for attention_prob_times_values (160x2048x2048x188): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x188): 60.429

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 532.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x189x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x189x2048): 66.565
Elapsed time for attention_prob_times_values (160x2048x2048x189): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x189): 61.537

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 536.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x190x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x190x2048): 67.998
Elapsed time for attention_prob_times_values (160x2048x2048x190): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x190): 63.414

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 552.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x191x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x191x2048): 64.013
Elapsed time for attention_prob_times_values (160x2048x2048x191): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x191): 58.618

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 517.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x192x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x192x2048): 77.282
Elapsed time for attention_prob_times_values (160x2048x2048x192): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x192): 65.706

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 603.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x193x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x193x2048): 66.100
Elapsed time for attention_prob_times_values (160x2048x2048x193): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x193): 51.549

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 494.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x194x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x194x2048): 67.158
Elapsed time for attention_prob_times_values (160x2048x2048x194): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x194): 52.683

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 506.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x195x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x195x2048): 66.313
Elapsed time for attention_prob_times_values (160x2048x2048x195): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x195): 52.177

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 503.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x196x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x196x2048): 67.038
Elapsed time for attention_prob_times_values (160x2048x2048x196): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x196): 52.550

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 509.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x197x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x197x2048): 66.190
Elapsed time for attention_prob_times_values (160x2048x2048x197): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x197): 52.660

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 510.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x198x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x198x2048): 67.321
Elapsed time for attention_prob_times_values (160x2048x2048x198): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x198): 53.152

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 518.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 7960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x199x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x199x2048): 66.501
Elapsed time for attention_prob_times_values (160x2048x2048x199): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x199): 53.066

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 517.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x200x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x200x2048): 69.487
Elapsed time for attention_prob_times_values (160x2048x2048x200): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x200): 50.151

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 513.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x201x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x201x2048): 66.844
Elapsed time for attention_prob_times_values (160x2048x2048x201): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x201): 51.351

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 514.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x202x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x202x2048): 65.144
Elapsed time for attention_prob_times_values (160x2048x2048x202): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x202): 54.287

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 526.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x203x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x203x2048): 67.322
Elapsed time for attention_prob_times_values (160x2048x2048x203): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x203): 51.505

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 521.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x204x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x204x2048): 68.677
Elapsed time for attention_prob_times_values (160x2048x2048x204): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x204): 54.464

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 544.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x205x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x205x2048): 62.317
Elapsed time for attention_prob_times_values (160x2048x2048x205): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x205): 51.170

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 506.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x206x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x206x2048): 69.115
Elapsed time for attention_prob_times_values (160x2048x2048x206): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x206): 54.529

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 551.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x207x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x207x2048): 68.290
Elapsed time for attention_prob_times_values (160x2048x2048x207): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x207): 51.517

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 533.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x208x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x208x2048): 69.017
Elapsed time for attention_prob_times_values (160x2048x2048x208): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x208): 53.764

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 551.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x209x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x209x2048): 66.744
Elapsed time for attention_prob_times_values (160x2048x2048x209): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x209): 53.262

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 542.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x210x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x210x2048): 66.558
Elapsed time for attention_prob_times_values (160x2048x2048x210): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x210): 54.681

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 552.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x211x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x211x2048): 68.016
Elapsed time for attention_prob_times_values (160x2048x2048x211): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x211): 53.617

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 554.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x212x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x212x2048): 68.034
Elapsed time for attention_prob_times_values (160x2048x2048x212): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x212): 56.399

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 572.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x213x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x213x2048): 68.339
Elapsed time for attention_prob_times_values (160x2048x2048x213): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x213): 53.887

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 561.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x214x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x214x2048): 68.101
Elapsed time for attention_prob_times_values (160x2048x2048x214): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x214): 54.367

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 565.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x215x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x215x2048): 69.044
Elapsed time for attention_prob_times_values (160x2048x2048x215): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x215): 52.865

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 562.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x216x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x216x2048): 71.055
Elapsed time for attention_prob_times_values (160x2048x2048x216): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x216): 54.474

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 582.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x217x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x217x2048): 67.623
Elapsed time for attention_prob_times_values (160x2048x2048x217): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x217): 52.723

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 561.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x218x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x218x2048): 69.876
Elapsed time for attention_prob_times_values (160x2048x2048x218): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x218): 57.452

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 600.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x219x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x219x2048): 67.230
Elapsed time for attention_prob_times_values (160x2048x2048x219): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x219): 54.457

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 574.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x220x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x220x2048): 68.740
Elapsed time for attention_prob_times_values (160x2048x2048x220): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x220): 58.142

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 604.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x221x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x221x2048): 69.504
Elapsed time for attention_prob_times_values (160x2048x2048x221): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x221): 55.621

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 595.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x222x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x222x2048): 67.781
Elapsed time for attention_prob_times_values (160x2048x2048x222): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x222): 53.545

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 578.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x223x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x223x2048): 67.356
Elapsed time for attention_prob_times_values (160x2048x2048x223): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x223): 55.305

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 589.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x224x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x224x2048): 79.750
Elapsed time for attention_prob_times_values (160x2048x2048x224): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x224): 59.155

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 662.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x225x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x225x2048): 66.133
Elapsed time for attention_prob_times_values (160x2048x2048x225): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x225): 56.001

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 593.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x226x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x226x2048): 68.379
Elapsed time for attention_prob_times_values (160x2048x2048x226): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x226): 56.775

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 609.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x227x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x227x2048): 66.857
Elapsed time for attention_prob_times_values (160x2048x2048x227): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x227): 56.063

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 601.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x228x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x228x2048): 67.847
Elapsed time for attention_prob_times_values (160x2048x2048x228): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x228): 58.761

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 623.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x229x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x229x2048): 69.035
Elapsed time for attention_prob_times_values (160x2048x2048x229): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x229): 57.459

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 623.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x230x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x230x2048): 66.433
Elapsed time for attention_prob_times_values (160x2048x2048x230): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x230): 59.863

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 628.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x231x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x231x2048): 67.921
Elapsed time for attention_prob_times_values (160x2048x2048x231): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x231): 57.794

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 625.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x232x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x232x2048): 68.347
Elapsed time for attention_prob_times_values (160x2048x2048x232): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x232): 58.564

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 634.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x233x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x233x2048): 66.847
Elapsed time for attention_prob_times_values (160x2048x2048x233): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x233): 57.931

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 627.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x234x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x234x2048): 70.316
Elapsed time for attention_prob_times_values (160x2048x2048x234): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x234): 59.780

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 655.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x235x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x235x2048): 68.567
Elapsed time for attention_prob_times_values (160x2048x2048x235): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x235): 58.315

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 641.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x236x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x236x2048): 70.872
Elapsed time for attention_prob_times_values (160x2048x2048x236): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x236): 61.314

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 671.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x237x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x237x2048): 68.847
Elapsed time for attention_prob_times_values (160x2048x2048x237): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x237): 58.690

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 649.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x238x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x238x2048): 70.924
Elapsed time for attention_prob_times_values (160x2048x2048x238): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x238): 61.381

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 677.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x239x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x239x2048): 69.821
Elapsed time for attention_prob_times_values (160x2048x2048x239): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x239): 59.122

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 661.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x240x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x240x2048): 55.761
Elapsed time for attention_prob_times_values (160x2048x2048x240): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x240): 59.522

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 597.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x241x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x241x2048): 52.153
Elapsed time for attention_prob_times_values (160x2048x2048x241): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x241): 59.641

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 579.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x242x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x242x2048): 54.040
Elapsed time for attention_prob_times_values (160x2048x2048x242): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x242): 59.909

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 593.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x243x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x243x2048): 53.018
Elapsed time for attention_prob_times_values (160x2048x2048x243): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x243): 60.028

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 590.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x244x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x244x2048): 54.874
Elapsed time for attention_prob_times_values (160x2048x2048x244): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x244): 62.016

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 613.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x245x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x245x2048): 53.727
Elapsed time for attention_prob_times_values (160x2048x2048x245): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x245): 59.740

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 598.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x246x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x246x2048): 54.625
Elapsed time for attention_prob_times_values (160x2048x2048x246): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x246): 59.961

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 606.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x247x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x247x2048): 53.987
Elapsed time for attention_prob_times_values (160x2048x2048x247): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x247): 60.062

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 605.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x248x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x248x2048): 55.822
Elapsed time for attention_prob_times_values (160x2048x2048x248): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x248): 62.039

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 628.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x249x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x249x2048): 51.819
Elapsed time for attention_prob_times_values (160x2048x2048x249): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x249): 59.720

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 595.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x250x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x250x2048): 45.462
Elapsed time for attention_prob_times_values (160x2048x2048x250): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x250): 63.832

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 571.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x251x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x251x2048): 52.943
Elapsed time for attention_prob_times_values (160x2048x2048x251): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x251): 62.137

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 617.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x252x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x252x2048): 54.181
Elapsed time for attention_prob_times_values (160x2048x2048x252): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x252): 64.157

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 637.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x253x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x253x2048): 52.432
Elapsed time for attention_prob_times_values (160x2048x2048x253): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x253): 56.357

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 591.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x254x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x254x2048): 54.737
Elapsed time for attention_prob_times_values (160x2048x2048x254): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x254): 60.826

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 629.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x255x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x255x2048): 53.069
Elapsed time for attention_prob_times_values (160x2048x2048x255): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x255): 65.895

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 644.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x256x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x256x2048): 72.782
Elapsed time for attention_prob_times_values (160x2048x2048x256): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x256): 67.093

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 768.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x257x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x257x2048): 55.575
Elapsed time for attention_prob_times_values (160x2048x2048x257): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x257): 53.232

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 600.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x258x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x258x2048): 56.123
Elapsed time for attention_prob_times_values (160x2048x2048x258): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x258): 55.300

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 617.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x259x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x259x2048): 56.775
Elapsed time for attention_prob_times_values (160x2048x2048x259): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x259): 53.984

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 615.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x260x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x260x2048): 56.635
Elapsed time for attention_prob_times_values (160x2048x2048x260): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x260): 56.676

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 632.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x261x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x261x2048): 54.110
Elapsed time for attention_prob_times_values (160x2048x2048x261): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x261): 54.388

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 607.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x262x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x262x2048): 57.268
Elapsed time for attention_prob_times_values (160x2048x2048x262): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x262): 56.969

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 641.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x263x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x263x2048): 54.980
Elapsed time for attention_prob_times_values (160x2048x2048x263): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x263): 54.901

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 619.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x264x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x264x2048): 57.225
Elapsed time for attention_prob_times_values (160x2048x2048x264): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x264): 52.143

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 617.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x265x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x265x2048): 54.667
Elapsed time for attention_prob_times_values (160x2048x2048x265): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x265): 54.874

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 621.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x266x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x266x2048): 54.472
Elapsed time for attention_prob_times_values (160x2048x2048x266): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x266): 55.827

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 628.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x267x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x267x2048): 53.656
Elapsed time for attention_prob_times_values (160x2048x2048x267): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x267): 55.368

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 622.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x268x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x268x2048): 56.688
Elapsed time for attention_prob_times_values (160x2048x2048x268): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x268): 57.825

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 656.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x269x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x269x2048): 54.324
Elapsed time for attention_prob_times_values (160x2048x2048x269): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x269): 55.699

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 632.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x270x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x270x2048): 55.095
Elapsed time for attention_prob_times_values (160x2048x2048x270): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x270): 57.459

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 649.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x271x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x271x2048): 54.034
Elapsed time for attention_prob_times_values (160x2048x2048x271): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x271): 54.614

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 629.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x272x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x272x2048): 57.931
Elapsed time for attention_prob_times_values (160x2048x2048x272): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x272): 72.378

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 748.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x273x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x273x2048): 54.580
Elapsed time for attention_prob_times_values (160x2048x2048x273): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x273): 55.752

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 643.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x274x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x274x2048): 54.534
Elapsed time for attention_prob_times_values (160x2048x2048x274): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x274): 59.079

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 663.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x275x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x275x2048): 53.983
Elapsed time for attention_prob_times_values (160x2048x2048x275): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x275): 56.782

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 649.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x276x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x276x2048): 56.155
Elapsed time for attention_prob_times_values (160x2048x2048x276): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x276): 59.073

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 678.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x277x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x277x2048): 55.789
Elapsed time for attention_prob_times_values (160x2048x2048x277): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x277): 56.485

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 663.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x278x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x278x2048): 54.926
Elapsed time for attention_prob_times_values (160x2048x2048x278): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x278): 57.389

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 665.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x279x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x279x2048): 55.296
Elapsed time for attention_prob_times_values (160x2048x2048x279): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x279): 57.676

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 671.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x280x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x280x2048): 57.883
Elapsed time for attention_prob_times_values (160x2048x2048x280): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x280): 74.932

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 779.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x281x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x281x2048): 55.814
Elapsed time for attention_prob_times_values (160x2048x2048x281): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x281): 27.097

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 436.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x282x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x282x2048): 55.413
Elapsed time for attention_prob_times_values (160x2048x2048x282): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x282): 60.678

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 696.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x283x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x283x2048): 56.431
Elapsed time for attention_prob_times_values (160x2048x2048x283): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x283): 58.400

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 691.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x284x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x284x2048): 57.506
Elapsed time for attention_prob_times_values (160x2048x2048x284): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x284): 61.366

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 718.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x285x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x285x2048): 54.518
Elapsed time for attention_prob_times_values (160x2048x2048x285): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x285): 57.255

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 677.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x286x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x286x2048): 54.698
Elapsed time for attention_prob_times_values (160x2048x2048x286): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x286): 61.454

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 704.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x287x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x287x2048): 54.392
Elapsed time for attention_prob_times_values (160x2048x2048x287): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x287): 59.165

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 692.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x288x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x288x2048): 76.071
Elapsed time for attention_prob_times_values (160x2048x2048x288): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x288): 73.375

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 915.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x289x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x289x2048): 55.238
Elapsed time for attention_prob_times_values (160x2048x2048x289): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x289): 59.759

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 705.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x290x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x290x2048): 61.317
Elapsed time for attention_prob_times_values (160x2048x2048x290): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x290): 62.138

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 760.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x291x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x291x2048): 60.512
Elapsed time for attention_prob_times_values (160x2048x2048x291): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x291): 59.936

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 744.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x292x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x292x2048): 62.209
Elapsed time for attention_prob_times_values (160x2048x2048x292): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x292): 62.736

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 775.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x293x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x293x2048): 57.697
Elapsed time for attention_prob_times_values (160x2048x2048x293): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x293): 60.537

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 735.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x294x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x294x2048): 60.014
Elapsed time for attention_prob_times_values (160x2048x2048x294): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x294): 61.978

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 761.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x295x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x295x2048): 60.159
Elapsed time for attention_prob_times_values (160x2048x2048x295): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x295): 60.952

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 758.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x296x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x296x2048): 62.077
Elapsed time for attention_prob_times_values (160x2048x2048x296): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x296): 79.082

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 873.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x297x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x297x2048): 59.205
Elapsed time for attention_prob_times_values (160x2048x2048x297): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x297): 61.085

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 757.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x298x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x298x2048): 57.020
Elapsed time for attention_prob_times_values (160x2048x2048x298): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x298): 61.938

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 750.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 11960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x299x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x299x2048): 58.152
Elapsed time for attention_prob_times_values (160x2048x2048x299): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x299): 61.461

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 757.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x300x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x300x2048): 56.444
Elapsed time for attention_prob_times_values (160x2048x2048x300): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x300): 64.554

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 766.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x301x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x301x2048): 59.926
Elapsed time for attention_prob_times_values (160x2048x2048x301): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x301): 62.003

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 777.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x302x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x302x2048): 60.625
Elapsed time for attention_prob_times_values (160x2048x2048x302): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x302): 64.709

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 801.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x303x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x303x2048): 60.076
Elapsed time for attention_prob_times_values (160x2048x2048x303): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x303): 63.103

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 790.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x304x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x304x2048): 58.994
Elapsed time for attention_prob_times_values (160x2048x2048x304): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x304): 77.602

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 863.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x305x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x305x2048): 59.287
Elapsed time for attention_prob_times_values (160x2048x2048x305): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x305): 60.953

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 776.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x306x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x306x2048): 58.881
Elapsed time for attention_prob_times_values (160x2048x2048x306): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x306): 64.076

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 794.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x307x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x307x2048): 57.809
Elapsed time for attention_prob_times_values (160x2048x2048x307): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x307): 61.403

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 773.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x308x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x308x2048): 61.259
Elapsed time for attention_prob_times_values (160x2048x2048x308): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x308): 66.179

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 829.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x309x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x309x2048): 58.761
Elapsed time for attention_prob_times_values (160x2048x2048x309): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x309): 62.554

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 792.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x310x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x310x2048): 60.669
Elapsed time for attention_prob_times_values (160x2048x2048x310): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x310): 62.523

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 807.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x311x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x311x2048): 59.459
Elapsed time for attention_prob_times_values (160x2048x2048x311): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x311): 63.972

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 810.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x312x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x312x2048): 60.974
Elapsed time for attention_prob_times_values (160x2048x2048x312): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x312): 81.011

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 917.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x313x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x313x2048): 58.660
Elapsed time for attention_prob_times_values (160x2048x2048x313): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x313): 63.472

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 806.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x314x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x314x2048): 58.197
Elapsed time for attention_prob_times_values (160x2048x2048x314): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x314): 66.389

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 822.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x315x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x315x2048): 58.025
Elapsed time for attention_prob_times_values (160x2048x2048x315): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x315): 64.861

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 814.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x316x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x316x2048): 61.410
Elapsed time for attention_prob_times_values (160x2048x2048x316): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x316): 67.185

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 856.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x317x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x317x2048): 59.073
Elapsed time for attention_prob_times_values (160x2048x2048x317): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x317): 65.061

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 828.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x318x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x318x2048): 59.701
Elapsed time for attention_prob_times_values (160x2048x2048x318): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x318): 67.733

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 851.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x319x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x319x2048): 60.141
Elapsed time for attention_prob_times_values (160x2048x2048x319): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x319): 64.403

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 837.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x320x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x320x2048): 78.313
Elapsed time for attention_prob_times_values (160x2048x2048x320): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x320): 80.540

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1072.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x321x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x321x2048): 61.482
Elapsed time for attention_prob_times_values (160x2048x2048x321): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x321): 57.130

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 801.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x322x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x322x2048): 61.949
Elapsed time for attention_prob_times_values (160x2048x2048x322): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x322): 58.701

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 818.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x323x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x323x2048): 62.861
Elapsed time for attention_prob_times_values (160x2048x2048x323): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x323): 58.484

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 825.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x324x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x324x2048): 64.553
Elapsed time for attention_prob_times_values (160x2048x2048x324): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x324): 61.166

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 857.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x325x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x325x2048): 62.664
Elapsed time for attention_prob_times_values (160x2048x2048x325): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x325): 58.791

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 830.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x326x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x326x2048): 63.251
Elapsed time for attention_prob_times_values (160x2048x2048x326): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x326): 59.810

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 844.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x327x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x327x2048): 61.848
Elapsed time for attention_prob_times_values (160x2048x2048x327): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x327): 57.226

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 818.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x328x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x328x2048): 64.404
Elapsed time for attention_prob_times_values (160x2048x2048x328): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x328): 70.302

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 928.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x329x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x329x2048): 59.634
Elapsed time for attention_prob_times_values (160x2048x2048x329): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x329): 58.007

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 814.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x330x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x330x2048): 61.484
Elapsed time for attention_prob_times_values (160x2048x2048x330): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x330): 60.115

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 844.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x331x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x331x2048): 59.059
Elapsed time for attention_prob_times_values (160x2048x2048x331): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x331): 56.592

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 805.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x332x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x332x2048): 61.353
Elapsed time for attention_prob_times_values (160x2048x2048x332): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x332): 62.215

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 863.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x333x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x333x2048): 60.646
Elapsed time for attention_prob_times_values (160x2048x2048x333): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x333): 57.594

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 827.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x334x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x334x2048): 61.270
Elapsed time for attention_prob_times_values (160x2048x2048x334): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x334): 57.514

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 833.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x335x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x335x2048): 60.408
Elapsed time for attention_prob_times_values (160x2048x2048x335): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x335): 60.071

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 848.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x336x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x336x2048): 63.801
Elapsed time for attention_prob_times_values (160x2048x2048x336): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x336): 74.986

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 973.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x337x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x337x2048): 61.331
Elapsed time for attention_prob_times_values (160x2048x2048x337): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x337): 59.464

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 855.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x338x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x338x2048): 62.095
Elapsed time for attention_prob_times_values (160x2048x2048x338): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x338): 63.194

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 889.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x339x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x339x2048): 60.425
Elapsed time for attention_prob_times_values (160x2048x2048x339): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x339): 59.470

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 853.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x340x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x340x2048): 61.180
Elapsed time for attention_prob_times_values (160x2048x2048x340): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x340): 62.304

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 881.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x341x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x341x2048): 61.130
Elapsed time for attention_prob_times_values (160x2048x2048x341): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x341): 58.972

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 859.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x342x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x342x2048): 61.861
Elapsed time for attention_prob_times_values (160x2048x2048x342): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x342): 62.220

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 890.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x343x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x343x2048): 62.312
Elapsed time for attention_prob_times_values (160x2048x2048x343): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x343): 61.420

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 890.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x344x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x344x2048): 62.949
Elapsed time for attention_prob_times_values (160x2048x2048x344): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x344): 75.384

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 990.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x345x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x345x2048): 61.461
Elapsed time for attention_prob_times_values (160x2048x2048x345): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x345): 59.539

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 875.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x346x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x346x2048): 59.883
Elapsed time for attention_prob_times_values (160x2048x2048x346): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x346): 62.304

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 886.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x347x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x347x2048): 59.483
Elapsed time for attention_prob_times_values (160x2048x2048x347): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x347): 59.143

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 863.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x348x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x348x2048): 61.509
Elapsed time for attention_prob_times_values (160x2048x2048x348): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x348): 62.795

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 906.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 13960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x349x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x349x2048): 59.764
Elapsed time for attention_prob_times_values (160x2048x2048x349): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x349): 60.391

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 879.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x350x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x350x2048): 62.042
Elapsed time for attention_prob_times_values (160x2048x2048x350): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x350): 62.767

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 915.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x351x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x351x2048): 60.176
Elapsed time for attention_prob_times_values (160x2048x2048x351): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x351): 62.045

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 898.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x352x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x352x2048): 78.194
Elapsed time for attention_prob_times_values (160x2048x2048x352): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x352): 74.036

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1121.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x353x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x353x2048): 62.246
Elapsed time for attention_prob_times_values (160x2048x2048x353): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x353): 59.558

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 900.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x354x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x354x2048): 65.679
Elapsed time for attention_prob_times_values (160x2048x2048x354): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x354): 63.349

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 956.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x355x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x355x2048): 63.700
Elapsed time for attention_prob_times_values (160x2048x2048x355): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x355): 63.113

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 942.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x356x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x356x2048): 64.401
Elapsed time for attention_prob_times_values (160x2048x2048x356): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x356): 63.916

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 956.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x357x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x357x2048): 64.503
Elapsed time for attention_prob_times_values (160x2048x2048x357): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x357): 63.864

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 959.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x358x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x358x2048): 64.876
Elapsed time for attention_prob_times_values (160x2048x2048x358): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x358): 63.990

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 965.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x359x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x359x2048): 63.410
Elapsed time for attention_prob_times_values (160x2048x2048x359): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x359): 63.591

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 953.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x360x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x360x2048): 63.582
Elapsed time for attention_prob_times_values (160x2048x2048x360): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x360): 79.898

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1066.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x361x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x361x2048): 61.233
Elapsed time for attention_prob_times_values (160x2048x2048x361): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x361): 63.411

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 940.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x362x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x362x2048): 63.994
Elapsed time for attention_prob_times_values (160x2048x2048x362): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x362): 64.582

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 973.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x363x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x363x2048): 63.570
Elapsed time for attention_prob_times_values (160x2048x2048x363): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x363): 62.239

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 954.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x364x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x364x2048): 64.154
Elapsed time for attention_prob_times_values (160x2048x2048x364): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x364): 65.164

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 983.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x365x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x365x2048): 62.347
Elapsed time for attention_prob_times_values (160x2048x2048x365): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x365): 62.972

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 956.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x366x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x366x2048): 64.516
Elapsed time for attention_prob_times_values (160x2048x2048x366): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x366): 62.932

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 974.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x367x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x367x2048): 62.226
Elapsed time for attention_prob_times_values (160x2048x2048x367): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x367): 64.426

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 970.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x368x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x368x2048): 63.739
Elapsed time for attention_prob_times_values (160x2048x2048x368): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x368): 81.520

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1099.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x369x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x369x2048): 63.103
Elapsed time for attention_prob_times_values (160x2048x2048x369): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x369): 63.869

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 978.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x370x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x370x2048): 63.502
Elapsed time for attention_prob_times_values (160x2048x2048x370): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x370): 63.597

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 982.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x371x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x371x2048): 61.667
Elapsed time for attention_prob_times_values (160x2048x2048x371): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x371): 64.154

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 974.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x372x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x372x2048): 62.453
Elapsed time for attention_prob_times_values (160x2048x2048x372): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x372): 66.284

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 998.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x373x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x373x2048): 62.283
Elapsed time for attention_prob_times_values (160x2048x2048x373): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x373): 64.425

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 986.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x374x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x374x2048): 61.908
Elapsed time for attention_prob_times_values (160x2048x2048x374): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x374): 65.343

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 992.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x375x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x375x2048): 63.792
Elapsed time for attention_prob_times_values (160x2048x2048x375): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x375): 63.923

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 999.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x376x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x376x2048): 63.954
Elapsed time for attention_prob_times_values (160x2048x2048x376): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x376): 81.118

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1121.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x377x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x377x2048): 61.074
Elapsed time for attention_prob_times_values (160x2048x2048x377): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x377): 64.393

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 985.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x378x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x378x2048): 60.880
Elapsed time for attention_prob_times_values (160x2048x2048x378): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x378): 64.780

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 989.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x379x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x379x2048): 60.921
Elapsed time for attention_prob_times_values (160x2048x2048x379): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x379): 63.273

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 981.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x380x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x380x2048): 63.358
Elapsed time for attention_prob_times_values (160x2048x2048x380): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x380): 65.521

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1020.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x381x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x381x2048): 60.791
Elapsed time for attention_prob_times_values (160x2048x2048x381): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x381): 64.822

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 996.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x382x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x382x2048): 64.061
Elapsed time for attention_prob_times_values (160x2048x2048x382): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x382): 66.810

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1041.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x383x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x383x2048): 63.145
Elapsed time for attention_prob_times_values (160x2048x2048x383): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x383): 64.672

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1019.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x384x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x384x2048): 78.015
Elapsed time for attention_prob_times_values (160x2048x2048x384): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x384): 79.513

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1260.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x385x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x385x2048): 59.882
Elapsed time for attention_prob_times_values (160x2048x2048x385): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x385): 57.893

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 944.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x386x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x386x2048): 65.296
Elapsed time for attention_prob_times_values (160x2048x2048x386): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x386): 61.015

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1014.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x387x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x387x2048): 65.479
Elapsed time for attention_prob_times_values (160x2048x2048x387): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x387): 58.197

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 993.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x388x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x388x2048): 65.684
Elapsed time for attention_prob_times_values (160x2048x2048x388): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x388): 30.830

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 677.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x389x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x389x2048): 63.567
Elapsed time for attention_prob_times_values (160x2048x2048x389): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x389): 58.625

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 987.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x390x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x390x2048): 64.623
Elapsed time for attention_prob_times_values (160x2048x2048x390): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x390): 61.798

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1025.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x391x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x391x2048): 42.828
Elapsed time for attention_prob_times_values (160x2048x2048x391): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x391): 56.447

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 792.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x392x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x392x2048): 66.704
Elapsed time for attention_prob_times_values (160x2048x2048x392): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x392): 73.375

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1139.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x393x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x393x2048): 62.584
Elapsed time for attention_prob_times_values (160x2048x2048x393): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x393): 57.562

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 980.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x394x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x394x2048): 64.264
Elapsed time for attention_prob_times_values (160x2048x2048x394): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x394): 61.725

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1032.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x395x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x395x2048): 62.375
Elapsed time for attention_prob_times_values (160x2048x2048x395): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x395): 57.893

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 986.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x396x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x396x2048): 64.180
Elapsed time for attention_prob_times_values (160x2048x2048x396): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x396): 62.204

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1040.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x397x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x397x2048): 63.299
Elapsed time for attention_prob_times_values (160x2048x2048x397): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x397): 59.669

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1014.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x398x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x398x2048): 64.742
Elapsed time for attention_prob_times_values (160x2048x2048x398): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x398): 62.889

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1055.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x399x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x399x2048): 65.157
Elapsed time for attention_prob_times_values (160x2048x2048x399): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x399): 59.163

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1028.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x400x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x400x2048): 66.912
Elapsed time for attention_prob_times_values (160x2048x2048x400): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x400): 76.265

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1185.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x401x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x401x2048): 64.415
Elapsed time for attention_prob_times_values (160x2048x2048x401): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x401): 60.201

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1037.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x402x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x402x2048): 65.090
Elapsed time for attention_prob_times_values (160x2048x2048x402): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x402): 63.371

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1072.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x403x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x403x2048): 62.886
Elapsed time for attention_prob_times_values (160x2048x2048x403): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x403): 59.314

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1022.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x404x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x404x2048): 65.728
Elapsed time for attention_prob_times_values (160x2048x2048x404): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x404): 63.858

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1087.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x405x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x405x2048): 64.932
Elapsed time for attention_prob_times_values (160x2048x2048x405): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x405): 59.868

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1047.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x406x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x406x2048): 65.674
Elapsed time for attention_prob_times_values (160x2048x2048x406): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x406): 62.974

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1083.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x407x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x407x2048): 65.293
Elapsed time for attention_prob_times_values (160x2048x2048x407): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x407): 60.953

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1065.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x408x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x408x2048): 66.747
Elapsed time for attention_prob_times_values (160x2048x2048x408): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x408): 75.082

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1196.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x409x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x409x2048): 62.734
Elapsed time for attention_prob_times_values (160x2048x2048x409): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x409): 57.079

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1014.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x410x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x410x2048): 63.512
Elapsed time for attention_prob_times_values (160x2048x2048x410): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x410): 60.531

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1054.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x411x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x411x2048): 60.302
Elapsed time for attention_prob_times_values (160x2048x2048x411): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x411): 61.092

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1035.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x412x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x412x2048): 64.298
Elapsed time for attention_prob_times_values (160x2048x2048x412): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x412): 63.332

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1090.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x413x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x413x2048): 64.971
Elapsed time for attention_prob_times_values (160x2048x2048x413): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x413): 60.644

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1074.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x414x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x414x2048): 63.543
Elapsed time for attention_prob_times_values (160x2048x2048x414): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x414): 64.838

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1102.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x415x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x415x2048): 63.028
Elapsed time for attention_prob_times_values (160x2048x2048x415): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x415): 61.987

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1075.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x416x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x416x2048): 80.067
Elapsed time for attention_prob_times_values (160x2048x2048x416): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x416): 75.958

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1344.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x417x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x417x2048): 68.146
Elapsed time for attention_prob_times_values (160x2048x2048x417): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x417): 62.588

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1128.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x418x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x418x2048): 66.714
Elapsed time for attention_prob_times_values (160x2048x2048x418): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x418): 64.041

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1132.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x419x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x419x2048): 68.176
Elapsed time for attention_prob_times_values (160x2048x2048x419): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x419): 62.843

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1135.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x420x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x420x2048): 69.853
Elapsed time for attention_prob_times_values (160x2048x2048x420): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x420): 66.070

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1182.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x421x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x421x2048): 68.032
Elapsed time for attention_prob_times_values (160x2048x2048x421): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x421): 61.092

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1123.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x422x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x422x2048): 68.712
Elapsed time for attention_prob_times_values (160x2048x2048x422): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x422): 66.078

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1177.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x423x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x423x2048): 67.740
Elapsed time for attention_prob_times_values (160x2048x2048x423): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x423): 62.176

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1136.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x424x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x424x2048): 69.705
Elapsed time for attention_prob_times_values (160x2048x2048x424): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x424): 77.409

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1288.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x425x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x425x2048): 65.983
Elapsed time for attention_prob_times_values (160x2048x2048x425): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x425): 63.098

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1135.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x426x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x426x2048): 66.645
Elapsed time for attention_prob_times_values (160x2048x2048x426): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x426): 64.850

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1159.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x427x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x427x2048): 64.705
Elapsed time for attention_prob_times_values (160x2048x2048x427): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x427): 61.926

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1118.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x428x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x428x2048): 67.593
Elapsed time for attention_prob_times_values (160x2048x2048x428): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x428): 66.643

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1189.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x429x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x429x2048): 66.426
Elapsed time for attention_prob_times_values (160x2048x2048x429): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x429): 64.097

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1158.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x430x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x430x2048): 68.279
Elapsed time for attention_prob_times_values (160x2048x2048x430): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x430): 65.696

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1191.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x431x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x431x2048): 66.765
Elapsed time for attention_prob_times_values (160x2048x2048x431): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x431): 64.321

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1168.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x432x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x432x2048): 68.319
Elapsed time for attention_prob_times_values (160x2048x2048x432): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x432): 79.152

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1310.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x433x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x433x2048): 65.724
Elapsed time for attention_prob_times_values (160x2048x2048x433): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x433): 63.608

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1158.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x434x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x434x2048): 65.486
Elapsed time for attention_prob_times_values (160x2048x2048x434): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x434): 65.161

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1172.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x435x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x435x2048): 65.052
Elapsed time for attention_prob_times_values (160x2048x2048x435): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x435): 63.787

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1158.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x436x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x436x2048): 63.126
Elapsed time for attention_prob_times_values (160x2048x2048x436): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x436): 66.852

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1170.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x437x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x437x2048): 65.810
Elapsed time for attention_prob_times_values (160x2048x2048x437): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x437): 63.740

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1170.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x438x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x438x2048): 65.607
Elapsed time for attention_prob_times_values (160x2048x2048x438): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x438): 63.622

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1169.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x439x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x439x2048): 63.757
Elapsed time for attention_prob_times_values (160x2048x2048x439): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x439): 62.610

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1146.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x440x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x440x2048): 66.127
Elapsed time for attention_prob_times_values (160x2048x2048x440): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x440): 78.892

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1308.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x441x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x441x2048): 65.054
Elapsed time for attention_prob_times_values (160x2048x2048x441): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x441): 62.222

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1159.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x442x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x442x2048): 63.075
Elapsed time for attention_prob_times_values (160x2048x2048x442): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x442): 64.651

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1166.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x443x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x443x2048): 64.685
Elapsed time for attention_prob_times_values (160x2048x2048x443): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x443): 64.220

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1179.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x444x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x444x2048): 65.776
Elapsed time for attention_prob_times_values (160x2048x2048x444): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x444): 68.709

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1232.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x445x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x445x2048): 65.916
Elapsed time for attention_prob_times_values (160x2048x2048x445): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x445): 64.147

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1195.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x446x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x446x2048): 67.568
Elapsed time for attention_prob_times_values (160x2048x2048x446): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x446): 69.136

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1259.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x447x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x447x2048): 66.054
Elapsed time for attention_prob_times_values (160x2048x2048x447): 0.0213
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x447): 28.131

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 728.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x448x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x448x2048): 83.251
Elapsed time for attention_prob_times_values (160x2048x2048x448): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x448): 80.772

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1516.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 17960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x449x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x449x2048): 68.748
Elapsed time for attention_prob_times_values (160x2048x2048x449): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x449): 59.692

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1184.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x450x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x450x2048): 70.669
Elapsed time for attention_prob_times_values (160x2048x2048x450): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x450): 63.367

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1241.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x451x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x451x2048): 68.403
Elapsed time for attention_prob_times_values (160x2048x2048x451): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x451): 59.154

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1181.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x452x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x452x2048): 71.413
Elapsed time for attention_prob_times_values (160x2048x2048x452): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x452): 63.795

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1257.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x453x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x453x2048): 67.988
Elapsed time for attention_prob_times_values (160x2048x2048x453): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x453): 60.360

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1195.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x454x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x454x2048): 70.139
Elapsed time for attention_prob_times_values (160x2048x2048x454): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x454): 62.368

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1236.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x455x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x455x2048): 69.218
Elapsed time for attention_prob_times_values (160x2048x2048x455): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x455): 61.170

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1219.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x456x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x456x2048): 71.382
Elapsed time for attention_prob_times_values (160x2048x2048x456): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x456): 79.631

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1416.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x457x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x457x2048): 66.816
Elapsed time for attention_prob_times_values (160x2048x2048x457): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x457): 60.160

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1193.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x458x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x458x2048): 67.382
Elapsed time for attention_prob_times_values (160x2048x2048x458): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x458): 62.624

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1226.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x459x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x459x2048): 67.203
Elapsed time for attention_prob_times_values (160x2048x2048x459): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x459): 60.387

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1204.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x460x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x460x2048): 68.656
Elapsed time for attention_prob_times_values (160x2048x2048x460): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x460): 63.278

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1249.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x461x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x461x2048): 67.425
Elapsed time for attention_prob_times_values (160x2048x2048x461): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x461): 60.359

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1210.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x462x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x462x2048): 66.674
Elapsed time for attention_prob_times_values (160x2048x2048x462): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x462): 62.012

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1223.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x463x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x463x2048): 68.701
Elapsed time for attention_prob_times_values (160x2048x2048x463): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x463): 61.908

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1243.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x464x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x464x2048): 68.051
Elapsed time for attention_prob_times_values (160x2048x2048x464): 0.0183
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x464): 34.095

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 868.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x465x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x465x2048): 67.917
Elapsed time for attention_prob_times_values (160x2048x2048x465): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x465): 62.257

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1244.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x466x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x466x2048): 68.769
Elapsed time for attention_prob_times_values (160x2048x2048x466): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x466): 63.856

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1271.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x467x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x467x2048): 66.186
Elapsed time for attention_prob_times_values (160x2048x2048x467): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x467): 60.424

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1215.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x468x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x468x2048): 69.660
Elapsed time for attention_prob_times_values (160x2048x2048x468): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x468): 63.244

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1278.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x469x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x469x2048): 66.647
Elapsed time for attention_prob_times_values (160x2048x2048x469): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x469): 61.555

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1236.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x470x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x470x2048): 66.606
Elapsed time for attention_prob_times_values (160x2048x2048x470): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x470): 63.951

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1263.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x471x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x471x2048): 67.200
Elapsed time for attention_prob_times_values (160x2048x2048x471): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x471): 60.178

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1231.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x472x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x472x2048): 67.957
Elapsed time for attention_prob_times_values (160x2048x2048x472): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x472): 82.532

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1448.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x473x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x473x2048): 65.512
Elapsed time for attention_prob_times_values (160x2048x2048x473): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x473): 61.900

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1239.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x474x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x474x2048): 66.919
Elapsed time for attention_prob_times_values (160x2048x2048x474): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x474): 65.234

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1289.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x475x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x475x2048): 67.968
Elapsed time for attention_prob_times_values (160x2048x2048x475): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x475): 63.260

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1281.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x476x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x476x2048): 68.334
Elapsed time for attention_prob_times_values (160x2048x2048x476): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x476): 63.254

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1287.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x477x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x477x2048): 68.159
Elapsed time for attention_prob_times_values (160x2048x2048x477): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x477): 61.783

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1272.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x478x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x478x2048): 67.187
Elapsed time for attention_prob_times_values (160x2048x2048x478): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x478): 65.552

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1305.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x479x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x479x2048): 66.614
Elapsed time for attention_prob_times_values (160x2048x2048x479): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x479): 63.490

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1281.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x480x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x480x2048): 83.459
Elapsed time for attention_prob_times_values (160x2048x2048x480): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x480): 83.169

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1645.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x481x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x481x2048): 68.844
Elapsed time for attention_prob_times_values (160x2048x2048x481): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x481): 63.414

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1306.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x482x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x482x2048): 67.912
Elapsed time for attention_prob_times_values (160x2048x2048x482): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x482): 64.542

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1312.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x483x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x483x2048): 67.200
Elapsed time for attention_prob_times_values (160x2048x2048x483): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x483): 63.304

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1295.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x484x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x484x2048): 68.464
Elapsed time for attention_prob_times_values (160x2048x2048x484): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x484): 67.578

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1353.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x485x2048): 0.0240
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x485x2048): 27.139
Elapsed time for attention_prob_times_values (160x2048x2048x485): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x485): 63.553

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 758.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0342
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x486x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x486x2048): 68.316
Elapsed time for attention_prob_times_values (160x2048x2048x486): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x486): 65.999

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1341.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x487x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x487x2048): 68.664
Elapsed time for attention_prob_times_values (160x2048x2048x487): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x487): 62.388

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1309.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x488x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x488x2048): 70.200
Elapsed time for attention_prob_times_values (160x2048x2048x488): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x488): 87.141

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1560.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x489x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x489x2048): 68.193
Elapsed time for attention_prob_times_values (160x2048x2048x489): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x489): 63.285

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1319.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x490x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x490x2048): 68.552
Elapsed time for attention_prob_times_values (160x2048x2048x490): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x490): 66.718

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1361.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x491x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x491x2048): 68.526
Elapsed time for attention_prob_times_values (160x2048x2048x491): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x491): 63.587

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1331.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x492x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x492x2048): 69.213
Elapsed time for attention_prob_times_values (160x2048x2048x492): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x492): 67.101

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1377.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x493x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x493x2048): 65.729
Elapsed time for attention_prob_times_values (160x2048x2048x493): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x493): 62.556

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1298.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x494x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x494x2048): 67.908
Elapsed time for attention_prob_times_values (160x2048x2048x494): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x494): 67.050

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1369.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x495x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x495x2048): 67.958
Elapsed time for attention_prob_times_values (160x2048x2048x495): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x495): 63.925

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1339.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x496x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x496x2048): 71.605
Elapsed time for attention_prob_times_values (160x2048x2048x496): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x496): 88.786

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1615.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x497x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x497x2048): 66.273
Elapsed time for attention_prob_times_values (160x2048x2048x497): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x497): 63.604

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1325.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x498x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x498x2048): 68.288
Elapsed time for attention_prob_times_values (160x2048x2048x498): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x498): 67.159

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1385.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 19960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x499x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x499x2048): 66.935
Elapsed time for attention_prob_times_values (160x2048x2048x499): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x499): 65.071

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1352.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x500x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x500x2048): 68.992
Elapsed time for attention_prob_times_values (160x2048x2048x500): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x500): 69.094

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1417.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x501x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x501x2048): 68.611
Elapsed time for attention_prob_times_values (160x2048x2048x501): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x501): 65.149

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1374.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x502x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x502x2048): 69.023
Elapsed time for attention_prob_times_values (160x2048x2048x502): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x502): 67.177

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1403.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x503x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x503x2048): 68.296
Elapsed time for attention_prob_times_values (160x2048x2048x503): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x503): 63.426

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1358.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x504x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x504x2048): 68.115
Elapsed time for attention_prob_times_values (160x2048x2048x504): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x504): 89.442

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1599.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x505x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x505x2048): 68.049
Elapsed time for attention_prob_times_values (160x2048x2048x505): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x505): 60.158

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1323.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x506x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x506x2048): 69.480
Elapsed time for attention_prob_times_values (160x2048x2048x506): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x506): 67.429

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1421.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x507x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x507x2048): 68.275
Elapsed time for attention_prob_times_values (160x2048x2048x507): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x507): 60.716

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1337.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x508x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x508x2048): 69.834
Elapsed time for attention_prob_times_values (160x2048x2048x508): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x508): 66.780

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1423.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x509x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x509x2048): 64.446
Elapsed time for attention_prob_times_values (160x2048x2048x509): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x509): 58.691

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1282.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x510x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x510x2048): 68.753
Elapsed time for attention_prob_times_values (160x2048x2048x510): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x510): 66.318

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1412.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x511x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x511x2048): 65.132
Elapsed time for attention_prob_times_values (160x2048x2048x511): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x511): 62.035

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1331.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x512x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x512x2048): 82.356
Elapsed time for attention_prob_times_values (160x2048x2048x512): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x512): 92.625

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1830.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x513x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x513x2048): 66.599
Elapsed time for attention_prob_times_values (160x2048x2048x513): 0.0231
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x513): 29.753

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 865.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x514x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x514x2048): 73.876
Elapsed time for attention_prob_times_values (160x2048x2048x514): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x514): 64.206

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1448.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x515x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x515x2048): 69.700
Elapsed time for attention_prob_times_values (160x2048x2048x515): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x515): 58.744

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1346.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x516x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x516x2048): 71.521
Elapsed time for attention_prob_times_values (160x2048x2048x516): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x516): 63.161

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1419.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x517x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x517x2048): 70.857
Elapsed time for attention_prob_times_values (160x2048x2048x517): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x517): 62.067

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1402.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x518x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x518x2048): 68.969
Elapsed time for attention_prob_times_values (160x2048x2048x518): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x518): 65.411

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1425.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x519x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x519x2048): 68.511
Elapsed time for attention_prob_times_values (160x2048x2048x519): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x519): 60.932

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1372.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x520x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x520x2048): 72.603
Elapsed time for attention_prob_times_values (160x2048x2048x520): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x520): 75.404

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1576.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x521x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x521x2048): 68.812
Elapsed time for attention_prob_times_values (160x2048x2048x521): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x521): 62.355

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1396.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x522x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x522x2048): 69.320
Elapsed time for attention_prob_times_values (160x2048x2048x522): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x522): 65.165

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1436.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x523x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x523x2048): 68.885
Elapsed time for attention_prob_times_values (160x2048x2048x523): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x523): 60.442

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1379.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x524x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x524x2048): 68.372
Elapsed time for attention_prob_times_values (160x2048x2048x524): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x524): 62.822

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1405.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x525x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x525x2048): 69.802
Elapsed time for attention_prob_times_values (160x2048x2048x525): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x525): 60.486

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1393.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x526x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x526x2048): 68.863
Elapsed time for attention_prob_times_values (160x2048x2048x526): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x526): 62.915

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1416.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x527x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x527x2048): 70.900
Elapsed time for attention_prob_times_values (160x2048x2048x527): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x527): 62.839

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1438.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x528x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x528x2048): 70.899
Elapsed time for attention_prob_times_values (160x2048x2048x528): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x528): 74.880

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1575.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x529x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x529x2048): 70.140
Elapsed time for attention_prob_times_values (160x2048x2048x529): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x529): 60.286

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1404.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x530x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x530x2048): 70.081
Elapsed time for attention_prob_times_values (160x2048x2048x530): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x530): 65.729

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1472.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x531x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x531x2048): 68.679
Elapsed time for attention_prob_times_values (160x2048x2048x531): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x531): 62.812

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1426.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x532x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x532x2048): 70.553
Elapsed time for attention_prob_times_values (160x2048x2048x532): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x532): 66.618

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1492.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x533x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x533x2048): 68.744
Elapsed time for attention_prob_times_values (160x2048x2048x533): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x533): 61.969

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1422.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x534x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x534x2048): 69.799
Elapsed time for attention_prob_times_values (160x2048x2048x534): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x534): 64.891

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1470.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x535x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x535x2048): 68.775
Elapsed time for attention_prob_times_values (160x2048x2048x535): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x535): 62.627

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1435.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x536x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x536x2048): 70.743
Elapsed time for attention_prob_times_values (160x2048x2048x536): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x536): 73.549

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1582.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x537x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x537x2048): 70.061
Elapsed time for attention_prob_times_values (160x2048x2048x537): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x537): 63.058

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1458.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x538x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x538x2048): 70.873
Elapsed time for attention_prob_times_values (160x2048x2048x538): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x538): 67.086

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1517.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x539x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x539x2048): 70.480
Elapsed time for attention_prob_times_values (160x2048x2048x539): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x539): 63.134

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1468.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x540x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x540x2048): 71.402
Elapsed time for attention_prob_times_values (160x2048x2048x540): 0.0250
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x540): 29.016

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 911.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x541x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x541x2048): 68.398
Elapsed time for attention_prob_times_values (160x2048x2048x541): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x541): 62.881

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1450.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x542x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x542x2048): 70.276
Elapsed time for attention_prob_times_values (160x2048x2048x542): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x542): 66.135

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1510.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x543x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x543x2048): 70.257
Elapsed time for attention_prob_times_values (160x2048x2048x543): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x543): 63.447

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1480.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x544x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x544x2048): 87.224
Elapsed time for attention_prob_times_values (160x2048x2048x544): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x544): 79.608

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1852.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x545x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x545x2048): 73.552
Elapsed time for attention_prob_times_values (160x2048x2048x545): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x545): 64.382

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1530.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x546x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x546x2048): 74.334
Elapsed time for attention_prob_times_values (160x2048x2048x546): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x546): 66.467

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1567.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x547x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x547x2048): 73.426
Elapsed time for attention_prob_times_values (160x2048x2048x547): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x547): 64.680

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1538.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x548x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x548x2048): 74.934
Elapsed time for attention_prob_times_values (160x2048x2048x548): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x548): 66.525

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1579.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x549x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x549x2048): 73.308
Elapsed time for attention_prob_times_values (160x2048x2048x549): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x549): 65.288

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1550.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x550x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x550x2048): 73.931
Elapsed time for attention_prob_times_values (160x2048x2048x550): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x550): 68.543

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1599.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x551x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x551x2048): 68.681
Elapsed time for attention_prob_times_values (160x2048x2048x551): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x551): 65.861

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1514.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x552x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x552x2048): 74.859
Elapsed time for attention_prob_times_values (160x2048x2048x552): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x552): 75.540

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1696.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x553x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x553x2048): 70.378
Elapsed time for attention_prob_times_values (160x2048x2048x553): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x553): 65.470

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1533.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x554x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x554x2048): 72.559
Elapsed time for attention_prob_times_values (160x2048x2048x554): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x554): 68.472

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1595.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x555x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x555x2048): 71.044
Elapsed time for attention_prob_times_values (160x2048x2048x555): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x555): 62.701

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1510.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x556x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x556x2048): 71.814
Elapsed time for attention_prob_times_values (160x2048x2048x556): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x556): 68.303

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1590.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x557x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x557x2048): 71.448
Elapsed time for attention_prob_times_values (160x2048x2048x557): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x557): 65.987

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1561.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x558x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x558x2048): 72.505
Elapsed time for attention_prob_times_values (160x2048x2048x558): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x558): 68.924

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1611.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x559x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x559x2048): 72.532
Elapsed time for attention_prob_times_values (160x2048x2048x559): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x559): 66.735

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1587.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x560x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x560x2048): 72.751
Elapsed time for attention_prob_times_values (160x2048x2048x560): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x560): 78.501

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1727.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x561x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x561x2048): 71.115
Elapsed time for attention_prob_times_values (160x2048x2048x561): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x561): 65.921

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1567.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x562x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x562x2048): 71.658
Elapsed time for attention_prob_times_values (160x2048x2048x562): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x562): 68.767

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1610.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x563x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x563x2048): 69.579
Elapsed time for attention_prob_times_values (160x2048x2048x563): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x563): 65.766

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1554.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x564x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x564x2048): 71.790
Elapsed time for attention_prob_times_values (160x2048x2048x564): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x564): 68.758

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1617.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x565x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x565x2048): 68.993
Elapsed time for attention_prob_times_values (160x2048x2048x565): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x565): 66.312

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1560.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x566x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x566x2048): 71.047
Elapsed time for attention_prob_times_values (160x2048x2048x566): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x566): 70.545

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1636.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x567x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x567x2048): 72.455
Elapsed time for attention_prob_times_values (160x2048x2048x567): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x567): 67.074

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1612.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x568x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x568x2048): 72.277
Elapsed time for attention_prob_times_values (160x2048x2048x568): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x568): 80.986

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1771.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x569x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x569x2048): 70.058
Elapsed time for attention_prob_times_values (160x2048x2048x569): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x569): 66.805

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1588.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x570x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x570x2048): 70.858
Elapsed time for attention_prob_times_values (160x2048x2048x570): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x570): 69.711

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1635.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x571x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x571x2048): 70.536
Elapsed time for attention_prob_times_values (160x2048x2048x571): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x571): 65.210

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1579.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x572x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x572x2048): 71.174
Elapsed time for attention_prob_times_values (160x2048x2048x572): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x572): 69.136

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1637.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x573x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x573x2048): 70.356
Elapsed time for attention_prob_times_values (160x2048x2048x573): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x573): 66.667

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1600.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x574x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x574x2048): 70.771
Elapsed time for attention_prob_times_values (160x2048x2048x574): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x574): 68.704

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1633.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x575x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x575x2048): 71.897
Elapsed time for attention_prob_times_values (160x2048x2048x575): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x575): 67.708

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1636.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x576x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x576x2048): 87.536
Elapsed time for attention_prob_times_values (160x2048x2048x576): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x576): 84.985

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2026.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x577x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x577x2048): 71.911
Elapsed time for attention_prob_times_values (160x2048x2048x577): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x577): 61.940

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1566.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x578x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x578x2048): 74.267
Elapsed time for attention_prob_times_values (160x2048x2048x578): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x578): 64.016

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1621.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x579x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x579x2048): 72.924
Elapsed time for attention_prob_times_values (160x2048x2048x579): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x579): 62.431

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1588.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x580x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x580x2048): 75.727
Elapsed time for attention_prob_times_values (160x2048x2048x580): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x580): 36.980

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1175.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x581x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x581x2048): 73.076
Elapsed time for attention_prob_times_values (160x2048x2048x581): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x581): 61.128

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1577.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x582x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x582x2048): 73.182
Elapsed time for attention_prob_times_values (160x2048x2048x582): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x582): 63.461

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1613.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x583x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x583x2048): 75.835
Elapsed time for attention_prob_times_values (160x2048x2048x583): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x583): 61.516

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1614.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x584x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x584x2048): 74.761
Elapsed time for attention_prob_times_values (160x2048x2048x584): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x584): 83.153

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1874.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x585x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x585x2048): 70.972
Elapsed time for attention_prob_times_values (160x2048x2048x585): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x585): 62.806

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1589.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x586x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x586x2048): 72.398
Elapsed time for attention_prob_times_values (160x2048x2048x586): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x586): 64.015

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1623.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x587x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x587x2048): 70.200
Elapsed time for attention_prob_times_values (160x2048x2048x587): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x587): 60.538

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1555.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x588x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x588x2048): 72.677
Elapsed time for attention_prob_times_values (160x2048x2048x588): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x588): 63.352

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1622.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x589x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x589x2048): 72.438
Elapsed time for attention_prob_times_values (160x2048x2048x589): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x589): 63.206

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1620.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x590x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x590x2048): 74.142
Elapsed time for attention_prob_times_values (160x2048x2048x590): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x590): 65.985

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1679.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x591x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x591x2048): 73.351
Elapsed time for attention_prob_times_values (160x2048x2048x591): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x591): 61.505

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1611.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x592x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x592x2048): 74.054
Elapsed time for attention_prob_times_values (160x2048x2048x592): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x592): 86.044

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1920.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x593x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x593x2048): 72.698
Elapsed time for attention_prob_times_values (160x2048x2048x593): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x593): 63.454

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1637.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x594x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x594x2048): 71.155
Elapsed time for attention_prob_times_values (160x2048x2048x594): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x594): 66.550

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1664.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x595x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x595x2048): 72.992
Elapsed time for attention_prob_times_values (160x2048x2048x595): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x595): 61.651

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1620.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x596x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x596x2048): 74.374
Elapsed time for attention_prob_times_values (160x2048x2048x596): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x596): 67.324

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1716.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x597x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x597x2048): 73.015
Elapsed time for attention_prob_times_values (160x2048x2048x597): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x597): 64.311

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1663.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x598x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x598x2048): 73.947
Elapsed time for attention_prob_times_values (160x2048x2048x598): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x598): 66.902

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1711.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 23960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x599x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x599x2048): 72.916
Elapsed time for attention_prob_times_values (160x2048x2048x599): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x599): 63.237

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1652.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x600x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x600x2048): 72.064
Elapsed time for attention_prob_times_values (160x2048x2048x600): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x600): 86.949

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1925.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x601x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x601x2048): 72.638
Elapsed time for attention_prob_times_values (160x2048x2048x601): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x601): 63.683

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1661.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x602x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x602x2048): 71.998
Elapsed time for attention_prob_times_values (160x2048x2048x602): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x602): 62.488

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1640.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x603x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x603x2048): 70.190
Elapsed time for attention_prob_times_values (160x2048x2048x603): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x603): 63.242

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1633.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x604x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x604x2048): 70.472
Elapsed time for attention_prob_times_values (160x2048x2048x604): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x604): 66.408

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1681.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x605x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x605x2048): 72.868
Elapsed time for attention_prob_times_values (160x2048x2048x605): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x605): 63.478

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1671.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x606x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x606x2048): 73.593
Elapsed time for attention_prob_times_values (160x2048x2048x606): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x606): 67.151

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1732.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x607x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x607x2048): 72.914
Elapsed time for attention_prob_times_values (160x2048x2048x607): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x607): 64.107

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1685.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x608x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x608x2048): 88.782
Elapsed time for attention_prob_times_values (160x2048x2048x608): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x608): 87.830

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2185.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x609x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x609x2048): 74.801
Elapsed time for attention_prob_times_values (160x2048x2048x609): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x609): 65.345

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1729.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x610x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x610x2048): 75.719
Elapsed time for attention_prob_times_values (160x2048x2048x610): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x610): 68.039

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1779.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x611x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x611x2048): 73.896
Elapsed time for attention_prob_times_values (160x2048x2048x611): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x611): 65.507

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1726.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x612x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x612x2048): 75.964
Elapsed time for attention_prob_times_values (160x2048x2048x612): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x612): 66.728

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1769.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x613x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x613x2048): 74.249
Elapsed time for attention_prob_times_values (160x2048x2048x613): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x613): 65.581

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1737.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x614x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x614x2048): 73.769
Elapsed time for attention_prob_times_values (160x2048x2048x614): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x614): 67.784

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1765.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x615x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x615x2048): 72.445
Elapsed time for attention_prob_times_values (160x2048x2048x615): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x615): 66.076

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1729.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x616x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x616x2048): 75.793
Elapsed time for attention_prob_times_values (160x2048x2048x616): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x616): 89.093

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2052.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x617x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x617x2048): 73.303
Elapsed time for attention_prob_times_values (160x2048x2048x617): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x617): 65.233

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1732.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x618x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x618x2048): 72.786
Elapsed time for attention_prob_times_values (160x2048x2048x618): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x618): 66.661

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1749.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x619x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x619x2048): 71.973
Elapsed time for attention_prob_times_values (160x2048x2048x619): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x619): 64.928

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1718.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x620x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x620x2048): 72.465
Elapsed time for attention_prob_times_values (160x2048x2048x620): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x620): 67.932

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1768.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x621x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x621x2048): 70.605
Elapsed time for attention_prob_times_values (160x2048x2048x621): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x621): 64.494

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1702.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x622x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x622x2048): 72.612
Elapsed time for attention_prob_times_values (160x2048x2048x622): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x622): 68.400

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1781.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x623x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x623x2048): 71.975
Elapsed time for attention_prob_times_values (160x2048x2048x623): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x623): 65.376

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1735.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x624x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x624x2048): 74.369
Elapsed time for attention_prob_times_values (160x2048x2048x624): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x624): 89.151

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2057.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x625x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x625x2048): 72.112
Elapsed time for attention_prob_times_values (160x2048x2048x625): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x625): 65.613

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1746.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x626x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x626x2048): 72.906
Elapsed time for attention_prob_times_values (160x2048x2048x626): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x626): 69.067

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1805.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x627x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x627x2048): 68.931
Elapsed time for attention_prob_times_values (160x2048x2048x627): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x627): 63.158

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1680.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x628x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x628x2048): 71.883
Elapsed time for attention_prob_times_values (160x2048x2048x628): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x628): 66.339

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1761.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x629x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x629x2048): 69.555
Elapsed time for attention_prob_times_values (160x2048x2048x629): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x629): 63.414

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1696.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x630x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x630x2048): 73.005
Elapsed time for attention_prob_times_values (160x2048x2048x630): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x630): 68.583

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1811.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x631x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x631x2048): 72.308
Elapsed time for attention_prob_times_values (160x2048x2048x631): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x631): 65.318

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1760.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x632x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x632x2048): 74.446
Elapsed time for attention_prob_times_values (160x2048x2048x632): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x632): 91.412

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2107.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x633x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x633x2048): 72.249
Elapsed time for attention_prob_times_values (160x2048x2048x633): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x633): 67.472

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1795.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x634x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x634x2048): 73.519
Elapsed time for attention_prob_times_values (160x2048x2048x634): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x634): 69.583

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1842.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x635x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x635x2048): 72.854
Elapsed time for attention_prob_times_values (160x2048x2048x635): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x635): 64.216

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 1761.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x636x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x636x2048): 72.583
Elapsed time for attention_prob_times_values (160x2048x2048x636): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x636): 68.943

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1827.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x637x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x637x2048): 71.832
Elapsed time for attention_prob_times_values (160x2048x2048x637): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x637): 66.491

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1787.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x638x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x638x2048): 73.692
Elapsed time for attention_prob_times_values (160x2048x2048x638): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x638): 69.717

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1857.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x639x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x639x2048): 72.317
Elapsed time for attention_prob_times_values (160x2048x2048x639): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x639): 62.913

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1746.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x640x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x640x2048): 80.632
Elapsed time for attention_prob_times_values (160x2048x2048x640): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x640): 90.872

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2221.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x641x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x641x2048): 69.286
Elapsed time for attention_prob_times_values (160x2048x2048x641): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x641): 60.456

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1681.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x642x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x642x2048): 74.199
Elapsed time for attention_prob_times_values (160x2048x2048x642): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x642): 61.946

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1760.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x643x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x643x2048): 71.023
Elapsed time for attention_prob_times_values (160x2048x2048x643): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x643): 60.387

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1704.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x644x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x644x2048): 72.599
Elapsed time for attention_prob_times_values (160x2048x2048x644): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x644): 64.610

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1788.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x645x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x645x2048): 72.848
Elapsed time for attention_prob_times_values (160x2048x2048x645): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x645): 59.018

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1708.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x646x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x646x2048): 72.019
Elapsed time for attention_prob_times_values (160x2048x2048x646): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x646): 63.383

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1768.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x647x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x647x2048): 72.773
Elapsed time for attention_prob_times_values (160x2048x2048x647): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x647): 61.555

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1752.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x648x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x648x2048): 76.051
Elapsed time for attention_prob_times_values (160x2048x2048x648): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x648): 77.714

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2022.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 25960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x649x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x649x2048): 71.790
Elapsed time for attention_prob_times_values (160x2048x2048x649): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x649): 62.494

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1760.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x650x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x650x2048): 72.844
Elapsed time for attention_prob_times_values (160x2048x2048x650): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x650): 65.128

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1814.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x651x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x651x2048): 72.357
Elapsed time for attention_prob_times_values (160x2048x2048x651): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x651): 60.004

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1733.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x652x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x652x2048): 72.971
Elapsed time for attention_prob_times_values (160x2048x2048x652): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x652): 65.488

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1827.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x653x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x653x2048): 71.181
Elapsed time for attention_prob_times_values (160x2048x2048x653): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x653): 62.534

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1764.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x654x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x654x2048): 73.440
Elapsed time for attention_prob_times_values (160x2048x2048x654): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x654): 64.897

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1829.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x655x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x655x2048): 71.562
Elapsed time for attention_prob_times_values (160x2048x2048x655): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x655): 62.438

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1773.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x656x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x656x2048): 75.545
Elapsed time for attention_prob_times_values (160x2048x2048x656): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x656): 75.155

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2006.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x657x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x657x2048): 70.260
Elapsed time for attention_prob_times_values (160x2048x2048x657): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x657): 59.821

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1723.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x658x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x658x2048): 73.582
Elapsed time for attention_prob_times_values (160x2048x2048x658): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x658): 64.605

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1837.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x659x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x659x2048): 70.979
Elapsed time for attention_prob_times_values (160x2048x2048x659): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x659): 63.475

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1792.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x660x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x660x2048): 73.467
Elapsed time for attention_prob_times_values (160x2048x2048x660): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x660): 66.520

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1869.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x661x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x661x2048): 73.011
Elapsed time for attention_prob_times_values (160x2048x2048x661): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x661): 62.758

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1810.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x662x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x662x2048): 71.807
Elapsed time for attention_prob_times_values (160x2048x2048x662): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x662): 66.480

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1854.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x663x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x663x2048): 74.326
Elapsed time for attention_prob_times_values (160x2048x2048x663): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x663): 64.065

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1851.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x664x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x664x2048): 75.881
Elapsed time for attention_prob_times_values (160x2048x2048x664): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x664): 80.233

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2101.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x665x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x665x2048): 73.753
Elapsed time for attention_prob_times_values (160x2048x2048x665): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x665): 64.344

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1854.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x666x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x666x2048): 74.396
Elapsed time for attention_prob_times_values (160x2048x2048x666): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x666): 66.656

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1899.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x667x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x667x2048): 73.863
Elapsed time for attention_prob_times_values (160x2048x2048x667): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x667): 64.583

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1864.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x668x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x668x2048): 75.096
Elapsed time for attention_prob_times_values (160x2048x2048x668): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x668): 67.154

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1921.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x669x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x669x2048): 74.002
Elapsed time for attention_prob_times_values (160x2048x2048x669): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x669): 62.672

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1841.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x670x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x670x2048): 73.174
Elapsed time for attention_prob_times_values (160x2048x2048x670): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x670): 66.799

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1897.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x671x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x671x2048): 72.088
Elapsed time for attention_prob_times_values (160x2048x2048x671): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x671): 63.788

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1841.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x672x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x672x2048): 89.433
Elapsed time for attention_prob_times_values (160x2048x2048x672): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x672): 80.493

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2308.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x673x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x673x2048): 73.736
Elapsed time for attention_prob_times_values (160x2048x2048x673): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x673): 62.861

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1851.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x674x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x674x2048): 77.217
Elapsed time for attention_prob_times_values (160x2048x2048x674): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x674): 65.914

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1943.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x675x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x675x2048): 73.352
Elapsed time for attention_prob_times_values (160x2048x2048x675): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x675): 62.839

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1852.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x676x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x676x2048): 77.751
Elapsed time for attention_prob_times_values (160x2048x2048x676): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x676): 66.564

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1965.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x677x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x677x2048): 73.713
Elapsed time for attention_prob_times_values (160x2048x2048x677): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x677): 65.066

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1897.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x678x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x678x2048): 77.018
Elapsed time for attention_prob_times_values (160x2048x2048x678): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x678): 67.373

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1975.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x679x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x679x2048): 75.089
Elapsed time for attention_prob_times_values (160x2048x2048x679): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x679): 64.077

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1903.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x680x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x680x2048): 80.202
Elapsed time for attention_prob_times_values (160x2048x2048x680): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x680): 79.526

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2201.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x681x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x681x2048): 73.898
Elapsed time for attention_prob_times_values (160x2048x2048x681): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x681): 64.348

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1898.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x682x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x682x2048): 73.650
Elapsed time for attention_prob_times_values (160x2048x2048x682): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x682): 65.903

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1922.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x683x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x683x2048): 72.081
Elapsed time for attention_prob_times_values (160x2048x2048x683): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x683): 64.877

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1890.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x684x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x684x2048): 76.801
Elapsed time for attention_prob_times_values (160x2048x2048x684): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x684): 68.171

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2002.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x685x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x685x2048): 74.731
Elapsed time for attention_prob_times_values (160x2048x2048x685): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x685): 65.258

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1933.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x686x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x686x2048): 76.199
Elapsed time for attention_prob_times_values (160x2048x2048x686): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x686): 68.219

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2001.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x687x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x687x2048): 75.557
Elapsed time for attention_prob_times_values (160x2048x2048x687): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x687): 64.694

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1940.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x688x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x688x2048): 77.368
Elapsed time for attention_prob_times_values (160x2048x2048x688): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x688): 82.223

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2222.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x689x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x689x2048): 72.444
Elapsed time for attention_prob_times_values (160x2048x2048x689): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x689): 65.431

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1919.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x690x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x690x2048): 74.547
Elapsed time for attention_prob_times_values (160x2048x2048x690): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x690): 68.450

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1994.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x691x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x691x2048): 74.329
Elapsed time for attention_prob_times_values (160x2048x2048x691): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x691): 65.370

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1947.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x692x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x692x2048): 74.981
Elapsed time for attention_prob_times_values (160x2048x2048x692): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x692): 68.222

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2002.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x693x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x693x2048): 74.698
Elapsed time for attention_prob_times_values (160x2048x2048x693): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x693): 66.376

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1973.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x694x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x694x2048): 75.347
Elapsed time for attention_prob_times_values (160x2048x2048x694): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x694): 68.807

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2021.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x695x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x695x2048): 75.301
Elapsed time for attention_prob_times_values (160x2048x2048x695): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x695): 66.168

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1982.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x696x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x696x2048): 77.235
Elapsed time for attention_prob_times_values (160x2048x2048x696): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x696): 82.232

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2245.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x697x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x697x2048): 73.492
Elapsed time for attention_prob_times_values (160x2048x2048x697): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x697): 65.977

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1962.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x698x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x698x2048): 73.933
Elapsed time for attention_prob_times_values (160x2048x2048x698): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x698): 68.325

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2007.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x699x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x699x2048): 74.067
Elapsed time for attention_prob_times_values (160x2048x2048x699): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x699): 63.518

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1935.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x700x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x700x2048): 75.351
Elapsed time for attention_prob_times_values (160x2048x2048x700): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x700): 67.949

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2025.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x701x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x701x2048): 75.076
Elapsed time for attention_prob_times_values (160x2048x2048x701): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x701): 66.901

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2008.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x702x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x702x2048): 76.007
Elapsed time for attention_prob_times_values (160x2048x2048x702): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x702): 69.339

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2061.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x703x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x703x2048): 75.204
Elapsed time for attention_prob_times_values (160x2048x2048x703): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x703): 67.019

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2017.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x704x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x704x2048): 89.780
Elapsed time for attention_prob_times_values (160x2048x2048x704): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x704): 86.557

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2511.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x705x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x705x2048): 76.388
Elapsed time for attention_prob_times_values (160x2048x2048x705): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x705): 63.224

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1974.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x706x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x706x2048): 77.104
Elapsed time for attention_prob_times_values (160x2048x2048x706): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x706): 65.307

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2020.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x707x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x707x2048): 75.168
Elapsed time for attention_prob_times_values (160x2048x2048x707): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x707): 63.082

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1963.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x708x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x708x2048): 77.375
Elapsed time for attention_prob_times_values (160x2048x2048x708): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x708): 66.146

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2043.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x709x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x709x2048): 76.000
Elapsed time for attention_prob_times_values (160x2048x2048x709): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x709): 60.644

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 1935.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x710x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x710x2048): 76.318
Elapsed time for attention_prob_times_values (160x2048x2048x710): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x710): 65.646

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2028.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x711x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x711x2048): 75.512
Elapsed time for attention_prob_times_values (160x2048x2048x711): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x711): 64.218

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1997.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x712x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x712x2048): 78.157
Elapsed time for attention_prob_times_values (160x2048x2048x712): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x712): 85.980

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2359.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x713x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x713x2048): 75.896
Elapsed time for attention_prob_times_values (160x2048x2048x713): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x713): 62.737

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1981.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x714x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x714x2048): 76.814
Elapsed time for attention_prob_times_values (160x2048x2048x714): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x714): 67.202

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2071.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x715x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x715x2048): 76.282
Elapsed time for attention_prob_times_values (160x2048x2048x715): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x715): 63.979

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2013.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x716x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x716x2048): 76.930
Elapsed time for attention_prob_times_values (160x2048x2048x716): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x716): 66.230

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2062.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x717x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x717x2048): 73.671
Elapsed time for attention_prob_times_values (160x2048x2048x717): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x717): 64.052

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1987.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x718x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x718x2048): 76.242
Elapsed time for attention_prob_times_values (160x2048x2048x718): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x718): 65.462

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2046.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x719x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x719x2048): 74.609
Elapsed time for attention_prob_times_values (160x2048x2048x719): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x719): 62.488

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1978.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x720x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x720x2048): 76.970
Elapsed time for attention_prob_times_values (160x2048x2048x720): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x720): 84.652

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2348.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x721x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x721x2048): 73.281
Elapsed time for attention_prob_times_values (160x2048x2048x721): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x721): 64.289

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 1997.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x722x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x722x2048): 74.566
Elapsed time for attention_prob_times_values (160x2048x2048x722): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x722): 67.123

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2063.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x723x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x723x2048): 74.464
Elapsed time for attention_prob_times_values (160x2048x2048x723): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x723): 64.020

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 2013.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x724x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x724x2048): 75.872
Elapsed time for attention_prob_times_values (160x2048x2048x724): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x724): 66.376

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2073.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x725x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x725x2048): 75.981
Elapsed time for attention_prob_times_values (160x2048x2048x725): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x725): 63.989

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2036.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x726x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x726x2048): 74.907
Elapsed time for attention_prob_times_values (160x2048x2048x726): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x726): 66.630

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2070.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x727x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x727x2048): 74.474
Elapsed time for attention_prob_times_values (160x2048x2048x727): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x727): 63.514

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2015.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x728x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x728x2048): 77.103
Elapsed time for attention_prob_times_values (160x2048x2048x728): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x728): 86.611

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2401.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x729x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x729x2048): 75.478
Elapsed time for attention_prob_times_values (160x2048x2048x729): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x729): 62.896

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2022.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x730x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x730x2048): 74.979
Elapsed time for attention_prob_times_values (160x2048x2048x730): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x730): 66.593

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2081.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x731x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x731x2048): 74.297
Elapsed time for attention_prob_times_values (160x2048x2048x731): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x731): 65.198

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2052.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x732x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x732x2048): 74.491
Elapsed time for attention_prob_times_values (160x2048x2048x732): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x732): 68.837

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2117.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x733x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x733x2048): 74.412
Elapsed time for attention_prob_times_values (160x2048x2048x733): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x733): 63.979

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 2038.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x734x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x734x2048): 73.231
Elapsed time for attention_prob_times_values (160x2048x2048x734): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x734): 66.420

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2066.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x735x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x735x2048): 73.606
Elapsed time for attention_prob_times_values (160x2048x2048x735): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x735): 63.228

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 2021.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x736x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x736x2048): 87.206
Elapsed time for attention_prob_times_values (160x2048x2048x736): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x736): 89.148

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2622.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x737x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x737x2048): 75.356
Elapsed time for attention_prob_times_values (160x2048x2048x737): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x737): 65.704

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 2091.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x738x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x738x2048): 75.324
Elapsed time for attention_prob_times_values (160x2048x2048x738): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x738): 67.410

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2122.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x739x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x739x2048): 74.722
Elapsed time for attention_prob_times_values (160x2048x2048x739): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x739): 65.868

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2091.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x740x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x740x2048): 76.176
Elapsed time for attention_prob_times_values (160x2048x2048x740): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x740): 67.313

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2137.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x741x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x741x2048): 75.080
Elapsed time for attention_prob_times_values (160x2048x2048x741): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x741): 65.882

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2101.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x742x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x742x2048): 77.562
Elapsed time for attention_prob_times_values (160x2048x2048x742): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x742): 67.652

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2166.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x743x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x743x2048): 74.642
Elapsed time for attention_prob_times_values (160x2048x2048x743): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x743): 65.975

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2102.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x744x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x744x2048): 79.042
Elapsed time for attention_prob_times_values (160x2048x2048x744): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x744): 89.854

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2528.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x745x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x745x2048): 76.193
Elapsed time for attention_prob_times_values (160x2048x2048x745): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x745): 65.935

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2127.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x746x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x746x2048): 77.155
Elapsed time for attention_prob_times_values (160x2048x2048x746): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x746): 67.541

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 2170.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x747x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x747x2048): 75.517
Elapsed time for attention_prob_times_values (160x2048x2048x747): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x747): 66.142

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 2128.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x748x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x748x2048): 75.096
Elapsed time for attention_prob_times_values (160x2048x2048x748): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x748): 68.460

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2164.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 29960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x749x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x749x2048): 74.275
Elapsed time for attention_prob_times_values (160x2048x2048x749): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x749): 63.061

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 2063.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x750x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x750x2048): 72.553
Elapsed time for attention_prob_times_values (160x2048x2048x750): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x750): 68.132

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 2129.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x751x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x751x2048): 74.119
Elapsed time for attention_prob_times_values (160x2048x2048x751): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x751): 65.283

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 2105.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x752x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x752x2048): 74.819
Elapsed time for attention_prob_times_values (160x2048x2048x752): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x752): 91.178

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2496.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x753x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x753x2048): 74.262
Elapsed time for attention_prob_times_values (160x2048x2048x753): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x753): 66.252

Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 2129.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x754x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x754x2048): 74.368
Elapsed time for attention_prob_times_values (160x2048x2048x754): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x754): 68.048

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2164.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x755x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x755x2048): 76.088
Elapsed time for attention_prob_times_values (160x2048x2048x755): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x755): 66.905

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2171.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x756x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x756x2048): 77.286
Elapsed time for attention_prob_times_values (160x2048x2048x756): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x756): 65.799

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2170.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x757x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x757x2048): 73.370
Elapsed time for attention_prob_times_values (160x2048x2048x757): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x757): 66.555

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2133.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x758x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x758x2048): 73.104
Elapsed time for attention_prob_times_values (160x2048x2048x758): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x758): 67.199

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2143.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x759x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x759x2048): 74.457
Elapsed time for attention_prob_times_values (160x2048x2048x759): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x759): 64.633

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 2120.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x760x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x760x2048): 74.672
Elapsed time for attention_prob_times_values (160x2048x2048x760): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x760): 90.298

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2508.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x761x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x761x2048): 75.111
Elapsed time for attention_prob_times_values (160x2048x2048x761): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x761): 65.864

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2156.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x762x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x762x2048): 75.674
Elapsed time for attention_prob_times_values (160x2048x2048x762): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x762): 68.502

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 2212.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x763x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x763x2048): 75.350
Elapsed time for attention_prob_times_values (160x2048x2048x763): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x763): 65.795

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 2164.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x764x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x764x2048): 76.183
Elapsed time for attention_prob_times_values (160x2048x2048x764): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x764): 67.997

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2216.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x765x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x765x2048): 74.957
Elapsed time for attention_prob_times_values (160x2048x2048x765): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x765): 66.522

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2176.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x766x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x766x2048): 75.827
Elapsed time for attention_prob_times_values (160x2048x2048x766): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x766): 64.675

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 2158.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x767x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x767x2048): 72.842
Elapsed time for attention_prob_times_values (160x2048x2048x767): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x767): 62.339

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2080.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x768x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x768x2048): 83.663
Elapsed time for attention_prob_times_values (160x2048x2048x768): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x768): 94.261

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2748.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x769x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x769x2048): 73.835
Elapsed time for attention_prob_times_values (160x2048x2048x769): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x769): 60.316

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2060.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x770x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x770x2048): 76.208
Elapsed time for attention_prob_times_values (160x2048x2048x770): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x770): 63.881

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 2159.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x771x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x771x2048): 75.601
Elapsed time for attention_prob_times_values (160x2048x2048x771): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x771): 62.090

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 2121.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x772x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x772x2048): 77.578
Elapsed time for attention_prob_times_values (160x2048x2048x772): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x772): 65.344

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 2210.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x773x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x773x2048): 76.568
Elapsed time for attention_prob_times_values (160x2048x2048x773): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x773): 62.014

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 2137.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x774x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x774x2048): 77.528
Elapsed time for attention_prob_times_values (160x2048x2048x774): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x774): 65.364

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 2215.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x775x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x775x2048): 76.133
Elapsed time for attention_prob_times_values (160x2048x2048x775): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x775): 62.517

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 2147.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x776x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x776x2048): 77.717
Elapsed time for attention_prob_times_values (160x2048x2048x776): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x776): 80.352

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2474.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x777x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x777x2048): 75.880
Elapsed time for attention_prob_times_values (160x2048x2048x777): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x777): 60.886

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2118.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x778x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x778x2048): 76.254
Elapsed time for attention_prob_times_values (160x2048x2048x778): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x778): 65.195

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 2206.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x779x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x779x2048): 74.719
Elapsed time for attention_prob_times_values (160x2048x2048x779): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x779): 61.875

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2127.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x780x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x780x2048): 77.658
Elapsed time for attention_prob_times_values (160x2048x2048x780): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x780): 65.837

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 2242.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x781x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x781x2048): 76.128
Elapsed time for attention_prob_times_values (160x2048x2048x781): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x781): 61.952

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2152.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x782x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x782x2048): 76.643
Elapsed time for attention_prob_times_values (160x2048x2048x782): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x782): 64.206

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2204.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x783x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x783x2048): 73.993
Elapsed time for attention_prob_times_values (160x2048x2048x783): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x783): 60.968

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 2111.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x784x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x784x2048): 76.170
Elapsed time for attention_prob_times_values (160x2048x2048x784): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x784): 82.253

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2501.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x785x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x785x2048): 73.873
Elapsed time for attention_prob_times_values (160x2048x2048x785): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x785): 62.057

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2135.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x786x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x786x2048): 77.261
Elapsed time for attention_prob_times_values (160x2048x2048x786): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x786): 66.046

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 2257.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x787x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x787x2048): 76.684
Elapsed time for attention_prob_times_values (160x2048x2048x787): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x787): 62.979

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2195.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x788x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x788x2048): 77.935
Elapsed time for attention_prob_times_values (160x2048x2048x788): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x788): 65.944

Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 2270.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x789x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x789x2048): 76.806
Elapsed time for attention_prob_times_values (160x2048x2048x789): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x789): 62.464

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2192.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x790x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x790x2048): 74.922
Elapsed time for attention_prob_times_values (160x2048x2048x790): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x790): 64.387

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2206.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x791x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x791x2048): 75.459
Elapsed time for attention_prob_times_values (160x2048x2048x791): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x791): 60.803

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 2148.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x792x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x792x2048): 77.587
Elapsed time for attention_prob_times_values (160x2048x2048x792): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x792): 81.789

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2543.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x793x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x793x2048): 75.301
Elapsed time for attention_prob_times_values (160x2048x2048x793): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x793): 62.826

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2190.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x794x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x794x2048): 75.321
Elapsed time for attention_prob_times_values (160x2048x2048x794): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x794): 63.774

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2211.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x795x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x795x2048): 73.568
Elapsed time for attention_prob_times_values (160x2048x2048x795): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x795): 62.635

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 2168.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x796x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x796x2048): 74.162
Elapsed time for attention_prob_times_values (160x2048x2048x796): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x796): 66.816

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 2256.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x797x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x797x2048): 75.494
Elapsed time for attention_prob_times_values (160x2048x2048x797): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x797): 62.800

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2203.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x798x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x798x2048): 74.917
Elapsed time for attention_prob_times_values (160x2048x2048x798): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x798): 65.483

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2248.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 31960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x799x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x799x2048): 75.613
Elapsed time for attention_prob_times_values (160x2048x2048x799): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x799): 63.114

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2216.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x800x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x800x2048): 87.853
Elapsed time for attention_prob_times_values (160x2048x2048x800): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x800): 85.966

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2802.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x801x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x801x2048): 75.918
Elapsed time for attention_prob_times_values (160x2048x2048x801): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x801): 65.198

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2265.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x802x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x802x2048): 75.968
Elapsed time for attention_prob_times_values (160x2048x2048x802): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x802): 67.038

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 2302.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x803x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x803x2048): 74.896
Elapsed time for attention_prob_times_values (160x2048x2048x803): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x803): 64.303

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2239.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x804x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x804x2048): 80.376
Elapsed time for attention_prob_times_values (160x2048x2048x804): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x804): 67.397

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 2375.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x805x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x805x2048): 77.932
Elapsed time for attention_prob_times_values (160x2048x2048x805): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x805): 64.435

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2288.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x806x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x806x2048): 79.409
Elapsed time for attention_prob_times_values (160x2048x2048x806): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x806): 67.309

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 2366.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x807x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x807x2048): 78.363
Elapsed time for attention_prob_times_values (160x2048x2048x807): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x807): 64.423

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2299.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x808x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x808x2048): 80.321
Elapsed time for attention_prob_times_values (160x2048x2048x808): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x808): 86.206

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2707.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x809x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x809x2048): 77.713
Elapsed time for attention_prob_times_values (160x2048x2048x809): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x809): 63.883

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 2286.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x810x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x810x2048): 77.049
Elapsed time for attention_prob_times_values (160x2048x2048x810): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x810): 66.322

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2326.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x811x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x811x2048): 76.200
Elapsed time for attention_prob_times_values (160x2048x2048x811): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x811): 63.369

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 2261.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x812x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x812x2048): 78.025
Elapsed time for attention_prob_times_values (160x2048x2048x812): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x812): 67.915

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2376.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x813x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x813x2048): 78.082
Elapsed time for attention_prob_times_values (160x2048x2048x813): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x813): 64.863

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 2321.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x814x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x814x2048): 79.063
Elapsed time for attention_prob_times_values (160x2048x2048x814): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x814): 80.733

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2620.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x815x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x815x2048): 78.447
Elapsed time for attention_prob_times_values (160x2048x2048x815): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x815): 78.455

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 2576.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x816x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x816x2048): 80.085
Elapsed time for attention_prob_times_values (160x2048x2048x816): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x816): 87.277

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2745.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x817x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x817x2048): 78.067
Elapsed time for attention_prob_times_values (160x2048x2048x817): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x817): 77.837

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2565.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x818x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x818x2048): 76.602
Elapsed time for attention_prob_times_values (160x2048x2048x818): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x818): 79.775

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2575.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 40, hidden_size: 32760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (160x2048x819x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (160x2048x819x2048): 76.956
Elapsed time for attention_prob_times_values (160x2048x2048x819): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (160x2048x2048x819): 77.079

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 2540.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 64, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x1x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x1x2048): 1.038
Elapsed time for attention_prob_times_values (256x2048x2048x1): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x1): 0.210

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 0.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x2x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x2x2048): 1.917
Elapsed time for attention_prob_times_values (256x2048x2048x2): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x2): 1.814

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 2.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x3x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x3x2048): 2.548
Elapsed time for attention_prob_times_values (256x2048x2048x3): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x3): 2.096

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 2.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x4x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x4x2048): 3.426
Elapsed time for attention_prob_times_values (256x2048x2048x4): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x4): 2.810

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 3.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x5x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x5x2048): 4.227
Elapsed time for attention_prob_times_values (256x2048x2048x5): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x5): 3.769

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 5.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x6x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x6x2048): 5.075
Elapsed time for attention_prob_times_values (256x2048x2048x6): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x6): 4.316

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 6.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x7x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x7x2048): 5.895
Elapsed time for attention_prob_times_values (256x2048x2048x7): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x7): 5.250

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 7.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x8x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x8x2048): 6.632
Elapsed time for attention_prob_times_values (256x2048x2048x8): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x8): 7.173

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 10.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x9x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x9x2048): 7.215
Elapsed time for attention_prob_times_values (256x2048x2048x9): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x9): 7.469

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 11.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x10x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x10x2048): 7.981
Elapsed time for attention_prob_times_values (256x2048x2048x10): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x10): 8.806

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 13.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x11x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x11x2048): 8.718
Elapsed time for attention_prob_times_values (256x2048x2048x11): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x11): 9.336

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 15.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x12x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x12x2048): 9.322
Elapsed time for attention_prob_times_values (256x2048x2048x12): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x12): 10.564

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 17.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x13x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x13x2048): 10.213
Elapsed time for attention_prob_times_values (256x2048x2048x13): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x13): 11.024

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 19.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x14x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x14x2048): 11.061
Elapsed time for attention_prob_times_values (256x2048x2048x14): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x14): 11.301

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 20.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x15x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x15x2048): 11.828
Elapsed time for attention_prob_times_values (256x2048x2048x15): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x15): 12.659

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 23.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x16x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x16x2048): 12.677
Elapsed time for attention_prob_times_values (256x2048x2048x16): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x16): 14.026

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 26.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x17x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x17x2048): 12.996
Elapsed time for attention_prob_times_values (256x2048x2048x17): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x17): 14.198

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 27.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x18x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x18x2048): 13.690
Elapsed time for attention_prob_times_values (256x2048x2048x18): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x18): 15.417

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 30.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x19x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x19x2048): 14.435
Elapsed time for attention_prob_times_values (256x2048x2048x19): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x19): 15.871

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 33.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x20x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x20x2048): 15.117
Elapsed time for attention_prob_times_values (256x2048x2048x20): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x20): 15.791

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 34.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x21x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x21x2048): 15.918
Elapsed time for attention_prob_times_values (256x2048x2048x21): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x21): 17.379

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 38.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x22x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x22x2048): 16.652
Elapsed time for attention_prob_times_values (256x2048x2048x22): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x22): 18.650

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 41.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x23x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x23x2048): 17.421
Elapsed time for attention_prob_times_values (256x2048x2048x23): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x23): 18.828

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 44.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x24x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x24x2048): 18.070
Elapsed time for attention_prob_times_values (256x2048x2048x24): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x24): 20.483

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 48.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x25x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x25x2048): 18.426
Elapsed time for attention_prob_times_values (256x2048x2048x25): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x25): 17.651

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 46.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x26x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x26x2048): 19.060
Elapsed time for attention_prob_times_values (256x2048x2048x26): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x26): 20.387

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 51.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x27x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x27x2048): 19.814
Elapsed time for attention_prob_times_values (256x2048x2048x27): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x27): 21.949

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 55.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x28x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x28x2048): 19.449
Elapsed time for attention_prob_times_values (256x2048x2048x28): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x28): 23.161

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 58.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x29x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x29x2048): 21.171
Elapsed time for attention_prob_times_values (256x2048x2048x29): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x29): 23.279

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 62.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x30x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x30x2048): 21.933
Elapsed time for attention_prob_times_values (256x2048x2048x30): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x30): 24.655

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 66.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x31x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x31x2048): 22.585
Elapsed time for attention_prob_times_values (256x2048x2048x31): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x31): 24.629

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 69.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x32x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x32x2048): 40.682
Elapsed time for attention_prob_times_values (256x2048x2048x32): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x32): 26.575

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 96.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x33x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x33x2048): 25.657
Elapsed time for attention_prob_times_values (256x2048x2048x33): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x33): 26.398

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 79.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x34x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x34x2048): 27.106
Elapsed time for attention_prob_times_values (256x2048x2048x34): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x34): 27.431

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 85.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x35x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x35x2048): 26.125
Elapsed time for attention_prob_times_values (256x2048x2048x35): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x35): 27.844

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 85.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x36x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x36x2048): 25.579
Elapsed time for attention_prob_times_values (256x2048x2048x36): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x36): 29.181

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 88.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x37x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x37x2048): 26.740
Elapsed time for attention_prob_times_values (256x2048x2048x37): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x37): 27.912

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 90.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x38x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x38x2048): 27.552
Elapsed time for attention_prob_times_values (256x2048x2048x38): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x38): 30.443

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 97.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x39x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x39x2048): 27.835
Elapsed time for attention_prob_times_values (256x2048x2048x39): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x39): 30.790

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 100.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x40x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x40x2048): 28.660
Elapsed time for attention_prob_times_values (256x2048x2048x40): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x40): 31.398

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 104.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x41x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x41x2048): 27.373
Elapsed time for attention_prob_times_values (256x2048x2048x41): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x41): 31.861

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 104.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x42x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x42x2048): 29.190
Elapsed time for attention_prob_times_values (256x2048x2048x42): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x42): 30.168

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 107.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x43x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x43x2048): 29.550
Elapsed time for attention_prob_times_values (256x2048x2048x43): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x43): 33.180

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 115.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x44x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x44x2048): 30.451
Elapsed time for attention_prob_times_values (256x2048x2048x44): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x44): 34.667

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 121.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x45x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x45x2048): 30.877
Elapsed time for attention_prob_times_values (256x2048x2048x45): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x45): 34.251

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 123.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x46x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x46x2048): 31.619
Elapsed time for attention_prob_times_values (256x2048x2048x46): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x46): 35.963

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 130.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x47x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x47x2048): 32.232
Elapsed time for attention_prob_times_values (256x2048x2048x47): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x47): 36.164

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 134.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x48x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x48x2048): 33.342
Elapsed time for attention_prob_times_values (256x2048x2048x48): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x48): 38.626

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 143.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x49x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x49x2048): 33.262
Elapsed time for attention_prob_times_values (256x2048x2048x49): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x49): 36.397

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 141.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x50x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x50x2048): 34.064
Elapsed time for attention_prob_times_values (256x2048x2048x50): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x50): 38.384

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 148.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x51x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x51x2048): 34.630
Elapsed time for attention_prob_times_values (256x2048x2048x51): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x51): 38.492

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 152.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x52x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x52x2048): 35.229
Elapsed time for attention_prob_times_values (256x2048x2048x52): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x52): 40.384

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 159.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x53x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x53x2048): 36.029
Elapsed time for attention_prob_times_values (256x2048x2048x53): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x53): 40.019

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 163.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x54x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x54x2048): 36.836
Elapsed time for attention_prob_times_values (256x2048x2048x54): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x54): 41.607

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 170.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x55x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x55x2048): 35.464
Elapsed time for attention_prob_times_values (256x2048x2048x55): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x55): 41.156

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 169.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x56x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x56x2048): 38.160
Elapsed time for attention_prob_times_values (256x2048x2048x56): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x56): 44.507

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 184.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x57x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x57x2048): 37.024
Elapsed time for attention_prob_times_values (256x2048x2048x57): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x57): 40.919

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 177.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x58x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x58x2048): 38.371
Elapsed time for attention_prob_times_values (256x2048x2048x58): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x58): 44.172

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 189.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x59x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x59x2048): 38.847
Elapsed time for attention_prob_times_values (256x2048x2048x59): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x59): 43.667

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 192.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x60x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x60x2048): 37.392
Elapsed time for attention_prob_times_values (256x2048x2048x60): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x60): 45.687

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 195.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x61x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x61x2048): 39.067
Elapsed time for attention_prob_times_values (256x2048x2048x61): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x61): 45.225

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 201.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x62x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x62x2048): 40.742
Elapsed time for attention_prob_times_values (256x2048x2048x62): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x62): 47.447

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 213.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x63x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x63x2048): 40.909
Elapsed time for attention_prob_times_values (256x2048x2048x63): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x63): 46.781

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 215.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x64x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x64x2048): 55.994
Elapsed time for attention_prob_times_values (256x2048x2048x64): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x64): 50.736

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 266.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x65x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x65x2048): 43.568
Elapsed time for attention_prob_times_values (256x2048x2048x65): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x65): 33.993

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 193.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x66x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x66x2048): 44.624
Elapsed time for attention_prob_times_values (256x2048x2048x66): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x66): 35.358

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 202.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x67x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x67x2048): 44.309
Elapsed time for attention_prob_times_values (256x2048x2048x67): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x67): 33.259

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 197.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x68x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x68x2048): 45.043
Elapsed time for attention_prob_times_values (256x2048x2048x68): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x68): 36.141

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 210.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x69x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x69x2048): 44.961
Elapsed time for attention_prob_times_values (256x2048x2048x69): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x69): 35.714

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 211.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x70x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x70x2048): 45.995
Elapsed time for attention_prob_times_values (256x2048x2048x70): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x70): 37.374

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 221.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x71x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x71x2048): 45.666
Elapsed time for attention_prob_times_values (256x2048x2048x71): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x71): 36.885

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 221.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x72x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x72x2048): 46.916
Elapsed time for attention_prob_times_values (256x2048x2048x72): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x72): 36.775

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 226.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x73x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x73x2048): 45.681
Elapsed time for attention_prob_times_values (256x2048x2048x73): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x73): 37.573

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 229.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x74x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x74x2048): 46.804
Elapsed time for attention_prob_times_values (256x2048x2048x74): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x74): 39.229

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 240.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x75x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x75x2048): 42.984
Elapsed time for attention_prob_times_values (256x2048x2048x75): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x75): 38.536

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 231.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x76x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x76x2048): 47.820
Elapsed time for attention_prob_times_values (256x2048x2048x76): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x76): 40.279

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 251.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x77x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x77x2048): 45.192
Elapsed time for attention_prob_times_values (256x2048x2048x77): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x77): 39.324

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 244.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x78x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x78x2048): 48.681
Elapsed time for attention_prob_times_values (256x2048x2048x78): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x78): 41.042

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 261.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x79x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x79x2048): 46.786
Elapsed time for attention_prob_times_values (256x2048x2048x79): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x79): 40.213

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 256.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x80x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x80x2048): 50.400
Elapsed time for attention_prob_times_values (256x2048x2048x80): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x80): 39.324

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 265.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x81x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x81x2048): 47.471
Elapsed time for attention_prob_times_values (256x2048x2048x81): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x81): 39.155

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 260.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x82x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x82x2048): 48.029
Elapsed time for attention_prob_times_values (256x2048x2048x82): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x82): 42.802

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 277.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x83x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x83x2048): 48.310
Elapsed time for attention_prob_times_values (256x2048x2048x83): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x83): 41.740

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 277.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x84x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x84x2048): 48.820
Elapsed time for attention_prob_times_values (256x2048x2048x84): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x84): 43.724

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 288.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x85x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x85x2048): 48.858
Elapsed time for attention_prob_times_values (256x2048x2048x85): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x85): 42.671

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 287.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x86x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x86x2048): 47.807
Elapsed time for attention_prob_times_values (256x2048x2048x86): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x86): 44.437

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 293.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x87x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x87x2048): 49.893
Elapsed time for attention_prob_times_values (256x2048x2048x87): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x87): 43.489

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 299.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x88x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x88x2048): 49.986
Elapsed time for attention_prob_times_values (256x2048x2048x88): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x88): 43.219

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 301.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x89x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x89x2048): 49.271
Elapsed time for attention_prob_times_values (256x2048x2048x89): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x89): 43.667

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 303.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x90x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x90x2048): 49.103
Elapsed time for attention_prob_times_values (256x2048x2048x90): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x90): 46.212

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 315.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x91x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x91x2048): 50.172
Elapsed time for attention_prob_times_values (256x2048x2048x91): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x91): 44.270

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 314.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x92x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x92x2048): 51.079
Elapsed time for attention_prob_times_values (256x2048x2048x92): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x92): 46.385

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 328.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x93x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x93x2048): 51.025
Elapsed time for attention_prob_times_values (256x2048x2048x93): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x93): 45.880

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 329.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x94x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x94x2048): 51.838
Elapsed time for attention_prob_times_values (256x2048x2048x94): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x94): 47.916

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 342.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x95x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x95x2048): 51.882
Elapsed time for attention_prob_times_values (256x2048x2048x95): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x95): 46.677

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 340.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x96x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x96x2048): 61.126
Elapsed time for attention_prob_times_values (256x2048x2048x96): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x96): 48.515

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 378.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x97x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x97x2048): 48.821
Elapsed time for attention_prob_times_values (256x2048x2048x97): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x97): 47.714

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 340.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x98x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x98x2048): 51.933
Elapsed time for attention_prob_times_values (256x2048x2048x98): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x98): 46.610

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 350.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x99x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x99x2048): 52.571
Elapsed time for attention_prob_times_values (256x2048x2048x99): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x99): 48.568

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 362.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x100x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x100x2048): 54.875
Elapsed time for attention_prob_times_values (256x2048x2048x100): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x100): 50.168

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 380.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x101x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x101x2048): 53.852
Elapsed time for attention_prob_times_values (256x2048x2048x101): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x101): 49.042

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 375.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x102x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x102x2048): 53.214
Elapsed time for attention_prob_times_values (256x2048x2048x102): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x102): 50.364

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 381.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x103x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x103x2048): 54.599
Elapsed time for attention_prob_times_values (256x2048x2048x103): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x103): 50.216

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 389.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x104x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x104x2048): 56.039
Elapsed time for attention_prob_times_values (256x2048x2048x104): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x104): 50.600

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 398.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x105x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x105x2048): 52.356
Elapsed time for attention_prob_times_values (256x2048x2048x105): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x105): 49.816

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 386.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x106x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x106x2048): 55.300
Elapsed time for attention_prob_times_values (256x2048x2048x106): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x106): 51.417

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 406.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x107x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x107x2048): 52.957
Elapsed time for attention_prob_times_values (256x2048x2048x107): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x107): 50.362

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 396.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x108x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x108x2048): 56.236
Elapsed time for attention_prob_times_values (256x2048x2048x108): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x108): 52.320

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 420.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x109x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x109x2048): 53.196
Elapsed time for attention_prob_times_values (256x2048x2048x109): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x109): 50.232

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 403.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x110x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x110x2048): 56.743
Elapsed time for attention_prob_times_values (256x2048x2048x110): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x110): 54.350

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 437.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x111x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x111x2048): 56.339
Elapsed time for attention_prob_times_values (256x2048x2048x111): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x111): 52.816

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 432.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x112x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x112x2048): 58.394
Elapsed time for attention_prob_times_values (256x2048x2048x112): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x112): 54.913

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 452.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x113x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x113x2048): 54.385
Elapsed time for attention_prob_times_values (256x2048x2048x113): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x113): 53.420

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 434.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x114x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x114x2048): 57.435
Elapsed time for attention_prob_times_values (256x2048x2048x114): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x114): 55.576

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 458.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x115x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x115x2048): 56.711
Elapsed time for attention_prob_times_values (256x2048x2048x115): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x115): 51.971

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 444.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x116x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x116x2048): 58.130
Elapsed time for attention_prob_times_values (256x2048x2048x116): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x116): 56.386

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 472.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x117x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x117x2048): 57.562
Elapsed time for attention_prob_times_values (256x2048x2048x117): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x117): 54.174

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 463.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x118x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x118x2048): 56.749
Elapsed time for attention_prob_times_values (256x2048x2048x118): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x118): 57.330

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 477.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x119x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x119x2048): 58.247
Elapsed time for attention_prob_times_values (256x2048x2048x119): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x119): 55.577

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 479.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x120x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x120x2048): 57.429
Elapsed time for attention_prob_times_values (256x2048x2048x120): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x120): 57.377

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 487.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x121x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x121x2048): 58.009
Elapsed time for attention_prob_times_values (256x2048x2048x121): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x121): 42.641

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 420.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x122x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x122x2048): 58.997
Elapsed time for attention_prob_times_values (256x2048x2048x122): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x122): 56.781

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 499.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x123x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x123x2048): 57.772
Elapsed time for attention_prob_times_values (256x2048x2048x123): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x123): 42.433

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 425.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x124x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x124x2048): 57.468
Elapsed time for attention_prob_times_values (256x2048x2048x124): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x124): 58.519

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 507.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x125x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x125x2048): 57.344
Elapsed time for attention_prob_times_values (256x2048x2048x125): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x125): 44.068

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 439.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x126x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x126x2048): 59.707
Elapsed time for attention_prob_times_values (256x2048x2048x126): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x126): 60.118

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 531.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x127x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x127x2048): 58.387
Elapsed time for attention_prob_times_values (256x2048x2048x127): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x127): 43.803

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 447.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 69.650
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 63.386

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 597.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x129x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x129x2048): 56.024
Elapsed time for attention_prob_times_values (256x2048x2048x129): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x129): 45.167

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 453.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 59.538
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 45.720

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 471.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x131x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x131x2048): 59.303
Elapsed time for attention_prob_times_values (256x2048x2048x131): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x131): 46.287

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 477.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 59.093
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 48.818

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 494.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x133x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x133x2048): 59.458
Elapsed time for attention_prob_times_values (256x2048x2048x133): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x133): 47.017

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 489.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 58.679
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 47.153

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 490.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x135x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x135x2048): 58.996
Elapsed time for attention_prob_times_values (256x2048x2048x135): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x135): 47.670

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 497.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 62.071
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 46.341

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 504.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x137x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x137x2048): 60.171
Elapsed time for attention_prob_times_values (256x2048x2048x137): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x137): 45.871

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 497.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 61.072
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 49.823

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 528.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x139x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x139x2048): 60.583
Elapsed time for attention_prob_times_values (256x2048x2048x139): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x139): 48.293

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 520.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 60.065
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 51.514

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 540.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x141x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x141x2048): 60.979
Elapsed time for attention_prob_times_values (256x2048x2048x141): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x141): 49.170

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 534.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 61.095
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 51.873

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 554.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x143x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x143x2048): 60.040
Elapsed time for attention_prob_times_values (256x2048x2048x143): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x143): 47.241

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 525.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 64.200
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 50.046

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 562.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x145x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x145x2048): 61.389
Elapsed time for attention_prob_times_values (256x2048x2048x145): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x145): 50.543

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 557.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 61.873
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 49.583

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 557.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x147x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x147x2048): 61.898
Elapsed time for attention_prob_times_values (256x2048x2048x147): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x147): 51.082

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 570.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 62.860
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 53.848

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 594.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x149x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x149x2048): 62.946
Elapsed time for attention_prob_times_values (256x2048x2048x149): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x149): 51.724

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 585.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 64.212
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 54.243

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 610.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x151x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x151x2048): 61.359
Elapsed time for attention_prob_times_values (256x2048x2048x151): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x151): 50.621

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 579.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 63.413
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 50.051

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 587.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x153x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x153x2048): 60.153
Elapsed time for attention_prob_times_values (256x2048x2048x153): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x153): 52.884

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 594.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 64.364
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 55.299

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 632.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x155x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x155x2048): 60.867
Elapsed time for attention_prob_times_values (256x2048x2048x155): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x155): 51.670

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 597.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 65.263
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 56.209

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 649.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x157x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x157x2048): 64.634
Elapsed time for attention_prob_times_values (256x2048x2048x157): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x157): 52.676

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 627.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 65.822
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 54.740

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 650.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x159x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x159x2048): 65.047
Elapsed time for attention_prob_times_values (256x2048x2048x159): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x159): 54.566

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 649.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 78.635
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 56.511

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 723.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x161x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x161x2048): 63.124
Elapsed time for attention_prob_times_values (256x2048x2048x161): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x161): 55.364

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 652.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 65.346
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 57.577

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 681.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x163x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x163x2048): 64.422
Elapsed time for attention_prob_times_values (256x2048x2048x163): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x163): 53.957

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 657.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 66.319
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 58.449

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 699.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x165x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x165x2048): 64.970
Elapsed time for attention_prob_times_values (256x2048x2048x165): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x165): 56.820

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 685.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 65.958
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 57.980

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 701.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x167x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x167x2048): 65.193
Elapsed time for attention_prob_times_values (256x2048x2048x167): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x167): 56.316

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 691.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 67.482
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 53.721

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 687.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x169x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x169x2048): 62.371
Elapsed time for attention_prob_times_values (256x2048x2048x169): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x169): 56.523

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 685.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 66.379
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 57.496

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 716.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x171x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x171x2048): 65.785
Elapsed time for attention_prob_times_values (256x2048x2048x171): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x171): 55.858

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 706.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 67.376
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 60.886

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 751.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x173x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x173x2048): 64.706
Elapsed time for attention_prob_times_values (256x2048x2048x173): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x173): 58.871

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 728.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 67.639
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 61.218

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 763.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x175x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x175x2048): 66.822
Elapsed time for attention_prob_times_values (256x2048x2048x175): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x175): 59.412

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 750.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 69.319
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 58.579

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 761.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x177x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x177x2048): 65.762
Elapsed time for attention_prob_times_values (256x2048x2048x177): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x177): 59.612

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 754.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 66.438
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 62.051

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 778.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x179x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x179x2048): 65.227
Elapsed time for attention_prob_times_values (256x2048x2048x179): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x179): 57.828

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 747.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0184
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 21.036
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 63.110

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 386.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x181x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x181x2048): 67.586
Elapsed time for attention_prob_times_values (256x2048x2048x181): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x181): 57.155

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 762.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 69.088
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 59.862

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 793.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x183x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x183x2048): 65.983
Elapsed time for attention_prob_times_values (256x2048x2048x183): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x183): 61.629

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 792.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 70.345
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 61.526

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 820.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x185x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x185x2048): 67.732
Elapsed time for attention_prob_times_values (256x2048x2048x185): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x185): 61.971

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 813.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 67.737
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 64.525

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 834.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x187x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x187x2048): 66.164
Elapsed time for attention_prob_times_values (256x2048x2048x187): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x187): 62.687

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 816.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 69.895
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 64.798

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 857.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x189x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x189x2048): 66.325
Elapsed time for attention_prob_times_values (256x2048x2048x189): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x189): 61.689

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 819.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 70.245
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 63.080

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 855.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x191x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x191x2048): 68.787
Elapsed time for attention_prob_times_values (256x2048x2048x191): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x191): 63.645

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 855.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 80.545
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 66.916

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 950.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x193x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x193x2048): 65.294
Elapsed time for attention_prob_times_values (256x2048x2048x193): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x193): 54.044

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 772.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 66.720
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 56.379

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 802.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x195x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x195x2048): 66.730
Elapsed time for attention_prob_times_values (256x2048x2048x195): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x195): 54.548

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 791.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 70.184
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 54.706

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 814.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x197x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x197x2048): 68.955
Elapsed time for attention_prob_times_values (256x2048x2048x197): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x197): 54.638

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 811.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 70.402
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 55.463

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 829.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x199x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x199x2048): 67.936
Elapsed time for attention_prob_times_values (256x2048x2048x199): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x199): 55.091

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 817.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 67.278
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 52.517

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 796.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x201x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x201x2048): 68.975
Elapsed time for attention_prob_times_values (256x2048x2048x201): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x201): 51.917

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 803.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 68.257
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 56.967

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 846.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x203x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x203x2048): 68.038
Elapsed time for attention_prob_times_values (256x2048x2048x203): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x203): 52.615

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 812.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 69.480
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 56.731

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 858.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x205x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x205x2048): 69.929
Elapsed time for attention_prob_times_values (256x2048x2048x205): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x205): 23.985

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 493.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 71.373
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 56.833

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 877.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x207x2048): 0.0188
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x207x2048): 23.663
Elapsed time for attention_prob_times_values (256x2048x2048x207): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x207): 54.219

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 459.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 73.641
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 55.357

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 884.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x209x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x209x2048): 68.012
Elapsed time for attention_prob_times_values (256x2048x2048x209): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x209): 54.050

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 847.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 70.147
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 57.561

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 893.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x211x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x211x2048): 68.977
Elapsed time for attention_prob_times_values (256x2048x2048x211): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x211): 55.148

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 869.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 69.564
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 58.315

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 904.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x213x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x213x2048): 70.920
Elapsed time for attention_prob_times_values (256x2048x2048x213): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x213): 53.492

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 872.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 72.041
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 58.458

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 927.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x215x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x215x2048): 71.420
Elapsed time for attention_prob_times_values (256x2048x2048x215): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x215): 55.981

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 906.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 73.810
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 56.422

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 927.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x217x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x217x2048): 71.230
Elapsed time for attention_prob_times_values (256x2048x2048x217): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x217): 53.980

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 894.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 71.658
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 57.825

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 936.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x219x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x219x2048): 69.506
Elapsed time for attention_prob_times_values (256x2048x2048x219): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x219): 56.819

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 918.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 73.264
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 60.115

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 974.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x221x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x221x2048): 70.943
Elapsed time for attention_prob_times_values (256x2048x2048x221): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x221): 55.709

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 924.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 73.458
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 60.035

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 982.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x223x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x223x2048): 72.552
Elapsed time for attention_prob_times_values (256x2048x2048x223): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x223): 55.628

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 940.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 83.403
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 59.689

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1043.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x225x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x225x2048): 68.949
Elapsed time for attention_prob_times_values (256x2048x2048x225): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x225): 58.605

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 954.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 67.936
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 60.989

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 972.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x227x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x227x2048): 71.197
Elapsed time for attention_prob_times_values (256x2048x2048x227): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x227): 58.821

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 978.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 70.164
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 59.721

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 983.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x229x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x229x2048): 71.732
Elapsed time for attention_prob_times_values (256x2048x2048x229): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x229): 59.246

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 993.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 70.173
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 59.028

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 985.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x231x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x231x2048): 71.938
Elapsed time for attention_prob_times_values (256x2048x2048x231): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x231): 59.594

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1006.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 72.390
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 59.984

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1016.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x233x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x233x2048): 69.594
Elapsed time for attention_prob_times_values (256x2048x2048x233): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x233): 59.542

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 998.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 73.034
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 62.511

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1052.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x235x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x235x2048): 70.117
Elapsed time for attention_prob_times_values (256x2048x2048x235): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x235): 59.453

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1009.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 71.931
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 63.143

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1059.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x237x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x237x2048): 72.333
Elapsed time for attention_prob_times_values (256x2048x2048x237): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x237): 58.929

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1026.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 73.690
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 63.088

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1079.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x239x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x239x2048): 70.764
Elapsed time for attention_prob_times_values (256x2048x2048x239): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x239): 59.317

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1028.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 56.886
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 62.445

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 952.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x241x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x241x2048): 54.432
Elapsed time for attention_prob_times_values (256x2048x2048x241): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x241): 61.381

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 926.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 53.778
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 61.800

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 927.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x243x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x243x2048): 53.876
Elapsed time for attention_prob_times_values (256x2048x2048x243): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x243): 60.053

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 919.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 55.839
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 61.895

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 954.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x245x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x245x2048): 52.460
Elapsed time for attention_prob_times_values (256x2048x2048x245): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x245): 61.082

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 920.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 55.331
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 64.762

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 977.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x247x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x247x2048): 55.178
Elapsed time for attention_prob_times_values (256x2048x2048x247): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x247): 61.565

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 956.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 56.795
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 62.800

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 984.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x249x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x249x2048): 54.670
Elapsed time for attention_prob_times_values (256x2048x2048x249): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x249): 61.664

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 959.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 55.179
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 62.688

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 975.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x251x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x251x2048): 53.753
Elapsed time for attention_prob_times_values (256x2048x2048x251): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x251): 63.769

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 973.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 53.975
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 65.832

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 993.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x253x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x253x2048): 52.541
Elapsed time for attention_prob_times_values (256x2048x2048x253): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x253): 66.103

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 984.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 55.573
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 66.514

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1021.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x255x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x255x2048): 54.707
Elapsed time for attention_prob_times_values (256x2048x2048x255): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x255): 65.020

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1006.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 77.263
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 68.749

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1236.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x257x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x257x2048): 57.184
Elapsed time for attention_prob_times_values (256x2048x2048x257): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x257): 54.869

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 955.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 56.929
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 57.668

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 981.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x259x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x259x2048): 58.024
Elapsed time for attention_prob_times_values (256x2048x2048x259): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x259): 55.775

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 977.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 59.219
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 58.490

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1015.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x261x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x261x2048): 55.154
Elapsed time for attention_prob_times_values (256x2048x2048x261): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x261): 55.223

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 955.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 58.688
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 58.909

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1021.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x263x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x263x2048): 57.822
Elapsed time for attention_prob_times_values (256x2048x2048x263): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x263): 56.938

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1000.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 56.426
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 55.522

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 979.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x265x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x265x2048): 55.609
Elapsed time for attention_prob_times_values (256x2048x2048x265): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x265): 56.795

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 986.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 57.438
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 59.795

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1032.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x267x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x267x2048): 57.225
Elapsed time for attention_prob_times_values (256x2048x2048x267): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x267): 57.449

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1014.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 58.327
Elapsed time for attention_prob_times_values (256x2048x2048x268): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 60.565

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1054.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x269x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x269x2048): 57.589
Elapsed time for attention_prob_times_values (256x2048x2048x269): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x269): 54.433

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 996.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 56.908
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 60.642

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1049.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x271x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x271x2048): 57.827
Elapsed time for attention_prob_times_values (256x2048x2048x271): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x271): 58.038

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1039.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 59.659
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 76.789

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1208.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x273x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x273x2048): 57.136
Elapsed time for attention_prob_times_values (256x2048x2048x273): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x273): 58.369

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1043.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 57.890
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 61.036

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1077.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x275x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x275x2048): 55.923
Elapsed time for attention_prob_times_values (256x2048x2048x275): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x275): 58.843

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1042.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 58.607
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 60.318

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1084.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x277x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x277x2048): 57.838
Elapsed time for attention_prob_times_values (256x2048x2048x277): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x277): 56.958

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1051.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 58.347
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 60.276

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1089.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x279x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x279x2048): 56.486
Elapsed time for attention_prob_times_values (256x2048x2048x279): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x279): 59.989

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1072.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 56.814
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 78.302

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1218.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x281x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x281x2048): 57.394
Elapsed time for attention_prob_times_values (256x2048x2048x281): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x281): 60.205

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1090.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 57.910
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 61.947

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1114.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x283x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x283x2048): 57.547
Elapsed time for attention_prob_times_values (256x2048x2048x283): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x283): 60.642

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1103.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 56.255
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 63.707

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1120.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x285x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x285x2048): 58.007
Elapsed time for attention_prob_times_values (256x2048x2048x285): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x285): 60.929

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1118.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 58.664
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 63.555

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1151.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x287x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x287x2048): 56.648
Elapsed time for attention_prob_times_values (256x2048x2048x287): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x287): 61.295

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1115.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 79.656
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 79.877

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1515.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x289x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x289x2048): 61.916
Elapsed time for attention_prob_times_values (256x2048x2048x289): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x289): 61.563

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1176.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 58.957
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 61.458

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1150.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x291x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x291x2048): 61.845
Elapsed time for attention_prob_times_values (256x2048x2048x291): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x291): 62.027

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1188.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 63.637
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 64.955

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1237.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x293x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x293x2048): 61.878
Elapsed time for attention_prob_times_values (256x2048x2048x293): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x293): 62.629

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1202.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 61.641
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 65.026

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1226.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x295x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x295x2048): 60.413
Elapsed time for attention_prob_times_values (256x2048x2048x295): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x295): 62.899

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1197.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 61.358
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 79.846

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1353.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x297x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x297x2048): 60.439
Elapsed time for attention_prob_times_values (256x2048x2048x297): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x297): 64.523

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1220.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 60.996
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 64.461

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1230.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x299x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x299x2048): 60.672
Elapsed time for attention_prob_times_values (256x2048x2048x299): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x299): 62.398

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1211.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 61.351
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 66.760

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1262.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x301x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x301x2048): 59.948
Elapsed time for attention_prob_times_values (256x2048x2048x301): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x301): 65.717

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1242.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 60.344
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 67.165

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1263.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x303x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x303x2048): 56.193
Elapsed time for attention_prob_times_values (256x2048x2048x303): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x303): 66.101

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1211.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 62.683
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 85.400

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1445.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x305x2048): 0.0293
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x305x2048): 22.391
Elapsed time for attention_prob_times_values (256x2048x2048x305): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x305): 66.571

Attention duration (in seconds): 0.0391
Attention throughput (in TFLOP/s): 672.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0391
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 59.773
Elapsed time for attention_prob_times_values (256x2048x2048x306): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 67.820

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1278.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x307x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x307x2048): 60.865
Elapsed time for attention_prob_times_values (256x2048x2048x307): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x307): 66.902

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1286.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 61.451
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 69.648

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1322.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x309x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x309x2048): 61.279
Elapsed time for attention_prob_times_values (256x2048x2048x309): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x309): 67.408

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1304.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 61.804
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 69.163

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1330.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x311x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x311x2048): 58.990
Elapsed time for attention_prob_times_values (256x2048x2048x311): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x311): 67.487

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1286.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 63.043
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 83.870

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1475.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x313x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x313x2048): 59.049
Elapsed time for attention_prob_times_values (256x2048x2048x313): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x313): 67.632

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1296.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 60.091
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 69.124

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1326.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x315x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x315x2048): 61.118
Elapsed time for attention_prob_times_values (256x2048x2048x315): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x315): 66.087

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1313.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 62.575
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 69.125

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1363.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x317x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x317x2048): 58.866
Elapsed time for attention_prob_times_values (256x2048x2048x317): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x317): 68.579

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1318.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 61.824
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 69.071

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1362.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x319x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x319x2048): 59.960
Elapsed time for attention_prob_times_values (256x2048x2048x319): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x319): 68.868

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1342.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 80.415
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 87.348

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1758.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x321x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x321x2048): 64.331
Elapsed time for attention_prob_times_values (256x2048x2048x321): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x321): 60.075

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1308.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 64.319
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 62.892

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1343.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x323x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x323x2048): 64.397
Elapsed time for attention_prob_times_values (256x2048x2048x323): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x323): 58.325

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1296.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 64.189
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 63.291

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1354.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x325x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x325x2048): 64.289
Elapsed time for attention_prob_times_values (256x2048x2048x325): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x325): 59.079

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1312.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 64.932
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 63.189

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1369.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x327x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x327x2048): 62.383
Elapsed time for attention_prob_times_values (256x2048x2048x327): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x327): 61.158

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1324.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 66.102
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 73.948

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1500.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x329x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x329x2048): 61.755
Elapsed time for attention_prob_times_values (256x2048x2048x329): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x329): 61.161

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1325.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 63.726
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 64.297

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1384.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x331x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x331x2048): 63.368
Elapsed time for attention_prob_times_values (256x2048x2048x331): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x331): 58.864

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1323.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 63.579
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 64.927

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1397.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x333x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x333x2048): 62.198
Elapsed time for attention_prob_times_values (256x2048x2048x333): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x333): 59.596

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1327.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 64.376
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 63.574

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1399.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x335x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x335x2048): 61.858
Elapsed time for attention_prob_times_values (256x2048x2048x335): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x335): 59.882

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1334.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 62.245
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 78.688

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1529.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x337x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x337x2048): 62.749
Elapsed time for attention_prob_times_values (256x2048x2048x337): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x337): 60.584

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1360.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 61.925
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 65.122

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1404.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x339x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x339x2048): 63.261
Elapsed time for attention_prob_times_values (256x2048x2048x339): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x339): 60.931

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1377.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 64.291
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 65.764

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1446.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x341x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x341x2048): 62.419
Elapsed time for attention_prob_times_values (256x2048x2048x341): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x341): 62.414

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1392.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 62.039
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 65.094

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1421.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x343x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x343x2048): 63.791
Elapsed time for attention_prob_times_values (256x2048x2048x343): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x343): 62.909

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1421.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 65.655
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 80.226

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1624.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x345x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x345x2048): 61.220
Elapsed time for attention_prob_times_values (256x2048x2048x345): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x345): 61.486

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1384.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 62.063
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 64.020

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1425.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x347x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x347x2048): 60.735
Elapsed time for attention_prob_times_values (256x2048x2048x347): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x347): 63.838

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1412.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 64.446
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 64.964

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1472.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x349x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x349x2048): 63.182
Elapsed time for attention_prob_times_values (256x2048x2048x349): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x349): 59.211

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1394.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 63.701
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 64.827

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1469.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x351x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x351x2048): 63.345
Elapsed time for attention_prob_times_values (256x2048x2048x351): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x351): 62.927

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1448.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 79.862
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 78.127

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1816.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x353x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x353x2048): 66.249
Elapsed time for attention_prob_times_values (256x2048x2048x353): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x353): 61.297

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1468.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 66.770
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 65.382

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1527.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x355x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x355x2048): 65.929
Elapsed time for attention_prob_times_values (256x2048x2048x355): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x355): 65.246

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1520.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 67.655
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 64.800

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1539.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x357x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x357x2048): 65.850
Elapsed time for attention_prob_times_values (256x2048x2048x357): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x357): 63.748

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1510.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 66.601
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 64.751

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1534.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x359x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x359x2048): 65.485
Elapsed time for attention_prob_times_values (256x2048x2048x359): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x359): 66.239

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1543.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 67.321
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 83.001

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1747.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x361x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x361x2048): 64.058
Elapsed time for attention_prob_times_values (256x2048x2048x361): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x361): 63.812

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1506.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 65.317
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 64.598

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1534.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x363x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x363x2048): 63.681
Elapsed time for attention_prob_times_values (256x2048x2048x363): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x363): 59.452

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1456.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 64.146
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 67.210

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 1558.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x365x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x365x2048): 65.081
Elapsed time for attention_prob_times_values (256x2048x2048x365): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x365): 64.466

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1542.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 63.202
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 67.226

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1555.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x367x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x367x2048): 65.227
Elapsed time for attention_prob_times_values (256x2048x2048x367): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x367): 64.996

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1558.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 67.162
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 82.526

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1777.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x369x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x369x2048): 64.327
Elapsed time for attention_prob_times_values (256x2048x2048x369): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x369): 65.120

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1557.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 65.038
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 67.745

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1601.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x371x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x371x2048): 64.680
Elapsed time for attention_prob_times_values (256x2048x2048x371): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x371): 66.812

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1589.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 65.343
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 68.459

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1621.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x373x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x373x2048): 64.494
Elapsed time for attention_prob_times_values (256x2048x2048x373): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x373): 66.033

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1586.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 64.765
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 66.690

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1601.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x375x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x375x2048): 65.276
Elapsed time for attention_prob_times_values (256x2048x2048x375): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x375): 64.204

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1581.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 66.572
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 87.084

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1848.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x377x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x377x2048): 64.621
Elapsed time for attention_prob_times_values (256x2048x2048x377): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x377): 66.250

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1607.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 64.065
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 69.047

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1636.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x379x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x379x2048): 61.425
Elapsed time for attention_prob_times_values (256x2048x2048x379): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x379): 65.398

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1563.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 63.087
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 69.250

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1634.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x381x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x381x2048): 61.743
Elapsed time for attention_prob_times_values (256x2048x2048x381): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x381): 66.814

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1592.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 62.818
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 69.731

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1644.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x383x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x383x2048): 63.361
Elapsed time for attention_prob_times_values (256x2048x2048x383): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x383): 66.551

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1618.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x384x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x384x2048): 77.769
Elapsed time for attention_prob_times_values (256x2048x2048x384): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x384): 85.189

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2032.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x385x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x385x2048): 67.234
Elapsed time for attention_prob_times_values (256x2048x2048x385): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x385): 57.904

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1559.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x386x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x386x2048): 68.277
Elapsed time for attention_prob_times_values (256x2048x2048x386): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x386): 60.783

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1615.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x387x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x387x2048): 67.231
Elapsed time for attention_prob_times_values (256x2048x2048x387): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x387): 60.219

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1600.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x388x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x388x2048): 68.782
Elapsed time for attention_prob_times_values (256x2048x2048x388): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x388): 62.642

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1655.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x389x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x389x2048): 64.667
Elapsed time for attention_prob_times_values (256x2048x2048x389): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x389): 60.789

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1586.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x390x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x390x2048): 68.255
Elapsed time for attention_prob_times_values (256x2048x2048x390): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x390): 63.852

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1674.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x391x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x391x2048): 67.256
Elapsed time for attention_prob_times_values (256x2048x2048x391): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x391): 60.955

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1626.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x392x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x392x2048): 68.850
Elapsed time for attention_prob_times_values (256x2048x2048x392): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x392): 77.971

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1864.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x393x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x393x2048): 66.010
Elapsed time for attention_prob_times_values (256x2048x2048x393): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x393): 59.894

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1605.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x394x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x394x2048): 64.229
Elapsed time for attention_prob_times_values (256x2048x2048x394): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x394): 64.238

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1645.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x395x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x395x2048): 66.579
Elapsed time for attention_prob_times_values (256x2048x2048x395): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x395): 59.063

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 1607.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x396x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x396x2048): 65.905
Elapsed time for attention_prob_times_values (256x2048x2048x396): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x396): 65.297

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1689.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x397x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x397x2048): 64.698
Elapsed time for attention_prob_times_values (256x2048x2048x397): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x397): 60.617

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 1615.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x398x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x398x2048): 66.689
Elapsed time for attention_prob_times_values (256x2048x2048x398): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x398): 65.223

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1706.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x399x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x399x2048): 62.991
Elapsed time for attention_prob_times_values (256x2048x2048x399): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x399): 61.532

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1614.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x400x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x400x2048): 69.033
Elapsed time for attention_prob_times_values (256x2048x2048x400): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x400): 79.862

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1925.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x401x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x401x2048): 65.630
Elapsed time for attention_prob_times_values (256x2048x2048x401): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x401): 62.032

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 1662.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x402x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x402x2048): 67.103
Elapsed time for attention_prob_times_values (256x2048x2048x402): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x402): 64.917

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1724.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x403x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x403x2048): 66.308
Elapsed time for attention_prob_times_values (256x2048x2048x403): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x403): 61.437

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 1670.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 67.747
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 66.039

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1755.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x405x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x405x2048): 65.710
Elapsed time for attention_prob_times_values (256x2048x2048x405): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x405): 62.494

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 1685.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 67.628
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 65.973

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1761.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x407x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x407x2048): 67.026
Elapsed time for attention_prob_times_values (256x2048x2048x407): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x407): 62.835

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1714.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 68.389
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 78.900

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1941.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x409x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x409x2048): 65.072
Elapsed time for attention_prob_times_values (256x2048x2048x409): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x409): 63.094

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1701.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 65.939
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 64.720

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 1739.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x411x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x411x2048): 65.156
Elapsed time for attention_prob_times_values (256x2048x2048x411): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x411): 61.586

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1689.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 65.704
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 67.253

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1778.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x413x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x413x2048): 64.722
Elapsed time for attention_prob_times_values (256x2048x2048x413): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x413): 61.445

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1690.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 67.347
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 65.550

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1785.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x415x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x415x2048): 66.391
Elapsed time for attention_prob_times_values (256x2048x2048x415): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x415): 63.843

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1753.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 85.511
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 82.074

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2261.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x417x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x417x2048): 69.344
Elapsed time for attention_prob_times_values (256x2048x2048x417): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x417): 64.409

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1807.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 70.812
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 67.553

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1875.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x419x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x419x2048): 69.929
Elapsed time for attention_prob_times_values (256x2048x2048x419): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x419): 64.686

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1827.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 71.360
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 68.302

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1901.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x421x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x421x2048): 69.789
Elapsed time for attention_prob_times_values (256x2048x2048x421): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x421): 63.967

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 1823.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 70.468
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 68.272

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1898.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x423x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x423x2048): 69.448
Elapsed time for attention_prob_times_values (256x2048x2048x423): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x423): 65.310

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 1846.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 71.442
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 84.079

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2124.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x425x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x425x2048): 68.246
Elapsed time for attention_prob_times_values (256x2048x2048x425): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x425): 65.336

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1840.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 68.166
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 68.326

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1885.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x427x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x427x2048): 68.682
Elapsed time for attention_prob_times_values (256x2048x2048x427): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x427): 65.578

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1857.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 70.332
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 69.382

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1938.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x429x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x429x2048): 69.103
Elapsed time for attention_prob_times_values (256x2048x2048x429): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x429): 62.582

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1826.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 69.055
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 69.485

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1930.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x431x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x431x2048): 69.121
Elapsed time for attention_prob_times_values (256x2048x2048x431): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x431): 64.565

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 1865.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 68.975
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 85.613

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2139.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x433x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x433x2048): 68.291
Elapsed time for attention_prob_times_values (256x2048x2048x433): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x433): 65.585

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1877.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 69.165
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 70.001

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 1956.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x435x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x435x2048): 66.997
Elapsed time for attention_prob_times_values (256x2048x2048x435): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x435): 66.793

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1885.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 70.245
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 68.923

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1965.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x437x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x437x2048): 66.203
Elapsed time for attention_prob_times_values (256x2048x2048x437): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x437): 67.164

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 1887.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 69.556
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 67.494

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1943.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x439x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x439x2048): 69.035
Elapsed time for attention_prob_times_values (256x2048x2048x439): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x439): 67.496

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1941.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 70.903
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 86.738

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2223.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x441x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x441x2048): 68.260
Elapsed time for attention_prob_times_values (256x2048x2048x441): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x441): 64.980

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1901.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 66.866
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 69.740

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1954.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x443x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x443x2048): 68.479
Elapsed time for attention_prob_times_values (256x2048x2048x443): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x443): 67.956

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1956.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 70.025
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 71.187

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2029.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x445x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x445x2048): 68.655
Elapsed time for attention_prob_times_values (256x2048x2048x445): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x445): 68.308

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 1973.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 67.211
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 71.562

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2001.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x447x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x447x2048): 68.756
Elapsed time for attention_prob_times_values (256x2048x2048x447): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x447): 66.180

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 1951.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 84.041
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 83.464

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2428.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x449x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x449x2048): 71.657
Elapsed time for attention_prob_times_values (256x2048x2048x449): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x449): 60.377

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 1904.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 72.736
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 65.545

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 2008.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x451x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x451x2048): 70.201
Elapsed time for attention_prob_times_values (256x2048x2048x451): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x451): 62.805

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1935.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 71.304
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 63.867

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 1970.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x453x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x453x2048): 71.200
Elapsed time for attention_prob_times_values (256x2048x2048x453): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x453): 62.480

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1950.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 71.699
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 65.712

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 2014.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x455x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x455x2048): 71.233
Elapsed time for attention_prob_times_values (256x2048x2048x455): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x455): 62.969

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 1967.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 73.460
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 85.430

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2330.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x457x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x457x2048): 69.655
Elapsed time for attention_prob_times_values (256x2048x2048x457): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x457): 61.516

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1931.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 69.148
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 66.305

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2005.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x459x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x459x2048): 69.736
Elapsed time for attention_prob_times_values (256x2048x2048x459): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x459): 63.506

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1973.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 72.119
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 66.200

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 2053.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x461x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x461x2048): 70.540
Elapsed time for attention_prob_times_values (256x2048x2048x461): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x461): 63.962

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 2000.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 68.458
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 67.145

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 2025.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x463x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x463x2048): 68.383
Elapsed time for attention_prob_times_values (256x2048x2048x463): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x463): 62.410

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1953.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 70.609
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 87.177

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2340.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x465x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x465x2048): 69.773
Elapsed time for attention_prob_times_values (256x2048x2048x465): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x465): 61.797

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 1970.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0208
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 48.021
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 67.612

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 1691.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0356
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x467x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x467x2048): 67.873
Elapsed time for attention_prob_times_values (256x2048x2048x467): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x467): 64.514

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1996.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 71.667
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 67.980

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 2110.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x469x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x469x2048): 69.512
Elapsed time for attention_prob_times_values (256x2048x2048x469): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x469): 64.666

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 2030.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 70.863
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 67.869

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 2106.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x471x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x471x2048): 70.492
Elapsed time for attention_prob_times_values (256x2048x2048x471): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x471): 64.752

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2054.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 71.879
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 85.105

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2376.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x473x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x473x2048): 67.394
Elapsed time for attention_prob_times_values (256x2048x2048x473): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x473): 64.886

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2020.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 66.450
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 68.081

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 2059.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x475x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x475x2048): 69.828
Elapsed time for attention_prob_times_values (256x2048x2048x475): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x475): 63.632

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2043.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 70.673
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 69.070

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 2148.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x477x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x477x2048): 68.701
Elapsed time for attention_prob_times_values (256x2048x2048x477): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x477): 64.132

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2044.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 70.622
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 69.066

Attention duration (in seconds): 0.0294
Attention throughput (in TFLOP/s): 2156.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x479x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x479x2048): 68.921
Elapsed time for attention_prob_times_values (256x2048x2048x479): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x479): 65.677

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2080.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 87.859
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 88.423

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2732.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x481x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x481x2048): 70.822
Elapsed time for attention_prob_times_values (256x2048x2048x481): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x481): 66.150

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 2124.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 72.076
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 69.538

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 2203.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x483x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x483x2048): 70.736
Elapsed time for attention_prob_times_values (256x2048x2048x483): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x483): 63.270

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2083.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 69.833
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 68.362

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 2159.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x485x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x485x2048): 70.916
Elapsed time for attention_prob_times_values (256x2048x2048x485): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x485): 66.470

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 2148.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x486x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x486x2048): 73.007
Elapsed time for attention_prob_times_values (256x2048x2048x486): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x486): 69.886

Attention duration (in seconds): 0.0292
Attention throughput (in TFLOP/s): 2240.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0292
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x487x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x487x2048): 70.465
Elapsed time for attention_prob_times_values (256x2048x2048x487): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x487): 66.849

Attention duration (in seconds): 0.0305
Attention throughput (in TFLOP/s): 2156.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0305
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x488x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x488x2048): 73.423
Elapsed time for attention_prob_times_values (256x2048x2048x488): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x488): 91.108

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2561.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x489x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x489x2048): 70.208
Elapsed time for attention_prob_times_values (256x2048x2048x489): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x489): 66.789

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2160.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x490x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x490x2048): 71.495
Elapsed time for attention_prob_times_values (256x2048x2048x490): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x490): 70.295

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 2241.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x491x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x491x2048): 71.302
Elapsed time for attention_prob_times_values (256x2048x2048x491): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x491): 66.805

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2185.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x492x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x492x2048): 72.470
Elapsed time for attention_prob_times_values (256x2048x2048x492): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x492): 68.519

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2236.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x493x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x493x2048): 71.142
Elapsed time for attention_prob_times_values (256x2048x2048x493): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x493): 67.145

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 2197.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x494x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x494x2048): 70.623
Elapsed time for attention_prob_times_values (256x2048x2048x494): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x494): 69.738

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 2236.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x495x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x495x2048): 71.481
Elapsed time for attention_prob_times_values (256x2048x2048x495): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x495): 65.183

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 2177.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0312
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x496x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x496x2048): 73.489
Elapsed time for attention_prob_times_values (256x2048x2048x496): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x496): 92.831

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2625.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x497x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x497x2048): 67.559
Elapsed time for attention_prob_times_values (256x2048x2048x497): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x497): 66.450

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 2148.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x498x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x498x2048): 71.788
Elapsed time for attention_prob_times_values (256x2048x2048x498): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x498): 71.089

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 2294.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x499x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x499x2048): 70.794
Elapsed time for attention_prob_times_values (256x2048x2048x499): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x499): 67.909

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 2231.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x500x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x500x2048): 72.125
Elapsed time for attention_prob_times_values (256x2048x2048x500): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x500): 71.460

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 2315.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x501x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x501x2048): 68.017
Elapsed time for attention_prob_times_values (256x2048x2048x501): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x501): 68.069

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 2198.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x502x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x502x2048): 69.595
Elapsed time for attention_prob_times_values (256x2048x2048x502): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x502): 70.854

Attention duration (in seconds): 0.0307
Attention throughput (in TFLOP/s): 2273.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0307
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x503x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x503x2048): 71.255
Elapsed time for attention_prob_times_values (256x2048x2048x503): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x503): 67.644

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2251.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x504x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x504x2048): 70.013
Elapsed time for attention_prob_times_values (256x2048x2048x504): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x504): 93.959

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2607.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x505x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x505x2048): 70.528
Elapsed time for attention_prob_times_values (256x2048x2048x505): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x505): 61.163

Attention duration (in seconds): 0.0331
Attention throughput (in TFLOP/s): 2133.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0331
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x506x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x506x2048): 70.904
Elapsed time for attention_prob_times_values (256x2048x2048x506): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x506): 69.460

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 2289.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x507x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x507x2048): 70.489
Elapsed time for attention_prob_times_values (256x2048x2048x507): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x507): 61.552

Attention duration (in seconds): 0.0331
Attention throughput (in TFLOP/s): 2148.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0331
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x508x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x508x2048): 70.114
Elapsed time for attention_prob_times_values (256x2048x2048x508): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x508): 68.758

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 2273.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x509x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x509x2048): 69.758
Elapsed time for attention_prob_times_values (256x2048x2048x509): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x509): 61.322

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 2141.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x510x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x510x2048): 71.377
Elapsed time for attention_prob_times_values (256x2048x2048x510): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x510): 69.690

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 2318.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x511x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x511x2048): 70.671
Elapsed time for attention_prob_times_values (256x2048x2048x511): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x511): 65.400

Attention duration (in seconds): 0.0323
Attention throughput (in TFLOP/s): 2237.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0323
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x1x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x1x2048): 1.044
Elapsed time for attention_prob_times_values (320x2048x2048x1): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x1): 0.216

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 0.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x2x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x2x2048): 1.939
Elapsed time for attention_prob_times_values (320x2048x2048x2): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x2): 1.416

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 1.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x3x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x3x2048): 2.666
Elapsed time for attention_prob_times_values (320x2048x2048x3): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x3): 2.013

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 2.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x4x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x4x2048): 3.420
Elapsed time for attention_prob_times_values (320x2048x2048x4): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x4): 2.913

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 4.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x5x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x5x2048): 4.252
Elapsed time for attention_prob_times_values (320x2048x2048x5): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x5): 3.604

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 5.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x6x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x6x2048): 5.086
Elapsed time for attention_prob_times_values (320x2048x2048x6): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x6): 4.552

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 7.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x7x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x7x2048): 5.944
Elapsed time for attention_prob_times_values (320x2048x2048x7): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x7): 5.249

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 8.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x8x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x8x2048): 6.193
Elapsed time for attention_prob_times_values (320x2048x2048x8): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x8): 7.210

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 10.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x9x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x9x2048): 7.213
Elapsed time for attention_prob_times_values (320x2048x2048x9): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x9): 7.669

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 12.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x10x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x10x2048): 7.961
Elapsed time for attention_prob_times_values (320x2048x2048x10): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x10): 8.095

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 14.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x11x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x11x2048): 8.741
Elapsed time for attention_prob_times_values (320x2048x2048x11): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x11): 8.822

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 16.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x12x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x12x2048): 9.509
Elapsed time for attention_prob_times_values (320x2048x2048x12): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x12): 9.878

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 18.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x13x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x13x2048): 10.281
Elapsed time for attention_prob_times_values (320x2048x2048x13): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x13): 10.382

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 20.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x14x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x14x2048): 11.062
Elapsed time for attention_prob_times_values (320x2048x2048x14): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x14): 12.290

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 24.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x15x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x15x2048): 11.839
Elapsed time for attention_prob_times_values (320x2048x2048x15): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x15): 12.683

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 26.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x16x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x16x2048): 12.605
Elapsed time for attention_prob_times_values (320x2048x2048x16): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x16): 13.262

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 29.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x17x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x17x2048): 12.940
Elapsed time for attention_prob_times_values (320x2048x2048x17): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x17): 14.293

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 31.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x18x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x18x2048): 13.713
Elapsed time for attention_prob_times_values (320x2048x2048x18): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x18): 15.526

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 35.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x19x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x19x2048): 14.452
Elapsed time for attention_prob_times_values (320x2048x2048x19): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x19): 15.866

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 37.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x20x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x20x2048): 15.218
Elapsed time for attention_prob_times_values (320x2048x2048x20): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x20): 17.029

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 41.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x21x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x21x2048): 15.958
Elapsed time for attention_prob_times_values (320x2048x2048x21): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x21): 16.198

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 42.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x22x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x22x2048): 16.708
Elapsed time for attention_prob_times_values (320x2048x2048x22): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x22): 18.609

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 47.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x23x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x23x2048): 17.461
Elapsed time for attention_prob_times_values (320x2048x2048x23): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x23): 18.902

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 50.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x24x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x24x2048): 18.188
Elapsed time for attention_prob_times_values (320x2048x2048x24): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x24): 20.462

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 55.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x25x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x25x2048): 18.455
Elapsed time for attention_prob_times_values (320x2048x2048x25): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x25): 20.492

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 57.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x26x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x26x2048): 17.911
Elapsed time for attention_prob_times_values (320x2048x2048x26): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x26): 21.461

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 59.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x27x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x27x2048): 19.849
Elapsed time for attention_prob_times_values (320x2048x2048x27): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x27): 21.966

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 64.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x28x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x28x2048): 20.540
Elapsed time for attention_prob_times_values (320x2048x2048x28): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x28): 23.094

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 69.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x29x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x29x2048): 21.183
Elapsed time for attention_prob_times_values (320x2048x2048x29): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x29): 23.342

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 72.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x30x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x30x2048): 21.967
Elapsed time for attention_prob_times_values (320x2048x2048x30): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x30): 22.560

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 74.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x31x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x31x2048): 5.098
Elapsed time for attention_prob_times_values (320x2048x2048x31): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x31): 24.845

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 28.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x32x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x32x2048): 40.854
Elapsed time for attention_prob_times_values (320x2048x2048x32): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x32): 26.649

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 112.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x33x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x33x2048): 26.877
Elapsed time for attention_prob_times_values (320x2048x2048x33): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x33): 26.362

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 95.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x34x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x34x2048): 27.244
Elapsed time for attention_prob_times_values (320x2048x2048x34): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x34): 27.404

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 99.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x35x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x35x2048): 26.220
Elapsed time for attention_prob_times_values (320x2048x2048x35): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x35): 27.948

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 101.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x36x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x36x2048): 26.889
Elapsed time for attention_prob_times_values (320x2048x2048x36): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x36): 29.141

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 106.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x37x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x37x2048): 25.340
Elapsed time for attention_prob_times_values (320x2048x2048x37): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x37): 29.386

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 105.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x38x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x38x2048): 27.649
Elapsed time for attention_prob_times_values (320x2048x2048x38): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x38): 30.541

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 115.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x39x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x39x2048): 26.129
Elapsed time for attention_prob_times_values (320x2048x2048x39): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x39): 30.853

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 114.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x40x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x40x2048): 28.836
Elapsed time for attention_prob_times_values (320x2048x2048x40): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x40): 32.571

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 126.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x41x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x41x2048): 28.298
Elapsed time for attention_prob_times_values (320x2048x2048x41): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x41): 32.166

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 126.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x42x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x42x2048): 29.174
Elapsed time for attention_prob_times_values (320x2048x2048x42): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x42): 33.108

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 132.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x43x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x43x2048): 29.618
Elapsed time for attention_prob_times_values (320x2048x2048x43): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x43): 33.313

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 136.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x44x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x44x2048): 30.465
Elapsed time for attention_prob_times_values (320x2048x2048x44): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x44): 34.684

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 143.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x45x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x45x2048): 30.899
Elapsed time for attention_prob_times_values (320x2048x2048x45): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x45): 34.429

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 147.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x46x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x46x2048): 31.836
Elapsed time for attention_prob_times_values (320x2048x2048x46): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x46): 36.046

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 155.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x47x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x47x2048): 32.280
Elapsed time for attention_prob_times_values (320x2048x2048x47): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x47): 36.161

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 159.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x48x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x48x2048): 33.354
Elapsed time for attention_prob_times_values (320x2048x2048x48): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x48): 38.660

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 170.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x49x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x49x2048): 33.391
Elapsed time for attention_prob_times_values (320x2048x2048x49): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x49): 37.418

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 170.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x50x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x50x2048): 34.048
Elapsed time for attention_prob_times_values (320x2048x2048x50): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x50): 38.571

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 177.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x51x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x51x2048): 33.487
Elapsed time for attention_prob_times_values (320x2048x2048x51): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x51): 38.571

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 178.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x52x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x52x2048): 35.554
Elapsed time for attention_prob_times_values (320x2048x2048x52): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x52): 40.324

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 191.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x53x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x53x2048): 34.234
Elapsed time for attention_prob_times_values (320x2048x2048x53): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x53): 40.119

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 189.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x54x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x54x2048): 35.420
Elapsed time for attention_prob_times_values (320x2048x2048x54): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x54): 41.649

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 199.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x55x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x55x2048): 37.523
Elapsed time for attention_prob_times_values (320x2048x2048x55): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x55): 41.327

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 208.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x56x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x56x2048): 38.294
Elapsed time for attention_prob_times_values (320x2048x2048x56): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x56): 44.568

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 221.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x57x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x57x2048): 37.753
Elapsed time for attention_prob_times_values (320x2048x2048x57): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x57): 43.141

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 219.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x58x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x58x2048): 38.458
Elapsed time for attention_prob_times_values (320x2048x2048x58): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x58): 44.487

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 228.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x59x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x59x2048): 38.876
Elapsed time for attention_prob_times_values (320x2048x2048x59): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x59): 44.027

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 231.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x60x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x60x2048): 39.478
Elapsed time for attention_prob_times_values (320x2048x2048x60): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x60): 46.099

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 241.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x61x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x61x2048): 40.143
Elapsed time for attention_prob_times_values (320x2048x2048x61): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x61): 45.468

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 245.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x62x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x62x2048): 40.877
Elapsed time for attention_prob_times_values (320x2048x2048x62): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x62): 47.342

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 256.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x63x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x63x2048): 41.328
Elapsed time for attention_prob_times_values (320x2048x2048x63): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x63): 46.841

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 260.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x64x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x64x2048): 57.646
Elapsed time for attention_prob_times_values (320x2048x2048x64): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x64): 50.795

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 324.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x65x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x65x2048): 44.174
Elapsed time for attention_prob_times_values (320x2048x2048x65): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x65): 34.042

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 233.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x66x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x66x2048): 44.892
Elapsed time for attention_prob_times_values (320x2048x2048x66): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x66): 35.325

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 243.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x67x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x67x2048): 44.442
Elapsed time for attention_prob_times_values (320x2048x2048x67): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x67): 34.840

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 243.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x68x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x68x2048): 45.329
Elapsed time for attention_prob_times_values (320x2048x2048x68): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x68): 36.165

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 253.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x69x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x69x2048): 45.026
Elapsed time for attention_prob_times_values (320x2048x2048x69): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x69): 35.030

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 251.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x70x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x70x2048): 46.125
Elapsed time for attention_prob_times_values (320x2048x2048x70): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x70): 35.749

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 260.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x71x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x71x2048): 45.760
Elapsed time for attention_prob_times_values (320x2048x2048x71): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x71): 36.842

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 267.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x72x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x72x2048): 47.210
Elapsed time for attention_prob_times_values (320x2048x2048x72): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x72): 35.359

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 267.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x73x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x73x2048): 44.151
Elapsed time for attention_prob_times_values (320x2048x2048x73): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x73): 37.463

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 271.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x74x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x74x2048): 46.957
Elapsed time for attention_prob_times_values (320x2048x2048x74): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x74): 39.039

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 289.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x75x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x75x2048): 46.705
Elapsed time for attention_prob_times_values (320x2048x2048x75): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x75): 38.039

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 287.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x76x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x76x2048): 47.911
Elapsed time for attention_prob_times_values (320x2048x2048x76): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x76): 40.167

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 303.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x77x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x77x2048): 47.585
Elapsed time for attention_prob_times_values (320x2048x2048x77): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x77): 39.262

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 301.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x78x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x78x2048): 48.775
Elapsed time for attention_prob_times_values (320x2048x2048x78): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x78): 40.953

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 315.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x79x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x79x2048): 48.604
Elapsed time for attention_prob_times_values (320x2048x2048x79): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x79): 40.037

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 314.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x80x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x80x2048): 50.202
Elapsed time for attention_prob_times_values (320x2048x2048x80): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x80): 38.726

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 316.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x81x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x81x2048): 47.506
Elapsed time for attention_prob_times_values (320x2048x2048x81): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x81): 40.940

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 322.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x82x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x82x2048): 48.332
Elapsed time for attention_prob_times_values (320x2048x2048x82): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x82): 42.705

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 335.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x83x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x83x2048): 48.344
Elapsed time for attention_prob_times_values (320x2048x2048x83): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x83): 41.719

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 335.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x84x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x84x2048): 47.595
Elapsed time for attention_prob_times_values (320x2048x2048x84): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x84): 43.650

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 344.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x85x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x85x2048): 49.186
Elapsed time for attention_prob_times_values (320x2048x2048x85): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x85): 40.949

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 341.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x86x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x86x2048): 49.612
Elapsed time for attention_prob_times_values (320x2048x2048x86): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x86): 44.465

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 361.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x87x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x87x2048): 50.040
Elapsed time for attention_prob_times_values (320x2048x2048x87): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x87): 43.485

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 362.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x88x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x88x2048): 50.865
Elapsed time for attention_prob_times_values (320x2048x2048x88): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x88): 43.406

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 368.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x89x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x89x2048): 49.238
Elapsed time for attention_prob_times_values (320x2048x2048x89): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x89): 44.362

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 371.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x90x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x90x2048): 50.057
Elapsed time for attention_prob_times_values (320x2048x2048x90): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x90): 44.724

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 379.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x91x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x91x2048): 50.123
Elapsed time for attention_prob_times_values (320x2048x2048x91): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x91): 45.135

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 385.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x92x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x92x2048): 50.966
Elapsed time for attention_prob_times_values (320x2048x2048x92): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x92): 46.981

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 400.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x93x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x93x2048): 49.606
Elapsed time for attention_prob_times_values (320x2048x2048x93): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x93): 45.883

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 394.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x94x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x94x2048): 51.821
Elapsed time for attention_prob_times_values (320x2048x2048x94): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x94): 47.558

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 413.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x95x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x95x2048): 51.857
Elapsed time for attention_prob_times_values (320x2048x2048x95): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x95): 46.642

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 413.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x96x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x96x2048): 67.762
Elapsed time for attention_prob_times_values (320x2048x2048x96): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x96): 48.367

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 479.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x97x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x97x2048): 53.150
Elapsed time for attention_prob_times_values (320x2048x2048x97): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x97): 47.660

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 431.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x98x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x98x2048): 54.178
Elapsed time for attention_prob_times_values (320x2048x2048x98): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x98): 49.426

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 447.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x99x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x99x2048): 53.684
Elapsed time for attention_prob_times_values (320x2048x2048x99): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x99): 48.490

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 445.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x100x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x100x2048): 54.981
Elapsed time for attention_prob_times_values (320x2048x2048x100): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x100): 50.423

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 463.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x101x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x101x2048): 52.252
Elapsed time for attention_prob_times_values (320x2048x2048x101): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x101): 47.588

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 442.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x102x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x102x2048): 55.218
Elapsed time for attention_prob_times_values (320x2048x2048x102): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x102): 48.420

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 462.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x103x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x103x2048): 54.725
Elapsed time for attention_prob_times_values (320x2048x2048x103): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x103): 50.248

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 473.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x104x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x104x2048): 54.712
Elapsed time for attention_prob_times_values (320x2048x2048x104): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x104): 50.683

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 480.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x105x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x105x2048): 53.004
Elapsed time for attention_prob_times_values (320x2048x2048x105): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x105): 49.371

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 470.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x106x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x106x2048): 55.440
Elapsed time for attention_prob_times_values (320x2048x2048x106): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x106): 52.652

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 501.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x107x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x107x2048): 54.937
Elapsed time for attention_prob_times_values (320x2048x2048x107): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x107): 51.380

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 496.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x108x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x108x2048): 56.326
Elapsed time for attention_prob_times_values (320x2048x2048x108): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x108): 53.647

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 518.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x109x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x109x2048): 55.637
Elapsed time for attention_prob_times_values (320x2048x2048x109): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x109): 50.539

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 503.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x110x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x110x2048): 56.754
Elapsed time for attention_prob_times_values (320x2048x2048x110): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x110): 52.465

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 523.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x111x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x111x2048): 56.275
Elapsed time for attention_prob_times_values (320x2048x2048x111): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x111): 52.699

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 526.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x112x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x112x2048): 56.535
Elapsed time for attention_prob_times_values (320x2048x2048x112): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x112): 54.765

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 542.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x113x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x113x2048): 56.198
Elapsed time for attention_prob_times_values (320x2048x2048x113): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x113): 53.644

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 539.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x114x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x114x2048): 57.490
Elapsed time for attention_prob_times_values (320x2048x2048x114): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x114): 55.820

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 561.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x115x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x115x2048): 55.401
Elapsed time for attention_prob_times_values (320x2048x2048x115): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x115): 54.100

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 546.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x116x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x116x2048): 58.285
Elapsed time for attention_prob_times_values (320x2048x2048x116): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x116): 56.551

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 577.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x117x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x117x2048): 57.632
Elapsed time for attention_prob_times_values (320x2048x2048x117): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x117): 53.130

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 560.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x118x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x118x2048): 56.847
Elapsed time for attention_prob_times_values (320x2048x2048x118): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x118): 57.130

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 582.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x119x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x119x2048): 58.316
Elapsed time for attention_prob_times_values (320x2048x2048x119): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x119): 55.484

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 585.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x120x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x120x2048): 58.745
Elapsed time for attention_prob_times_values (320x2048x2048x120): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x120): 56.328

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 596.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x121x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x121x2048): 58.175
Elapsed time for attention_prob_times_values (320x2048x2048x121): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x121): 42.960

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 516.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x122x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x122x2048): 59.079
Elapsed time for attention_prob_times_values (320x2048x2048x122): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x122): 58.663

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 619.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x123x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x123x2048): 57.717
Elapsed time for attention_prob_times_values (320x2048x2048x123): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x123): 44.887

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 535.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x124x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x124x2048): 60.126
Elapsed time for attention_prob_times_values (320x2048x2048x124): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x124): 57.662

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 629.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x125x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x125x2048): 58.928
Elapsed time for attention_prob_times_values (320x2048x2048x125): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x125): 43.403

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 538.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x126x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x126x2048): 59.919
Elapsed time for attention_prob_times_values (320x2048x2048x126): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x126): 56.540

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 630.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x127x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x127x2048): 58.473
Elapsed time for attention_prob_times_values (320x2048x2048x127): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x127): 43.532

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 545.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x128x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x128x2048): 69.725
Elapsed time for attention_prob_times_values (320x2048x2048x128): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x128): 63.495

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 731.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x129x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x129x2048): 57.197
Elapsed time for attention_prob_times_values (320x2048x2048x129): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x129): 42.862

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 542.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x130x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x130x2048): 59.119
Elapsed time for attention_prob_times_values (320x2048x2048x130): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x130): 47.978

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 590.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x131x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x131x2048): 59.251
Elapsed time for attention_prob_times_values (320x2048x2048x131): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x131): 46.341

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 584.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x132x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x132x2048): 60.889
Elapsed time for attention_prob_times_values (320x2048x2048x132): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x132): 48.877

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 613.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x133x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x133x2048): 59.522
Elapsed time for attention_prob_times_values (320x2048x2048x133): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x133): 47.013

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 598.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x134x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x134x2048): 60.948
Elapsed time for attention_prob_times_values (320x2048x2048x134): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x134): 49.306

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 625.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x135x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x135x2048): 59.383
Elapsed time for attention_prob_times_values (320x2048x2048x135): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x135): 46.926

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 605.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x136x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x136x2048): 60.406
Elapsed time for attention_prob_times_values (320x2048x2048x136): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x136): 46.442

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 610.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x137x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x137x2048): 59.624
Elapsed time for attention_prob_times_values (320x2048x2048x137): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x137): 47.865

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 621.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x138x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x138x2048): 61.265
Elapsed time for attention_prob_times_values (320x2048x2048x138): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x138): 50.657

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 653.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x139x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x139x2048): 60.619
Elapsed time for attention_prob_times_values (320x2048x2048x139): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x139): 48.532

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 639.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x140x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x140x2048): 59.818
Elapsed time for attention_prob_times_values (320x2048x2048x140): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x140): 51.524

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 660.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x141x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x141x2048): 61.508
Elapsed time for attention_prob_times_values (320x2048x2048x141): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x141): 46.600

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 637.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x142x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x142x2048): 62.572
Elapsed time for attention_prob_times_values (320x2048x2048x142): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x142): 50.013

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 672.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x143x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x143x2048): 61.936
Elapsed time for attention_prob_times_values (320x2048x2048x143): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x143): 49.904

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 672.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x144x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x144x2048): 64.396
Elapsed time for attention_prob_times_values (320x2048x2048x144): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x144): 49.944

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 689.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x145x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x145x2048): 61.671
Elapsed time for attention_prob_times_values (320x2048x2048x145): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x145): 50.528

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 684.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x146x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x146x2048): 63.066
Elapsed time for attention_prob_times_values (320x2048x2048x146): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x146): 53.182

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 715.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x147x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x147x2048): 62.380
Elapsed time for attention_prob_times_values (320x2048x2048x147): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x147): 50.971

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 700.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x148x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x148x2048): 61.695
Elapsed time for attention_prob_times_values (320x2048x2048x148): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x148): 52.104

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 709.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x149x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x149x2048): 61.123
Elapsed time for attention_prob_times_values (320x2048x2048x149): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x149): 49.229

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 689.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x150x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x150x2048): 62.301
Elapsed time for attention_prob_times_values (320x2048x2048x150): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x150): 51.723

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 718.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x151x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x151x2048): 59.960
Elapsed time for attention_prob_times_values (320x2048x2048x151): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x151): 50.678

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 702.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x152x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x152x2048): 63.433
Elapsed time for attention_prob_times_values (320x2048x2048x152): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x152): 50.745

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 725.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x153x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x153x2048): 59.295
Elapsed time for attention_prob_times_values (320x2048x2048x153): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x153): 51.038

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 710.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x154x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x154x2048): 62.444
Elapsed time for attention_prob_times_values (320x2048x2048x154): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x154): 53.583

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 751.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x155x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x155x2048): 59.517
Elapsed time for attention_prob_times_values (320x2048x2048x155): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x155): 51.825

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 726.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x156x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x156x2048): 63.401
Elapsed time for attention_prob_times_values (320x2048x2048x156): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x156): 53.521

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 765.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x157x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x157x2048): 61.369
Elapsed time for attention_prob_times_values (320x2048x2048x157): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x157): 51.989

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 746.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x158x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x158x2048): 61.020
Elapsed time for attention_prob_times_values (320x2048x2048x158): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x158): 52.654

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 754.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x159x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x159x2048): 63.224
Elapsed time for attention_prob_times_values (320x2048x2048x159): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x159): 53.118

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 774.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x160x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x160x2048): 74.036
Elapsed time for attention_prob_times_values (320x2048x2048x160): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x160): 53.798

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 841.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x161x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x161x2048): 62.254
Elapsed time for attention_prob_times_values (320x2048x2048x161): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x161): 52.710

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 775.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x162x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x162x2048): 62.372
Elapsed time for attention_prob_times_values (320x2048x2048x162): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x162): 54.659

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 795.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x163x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x163x2048): 59.394
Elapsed time for attention_prob_times_values (320x2048x2048x163): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x163): 54.524

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 780.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x164x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x164x2048): 62.324
Elapsed time for attention_prob_times_values (320x2048x2048x164): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x164): 55.345

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 809.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x165x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x165x2048): 63.047
Elapsed time for attention_prob_times_values (320x2048x2048x165): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x165): 55.149

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 817.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x166x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x166x2048): 63.712
Elapsed time for attention_prob_times_values (320x2048x2048x166): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x166): 55.396

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 827.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x167x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x167x2048): 61.771
Elapsed time for attention_prob_times_values (320x2048x2048x167): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x167): 55.677

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 822.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x168x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x168x2048): 65.550
Elapsed time for attention_prob_times_values (320x2048x2048x168): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x168): 55.540

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 849.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x169x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x169x2048): 63.257
Elapsed time for attention_prob_times_values (320x2048x2048x169): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x169): 55.641

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 840.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x170x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x170x2048): 64.545
Elapsed time for attention_prob_times_values (320x2048x2048x170): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x170): 58.305

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 874.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x171x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x171x2048): 63.690
Elapsed time for attention_prob_times_values (320x2048x2048x171): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x171): 56.409

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 859.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x172x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x172x2048): 65.395
Elapsed time for attention_prob_times_values (320x2048x2048x172): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x172): 59.243

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 897.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x173x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x173x2048): 62.885
Elapsed time for attention_prob_times_values (320x2048x2048x173): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x173): 56.004

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 859.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x174x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x174x2048): 63.846
Elapsed time for attention_prob_times_values (320x2048x2048x174): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x174): 59.479

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 898.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x175x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x175x2048): 64.811
Elapsed time for attention_prob_times_values (320x2048x2048x175): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x175): 56.456

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 885.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x176x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x176x2048): 67.786
Elapsed time for attention_prob_times_values (320x2048x2048x176): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x176): 58.790

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 928.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x177x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x177x2048): 63.215
Elapsed time for attention_prob_times_values (320x2048x2048x177): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x177): 56.924

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 888.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x178x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x178x2048): 65.491
Elapsed time for attention_prob_times_values (320x2048x2048x178): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x178): 60.514

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 937.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x179x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x179x2048): 61.188
Elapsed time for attention_prob_times_values (320x2048x2048x179): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x179): 55.647

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 873.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x180x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x180x2048): 66.407
Elapsed time for attention_prob_times_values (320x2048x2048x180): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x180): 59.542

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 945.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x181x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x181x2048): 60.969
Elapsed time for attention_prob_times_values (320x2048x2048x181): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x181): 59.465

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 911.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x182x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x182x2048): 66.850
Elapsed time for attention_prob_times_values (320x2048x2048x182): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x182): 59.260

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 956.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x183x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x183x2048): 62.287
Elapsed time for attention_prob_times_values (320x2048x2048x183): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x183): 56.802

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 908.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x184x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x184x2048): 68.046
Elapsed time for attention_prob_times_values (320x2048x2048x184): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x184): 60.254

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 982.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x185x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x185x2048): 64.206
Elapsed time for attention_prob_times_values (320x2048x2048x185): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x185): 60.555

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 963.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x186x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x186x2048): 66.941
Elapsed time for attention_prob_times_values (320x2048x2048x186): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x186): 60.934

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 990.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x187x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x187x2048): 63.317
Elapsed time for attention_prob_times_values (320x2048x2048x187): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x187): 58.465

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 948.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x188x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x188x2048): 67.622
Elapsed time for attention_prob_times_values (320x2048x2048x188): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x188): 63.183

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1024.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x189x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x189x2048): 66.351
Elapsed time for attention_prob_times_values (320x2048x2048x189): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x189): 61.448

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1005.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x190x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x190x2048): 67.868
Elapsed time for attention_prob_times_values (320x2048x2048x190): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x190): 62.592

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1031.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x191x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x191x2048): 67.190
Elapsed time for attention_prob_times_values (320x2048x2048x191): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x191): 61.089

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1018.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x192x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x192x2048): 77.260
Elapsed time for attention_prob_times_values (320x2048x2048x192): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x192): 65.654

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1135.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x193x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x193x2048): 65.733
Elapsed time for attention_prob_times_values (320x2048x2048x193): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x193): 52.483

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 938.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x194x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x194x2048): 67.415
Elapsed time for attention_prob_times_values (320x2048x2048x194): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x194): 52.955

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 958.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x195x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x195x2048): 66.243
Elapsed time for attention_prob_times_values (320x2048x2048x195): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x195): 52.274

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 948.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x196x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x196x2048): 67.905
Elapsed time for attention_prob_times_values (320x2048x2048x196): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x196): 52.810

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 969.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x197x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x197x2048): 66.915
Elapsed time for attention_prob_times_values (320x2048x2048x197): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x197): 52.898

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 968.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x198x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x198x2048): 67.718
Elapsed time for attention_prob_times_values (320x2048x2048x198): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x198): 53.275

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 982.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x199x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x199x2048): 67.409
Elapsed time for attention_prob_times_values (320x2048x2048x199): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x199): 52.575

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 977.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x200x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x200x2048): 67.699
Elapsed time for attention_prob_times_values (320x2048x2048x200): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x200): 51.117

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 968.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x201x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x201x2048): 66.946
Elapsed time for attention_prob_times_values (320x2048x2048x201): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x201): 51.493

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 972.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x202x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x202x2048): 68.310
Elapsed time for attention_prob_times_values (320x2048x2048x202): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x202): 54.091

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1013.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x203x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x203x2048): 67.371
Elapsed time for attention_prob_times_values (320x2048x2048x203): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x203): 51.879

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 988.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x204x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x204x2048): 66.992
Elapsed time for attention_prob_times_values (320x2048x2048x204): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x204): 53.316

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1005.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x205x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x205x2048): 67.662
Elapsed time for attention_prob_times_values (320x2048x2048x205): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x205): 52.341

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1004.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x206x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x206x2048): 69.168
Elapsed time for attention_prob_times_values (320x2048x2048x206): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x206): 54.788

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1045.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x207x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x207x2048): 66.776
Elapsed time for attention_prob_times_values (320x2048x2048x207): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x207): 52.630

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1010.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x208x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x208x2048): 69.607
Elapsed time for attention_prob_times_values (320x2048x2048x208): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x208): 53.108

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1039.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x209x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x209x2048): 67.016
Elapsed time for attention_prob_times_values (320x2048x2048x209): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x209): 51.441

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1008.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x210x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x210x2048): 67.968
Elapsed time for attention_prob_times_values (320x2048x2048x210): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x210): 54.879

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1057.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x211x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x211x2048): 67.223
Elapsed time for attention_prob_times_values (320x2048x2048x211): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x211): 53.007

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1036.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x212x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x212x2048): 66.627
Elapsed time for attention_prob_times_values (320x2048x2048x212): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x212): 56.324

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1072.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x213x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x213x2048): 67.138
Elapsed time for attention_prob_times_values (320x2048x2048x213): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x213): 53.460

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1050.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x214x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x214x2048): 69.901
Elapsed time for attention_prob_times_values (320x2048x2048x214): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x214): 56.494

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1107.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x215x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x215x2048): 66.032
Elapsed time for attention_prob_times_values (320x2048x2048x215): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x215): 53.204

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1048.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x216x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x216x2048): 70.446
Elapsed time for attention_prob_times_values (320x2048x2048x216): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x216): 53.118

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1082.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x217x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x217x2048): 65.632
Elapsed time for attention_prob_times_values (320x2048x2048x217): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x217): 51.439

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1035.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x218x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x218x2048): 69.806
Elapsed time for attention_prob_times_values (320x2048x2048x218): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x218): 57.500

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1137.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x219x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x219x2048): 67.011
Elapsed time for attention_prob_times_values (320x2048x2048x219): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x219): 54.997

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1094.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x220x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x220x2048): 66.375
Elapsed time for attention_prob_times_values (320x2048x2048x220): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x220): 58.058

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1126.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x221x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x221x2048): 68.696
Elapsed time for attention_prob_times_values (320x2048x2048x221): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x221): 54.256

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1107.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x222x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x222x2048): 67.978
Elapsed time for attention_prob_times_values (320x2048x2048x222): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x222): 55.458

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1120.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x223x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x223x2048): 69.875
Elapsed time for attention_prob_times_values (320x2048x2048x223): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x223): 55.975

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1145.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x224x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x224x2048): 79.892
Elapsed time for attention_prob_times_values (320x2048x2048x224): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x224): 56.979

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1230.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x225x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x225x2048): 66.697
Elapsed time for attention_prob_times_values (320x2048x2048x225): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x225): 55.723

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1128.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x226x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x226x2048): 68.634
Elapsed time for attention_prob_times_values (320x2048x2048x226): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x226): 59.159

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1185.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x227x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x227x2048): 67.009
Elapsed time for attention_prob_times_values (320x2048x2048x227): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x227): 56.063

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1143.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x228x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x228x2048): 69.361
Elapsed time for attention_prob_times_values (320x2048x2048x228): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x228): 59.718

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1207.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x229x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x229x2048): 68.968
Elapsed time for attention_prob_times_values (320x2048x2048x229): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x229): 55.276

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1159.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x230x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x230x2048): 70.289
Elapsed time for attention_prob_times_values (320x2048x2048x230): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x230): 59.865

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1226.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x231x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x231x2048): 67.716
Elapsed time for attention_prob_times_values (320x2048x2048x231): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x231): 57.804

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1187.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x232x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x232x2048): 71.735
Elapsed time for attention_prob_times_values (320x2048x2048x232): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x232): 58.606

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1233.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x233x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x233x2048): 67.471
Elapsed time for attention_prob_times_values (320x2048x2048x233): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x233): 57.940

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1197.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x234x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x234x2048): 70.053
Elapsed time for attention_prob_times_values (320x2048x2048x234): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x234): 56.568

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1206.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x235x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x235x2048): 68.679
Elapsed time for attention_prob_times_values (320x2048x2048x235): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x235): 57.080

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1206.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x236x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x236x2048): 68.107
Elapsed time for attention_prob_times_values (320x2048x2048x236): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x236): 59.768

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1237.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x237x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x237x2048): 69.404
Elapsed time for attention_prob_times_values (320x2048x2048x237): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x237): 58.827

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1242.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x238x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x238x2048): 70.836
Elapsed time for attention_prob_times_values (320x2048x2048x238): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x238): 61.429

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1289.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x239x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x239x2048): 66.491
Elapsed time for attention_prob_times_values (320x2048x2048x239): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x239): 56.276

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1199.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x240x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x240x2048): 54.104
Elapsed time for attention_prob_times_values (320x2048x2048x240): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x240): 59.348

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1117.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x241x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x241x2048): 52.433
Elapsed time for attention_prob_times_values (320x2048x2048x241): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x241): 59.619

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1106.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x242x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x242x2048): 53.841
Elapsed time for attention_prob_times_values (320x2048x2048x242): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x242): 61.426

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1142.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x243x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x243x2048): 53.097
Elapsed time for attention_prob_times_values (320x2048x2048x243): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x243): 58.215

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1109.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x244x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x244x2048): 54.789
Elapsed time for attention_prob_times_values (320x2048x2048x244): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x244): 62.960

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1175.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x245x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x245x2048): 54.180
Elapsed time for attention_prob_times_values (320x2048x2048x245): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x245): 59.248

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1139.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x246x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x246x2048): 53.037
Elapsed time for attention_prob_times_values (320x2048x2048x246): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x246): 63.050

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1164.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x247x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x247x2048): 54.465
Elapsed time for attention_prob_times_values (320x2048x2048x247): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x247): 60.808

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1166.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x248x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x248x2048): 53.374
Elapsed time for attention_prob_times_values (320x2048x2048x248): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x248): 59.763

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1148.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x249x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x249x2048): 52.605
Elapsed time for attention_prob_times_values (320x2048x2048x249): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x249): 62.448

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1167.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x250x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x250x2048): 53.331
Elapsed time for attention_prob_times_values (320x2048x2048x250): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x250): 63.990

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1194.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x251x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x251x2048): 53.376
Elapsed time for attention_prob_times_values (320x2048x2048x251): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x251): 44.402

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 999.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x252x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x252x2048): 51.545
Elapsed time for attention_prob_times_values (320x2048x2048x252): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x252): 64.315

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1183.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x253x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x253x2048): 52.360
Elapsed time for attention_prob_times_values (320x2048x2048x253): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x253): 56.187

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 1125.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x254x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x254x2048): 54.114
Elapsed time for attention_prob_times_values (320x2048x2048x254): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x254): 64.561

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1227.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x255x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x255x2048): 53.565
Elapsed time for attention_prob_times_values (320x2048x2048x255): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x255): 66.255

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1239.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x256x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x256x2048): 74.702
Elapsed time for attention_prob_times_values (320x2048x2048x256): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x256): 67.149

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1485.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x257x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x257x2048): 55.672
Elapsed time for attention_prob_times_values (320x2048x2048x257): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x257): 50.598

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1117.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x258x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x258x2048): 54.366
Elapsed time for attention_prob_times_values (320x2048x2048x258): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x258): 55.522

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1162.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x259x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x259x2048): 56.729
Elapsed time for attention_prob_times_values (320x2048x2048x259): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x259): 53.923

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 1174.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x260x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x260x2048): 57.717
Elapsed time for attention_prob_times_values (320x2048x2048x260): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x260): 56.622

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1218.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x261x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x261x2048): 52.874
Elapsed time for attention_prob_times_values (320x2048x2048x261): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x261): 53.491

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1137.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x262x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x262x2048): 57.059
Elapsed time for attention_prob_times_values (320x2048x2048x262): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x262): 57.003

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1224.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x263x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x263x2048): 55.329
Elapsed time for attention_prob_times_values (320x2048x2048x263): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x263): 54.843

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 1186.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x264x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x264x2048): 57.614
Elapsed time for attention_prob_times_values (320x2048x2048x264): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x264): 53.884

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1204.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x265x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x265x2048): 53.642
Elapsed time for attention_prob_times_values (320x2048x2048x265): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x265): 54.470

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1173.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x266x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x266x2048): 53.709
Elapsed time for attention_prob_times_values (320x2048x2048x266): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x266): 57.668

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1211.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x267x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x267x2048): 54.485
Elapsed time for attention_prob_times_values (320x2048x2048x267): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x267): 55.125

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1197.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x268x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x268x2048): 56.727
Elapsed time for attention_prob_times_values (320x2048x2048x268): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x268): 56.731

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1244.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x269x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x269x2048): 56.215
Elapsed time for attention_prob_times_values (320x2048x2048x269): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x269): 55.646

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1231.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x270x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x270x2048): 56.225
Elapsed time for attention_prob_times_values (320x2048x2048x270): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x270): 57.373

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1254.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x271x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x271x2048): 56.218
Elapsed time for attention_prob_times_values (320x2048x2048x271): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x271): 55.830

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1242.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x272x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x272x2048): 58.234
Elapsed time for attention_prob_times_values (320x2048x2048x272): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x272): 72.025

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1432.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x273x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x273x2048): 54.125
Elapsed time for attention_prob_times_values (320x2048x2048x273): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x273): 56.386

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1233.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x274x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x274x2048): 56.456
Elapsed time for attention_prob_times_values (320x2048x2048x274): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x274): 58.746

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1290.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x275x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x275x2048): 54.118
Elapsed time for attention_prob_times_values (320x2048x2048x275): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x275): 55.769

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1235.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x276x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x276x2048): 55.457
Elapsed time for attention_prob_times_values (320x2048x2048x276): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x276): 58.521

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1284.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x277x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x277x2048): 55.932
Elapsed time for attention_prob_times_values (320x2048x2048x277): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x277): 56.967

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1277.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x278x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x278x2048): 57.015
Elapsed time for attention_prob_times_values (320x2048x2048x278): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x278): 57.450

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1300.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x279x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x279x2048): 56.655
Elapsed time for attention_prob_times_values (320x2048x2048x279): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x279): 57.384

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1299.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x280x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x280x2048): 56.511
Elapsed time for attention_prob_times_values (320x2048x2048x280): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x280): 75.304

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1476.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x281x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x281x2048): 56.128
Elapsed time for attention_prob_times_values (320x2048x2048x281): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x281): 57.962

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1309.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x282x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x282x2048): 54.721
Elapsed time for attention_prob_times_values (320x2048x2048x282): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x282): 60.394

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1322.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x283x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x283x2048): 56.379
Elapsed time for attention_prob_times_values (320x2048x2048x283): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x283): 57.277

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1313.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x284x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x284x2048): 55.563
Elapsed time for attention_prob_times_values (320x2048x2048x284): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x284): 60.992

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1348.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x285x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x285x2048): 56.755
Elapsed time for attention_prob_times_values (320x2048x2048x285): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x285): 58.766

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1343.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x286x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x286x2048): 57.374
Elapsed time for attention_prob_times_values (320x2048x2048x286): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x286): 60.499

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1374.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x287x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x287x2048): 54.494
Elapsed time for attention_prob_times_values (320x2048x2048x287): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x287): 57.702

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1312.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x288x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x288x2048): 77.428
Elapsed time for attention_prob_times_values (320x2048x2048x288): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x288): 77.919

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1825.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x289x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x289x2048): 60.492
Elapsed time for attention_prob_times_values (320x2048x2048x289): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x289): 59.545

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1415.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x290x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x290x2048): 61.296
Elapsed time for attention_prob_times_values (320x2048x2048x290): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x290): 61.249

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1449.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x291x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x291x2048): 56.496
Elapsed time for attention_prob_times_values (320x2048x2048x291): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x291): 57.144

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1348.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x292x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x292x2048): 62.188
Elapsed time for attention_prob_times_values (320x2048x2048x292): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x292): 62.669

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 1486.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x293x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x293x2048): 59.170
Elapsed time for attention_prob_times_values (320x2048x2048x293): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x293): 60.488

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1429.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x294x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x294x2048): 59.588
Elapsed time for attention_prob_times_values (320x2048x2048x294): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x294): 61.226

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1447.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x295x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x295x2048): 57.866
Elapsed time for attention_prob_times_values (320x2048x2048x295): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x295): 58.406

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 1397.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x296x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x296x2048): 61.024
Elapsed time for attention_prob_times_values (320x2048x2048x296): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x296): 78.911

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1660.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x297x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x297x2048): 59.218
Elapsed time for attention_prob_times_values (320x2048x2048x297): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x297): 59.347

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1434.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x298x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x298x2048): 59.808
Elapsed time for attention_prob_times_values (320x2048x2048x298): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x298): 63.668

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1497.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x299x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x299x2048): 59.191
Elapsed time for attention_prob_times_values (320x2048x2048x299): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x299): 61.358

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1467.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x300x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x300x2048): 61.008
Elapsed time for attention_prob_times_values (320x2048x2048x300): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x300): 64.476

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1532.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x301x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x301x2048): 57.757
Elapsed time for attention_prob_times_values (320x2048x2048x301): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x301): 60.574

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1449.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x302x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x302x2048): 59.231
Elapsed time for attention_prob_times_values (320x2048x2048x302): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x302): 64.656

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1520.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x303x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x303x2048): 60.036
Elapsed time for attention_prob_times_values (320x2048x2048x303): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x303): 63.864

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1526.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x304x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x304x2048): 61.455
Elapsed time for attention_prob_times_values (320x2048x2048x304): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x304): 81.872

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1737.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x305x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x305x2048): 57.898
Elapsed time for attention_prob_times_values (320x2048x2048x305): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x305): 63.824

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 1507.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x306x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x306x2048): 58.062
Elapsed time for attention_prob_times_values (320x2048x2048x306): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x306): 65.355

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1531.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x307x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x307x2048): 57.852
Elapsed time for attention_prob_times_values (320x2048x2048x307): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x307): 63.365

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1511.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x308x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x308x2048): 61.180
Elapsed time for attention_prob_times_values (320x2048x2048x308): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x308): 66.073

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1592.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x309x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x309x2048): 60.043
Elapsed time for attention_prob_times_values (320x2048x2048x309): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x309): 65.133

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1570.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x310x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x310x2048): 60.641
Elapsed time for attention_prob_times_values (320x2048x2048x310): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x310): 66.174

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1596.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x311x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x311x2048): 60.360
Elapsed time for attention_prob_times_values (320x2048x2048x311): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x311): 65.559

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1589.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x312x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x312x2048): 61.580
Elapsed time for attention_prob_times_values (320x2048x2048x312): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x312): 81.519

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1780.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x313x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x313x2048): 57.123
Elapsed time for attention_prob_times_values (320x2048x2048x313): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x313): 64.370

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1540.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x314x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x314x2048): 59.377
Elapsed time for attention_prob_times_values (320x2048x2048x314): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x314): 65.232

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 1587.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x315x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x315x2048): 57.719
Elapsed time for attention_prob_times_values (320x2048x2048x315): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x315): 65.289

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1569.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x316x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x316x2048): 60.142
Elapsed time for attention_prob_times_values (320x2048x2048x316): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x316): 67.196

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1630.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x317x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x317x2048): 59.218
Elapsed time for attention_prob_times_values (320x2048x2048x317): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x317): 65.627

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1604.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x318x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x318x2048): 60.735
Elapsed time for attention_prob_times_values (320x2048x2048x318): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x318): 67.707

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1654.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x319x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x319x2048): 58.692
Elapsed time for attention_prob_times_values (320x2048x2048x319): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x319): 66.897

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1620.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x320x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x320x2048): 78.289
Elapsed time for attention_prob_times_values (320x2048x2048x320): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x320): 80.429

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2062.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x321x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x321x2048): 62.807
Elapsed time for attention_prob_times_values (320x2048x2048x321): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x321): 58.195

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 1575.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x322x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x322x2048): 63.514
Elapsed time for attention_prob_times_values (320x2048x2048x322): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x322): 60.740

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1624.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x323x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x323x2048): 62.851
Elapsed time for attention_prob_times_values (320x2048x2048x323): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x323): 58.488

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 1589.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x324x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x324x2048): 61.233
Elapsed time for attention_prob_times_values (320x2048x2048x324): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x324): 60.740

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 1604.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x325x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x325x2048): 62.669
Elapsed time for attention_prob_times_values (320x2048x2048x325): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x325): 58.006

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 1589.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x326x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x326x2048): 63.179
Elapsed time for attention_prob_times_values (320x2048x2048x326): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x326): 60.214

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1632.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x327x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x327x2048): 62.293
Elapsed time for attention_prob_times_values (320x2048x2048x327): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x327): 56.816

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 1577.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x328x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x328x2048): 61.949
Elapsed time for attention_prob_times_values (320x2048x2048x328): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x328): 72.654

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1780.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x329x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x329x2048): 60.311
Elapsed time for attention_prob_times_values (320x2048x2048x329): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x329): 57.477

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1571.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x330x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x330x2048): 61.240
Elapsed time for attention_prob_times_values (320x2048x2048x330): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x330): 61.328

Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 1641.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x331x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x331x2048): 61.472
Elapsed time for attention_prob_times_values (320x2048x2048x331): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x331): 58.423

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1609.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x332x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x332x2048): 61.570
Elapsed time for attention_prob_times_values (320x2048x2048x332): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x332): 62.311

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 1668.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x333x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x333x2048): 61.968
Elapsed time for attention_prob_times_values (320x2048x2048x333): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x333): 57.084

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1605.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x334x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x334x2048): 61.905
Elapsed time for attention_prob_times_values (320x2048x2048x334): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x334): 62.247

Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 1681.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x335x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x335x2048): 60.622
Elapsed time for attention_prob_times_values (320x2048x2048x335): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x335): 58.635

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 1619.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x336x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x336x2048): 63.367
Elapsed time for attention_prob_times_values (320x2048x2048x336): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x336): 75.463

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1877.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x337x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x337x2048): 61.284
Elapsed time for attention_prob_times_values (320x2048x2048x337): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x337): 58.870

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1641.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x338x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x338x2048): 62.014
Elapsed time for attention_prob_times_values (320x2048x2048x338): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x338): 62.889

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 1711.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x339x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x339x2048): 61.738
Elapsed time for attention_prob_times_values (320x2048x2048x339): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x339): 59.851

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1670.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x340x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x340x2048): 61.974
Elapsed time for attention_prob_times_values (320x2048x2048x340): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x340): 62.783

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 1719.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x341x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x341x2048): 61.272
Elapsed time for attention_prob_times_values (320x2048x2048x341): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x341): 60.474

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1682.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x342x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x342x2048): 61.407
Elapsed time for attention_prob_times_values (320x2048x2048x342): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x342): 62.084

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1711.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x343x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x343x2048): 60.221
Elapsed time for attention_prob_times_values (320x2048x2048x343): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x343): 60.242

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1674.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x344x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x344x2048): 63.668
Elapsed time for attention_prob_times_values (320x2048x2048x344): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x344): 76.879

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1941.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x345x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x345x2048): 61.202
Elapsed time for attention_prob_times_values (320x2048x2048x345): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x345): 57.948

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1664.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x346x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x346x2048): 61.617
Elapsed time for attention_prob_times_values (320x2048x2048x346): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x346): 62.230

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1735.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x347x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x347x2048): 60.672
Elapsed time for attention_prob_times_values (320x2048x2048x347): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x347): 60.465

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1702.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x348x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x348x2048): 63.092
Elapsed time for attention_prob_times_values (320x2048x2048x348): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x348): 62.763

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1773.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x349x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x349x2048): 59.796
Elapsed time for attention_prob_times_values (320x2048x2048x349): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x349): 60.574

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1701.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x350x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x350x2048): 62.484
Elapsed time for attention_prob_times_values (320x2048x2048x350): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x350): 62.747

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1774.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x351x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x351x2048): 61.160
Elapsed time for attention_prob_times_values (320x2048x2048x351): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x351): 61.239

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1739.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x352x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x352x2048): 79.195
Elapsed time for attention_prob_times_values (320x2048x2048x352): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x352): 72.629

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2159.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x353x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x353x2048): 64.487
Elapsed time for attention_prob_times_values (320x2048x2048x353): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x353): 63.198

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1824.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x354x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x354x2048): 65.031
Elapsed time for attention_prob_times_values (320x2048x2048x354): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x354): 62.220

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1822.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x355x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x355x2048): 62.444
Elapsed time for attention_prob_times_values (320x2048x2048x355): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x355): 62.761

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1798.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x356x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x356x2048): 65.812
Elapsed time for attention_prob_times_values (320x2048x2048x356): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x356): 63.918

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 1868.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x357x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x357x2048): 64.224
Elapsed time for attention_prob_times_values (320x2048x2048x357): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x357): 63.076

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1838.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x358x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x358x2048): 61.271
Elapsed time for attention_prob_times_values (320x2048x2048x358): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x358): 59.436

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 1747.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x359x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x359x2048): 60.218
Elapsed time for attention_prob_times_values (320x2048x2048x359): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x359): 61.946

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 1773.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x360x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x360x2048): 65.476
Elapsed time for attention_prob_times_values (320x2048x2048x360): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x360): 80.183

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2099.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x361x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x361x2048): 62.081
Elapsed time for attention_prob_times_values (320x2048x2048x361): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x361): 61.815

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1809.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x362x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x362x2048): 63.825
Elapsed time for attention_prob_times_values (320x2048x2048x362): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x362): 64.513

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1878.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x363x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x363x2048): 61.077
Elapsed time for attention_prob_times_values (320x2048x2048x363): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x363): 61.967

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1806.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x364x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x364x2048): 62.389
Elapsed time for attention_prob_times_values (320x2048x2048x364): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x364): 63.802

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 1857.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x365x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x365x2048): 63.438
Elapsed time for attention_prob_times_values (320x2048x2048x365): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x365): 62.446

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1857.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x366x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x366x2048): 63.314
Elapsed time for attention_prob_times_values (320x2048x2048x366): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x366): 65.129

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1900.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x367x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x367x2048): 61.372
Elapsed time for attention_prob_times_values (320x2048x2048x367): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x367): 63.821

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1856.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x368x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x368x2048): 64.920
Elapsed time for attention_prob_times_values (320x2048x2048x368): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x368): 82.065

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2156.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x369x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x369x2048): 62.226
Elapsed time for attention_prob_times_values (320x2048x2048x369): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x369): 62.997

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 1867.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x370x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x370x2048): 62.754
Elapsed time for attention_prob_times_values (320x2048x2048x370): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x370): 63.573

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1888.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x371x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x371x2048): 62.812
Elapsed time for attention_prob_times_values (320x2048x2048x371): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x371): 64.109

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1902.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x372x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x372x2048): 63.402
Elapsed time for attention_prob_times_values (320x2048x2048x372): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x372): 65.359

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 1934.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x373x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x373x2048): 62.336
Elapsed time for attention_prob_times_values (320x2048x2048x373): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x373): 63.865

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1901.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x374x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x374x2048): 63.937
Elapsed time for attention_prob_times_values (320x2048x2048x374): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x374): 66.103

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1964.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x375x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x375x2048): 62.524
Elapsed time for attention_prob_times_values (320x2048x2048x375): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x375): 63.953

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 1915.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x376x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x376x2048): 65.255
Elapsed time for attention_prob_times_values (320x2048x2048x376): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x376): 83.556

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2225.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x377x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x377x2048): 62.914
Elapsed time for attention_prob_times_values (320x2048x2048x377): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x377): 61.738

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 1897.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x378x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x378x2048): 62.525
Elapsed time for attention_prob_times_values (320x2048x2048x378): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x378): 66.826

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1972.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x379x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x379x2048): 62.102
Elapsed time for attention_prob_times_values (320x2048x2048x379): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x379): 63.662

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 1924.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0324
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x380x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x380x2048): 64.291
Elapsed time for attention_prob_times_values (320x2048x2048x380): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x380): 65.739

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1994.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x381x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x381x2048): 61.257
Elapsed time for attention_prob_times_values (320x2048x2048x381): 0.0275
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x381): 37.221

Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 1424.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0442
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x382x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x382x2048): 62.766
Elapsed time for attention_prob_times_values (320x2048x2048x382): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x382): 65.896

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 1983.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x383x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x383x2048): 61.451
Elapsed time for attention_prob_times_values (320x2048x2048x383): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x383): 64.639

Attention duration (in seconds): 0.0326
Attention throughput (in TFLOP/s): 1948.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0326
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x384x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x384x2048): 77.547
Elapsed time for attention_prob_times_values (320x2048x2048x384): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x384): 81.595

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2465.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x385x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x385x2048): 65.250
Elapsed time for attention_prob_times_values (320x2048x2048x385): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x385): 57.579

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 1901.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x386x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x386x2048): 65.242
Elapsed time for attention_prob_times_values (320x2048x2048x386): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x386): 61.471

Attention duration (in seconds): 0.0327
Attention throughput (in TFLOP/s): 1972.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0327
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x387x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x387x2048): 65.224
Elapsed time for attention_prob_times_values (320x2048x2048x387): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x387): 58.121

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 1919.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x388x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x388x2048): 66.804
Elapsed time for attention_prob_times_values (320x2048x2048x388): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x388): 61.769

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 2009.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x389x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x389x2048): 65.416
Elapsed time for attention_prob_times_values (320x2048x2048x389): 0.0180
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x389): 58.173

Attention duration (in seconds): 0.0339
Attention throughput (in TFLOP/s): 1933.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0339
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x390x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x390x2048): 63.904
Elapsed time for attention_prob_times_values (320x2048x2048x390): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x390): 61.498

Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 1972.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0334
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x391x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x391x2048): 64.353
Elapsed time for attention_prob_times_values (320x2048x2048x391): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x391): 58.827

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 1939.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0342
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x392x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x392x2048): 65.573
Elapsed time for attention_prob_times_values (320x2048x2048x392): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x392): 74.994

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 2212.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x393x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x393x2048): 64.557
Elapsed time for attention_prob_times_values (320x2048x2048x393): 0.0180
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x393): 58.597

Attention duration (in seconds): 0.0343
Attention throughput (in TFLOP/s): 1947.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x394x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x394x2048): 65.146
Elapsed time for attention_prob_times_values (320x2048x2048x394): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x394): 60.035

Attention duration (in seconds): 0.0339
Attention throughput (in TFLOP/s): 1985.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0339
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x395x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x395x2048): 61.376
Elapsed time for attention_prob_times_values (320x2048x2048x395): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x395): 59.407

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 1923.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x396x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x396x2048): 65.100
Elapsed time for attention_prob_times_values (320x2048x2048x396): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x396): 61.917

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 2027.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x397x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x397x2048): 63.823
Elapsed time for attention_prob_times_values (320x2048x2048x397): 0.0185
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x397): 57.715

Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 1940.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x398x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x398x2048): 61.666
Elapsed time for attention_prob_times_values (320x2048x2048x398): 0.0180
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x398): 59.406

Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 1942.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0353
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x399x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x399x2048): 65.104
Elapsed time for attention_prob_times_values (320x2048x2048x399): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x399): 59.904

Attention duration (in seconds): 0.0343
Attention throughput (in TFLOP/s): 2007.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x400x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x400x2048): 66.853
Elapsed time for attention_prob_times_values (320x2048x2048x400): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x400): 76.807

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2305.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x401x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x401x2048): 64.381
Elapsed time for attention_prob_times_values (320x2048x2048x401): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x401): 60.209

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 2011.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x402x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x402x2048): 63.490
Elapsed time for attention_prob_times_values (320x2048x2048x402): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x402): 62.116

Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 2034.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0344
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x403x2048): 0.0169
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x403x2048): 63.969
Elapsed time for attention_prob_times_values (320x2048x2048x403): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x403): 60.439

Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 2019.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0348
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x404x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x404x2048): 65.721
Elapsed time for attention_prob_times_values (320x2048x2048x404): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x404): 63.894

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 2109.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x405x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x405x2048): 64.973
Elapsed time for attention_prob_times_values (320x2048x2048x405): 0.0183
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x405): 59.275

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 2023.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x406x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x406x2048): 64.269
Elapsed time for attention_prob_times_values (320x2048x2048x406): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x406): 63.961

Attention duration (in seconds): 0.0340
Attention throughput (in TFLOP/s): 2097.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x407x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x407x2048): 64.973
Elapsed time for attention_prob_times_values (320x2048x2048x407): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x407): 60.962

Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 2063.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x408x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x408x2048): 66.738
Elapsed time for attention_prob_times_values (320x2048x2048x408): 0.0231
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x408): 47.407

Attention duration (in seconds): 0.0395
Attention throughput (in TFLOP/s): 1822.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0395
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x409x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x409x2048): 64.716
Elapsed time for attention_prob_times_values (320x2048x2048x409): 0.0181
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x409): 60.576

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 2062.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
