1.13.1 

here
num_attention_heads: 24, hidden_size: 24, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1x2048): 0.912
Elapsed time for attention_prob_times_values (96x2048x2048x1): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1): 1.366

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 1.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 48, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x2x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x2x2048): 1.196
Elapsed time for attention_prob_times_values (96x2048x2048x2): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x2): 2.639

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 1.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 72, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x3x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x3x2048): 2.745
Elapsed time for attention_prob_times_values (96x2048x2048x3): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x3): 3.108

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 3.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x4x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x4x2048): 2.378
Elapsed time for attention_prob_times_values (96x2048x2048x4): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x4): 5.241

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 3.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x5x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x5x2048): 4.566
Elapsed time for attention_prob_times_values (96x2048x2048x5): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x5): 4.969

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 5.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x6x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x6x2048): 3.593
Elapsed time for attention_prob_times_values (96x2048x2048x6): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x6): 7.777

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 5.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x7x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x7x2048): 6.387
Elapsed time for attention_prob_times_values (96x2048x2048x7): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x7): 6.717

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 7.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x8x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x8x2048): 10.128
Elapsed time for attention_prob_times_values (96x2048x2048x8): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x8): 9.515

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 11.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x9x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x9x2048): 5.723
Elapsed time for attention_prob_times_values (96x2048x2048x9): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x9): 7.904

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 8.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x10x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x10x2048): 5.928
Elapsed time for attention_prob_times_values (96x2048x2048x10): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x10): 12.843

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 10.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x11x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x11x2048): 7.546
Elapsed time for attention_prob_times_values (96x2048x2048x11): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x11): 10.422

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 11.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x12x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x12x2048): 7.183
Elapsed time for attention_prob_times_values (96x2048x2048x12): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x12): 15.018

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 12.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x13x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x13x2048): 8.393
Elapsed time for attention_prob_times_values (96x2048x2048x13): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x13): 11.700

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 12.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x14x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x14x2048): 8.432
Elapsed time for attention_prob_times_values (96x2048x2048x14): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x14): 17.322

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 15.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x15x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x15x2048): 9.579
Elapsed time for attention_prob_times_values (96x2048x2048x15): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x15): 13.187

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 14.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x16x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x16x2048): 16.557
Elapsed time for attention_prob_times_values (96x2048x2048x16): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x16): 20.667

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 25.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x17x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x17x2048): 11.375
Elapsed time for attention_prob_times_values (96x2048x2048x17): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x17): 15.832

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 18.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x18x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x18x2048): 10.153
Elapsed time for attention_prob_times_values (96x2048x2048x18): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x18): 2.129

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 5.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x19x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x19x2048): 12.063
Elapsed time for attention_prob_times_values (96x2048x2048x19): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x19): 17.064

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 20.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x20x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x20x2048): 11.810
Elapsed time for attention_prob_times_values (96x2048x2048x20): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x20): 23.972

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 23.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x21x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x21x2048): 13.993
Elapsed time for attention_prob_times_values (96x2048x2048x21): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x21): 18.525

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 23.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x22x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x22x2048): 13.182
Elapsed time for attention_prob_times_values (96x2048x2048x22): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x22): 26.520

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 26.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x23x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x23x2048): 15.317
Elapsed time for attention_prob_times_values (96x2048x2048x23): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x23): 13.895

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 22.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x24x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x24x2048): 28.334
Elapsed time for attention_prob_times_values (96x2048x2048x24): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x24): 29.683

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 45.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x25x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x25x2048): 15.568
Elapsed time for attention_prob_times_values (96x2048x2048x25): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x25): 22.934

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 29.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x26x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x26x2048): 15.415
Elapsed time for attention_prob_times_values (96x2048x2048x26): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x26): 27.915

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 31.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x27x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x27x2048): 17.788
Elapsed time for attention_prob_times_values (96x2048x2048x27): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x27): 23.384

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 32.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x28x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x28x2048): 16.161
Elapsed time for attention_prob_times_values (96x2048x2048x28): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x28): 29.920

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 34.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x29x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x29x2048): 17.966
Elapsed time for attention_prob_times_values (96x2048x2048x29): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x29): 22.706

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 33.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x30x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x30x2048): 16.517
Elapsed time for attention_prob_times_values (96x2048x2048x30): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x30): 30.457

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 36.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x31x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x31x2048): 20.348
Elapsed time for attention_prob_times_values (96x2048x2048x31): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x31): 27.548

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 40.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x32x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x32x2048): 38.018
Elapsed time for attention_prob_times_values (96x2048x2048x32): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x32): 39.870

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 68.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x33x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x33x2048): 17.177
Elapsed time for attention_prob_times_values (96x2048x2048x33): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x33): 21.937

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 34.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x34x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x34x2048): 14.243
Elapsed time for attention_prob_times_values (96x2048x2048x34): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x34): 35.698

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 36.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x35x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x35x2048): 18.547
Elapsed time for attention_prob_times_values (96x2048x2048x35): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x35): 22.849

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 37.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x36x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x36x2048): 12.606
Elapsed time for attention_prob_times_values (96x2048x2048x36): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x36): 24.650

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 30.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x37x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x37x2048): 19.885
Elapsed time for attention_prob_times_values (96x2048x2048x37): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x37): 25.095

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 41.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x38x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x38x2048): 12.390
Elapsed time for attention_prob_times_values (96x2048x2048x38): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x38): 33.956

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 34.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x39x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x39x2048): 20.949
Elapsed time for attention_prob_times_values (96x2048x2048x39): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x39): 25.931

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 44.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x40x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x40x2048): 37.941
Elapsed time for attention_prob_times_values (96x2048x2048x40): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x40): 50.040

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 83.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x41x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x41x2048): 21.638
Elapsed time for attention_prob_times_values (96x2048x2048x41): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x41): 25.798

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 46.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x42x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x42x2048): 17.279
Elapsed time for attention_prob_times_values (96x2048x2048x42): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x42): 43.597

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 49.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x43x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x43x2048): 22.290
Elapsed time for attention_prob_times_values (96x2048x2048x43): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x43): 19.635

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 41.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x44x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x44x2048): 17.476
Elapsed time for attention_prob_times_values (96x2048x2048x44): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x44): 45.799

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 51.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x45x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x45x2048): 19.477
Elapsed time for attention_prob_times_values (96x2048x2048x45): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x45): 29.980

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 48.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x46x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x46x2048): 17.733
Elapsed time for attention_prob_times_values (96x2048x2048x46): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x46): 47.822

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 53.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x47x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x47x2048): 2.303
Elapsed time for attention_prob_times_values (96x2048x2048x47): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x47): 9.778

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 7.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x48x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x48x2048): 36.393
Elapsed time for attention_prob_times_values (96x2048x2048x48): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x48): 55.868

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 93.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x49x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x49x2048): 24.618
Elapsed time for attention_prob_times_values (96x2048x2048x49): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x49): 31.749

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 59.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x50x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x50x2048): 18.203
Elapsed time for attention_prob_times_values (96x2048x2048x50): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x50): 51.317

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 58.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x51x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x51x2048): 25.834
Elapsed time for attention_prob_times_values (96x2048x2048x51): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x51): 34.111

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 64.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x52x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x52x2048): 18.444
Elapsed time for attention_prob_times_values (96x2048x2048x52): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x52): 52.856

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 60.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x53x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x53x2048): 26.145
Elapsed time for attention_prob_times_values (96x2048x2048x53): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x53): 35.323

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 67.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x54x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x54x2048): 20.692
Elapsed time for attention_prob_times_values (96x2048x2048x54): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x54): 55.238

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 68.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x55x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x55x2048): 26.217
Elapsed time for attention_prob_times_values (96x2048x2048x55): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x55): 35.178

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 68.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x56x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x56x2048): 49.464
Elapsed time for attention_prob_times_values (96x2048x2048x56): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x56): 64.206

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 129.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x57x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x57x2048): 27.520
Elapsed time for attention_prob_times_values (96x2048x2048x57): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x57): 2.676

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 11.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x58x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x58x2048): 21.113
Elapsed time for attention_prob_times_values (96x2048x2048x58): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x58): 58.937

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 73.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x59x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x59x2048): 18.495
Elapsed time for attention_prob_times_values (96x2048x2048x59): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x59): 38.908

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 59.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x60x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x60x2048): 19.678
Elapsed time for attention_prob_times_values (96x2048x2048x60): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x60): 58.999

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 71.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x61x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x61x2048): 29.506
Elapsed time for attention_prob_times_values (96x2048x2048x61): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x61): 36.786

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 79.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x62x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x62x2048): 22.647
Elapsed time for attention_prob_times_values (96x2048x2048x62): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x62): 62.513

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 81.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x63x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x63x2048): 29.588
Elapsed time for attention_prob_times_values (96x2048x2048x63): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x63): 39.546

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 83.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x64x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x64x2048): 59.161
Elapsed time for attention_prob_times_values (96x2048x2048x64): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x64): 76.440

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 166.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x65x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x65x2048): 25.742
Elapsed time for attention_prob_times_values (96x2048x2048x65): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x65): 36.265

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 75.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x66x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x66x2048): 18.905
Elapsed time for attention_prob_times_values (96x2048x2048x66): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x66): 63.027

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 74.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x67x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x67x2048): 27.142
Elapsed time for attention_prob_times_values (96x2048x2048x67): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x67): 36.601

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 80.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x68x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x68x2048): 19.001
Elapsed time for attention_prob_times_values (96x2048x2048x68): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x68): 64.718

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 76.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x69x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x69x2048): 27.630
Elapsed time for attention_prob_times_values (96x2048x2048x69): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x69): 38.529

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 84.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x70x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x70x2048): 20.051
Elapsed time for attention_prob_times_values (96x2048x2048x70): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x70): 66.696

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 81.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x71x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x71x2048): 27.638
Elapsed time for attention_prob_times_values (96x2048x2048x71): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x71): 39.276

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 86.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x72x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x72x2048): 50.216
Elapsed time for attention_prob_times_values (96x2048x2048x72): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x72): 80.555

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 166.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x73x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x73x2048): 27.789
Elapsed time for attention_prob_times_values (96x2048x2048x73): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x73): 39.686

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 88.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x74x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x74x2048): 49.613
Elapsed time for attention_prob_times_values (96x2048x2048x74): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x74): 69.315

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 158.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x75x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x75x2048): 29.416
Elapsed time for attention_prob_times_values (96x2048x2048x75): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x75): 41.005

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 94.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x76x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x76x2048): 50.823
Elapsed time for attention_prob_times_values (96x2048x2048x76): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x76): 53.592

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 145.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x77x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x77x2048): 29.154
Elapsed time for attention_prob_times_values (96x2048x2048x77): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x77): 38.865

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 93.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x78x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x78x2048): 52.545
Elapsed time for attention_prob_times_values (96x2048x2048x78): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x78): 67.055

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 166.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x79x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x79x2048): 29.622
Elapsed time for attention_prob_times_values (96x2048x2048x79): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x79): 43.290

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 100.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x80x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x80x2048): 77.050
Elapsed time for attention_prob_times_values (96x2048x2048x80): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x80): 81.415

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 227.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x81x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x81x2048): 28.049
Elapsed time for attention_prob_times_values (96x2048x2048x81): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x81): 42.078

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 97.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x82x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x82x2048): 49.770
Elapsed time for attention_prob_times_values (96x2048x2048x82): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x82): 75.080

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 174.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x83x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x83x2048): 30.899
Elapsed time for attention_prob_times_values (96x2048x2048x83): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x83): 45.526

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 108.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x84x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x84x2048): 56.128
Elapsed time for attention_prob_times_values (96x2048x2048x84): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x84): 78.356

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 194.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x85x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x85x2048): 29.693
Elapsed time for attention_prob_times_values (96x2048x2048x85): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x85): 41.138

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 103.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x86x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x86x2048): 56.702
Elapsed time for attention_prob_times_values (96x2048x2048x86): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x86): 80.045

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 200.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x87x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x87x2048): 30.075
Elapsed time for attention_prob_times_values (96x2048x2048x87): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x87): 47.496

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 111.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x88x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x88x2048): 75.326
Elapsed time for attention_prob_times_values (96x2048x2048x88): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x88): 97.232

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 259.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x89x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x89x2048): 31.950
Elapsed time for attention_prob_times_values (96x2048x2048x89): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x89): 48.362

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 118.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x90x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x90x2048): 59.212
Elapsed time for attention_prob_times_values (96x2048x2048x90): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x90): 82.271

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 214.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x91x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x91x2048): 31.155
Elapsed time for attention_prob_times_values (96x2048x2048x91): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x91): 49.688

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 119.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x92x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x92x2048): 57.291
Elapsed time for attention_prob_times_values (96x2048x2048x92): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x92): 84.603

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 215.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x93x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x93x2048): 15.558
Elapsed time for attention_prob_times_values (96x2048x2048x93): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x93): 50.519

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 75.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x94x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x94x2048): 27.647
Elapsed time for attention_prob_times_values (96x2048x2048x94): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x94): 87.035

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 134.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x95x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x95x2048): 33.335
Elapsed time for attention_prob_times_values (96x2048x2048x95): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x95): 51.655

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 130.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x96x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x96x2048): 71.281
Elapsed time for attention_prob_times_values (96x2048x2048x96): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x96): 104.870

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 275.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x97x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x97x2048): 30.281
Elapsed time for attention_prob_times_values (96x2048x2048x97): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x97): 44.443

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 117.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x98x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x98x2048): 58.774
Elapsed time for attention_prob_times_values (96x2048x2048x98): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x98): 70.654

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 211.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x99x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x99x2048): 30.082
Elapsed time for attention_prob_times_values (96x2048x2048x99): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x99): 52.232

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 126.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x100x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x100x2048): 49.037
Elapsed time for attention_prob_times_values (96x2048x2048x100): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x100): 92.489

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 214.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x101x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x101x2048): 29.975
Elapsed time for attention_prob_times_values (96x2048x2048x101): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x101): 54.820

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 130.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x102x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x102x2048): 59.136
Elapsed time for attention_prob_times_values (96x2048x2048x102): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x102): 93.851

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 246.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x103x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x103x2048): 28.714
Elapsed time for attention_prob_times_values (96x2048x2048x103): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x103): 42.972

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 117.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x104x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x104x2048): 68.449
Elapsed time for attention_prob_times_values (96x2048x2048x104): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x104): 113.757

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 293.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x105x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x105x2048): 30.864
Elapsed time for attention_prob_times_values (96x2048x2048x105): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x105): 53.663

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 135.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x106x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x106x2048): 59.882
Elapsed time for attention_prob_times_values (96x2048x2048x106): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x106): 92.612

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 253.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x107x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x107x2048): 30.719
Elapsed time for attention_prob_times_values (96x2048x2048x107): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x107): 57.873

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 140.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x108x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x108x2048): 61.330
Elapsed time for attention_prob_times_values (96x2048x2048x108): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x108): 99.998

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 268.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x109x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x109x2048): 32.146
Elapsed time for attention_prob_times_values (96x2048x2048x109): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x109): 56.355

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 145.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x110x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x110x2048): 62.529
Elapsed time for attention_prob_times_values (96x2048x2048x110): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x110): 99.106

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 274.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x111x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x111x2048): 31.728
Elapsed time for attention_prob_times_values (96x2048x2048x111): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x111): 59.034

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 148.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x112x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x112x2048): 80.782
Elapsed time for attention_prob_times_values (96x2048x2048x112): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x112): 121.680

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 351.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x113x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x113x2048): 31.539
Elapsed time for attention_prob_times_values (96x2048x2048x113): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x113): 53.994

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 145.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x114x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x114x2048): 66.184
Elapsed time for attention_prob_times_values (96x2048x2048x114): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x114): 94.031

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 285.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x115x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x115x2048): 33.675
Elapsed time for attention_prob_times_values (96x2048x2048x115): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x115): 61.922

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 161.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x116x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x116x2048): 67.636
Elapsed time for attention_prob_times_values (96x2048x2048x116): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x116): 104.959

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 305.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x117x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x117x2048): 34.266
Elapsed time for attention_prob_times_values (96x2048x2048x117): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x117): 52.861

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 155.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x118x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x118x2048): 68.096
Elapsed time for attention_prob_times_values (96x2048x2048x118): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x118): 105.414

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 311.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x119x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x119x2048): 33.605
Elapsed time for attention_prob_times_values (96x2048x2048x119): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x119): 58.253

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 161.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x120x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x120x2048): 85.349
Elapsed time for attention_prob_times_values (96x2048x2048x120): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x120): 126.585

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 388.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x121x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x121x2048): 34.324
Elapsed time for attention_prob_times_values (96x2048x2048x121): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x121): 60.307

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 167.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x122x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x122x2048): 57.075
Elapsed time for attention_prob_times_values (96x2048x2048x122): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x122): 109.654

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 289.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x123x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x123x2048): 34.720
Elapsed time for attention_prob_times_values (96x2048x2048x123): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x123): 61.613

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 172.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x124x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x124x2048): 72.714
Elapsed time for attention_prob_times_values (96x2048x2048x124): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x124): 87.403

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 310.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x125x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x125x2048): 36.031
Elapsed time for attention_prob_times_values (96x2048x2048x125): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x125): 63.778

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 180.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x126x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x126x2048): 70.462
Elapsed time for attention_prob_times_values (96x2048x2048x126): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x126): 107.391

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 336.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x127x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x127x2048): 35.216
Elapsed time for attention_prob_times_values (96x2048x2048x127): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x127): 66.651

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 183.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x128x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x128x2048): 85.007
Elapsed time for attention_prob_times_values (96x2048x2048x128): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x128): 134.981

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 417.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x129x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x129x2048): 33.538
Elapsed time for attention_prob_times_values (96x2048x2048x129): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x129): 52.276

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 164.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x130x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x130x2048): 65.724
Elapsed time for attention_prob_times_values (96x2048x2048x130): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x130): 90.555

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 308.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x131x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x131x2048): 34.381
Elapsed time for attention_prob_times_values (96x2048x2048x131): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x131): 55.165

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 172.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x132x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x132x2048): 70.760
Elapsed time for attention_prob_times_values (96x2048x2048x132): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x132): 90.254

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 324.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x133x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x133x2048): 33.480
Elapsed time for attention_prob_times_values (96x2048x2048x133): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x133): 53.328

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 169.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x134x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x134x2048): 66.865
Elapsed time for attention_prob_times_values (96x2048x2048x134): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x134): 89.555

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 317.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x135x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x135x2048): 32.711
Elapsed time for attention_prob_times_values (96x2048x2048x135): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x135): 53.931

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 169.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x136x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x136x2048): 90.462
Elapsed time for attention_prob_times_values (96x2048x2048x136): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x136): 101.271

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 400.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x137x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x137x2048): 33.210
Elapsed time for attention_prob_times_values (96x2048x2048x137): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x137): 49.962

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 168.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x138x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x138x2048): 68.547
Elapsed time for attention_prob_times_values (96x2048x2048x138): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x138): 75.242

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 303.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x139x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x139x2048): 34.484
Elapsed time for attention_prob_times_values (96x2048x2048x139): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x139): 55.483

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 181.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x140x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x140x2048): 74.858
Elapsed time for attention_prob_times_values (96x2048x2048x140): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x140): 79.609

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 330.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x141x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x141x2048): 34.436
Elapsed time for attention_prob_times_values (96x2048x2048x141): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x141): 55.224

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 182.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x142x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x142x2048): 75.319
Elapsed time for attention_prob_times_values (96x2048x2048x142): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x142): 85.192

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 346.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x143x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x143x2048): 33.706
Elapsed time for attention_prob_times_values (96x2048x2048x143): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x143): 56.125

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 183.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x144x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x144x2048): 87.354
Elapsed time for attention_prob_times_values (96x2048x2048x144): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x144): 129.912

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 457.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x145x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x145x2048): 54.814
Elapsed time for attention_prob_times_values (96x2048x2048x145): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x145): 54.052

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 239.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x146x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x146x2048): 76.504
Elapsed time for attention_prob_times_values (96x2048x2048x146): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x146): 92.836

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 370.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x147x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x147x2048): 56.752
Elapsed time for attention_prob_times_values (96x2048x2048x147): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x147): 55.789

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 250.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x148x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x148x2048): 71.424
Elapsed time for attention_prob_times_values (96x2048x2048x148): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x148): 94.660

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 363.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x149x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x149x2048): 56.352
Elapsed time for attention_prob_times_values (96x2048x2048x149): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x149): 56.688

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 253.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x150x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x150x2048): 71.239
Elapsed time for attention_prob_times_values (96x2048x2048x150): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x150): 94.684

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 367.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x151x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x151x2048): 56.645
Elapsed time for attention_prob_times_values (96x2048x2048x151): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x151): 54.236

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 251.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x152x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x152x2048): 98.771
Elapsed time for attention_prob_times_values (96x2048x2048x152): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x152): 120.378

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 495.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x153x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x153x2048): 57.364
Elapsed time for attention_prob_times_values (96x2048x2048x153): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x153): 52.890

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 252.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x154x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x154x2048): 77.142
Elapsed time for attention_prob_times_values (96x2048x2048x154): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x154): 93.036

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 388.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x155x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x155x2048): 38.661
Elapsed time for attention_prob_times_values (96x2048x2048x155): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x155): 54.695

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 209.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x156x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x156x2048): 73.123
Elapsed time for attention_prob_times_values (96x2048x2048x156): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x156): 95.821

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 386.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x157x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x157x2048): 56.259
Elapsed time for attention_prob_times_values (96x2048x2048x157): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x157): 55.604

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 261.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x158x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x158x2048): 83.024
Elapsed time for attention_prob_times_values (96x2048x2048x158): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x158): 95.916

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 418.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x159x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x159x2048): 57.822
Elapsed time for attention_prob_times_values (96x2048x2048x159): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x159): 57.866

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 273.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x160x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x160x2048): 101.299
Elapsed time for attention_prob_times_values (96x2048x2048x160): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x160): 143.808

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 564.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x161x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x161x2048): 55.508
Elapsed time for attention_prob_times_values (96x2048x2048x161): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x161): 58.569

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 272.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x162x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x162x2048): 78.811
Elapsed time for attention_prob_times_values (96x2048x2048x162): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x162): 86.979

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 396.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x163x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x163x2048): 54.555
Elapsed time for attention_prob_times_values (96x2048x2048x163): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x163): 57.542

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 269.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x164x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x164x2048): 79.101
Elapsed time for attention_prob_times_values (96x2048x2048x164): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x164): 101.790

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 431.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x165x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x165x2048): 56.893
Elapsed time for attention_prob_times_values (96x2048x2048x165): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x165): 60.063

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 284.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x166x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x166x2048): 75.955
Elapsed time for attention_prob_times_values (96x2048x2048x166): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x166): 104.026

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 429.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x167x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x167x2048): 58.068
Elapsed time for attention_prob_times_values (96x2048x2048x167): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x167): 60.738

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 291.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x168x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x168x2048): 101.168
Elapsed time for attention_prob_times_values (96x2048x2048x168): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x168): 139.595

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 579.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x169x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x169x2048): 56.669
Elapsed time for attention_prob_times_values (96x2048x2048x169): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x169): 60.056

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 289.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x170x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x170x2048): 82.206
Elapsed time for attention_prob_times_values (96x2048x2048x170): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x170): 94.676

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 438.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x171x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x171x2048): 59.319
Elapsed time for attention_prob_times_values (96x2048x2048x171): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x171): 61.860

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 303.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x172x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x172x2048): 83.340
Elapsed time for attention_prob_times_values (96x2048x2048x172): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x172): 107.209

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 471.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x173x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x173x2048): 59.749
Elapsed time for attention_prob_times_values (96x2048x2048x173): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x173): 64.783

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 314.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x174x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x174x2048): 82.499
Elapsed time for attention_prob_times_values (96x2048x2048x174): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x174): 106.432

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 472.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x175x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x175x2048): 58.311
Elapsed time for attention_prob_times_values (96x2048x2048x175): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x175): 57.985

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 296.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x176x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x176x2048): 101.118
Elapsed time for attention_prob_times_values (96x2048x2048x176): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x176): 155.622

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 628.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x177x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x177x2048): 58.538
Elapsed time for attention_prob_times_values (96x2048x2048x177): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x177): 61.711

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 309.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x178x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x178x2048): 78.418
Elapsed time for attention_prob_times_values (96x2048x2048x178): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x178): 107.267

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 468.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x179x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x179x2048): 58.197
Elapsed time for attention_prob_times_values (96x2048x2048x179): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x179): 66.382

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 322.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x180x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x180x2048): 85.572
Elapsed time for attention_prob_times_values (96x2048x2048x180): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x180): 110.322

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 503.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x181x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x181x2048): 60.168
Elapsed time for attention_prob_times_values (96x2048x2048x181): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x181): 66.194

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 330.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x182x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x182x2048): 84.304
Elapsed time for attention_prob_times_values (96x2048x2048x182): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x182): 111.346

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 505.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x183x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x183x2048): 59.452
Elapsed time for attention_prob_times_values (96x2048x2048x183): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x183): 64.833

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 328.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x184x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x184x2048): 109.438
Elapsed time for attention_prob_times_values (96x2048x2048x184): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x184): 155.764

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 682.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x185x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x185x2048): 58.784
Elapsed time for attention_prob_times_values (96x2048x2048x185): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x185): 65.666

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 331.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x186x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x186x2048): 84.602
Elapsed time for attention_prob_times_values (96x2048x2048x186): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x186): 113.629

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 519.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x187x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x187x2048): 59.130
Elapsed time for attention_prob_times_values (96x2048x2048x187): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x187): 69.334

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 343.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x188x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x188x2048): 88.281
Elapsed time for attention_prob_times_values (96x2048x2048x188): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x188): 114.169

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 538.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x189x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x189x2048): 60.078
Elapsed time for attention_prob_times_values (96x2048x2048x189): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x189): 70.353

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 351.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x190x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x190x2048): 90.237
Elapsed time for attention_prob_times_values (96x2048x2048x190): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x190): 114.970

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 551.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x191x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x191x2048): 59.301
Elapsed time for attention_prob_times_values (96x2048x2048x191): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x191): 65.067

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 339.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x192x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x192x2048): 110.012
Elapsed time for attention_prob_times_values (96x2048x2048x192): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x192): 167.058

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 729.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x193x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x193x2048): 60.845
Elapsed time for attention_prob_times_values (96x2048x2048x193): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x193): 65.768

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 349.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x194x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x194x2048): 84.541
Elapsed time for attention_prob_times_values (96x2048x2048x194): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x194): 115.691

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 541.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x195x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x195x2048): 57.944
Elapsed time for attention_prob_times_values (96x2048x2048x195): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x195): 67.846

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 348.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x196x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x196x2048): 69.062
Elapsed time for attention_prob_times_values (96x2048x2048x196): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x196): 116.064

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 484.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x197x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x197x2048): 61.238
Elapsed time for attention_prob_times_values (96x2048x2048x197): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x197): 70.931

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 369.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x198x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x198x2048): 85.022
Elapsed time for attention_prob_times_values (96x2048x2048x198): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x198): 103.048

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 525.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x199x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x199x2048): 59.940
Elapsed time for attention_prob_times_values (96x2048x2048x199): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x199): 67.813

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 360.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x200x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x200x2048): 110.999
Elapsed time for attention_prob_times_values (96x2048x2048x200): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x200): 164.405

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 753.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x201x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x201x2048): 61.720
Elapsed time for attention_prob_times_values (96x2048x2048x201): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x201): 68.405

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 370.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x202x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x202x2048): 89.061
Elapsed time for attention_prob_times_values (96x2048x2048x202): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x202): 103.425

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 548.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x203x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x203x2048): 60.005
Elapsed time for attention_prob_times_values (96x2048x2048x203): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x203): 71.843

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 376.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x204x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x204x2048): 89.767
Elapsed time for attention_prob_times_values (96x2048x2048x204): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x204): 110.709

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 573.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x205x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x205x2048): 61.774
Elapsed time for attention_prob_times_values (96x2048x2048x205): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x205): 71.392

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 384.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x206x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x206x2048): 89.747
Elapsed time for attention_prob_times_values (96x2048x2048x206): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x206): 109.455

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 574.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x207x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x207x2048): 62.472
Elapsed time for attention_prob_times_values (96x2048x2048x207): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x207): 70.739

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 388.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x208x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x208x2048): 115.080
Elapsed time for attention_prob_times_values (96x2048x2048x208): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x208): 179.042

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 823.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x209x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x209x2048): 61.854
Elapsed time for attention_prob_times_values (96x2048x2048x209): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x209): 70.229

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 387.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x210x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x210x2048): 87.852
Elapsed time for attention_prob_times_values (96x2048x2048x210): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x210): 114.646

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 589.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x211x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x211x2048): 62.998
Elapsed time for attention_prob_times_values (96x2048x2048x211): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x211): 71.198

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 397.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x212x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x212x2048): 92.540
Elapsed time for attention_prob_times_values (96x2048x2048x212): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x212): 111.105

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 602.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x213x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x213x2048): 64.490
Elapsed time for attention_prob_times_values (96x2048x2048x213): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x213): 72.525

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 409.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x214x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x214x2048): 79.879
Elapsed time for attention_prob_times_values (96x2048x2048x214): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x214): 11.126

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 117.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x215x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x215x2048): 63.552
Elapsed time for attention_prob_times_values (96x2048x2048x215): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x215): 72.130

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 408.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x216x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x216x2048): 113.413
Elapsed time for attention_prob_times_values (96x2048x2048x216): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x216): 182.168

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 847.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x217x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x217x2048): 64.238
Elapsed time for attention_prob_times_values (96x2048x2048x217): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x217): 69.547

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 406.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x218x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x218x2048): 94.005
Elapsed time for attention_prob_times_values (96x2048x2048x218): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x218): 109.136

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 617.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x219x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x219x2048): 65.987
Elapsed time for attention_prob_times_values (96x2048x2048x219): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x219): 72.742

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 424.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x220x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x220x2048): 94.613
Elapsed time for attention_prob_times_values (96x2048x2048x220): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x220): 123.602

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 659.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x221x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x221x2048): 65.966
Elapsed time for attention_prob_times_values (96x2048x2048x221): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x221): 77.115

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 439.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x222x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x222x2048): 97.649
Elapsed time for attention_prob_times_values (96x2048x2048x222): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x222): 121.041

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 670.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x223x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x223x2048): 67.663
Elapsed time for attention_prob_times_values (96x2048x2048x223): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x223): 76.446

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 446.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x224x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x224x2048): 114.672
Elapsed time for attention_prob_times_values (96x2048x2048x224): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x224): 174.574

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 865.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x225x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x225x2048): 57.069
Elapsed time for attention_prob_times_values (96x2048x2048x225): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x225): 74.058

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 404.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x226x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x226x2048): 92.405
Elapsed time for attention_prob_times_values (96x2048x2048x226): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x226): 120.271

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 658.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x227x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x227x2048): 63.630
Elapsed time for attention_prob_times_values (96x2048x2048x227): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x227): 76.490

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 439.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x228x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x228x2048): 93.404
Elapsed time for attention_prob_times_values (96x2048x2048x228): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x228): 128.310

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 685.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x229x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x229x2048): 64.324
Elapsed time for attention_prob_times_values (96x2048x2048x229): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x229): 79.325

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 452.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x230x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x230x2048): 94.166
Elapsed time for attention_prob_times_values (96x2048x2048x230): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x230): 124.002

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 684.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x231x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x231x2048): 63.850
Elapsed time for attention_prob_times_values (96x2048x2048x231): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x231): 77.668

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 449.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x232x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x232x2048): 119.546
Elapsed time for attention_prob_times_values (96x2048x2048x232): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x232): 194.545

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 953.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x233x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x233x2048): 66.414
Elapsed time for attention_prob_times_values (96x2048x2048x233): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x233): 75.153

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 455.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x234x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x234x2048): 96.388
Elapsed time for attention_prob_times_values (96x2048x2048x234): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x234): 128.018

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 713.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x235x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x235x2048): 65.698
Elapsed time for attention_prob_times_values (96x2048x2048x235): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x235): 78.559

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 465.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x236x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x236x2048): 96.296
Elapsed time for attention_prob_times_values (96x2048x2048x236): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x236): 125.198

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 711.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x237x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x237x2048): 66.251
Elapsed time for attention_prob_times_values (96x2048x2048x237): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x237): 79.157

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 472.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x238x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x238x2048): 96.506
Elapsed time for attention_prob_times_values (96x2048x2048x238): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x238): 123.108

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 711.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x239x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x239x2048): 65.573
Elapsed time for attention_prob_times_values (96x2048x2048x239): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x239): 77.839

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 469.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x240x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x240x2048): 120.043
Elapsed time for attention_prob_times_values (96x2048x2048x240): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x240): 199.815

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 993.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x241x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x241x2048): 66.213
Elapsed time for attention_prob_times_values (96x2048x2048x241): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x241): 82.109

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 487.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x242x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x242x2048): 98.684
Elapsed time for attention_prob_times_values (96x2048x2048x242): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x242): 126.533

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 739.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x243x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x243x2048): 65.437
Elapsed time for attention_prob_times_values (96x2048x2048x243): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x243): 84.408

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 493.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x244x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x244x2048): 99.934
Elapsed time for attention_prob_times_values (96x2048x2048x244): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x244): 127.480

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 752.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x245x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x245x2048): 65.719
Elapsed time for attention_prob_times_values (96x2048x2048x245): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x245): 85.243

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 500.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x246x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x246x2048): 99.774
Elapsed time for attention_prob_times_values (96x2048x2048x246): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x246): 135.174

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 776.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x247x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x247x2048): 69.501
Elapsed time for attention_prob_times_values (96x2048x2048x247): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x247): 84.907

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 518.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x248x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x248x2048): 121.754
Elapsed time for attention_prob_times_values (96x2048x2048x248): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x248): 205.060

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 1040.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x249x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x249x2048): 68.612
Elapsed time for attention_prob_times_values (96x2048x2048x249): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x249): 84.911

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 518.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x250x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x250x2048): 102.119
Elapsed time for attention_prob_times_values (96x2048x2048x250): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x250): 140.996

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 812.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x251x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x251x2048): 71.059
Elapsed time for attention_prob_times_values (96x2048x2048x251): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x251): 84.577

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 531.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x252x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x252x2048): 103.840
Elapsed time for attention_prob_times_values (96x2048x2048x252): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x252): 141.911

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 828.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x253x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x253x2048): 71.517
Elapsed time for attention_prob_times_values (96x2048x2048x253): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x253): 86.907

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 543.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x254x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x254x2048): 104.588
Elapsed time for attention_prob_times_values (96x2048x2048x254): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x254): 117.574

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 769.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x255x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x255x2048): 72.149
Elapsed time for attention_prob_times_values (96x2048x2048x255): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x255): 85.763

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 546.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x256x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x256x2048): 123.492
Elapsed time for attention_prob_times_values (96x2048x2048x256): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x256): 214.192

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 1096.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x257x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x257x2048): 69.384
Elapsed time for attention_prob_times_values (96x2048x2048x257): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x257): 53.018

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 422.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x258x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x258x2048): 98.558
Elapsed time for attention_prob_times_values (96x2048x2048x258): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x258): 94.887

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 681.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x259x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x259x2048): 66.466
Elapsed time for attention_prob_times_values (96x2048x2048x259): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x259): 54.629

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 423.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x260x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x260x2048): 100.148
Elapsed time for attention_prob_times_values (96x2048x2048x260): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x260): 96.010

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 695.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x261x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x261x2048): 66.100
Elapsed time for attention_prob_times_values (96x2048x2048x261): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x261): 55.044

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 427.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x262x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x262x2048): 96.886
Elapsed time for attention_prob_times_values (96x2048x2048x262): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x262): 93.994

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 681.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x263x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x263x2048): 68.975
Elapsed time for attention_prob_times_values (96x2048x2048x263): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x263): 53.495

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 431.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x264x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x264x2048): 124.802
Elapsed time for attention_prob_times_values (96x2048x2048x264): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x264): 130.215

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 916.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x265x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x265x2048): 69.167
Elapsed time for attention_prob_times_values (96x2048x2048x265): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x265): 13.145

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 159.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x266x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x266x2048): 14.210
Elapsed time for attention_prob_times_values (96x2048x2048x266): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x266): 95.612

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 178.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x267x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x267x2048): 70.407
Elapsed time for attention_prob_times_values (96x2048x2048x267): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x267): 56.348

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 454.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x268x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x268x2048): 100.213
Elapsed time for attention_prob_times_values (96x2048x2048x268): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x268): 96.836

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 717.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x269x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x269x2048): 68.589
Elapsed time for attention_prob_times_values (96x2048x2048x269): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x269): 56.909

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 454.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x270x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x270x2048): 102.805
Elapsed time for attention_prob_times_values (96x2048x2048x270): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x270): 91.869

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 711.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x271x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x271x2048): 70.369
Elapsed time for attention_prob_times_values (96x2048x2048x271): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x271): 54.469

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 451.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x272x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x272x2048): 128.566
Elapsed time for attention_prob_times_values (96x2048x2048x272): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x272): 123.221

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 928.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x273x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x273x2048): 68.615
Elapsed time for attention_prob_times_values (96x2048x2048x273): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x273): 47.860

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 417.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x274x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x274x2048): 98.070
Elapsed time for attention_prob_times_values (96x2048x2048x274): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x274): 96.829

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 723.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x275x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x275x2048): 67.900
Elapsed time for attention_prob_times_values (96x2048x2048x275): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x275): 56.759

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 460.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x276x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x276x2048): 104.045
Elapsed time for attention_prob_times_values (96x2048x2048x276): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x276): 93.308

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 734.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x277x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x277x2048): 67.574
Elapsed time for attention_prob_times_values (96x2048x2048x277): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x277): 56.210

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 459.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x278x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x278x2048): 100.439
Elapsed time for attention_prob_times_values (96x2048x2048x278): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x278): 70.017

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 620.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x279x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x279x2048): 59.959
Elapsed time for attention_prob_times_values (96x2048x2048x279): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x279): 55.964

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 436.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x280x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x280x2048): 122.365
Elapsed time for attention_prob_times_values (96x2048x2048x280): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x280): 136.848

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 977.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x281x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x281x2048): 68.828
Elapsed time for attention_prob_times_values (96x2048x2048x281): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x281): 55.157

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 464.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x282x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x282x2048): 98.665
Elapsed time for attention_prob_times_values (96x2048x2048x282): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x282): 95.595

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 738.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x283x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x283x2048): 72.570
Elapsed time for attention_prob_times_values (96x2048x2048x283): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x283): 49.929

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 451.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x284x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x284x2048): 105.379
Elapsed time for attention_prob_times_values (96x2048x2048x284): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x284): 92.255

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 753.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x285x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x285x2048): 71.641
Elapsed time for attention_prob_times_values (96x2048x2048x285): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x285): 58.008

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 492.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x286x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x286x2048): 106.743
Elapsed time for attention_prob_times_values (96x2048x2048x286): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x286): 100.460

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 797.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x287x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x287x2048): 74.045
Elapsed time for attention_prob_times_values (96x2048x2048x287): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x287): 55.515

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 490.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x288x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x288x2048): 128.674
Elapsed time for attention_prob_times_values (96x2048x2048x288): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x288): 128.947

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 998.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x289x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x289x2048): 68.972
Elapsed time for attention_prob_times_values (96x2048x2048x289): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x289): 56.455

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 482.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x290x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x290x2048): 103.567
Elapsed time for attention_prob_times_values (96x2048x2048x290): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x290): 100.640

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 795.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x291x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x291x2048): 68.400
Elapsed time for attention_prob_times_values (96x2048x2048x291): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x291): 59.484

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 497.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x292x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x292x2048): 103.028
Elapsed time for attention_prob_times_values (96x2048x2048x292): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x292): 14.704

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 201.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x293x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x293x2048): 68.239
Elapsed time for attention_prob_times_values (96x2048x2048x293): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x293): 58.222

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 494.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x294x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x294x2048): 103.940
Elapsed time for attention_prob_times_values (96x2048x2048x294): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x294): 97.558

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 794.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x295x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x295x2048): 69.033
Elapsed time for attention_prob_times_values (96x2048x2048x295): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x295): 57.570

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 496.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x296x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x296x2048): 129.491
Elapsed time for attention_prob_times_values (96x2048x2048x296): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x296): 138.056

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 1060.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x297x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x297x2048): 70.661
Elapsed time for attention_prob_times_values (96x2048x2048x297): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x297): 59.000

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 511.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x298x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x298x2048): 105.090
Elapsed time for attention_prob_times_values (96x2048x2048x298): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x298): 102.532

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 828.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x299x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x299x2048): 68.796
Elapsed time for attention_prob_times_values (96x2048x2048x299): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x299): 58.995

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 508.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x300x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x300x2048): 100.129
Elapsed time for attention_prob_times_values (96x2048x2048x300): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x300): 101.626

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 810.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x301x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x301x2048): 67.852
Elapsed time for attention_prob_times_values (96x2048x2048x301): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x301): 34.577

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 368.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x302x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x302x2048): 100.957
Elapsed time for attention_prob_times_values (96x2048x2048x302): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x302): 99.840

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 811.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x303x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x303x2048): 70.013
Elapsed time for attention_prob_times_values (96x2048x2048x303): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x303): 59.077

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 519.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x304x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x304x2048): 40.744
Elapsed time for attention_prob_times_values (96x2048x2048x304): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x304): 143.211

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 515.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x305x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x305x2048): 70.736
Elapsed time for attention_prob_times_values (96x2048x2048x305): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x305): 60.507

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 531.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x306x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x306x2048): 102.731
Elapsed time for attention_prob_times_values (96x2048x2048x306): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x306): 103.420

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 842.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x307x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x307x2048): 67.797
Elapsed time for attention_prob_times_values (96x2048x2048x307): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x307): 59.822

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 520.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x308x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x308x2048): 102.739
Elapsed time for attention_prob_times_values (96x2048x2048x308): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x308): 100.934

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 836.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x309x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x309x2048): 68.167
Elapsed time for attention_prob_times_values (96x2048x2048x309): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x309): 59.525

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 523.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x310x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x310x2048): 102.135
Elapsed time for attention_prob_times_values (96x2048x2048x310): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x310): 101.060

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 839.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x311x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x311x2048): 66.781
Elapsed time for attention_prob_times_values (96x2048x2048x311): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x311): 58.359

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 516.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x312x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x312x2048): 130.571
Elapsed time for attention_prob_times_values (96x2048x2048x312): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x312): 151.414

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 1165.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x313x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x313x2048): 69.954
Elapsed time for attention_prob_times_values (96x2048x2048x313): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x313): 58.578

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 531.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x314x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x314x2048): 109.272
Elapsed time for attention_prob_times_values (96x2048x2048x314): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x314): 107.185

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 904.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x315x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x315x2048): 70.537
Elapsed time for attention_prob_times_values (96x2048x2048x315): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x315): 55.934

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 523.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x316x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x316x2048): 109.304
Elapsed time for attention_prob_times_values (96x2048x2048x316): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x316): 107.175

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 909.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x317x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x317x2048): 69.124
Elapsed time for attention_prob_times_values (96x2048x2048x317): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x317): 62.590

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 553.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x318x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x318x2048): 106.717
Elapsed time for attention_prob_times_values (96x2048x2048x318): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x318): 104.955

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 894.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x319x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x319x2048): 69.426
Elapsed time for attention_prob_times_values (96x2048x2048x319): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x319): 61.606

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 553.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x320x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x320x2048): 135.447
Elapsed time for attention_prob_times_values (96x2048x2048x320): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x320): 157.285

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 1237.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x321x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x321x2048): 70.019
Elapsed time for attention_prob_times_values (96x2048x2048x321): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x321): 63.147

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 566.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x322x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x322x2048): 102.297
Elapsed time for attention_prob_times_values (96x2048x2048x322): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x322): 108.124

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 898.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x323x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x323x2048): 67.671
Elapsed time for attention_prob_times_values (96x2048x2048x323): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x323): 63.129

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 559.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x324x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x324x2048): 103.752
Elapsed time for attention_prob_times_values (96x2048x2048x324): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x324): 109.045

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 913.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x325x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x325x2048): 67.010
Elapsed time for attention_prob_times_values (96x2048x2048x325): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x325): 63.147

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 560.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x326x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x326x2048): 107.323
Elapsed time for attention_prob_times_values (96x2048x2048x326): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x326): 109.369

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 936.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x327x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x327x2048): 70.127
Elapsed time for attention_prob_times_values (96x2048x2048x327): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x327): 61.440

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 567.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x328x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x328x2048): 132.475
Elapsed time for attention_prob_times_values (96x2048x2048x328): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x328): 118.376

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 1086.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x329x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x329x2048): 67.801
Elapsed time for attention_prob_times_values (96x2048x2048x329): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x329): 63.672

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 572.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x330x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x330x2048): 105.214
Elapsed time for attention_prob_times_values (96x2048x2048x330): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x330): 108.852

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 934.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x331x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x331x2048): 70.931
Elapsed time for attention_prob_times_values (96x2048x2048x331): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x331): 63.365

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 586.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x332x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x332x2048): 108.243
Elapsed time for attention_prob_times_values (96x2048x2048x332): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x332): 107.795

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 948.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x333x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x333x2048): 72.106
Elapsed time for attention_prob_times_values (96x2048x2048x333): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x333): 56.916

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 560.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x334x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x334x2048): 106.490
Elapsed time for attention_prob_times_values (96x2048x2048x334): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x334): 111.039

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 959.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x335x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x335x2048): 71.085
Elapsed time for attention_prob_times_values (96x2048x2048x335): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x335): 63.031

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 591.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x336x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x336x2048): 132.428
Elapsed time for attention_prob_times_values (96x2048x2048x336): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x336): 156.108

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 1271.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x337x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x337x2048): 73.533
Elapsed time for attention_prob_times_values (96x2048x2048x337): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x337): 63.315

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 605.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x338x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x338x2048): 15.760
Elapsed time for attention_prob_times_values (96x2048x2048x338): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x338): 112.148

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 246.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x339x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x339x2048): 71.333
Elapsed time for attention_prob_times_values (96x2048x2048x339): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x339): 67.130

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 618.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x340x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x340x2048): 112.106
Elapsed time for attention_prob_times_values (96x2048x2048x340): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x340): 107.631

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 984.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x341x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x341x2048): 72.064
Elapsed time for attention_prob_times_values (96x2048x2048x341): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x341): 64.839

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 613.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x342x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x342x2048): 113.408
Elapsed time for attention_prob_times_values (96x2048x2048x342): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x342): 112.316

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 1017.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x343x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x343x2048): 72.980
Elapsed time for attention_prob_times_values (96x2048x2048x343): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x343): 64.989

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 621.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x344x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x344x2048): 168.738
Elapsed time for attention_prob_times_values (96x2048x2048x344): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x344): 156.573

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 1472.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x345x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x345x2048): 74.290
Elapsed time for attention_prob_times_values (96x2048x2048x345): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x345): 67.053

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 640.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x346x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x346x2048): 114.532
Elapsed time for attention_prob_times_values (96x2048x2048x346): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x346): 110.389

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 1024.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x347x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x347x2048): 73.599
Elapsed time for attention_prob_times_values (96x2048x2048x347): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x347): 68.558

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 648.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x348x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x348x2048): 109.414
Elapsed time for attention_prob_times_values (96x2048x2048x348): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x348): 114.342

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 1023.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x349x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x349x2048): 75.517
Elapsed time for attention_prob_times_values (96x2048x2048x349): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x349): 68.584

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 659.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x350x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x350x2048): 112.579
Elapsed time for attention_prob_times_values (96x2048x2048x350): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x350): 110.827

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 1027.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x351x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x351x2048): 73.730
Elapsed time for attention_prob_times_values (96x2048x2048x351): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x351): 68.841

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 656.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x352x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x352x2048): 198.721
Elapsed time for attention_prob_times_values (96x2048x2048x352): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x352): 168.359

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 1686.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x353x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x353x2048): 73.783
Elapsed time for attention_prob_times_values (96x2048x2048x353): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x353): 50.460

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 555.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x354x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x354x2048): 106.979
Elapsed time for attention_prob_times_values (96x2048x2048x354): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x354): 113.617

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 1024.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x355x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x355x2048): 71.069
Elapsed time for attention_prob_times_values (96x2048x2048x355): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x355): 69.508

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 655.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x356x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x356x2048): 106.952
Elapsed time for attention_prob_times_values (96x2048x2048x356): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x356): 116.687

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 1042.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x357x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x357x2048): 70.944
Elapsed time for attention_prob_times_values (96x2048x2048x357): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x357): 62.279

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 621.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x358x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x358x2048): 111.060
Elapsed time for attention_prob_times_values (96x2048x2048x358): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x358): 117.731

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 1073.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x359x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x359x2048): 71.145
Elapsed time for attention_prob_times_values (96x2048x2048x359): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x359): 66.593

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 647.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x360x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x360x2048): 169.639
Elapsed time for attention_prob_times_values (96x2048x2048x360): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x360): 169.947

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 1602.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x361x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x361x2048): 72.749
Elapsed time for attention_prob_times_values (96x2048x2048x361): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x361): 66.675

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 658.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x362x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x362x2048): 105.580
Elapsed time for attention_prob_times_values (96x2048x2048x362): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x362): 118.746

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 1060.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x363x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x363x2048): 72.697
Elapsed time for attention_prob_times_values (96x2048x2048x363): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x363): 70.636

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 681.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x364x2048): 0.0185
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x364x2048): 15.872
Elapsed time for attention_prob_times_values (96x2048x2048x364): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x364): 113.894

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 265.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x365x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x365x2048): 66.098
Elapsed time for attention_prob_times_values (96x2048x2048x365): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x365): 71.025

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 654.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x366x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x366x2048): 112.069
Elapsed time for attention_prob_times_values (96x2048x2048x366): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x366): 116.024

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 1092.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x367x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x367x2048): 70.453
Elapsed time for attention_prob_times_values (96x2048x2048x367): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x367): 70.429

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 676.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x368x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x368x2048): 187.029
Elapsed time for attention_prob_times_values (96x2048x2048x368): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x368): 169.322

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 1710.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x369x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x369x2048): 71.565
Elapsed time for attention_prob_times_values (96x2048x2048x369): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x369): 69.805

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 681.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x370x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x370x2048): 109.244
Elapsed time for attention_prob_times_values (96x2048x2048x370): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x370): 119.984

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 1106.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x371x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x371x2048): 72.511
Elapsed time for attention_prob_times_values (96x2048x2048x371): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x371): 71.583

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 698.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x372x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x372x2048): 112.399
Elapsed time for attention_prob_times_values (96x2048x2048x372): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x372): 118.650

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 1121.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x373x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x373x2048): 72.952
Elapsed time for attention_prob_times_values (96x2048x2048x373): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x373): 72.249

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 707.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x374x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x374x2048): 110.541
Elapsed time for attention_prob_times_values (96x2048x2048x374): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x374): 122.385

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 1134.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x375x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x375x2048): 71.923
Elapsed time for attention_prob_times_values (96x2048x2048x375): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x375): 71.187

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 700.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x376x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x376x2048): 172.627
Elapsed time for attention_prob_times_values (96x2048x2048x376): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x376): 175.078

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 1705.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x377x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x377x2048): 72.484
Elapsed time for attention_prob_times_values (96x2048x2048x377): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x377): 71.004

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 705.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x378x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x378x2048): 109.538
Elapsed time for attention_prob_times_values (96x2048x2048x378): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x378): 121.458

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 1135.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x379x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x379x2048): 74.014
Elapsed time for attention_prob_times_values (96x2048x2048x379): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x379): 75.232

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 737.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x380x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x380x2048): 114.867
Elapsed time for attention_prob_times_values (96x2048x2048x380): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x380): 125.996

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 1190.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x381x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x381x2048): 73.470
Elapsed time for attention_prob_times_values (96x2048x2048x381): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x381): 18.147

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 289.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x382x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x382x2048): 110.640
Elapsed time for attention_prob_times_values (96x2048x2048x382): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x382): 126.959

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 1176.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x383x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x383x2048): 36.189
Elapsed time for attention_prob_times_values (96x2048x2048x383): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x383): 73.158

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 483.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x384x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x384x2048): 199.146
Elapsed time for attention_prob_times_values (96x2048x2048x384): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x384): 183.456

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 1909.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x385x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x385x2048): 66.463
Elapsed time for attention_prob_times_values (96x2048x2048x385): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x385): 64.316

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 655.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x386x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x386x2048): 107.600
Elapsed time for attention_prob_times_values (96x2048x2048x386): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x386): 125.885

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 1165.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x387x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x387x2048): 71.344
Elapsed time for attention_prob_times_values (96x2048x2048x387): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x387): 74.470

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 733.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x388x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x388x2048): 85.100
Elapsed time for attention_prob_times_values (96x2048x2048x388): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x388): 120.710

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 1007.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x389x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x389x2048): 71.890
Elapsed time for attention_prob_times_values (96x2048x2048x389): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x389): 72.285

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 729.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x390x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x390x2048): 105.833
Elapsed time for attention_prob_times_values (96x2048x2048x390): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x390): 122.301

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1150.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x391x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x391x2048): 73.802
Elapsed time for attention_prob_times_values (96x2048x2048x391): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x391): 70.731

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 734.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x392x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x392x2048): 183.667
Elapsed time for attention_prob_times_values (96x2048x2048x392): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x392): 179.509

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 1849.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x393x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x393x2048): 74.853
Elapsed time for attention_prob_times_values (96x2048x2048x393): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x393): 71.406

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 746.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x394x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x394x2048): 90.569
Elapsed time for attention_prob_times_values (96x2048x2048x394): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x394): 87.542

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 911.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x395x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x395x2048): 70.110
Elapsed time for attention_prob_times_values (96x2048x2048x395): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x395): 69.392

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 715.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x396x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x396x2048): 107.023
Elapsed time for attention_prob_times_values (96x2048x2048x396): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x396): 125.570

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1188.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x397x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x397x2048): 74.016
Elapsed time for attention_prob_times_values (96x2048x2048x397): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x397): 72.933

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 757.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x398x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x398x2048): 65.135
Elapsed time for attention_prob_times_values (96x2048x2048x398): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x398): 99.359

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 812.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x399x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x399x2048): 69.771
Elapsed time for attention_prob_times_values (96x2048x2048x399): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x399): 65.413

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 698.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x400x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x400x2048): 180.457
Elapsed time for attention_prob_times_values (96x2048x2048x400): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x400): 185.079

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 1895.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x401x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x401x2048): 76.753
Elapsed time for attention_prob_times_values (96x2048x2048x401): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x401): 73.099

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 778.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x402x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x402x2048): 106.773
Elapsed time for attention_prob_times_values (96x2048x2048x402): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x402): 109.839

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1128.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x403x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x403x2048): 75.694
Elapsed time for attention_prob_times_values (96x2048x2048x403): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x403): 73.357

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 778.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x404x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x404x2048): 112.395
Elapsed time for attention_prob_times_values (96x2048x2048x404): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x404): 125.769

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1242.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x405x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x405x2048): 75.163
Elapsed time for attention_prob_times_values (96x2048x2048x405): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x405): 76.201

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 794.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x406x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x406x2048): 110.923
Elapsed time for attention_prob_times_values (96x2048x2048x406): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x406): 126.147

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1241.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x407x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x407x2048): 78.234
Elapsed time for attention_prob_times_values (96x2048x2048x407): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x407): 73.755

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 800.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x408x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x408x2048): 176.408
Elapsed time for attention_prob_times_values (96x2048x2048x408): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x408): 187.039

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 1917.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x409x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x409x2048): 76.435
Elapsed time for attention_prob_times_values (96x2048x2048x409): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x409): 74.034

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 796.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x410x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x410x2048): 112.307
Elapsed time for attention_prob_times_values (96x2048x2048x410): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x410): 124.616

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 1253.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x411x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x411x2048): 78.321
Elapsed time for attention_prob_times_values (96x2048x2048x411): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x411): 74.749

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 813.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x412x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x412x2048): 36.648
Elapsed time for attention_prob_times_values (96x2048x2048x412): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x412): 127.589

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 606.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x413x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x413x2048): 78.410
Elapsed time for attention_prob_times_values (96x2048x2048x413): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x413): 75.782

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 823.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x414x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x414x2048): 101.331
Elapsed time for attention_prob_times_values (96x2048x2048x414): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x414): 117.205

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 1163.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x415x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x415x2048): 72.629
Elapsed time for attention_prob_times_values (96x2048x2048x415): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x415): 76.506

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 799.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x416x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x416x2048): 200.847
Elapsed time for attention_prob_times_values (96x2048x2048x416): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x416): 190.267

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 2100.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x417x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x417x2048): 75.450
Elapsed time for attention_prob_times_values (96x2048x2048x417): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x417): 76.545

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 818.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x418x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x418x2048): 95.883
Elapsed time for attention_prob_times_values (96x2048x2048x418): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x418): 129.305

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 1188.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x419x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x419x2048): 75.208
Elapsed time for attention_prob_times_values (96x2048x2048x419): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x419): 77.033

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 823.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x420x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x420x2048): 109.395
Elapsed time for attention_prob_times_values (96x2048x2048x420): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x420): 129.272

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1285.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x421x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x421x2048): 74.807
Elapsed time for attention_prob_times_values (96x2048x2048x421): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x421): 70.726

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 790.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x422x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x422x2048): 104.210
Elapsed time for attention_prob_times_values (96x2048x2048x422): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x422): 109.384

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 1162.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x423x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x423x2048): 75.278
Elapsed time for attention_prob_times_values (96x2048x2048x423): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x423): 77.705

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 834.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x424x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x424x2048): 185.056
Elapsed time for attention_prob_times_values (96x2048x2048x424): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x424): 195.062

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 2077.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x425x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x425x2048): 76.236
Elapsed time for attention_prob_times_values (96x2048x2048x425): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x425): 76.541

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 837.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x426x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x426x2048): 112.652
Elapsed time for attention_prob_times_values (96x2048x2048x426): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x426): 126.330

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 1308.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x427x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x427x2048): 75.897
Elapsed time for attention_prob_times_values (96x2048x2048x427): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x427): 78.193

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 847.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x428x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x428x2048): 105.886
Elapsed time for attention_prob_times_values (96x2048x2048x428): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x428): 127.348

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1275.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x429x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x429x2048): 74.313
Elapsed time for attention_prob_times_values (96x2048x2048x429): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x429): 79.005

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 846.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x430x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x430x2048): 118.787
Elapsed time for attention_prob_times_values (96x2048x2048x430): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x430): 130.331

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 1376.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x431x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x431x2048): 75.688
Elapsed time for attention_prob_times_values (96x2048x2048x431): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x431): 79.988

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 863.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x432x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x432x2048): 189.330
Elapsed time for attention_prob_times_values (96x2048x2048x432): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x432): 197.185

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 2149.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x433x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x433x2048): 75.099
Elapsed time for attention_prob_times_values (96x2048x2048x433): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x433): 78.784

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 857.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x434x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x434x2048): 117.386
Elapsed time for attention_prob_times_values (96x2048x2048x434): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x434): 131.674

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 1386.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x435x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x435x2048): 78.321
Elapsed time for attention_prob_times_values (96x2048x2048x435): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x435): 80.554

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 889.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x436x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x436x2048): 117.438
Elapsed time for attention_prob_times_values (96x2048x2048x436): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x436): 129.194

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1380.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x437x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x437x2048): 78.247
Elapsed time for attention_prob_times_values (96x2048x2048x437): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x437): 44.028

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 633.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x438x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x438x2048): 116.052
Elapsed time for attention_prob_times_values (96x2048x2048x438): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x438): 135.096

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1406.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x439x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x439x2048): 76.022
Elapsed time for attention_prob_times_values (96x2048x2048x439): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x439): 79.020

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 874.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x440x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x440x2048): 186.216
Elapsed time for attention_prob_times_values (96x2048x2048x440): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x440): 196.586

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 2163.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x441x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x441x2048): 79.588
Elapsed time for attention_prob_times_values (96x2048x2048x441): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x441): 78.102

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 893.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x442x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x442x2048): 119.207
Elapsed time for attention_prob_times_values (96x2048x2048x442): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x442): 133.382

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 1430.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x443x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x443x2048): 80.039
Elapsed time for attention_prob_times_values (96x2048x2048x443): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x443): 82.301

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 923.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x444x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x444x2048): 122.454
Elapsed time for attention_prob_times_values (96x2048x2048x444): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x444): 138.081

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1480.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x445x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x445x2048): 80.993
Elapsed time for attention_prob_times_values (96x2048x2048x445): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x445): 83.444

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 939.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x446x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x446x2048): 121.724
Elapsed time for attention_prob_times_values (96x2048x2048x446): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x446): 138.486

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1483.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x447x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x447x2048): 27.834
Elapsed time for attention_prob_times_values (96x2048x2048x447): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x447): 82.034

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 477.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x448x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x448x2048): 181.942
Elapsed time for attention_prob_times_values (96x2048x2048x448): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x448): 203.061

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 2207.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x449x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x449x2048): 74.867
Elapsed time for attention_prob_times_values (96x2048x2048x449): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x449): 80.588

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 894.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x450x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x450x2048): 109.602
Elapsed time for attention_prob_times_values (96x2048x2048x450): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x450): 137.528

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1408.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x451x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x451x2048): 78.020
Elapsed time for attention_prob_times_values (96x2048x2048x451): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x451): 82.853

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 929.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x452x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x452x2048): 111.578
Elapsed time for attention_prob_times_values (96x2048x2048x452): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x452): 133.036

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1407.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x453x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x453x2048): 78.753
Elapsed time for attention_prob_times_values (96x2048x2048x453): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x453): 82.860

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 938.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x454x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x454x2048): 112.615
Elapsed time for attention_prob_times_values (96x2048x2048x454): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x454): 136.043

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1434.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x455x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x455x2048): 78.009
Elapsed time for attention_prob_times_values (96x2048x2048x455): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x455): 82.259

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 934.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x456x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x456x2048): 186.356
Elapsed time for attention_prob_times_values (96x2048x2048x456): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x456): 197.795

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 2242.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x457x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x457x2048): 78.245
Elapsed time for attention_prob_times_values (96x2048x2048x457): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x457): 79.971

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 926.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x458x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x458x2048): 114.295
Elapsed time for attention_prob_times_values (96x2048x2048x458): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x458): 135.617

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1455.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x459x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x459x2048): 80.534
Elapsed time for attention_prob_times_values (96x2048x2048x459): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x459): 82.796

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 960.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x460x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x460x2048): 113.894
Elapsed time for attention_prob_times_values (96x2048x2048x460): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x460): 137.063

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1465.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x461x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x461x2048): 78.623
Elapsed time for attention_prob_times_values (96x2048x2048x461): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x461): 85.768

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 968.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x462x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x462x2048): 117.049
Elapsed time for attention_prob_times_values (96x2048x2048x462): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x462): 136.574

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1491.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x463x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x463x2048): 77.979
Elapsed time for attention_prob_times_values (96x2048x2048x463): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x463): 82.522

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 950.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x464x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x464x2048): 197.188
Elapsed time for attention_prob_times_values (96x2048x2048x464): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x464): 207.172

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 2399.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x465x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x465x2048): 80.644
Elapsed time for attention_prob_times_values (96x2048x2048x465): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x465): 83.942

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 978.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x466x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x466x2048): 116.421
Elapsed time for attention_prob_times_values (96x2048x2048x466): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x466): 134.761

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1489.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x467x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x467x2048): 78.294
Elapsed time for attention_prob_times_values (96x2048x2048x467): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x467): 86.022

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 979.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x468x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x468x2048): 117.765
Elapsed time for attention_prob_times_values (96x2048x2048x468): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x468): 134.958

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1505.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x469x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x469x2048): 79.338
Elapsed time for attention_prob_times_values (96x2048x2048x469): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x469): 84.209

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 979.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x470x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x470x2048): 115.288
Elapsed time for attention_prob_times_values (96x2048x2048x470): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x470): 127.737

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 1456.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x471x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x471x2048): 79.808
Elapsed time for attention_prob_times_values (96x2048x2048x471): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x471): 81.803

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 972.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x472x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x472x2048): 174.371
Elapsed time for attention_prob_times_values (96x2048x2048x472): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x472): 210.911

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 2302.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x473x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x473x2048): 78.861
Elapsed time for attention_prob_times_values (96x2048x2048x473): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x473): 82.273

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 973.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x474x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x474x2048): 113.556
Elapsed time for attention_prob_times_values (96x2048x2048x474): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x474): 140.874

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 1522.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x475x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x475x2048): 79.101
Elapsed time for attention_prob_times_values (96x2048x2048x475): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x475): 81.895

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 976.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x476x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x476x2048): 115.643
Elapsed time for attention_prob_times_values (96x2048x2048x476): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x476): 135.988

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 1519.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x477x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x477x2048): 77.583
Elapsed time for attention_prob_times_values (96x2048x2048x477): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x477): 88.317

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1006.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x478x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x478x2048): 121.549
Elapsed time for attention_prob_times_values (96x2048x2048x478): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x478): 135.973

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1566.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x479x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x479x2048): 80.602
Elapsed time for attention_prob_times_values (96x2048x2048x479): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x479): 84.118

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1006.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x480x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x480x2048): 194.563
Elapsed time for attention_prob_times_values (96x2048x2048x480): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x480): 220.915

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 2534.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x481x2048): 0.0182
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x481x2048): 21.262
Elapsed time for attention_prob_times_values (96x2048x2048x481): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x481): 85.875

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 418.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x482x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x482x2048): 121.342
Elapsed time for attention_prob_times_values (96x2048x2048x482): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x482): 136.022

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 1577.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x483x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x483x2048): 78.593
Elapsed time for attention_prob_times_values (96x2048x2048x483): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x483): 28.946

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 521.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x484x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x484x2048): 121.394
Elapsed time for attention_prob_times_values (96x2048x2048x484): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x484): 141.297

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1611.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x485x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x485x2048): 79.044
Elapsed time for attention_prob_times_values (96x2048x2048x485): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x485): 89.396

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1037.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x486x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x486x2048): 122.587
Elapsed time for attention_prob_times_values (96x2048x2048x486): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x486): 144.415

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1643.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x487x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x487x2048): 77.974
Elapsed time for attention_prob_times_values (96x2048x2048x487): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x487): 87.441

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 1023.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x488x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x488x2048): 189.789
Elapsed time for attention_prob_times_values (96x2048x2048x488): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x488): 215.297

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 2509.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x489x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x489x2048): 79.097
Elapsed time for attention_prob_times_values (96x2048x2048x489): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x489): 87.234

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 1033.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x490x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x490x2048): 124.714
Elapsed time for attention_prob_times_values (96x2048x2048x490): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x490): 145.821

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1678.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x491x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x491x2048): 79.534
Elapsed time for attention_prob_times_values (96x2048x2048x491): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x491): 90.477

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1058.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x492x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x492x2048): 121.657
Elapsed time for attention_prob_times_values (96x2048x2048x492): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x492): 143.944

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1652.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x493x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x493x2048): 82.006
Elapsed time for attention_prob_times_values (96x2048x2048x493): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x493): 88.849

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1070.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x494x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x494x2048): 125.665
Elapsed time for attention_prob_times_values (96x2048x2048x494): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x494): 141.706

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1675.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x495x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x495x2048): 79.094
Elapsed time for attention_prob_times_values (96x2048x2048x495): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x495): 86.446

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1040.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x496x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x496x2048): 170.831
Elapsed time for attention_prob_times_values (96x2048x2048x496): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x496): 227.659

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 2464.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x497x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x497x2048): 81.004
Elapsed time for attention_prob_times_values (96x2048x2048x497): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x497): 89.809

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1077.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x498x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x498x2048): 126.635
Elapsed time for attention_prob_times_values (96x2048x2048x498): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x498): 145.661

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1716.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x499x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x499x2048): 75.544
Elapsed time for attention_prob_times_values (96x2048x2048x499): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x499): 92.649

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1056.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x500x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x500x2048): 128.322
Elapsed time for attention_prob_times_values (96x2048x2048x500): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x500): 146.767

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1741.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x501x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x501x2048): 82.774
Elapsed time for attention_prob_times_values (96x2048x2048x501): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x501): 89.950

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1098.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x502x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x502x2048): 125.796
Elapsed time for attention_prob_times_values (96x2048x2048x502): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x502): 147.778

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1734.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x503x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x503x2048): 82.188
Elapsed time for attention_prob_times_values (96x2048x2048x503): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x503): 90.801

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1103.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x504x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x504x2048): 188.941
Elapsed time for attention_prob_times_values (96x2048x2048x504): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x504): 224.438

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 2628.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x505x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x505x2048): 81.264
Elapsed time for attention_prob_times_values (96x2048x2048x505): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x505): 88.495

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 1087.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x506x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x506x2048): 128.922
Elapsed time for attention_prob_times_values (96x2048x2048x506): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x506): 152.491

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 1796.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x507x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x507x2048): 83.749
Elapsed time for attention_prob_times_values (96x2048x2048x507): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x507): 92.738

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1133.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x508x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x508x2048): 129.013
Elapsed time for attention_prob_times_values (96x2048x2048x508): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x508): 153.986

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 1812.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x509x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x509x2048): 84.042
Elapsed time for attention_prob_times_values (96x2048x2048x509): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x509): 93.667

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1145.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x510x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x510x2048): 130.236
Elapsed time for attention_prob_times_values (96x2048x2048x510): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x510): 151.386

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 1813.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x511x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x511x2048): 83.241
Elapsed time for attention_prob_times_values (96x2048x2048x511): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x511): 93.693

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1143.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x512x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x512x2048): 206.864
Elapsed time for attention_prob_times_values (96x2048x2048x512): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x512): 224.653

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 2800.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x513x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x513x2048): 81.103
Elapsed time for attention_prob_times_values (96x2048x2048x513): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x513): 69.886

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 977.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x514x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x514x2048): 122.289
Elapsed time for attention_prob_times_values (96x2048x2048x514): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x514): 121.374

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 1589.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x515x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x515x2048): 84.140
Elapsed time for attention_prob_times_values (96x2048x2048x515): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x515): 71.665

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1011.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x516x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x516x2048): 124.279
Elapsed time for attention_prob_times_values (96x2048x2048x516): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x516): 118.218

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 1586.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x517x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x517x2048): 83.112
Elapsed time for attention_prob_times_values (96x2048x2048x517): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x517): 73.373

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1022.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x518x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x518x2048): 119.463
Elapsed time for attention_prob_times_values (96x2048x2048x518): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x518): 119.757

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 1571.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x519x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x519x2048): 82.624
Elapsed time for attention_prob_times_values (96x2048x2048x519): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x519): 69.808

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 996.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x520x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x520x2048): 194.918
Elapsed time for attention_prob_times_values (96x2048x2048x520): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x520): 164.211

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 2350.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x521x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x521x2048): 81.827
Elapsed time for attention_prob_times_values (96x2048x2048x521): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x521): 71.269

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1006.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x522x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x522x2048): 121.841
Elapsed time for attention_prob_times_values (96x2048x2048x522): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x522): 116.020

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 1573.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x523x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x523x2048): 82.468
Elapsed time for attention_prob_times_values (96x2048x2048x523): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x523): 72.790

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1025.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x524x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x524x2048): 123.116
Elapsed time for attention_prob_times_values (96x2048x2048x524): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x524): 116.710

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 1591.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x525x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x525x2048): 80.987
Elapsed time for attention_prob_times_values (96x2048x2048x525): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x525): 73.089

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1022.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x526x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x526x2048): 121.432
Elapsed time for attention_prob_times_values (96x2048x2048x526): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x526): 116.986

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 1588.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x527x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x527x2048): 81.277
Elapsed time for attention_prob_times_values (96x2048x2048x527): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x527): 70.755

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1010.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x528x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x528x2048): 193.682
Elapsed time for attention_prob_times_values (96x2048x2048x528): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x528): 173.755

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 2450.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x529x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x529x2048): 82.688
Elapsed time for attention_prob_times_values (96x2048x2048x529): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x529): 71.352

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1026.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x530x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x530x2048): 120.885
Elapsed time for attention_prob_times_values (96x2048x2048x530): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x530): 119.171

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 1610.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x531x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x531x2048): 80.733
Elapsed time for attention_prob_times_values (96x2048x2048x531): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x531): 72.406

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1026.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x532x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x532x2048): 121.521
Elapsed time for attention_prob_times_values (96x2048x2048x532): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x532): 117.439

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1608.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x533x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x533x2048): 71.300
Elapsed time for attention_prob_times_values (96x2048x2048x533): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x533): 68.289

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 941.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x534x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x534x2048): 122.226
Elapsed time for attention_prob_times_values (96x2048x2048x534): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x534): 117.719

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1620.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x535x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x535x2048): 73.479
Elapsed time for attention_prob_times_values (96x2048x2048x535): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x535): 71.462

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 980.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x536x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x536x2048): 195.217
Elapsed time for attention_prob_times_values (96x2048x2048x536): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x536): 170.491

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 2468.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x537x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x537x2048): 81.094
Elapsed time for attention_prob_times_values (96x2048x2048x537): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x537): 69.948

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1020.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x538x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x538x2048): 122.247
Elapsed time for attention_prob_times_values (96x2048x2048x538): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x538): 116.884

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 1626.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x539x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x539x2048): 83.162
Elapsed time for attention_prob_times_values (96x2048x2048x539): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x539): 67.930

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1019.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x540x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x540x2048): 123.776
Elapsed time for attention_prob_times_values (96x2048x2048x540): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x540): 116.868

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1641.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x541x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x541x2048): 82.502
Elapsed time for attention_prob_times_values (96x2048x2048x541): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x541): 73.733

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1065.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x542x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x542x2048): 122.382
Elapsed time for attention_prob_times_values (96x2048x2048x542): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x542): 121.191

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1668.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x543x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x543x2048): 83.718
Elapsed time for attention_prob_times_values (96x2048x2048x543): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x543): 72.474

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1066.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x544x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x544x2048): 198.302
Elapsed time for attention_prob_times_values (96x2048x2048x544): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x544): 176.357

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 2566.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x545x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x545x2048): 81.145
Elapsed time for attention_prob_times_values (96x2048x2048x545): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x545): 70.875

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1042.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x546x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x546x2048): 126.542
Elapsed time for attention_prob_times_values (96x2048x2048x546): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x546): 107.742

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 1605.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x547x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x547x2048): 80.379
Elapsed time for attention_prob_times_values (96x2048x2048x547): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x547): 73.792

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1063.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x548x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x548x2048): 123.374
Elapsed time for attention_prob_times_values (96x2048x2048x548): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x548): 121.169

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1692.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x549x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x549x2048): 80.074
Elapsed time for attention_prob_times_values (96x2048x2048x549): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x549): 75.133

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1075.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x550x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x550x2048): 124.965
Elapsed time for attention_prob_times_values (96x2048x2048x550): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x550): 123.102

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 1722.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x551x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x551x2048): 79.470
Elapsed time for attention_prob_times_values (96x2048x2048x551): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x551): 71.736

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1049.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x552x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x552x2048): 188.771
Elapsed time for attention_prob_times_values (96x2048x2048x552): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x552): 176.111

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 2539.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x553x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x553x2048): 81.641
Elapsed time for attention_prob_times_values (96x2048x2048x553): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x553): 71.971

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1068.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x554x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x554x2048): 123.891
Elapsed time for attention_prob_times_values (96x2048x2048x554): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x554): 121.423

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 1715.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x555x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x555x2048): 81.961
Elapsed time for attention_prob_times_values (96x2048x2048x555): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x555): 74.737

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1095.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x556x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x556x2048): 118.898
Elapsed time for attention_prob_times_values (96x2048x2048x556): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x556): 122.633

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1694.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x557x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x557x2048): 81.501
Elapsed time for attention_prob_times_values (96x2048x2048x557): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x557): 74.398

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1093.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x558x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x558x2048): 129.421
Elapsed time for attention_prob_times_values (96x2048x2048x558): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x558): 120.278

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1755.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x559x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x559x2048): 81.098
Elapsed time for attention_prob_times_values (96x2048x2048x559): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x559): 73.663

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1088.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x560x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x560x2048): 205.086
Elapsed time for attention_prob_times_values (96x2048x2048x560): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x560): 180.712

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 2713.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x561x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x561x2048): 77.561
Elapsed time for attention_prob_times_values (96x2048x2048x561): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x561): 73.768

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1069.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x562x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x562x2048): 125.805
Elapsed time for attention_prob_times_values (96x2048x2048x562): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x562): 123.690

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 1767.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x563x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x563x2048): 81.542
Elapsed time for attention_prob_times_values (96x2048x2048x563): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x563): 75.681

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1114.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x564x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x564x2048): 130.098
Elapsed time for attention_prob_times_values (96x2048x2048x564): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x564): 122.604

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1794.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x565x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x565x2048): 82.184
Elapsed time for attention_prob_times_values (96x2048x2048x565): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x565): 75.057

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1117.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x566x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x566x2048): 128.170
Elapsed time for attention_prob_times_values (96x2048x2048x566): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x566): 118.589

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1757.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x567x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x567x2048): 83.037
Elapsed time for attention_prob_times_values (96x2048x2048x567): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x567): 73.996

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1118.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x568x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x568x2048): 198.565
Elapsed time for attention_prob_times_values (96x2048x2048x568): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x568): 182.059

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 2718.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x569x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x569x2048): 83.824
Elapsed time for attention_prob_times_values (96x2048x2048x569): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x569): 74.225

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1128.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x570x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x570x2048): 130.981
Elapsed time for attention_prob_times_values (96x2048x2048x570): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x570): 124.679

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 1834.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x571x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x571x2048): 82.733
Elapsed time for attention_prob_times_values (96x2048x2048x571): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x571): 75.827

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1138.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x572x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x572x2048): 127.603
Elapsed time for attention_prob_times_values (96x2048x2048x572): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x572): 72.546

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1332.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x573x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x573x2048): 83.348
Elapsed time for attention_prob_times_values (96x2048x2048x573): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x573): 75.887

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1146.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x574x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x574x2048): 127.360
Elapsed time for attention_prob_times_values (96x2048x2048x574): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x574): 122.037

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 1801.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x575x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x575x2048): 83.259
Elapsed time for attention_prob_times_values (96x2048x2048x575): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x575): 74.067

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1134.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x576x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x576x2048): 211.290
Elapsed time for attention_prob_times_values (96x2048x2048x576): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x576): 188.742

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 2891.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x577x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x577x2048): 80.979
Elapsed time for attention_prob_times_values (96x2048x2048x577): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x577): 74.726

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1128.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x578x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x578x2048): 119.240
Elapsed time for attention_prob_times_values (96x2048x2048x578): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x578): 125.438

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 1778.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x579x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x579x2048): 80.827
Elapsed time for attention_prob_times_values (96x2048x2048x579): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x579): 76.103

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1142.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x580x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x580x2048): 124.401
Elapsed time for attention_prob_times_values (96x2048x2048x580): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x580): 124.393

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1815.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x581x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x581x2048): 83.530
Elapsed time for attention_prob_times_values (96x2048x2048x581): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x581): 76.685

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1168.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x582x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x582x2048): 123.909
Elapsed time for attention_prob_times_values (96x2048x2048x582): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x582): 126.468

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1832.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x583x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x583x2048): 82.778
Elapsed time for attention_prob_times_values (96x2048x2048x583): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x583): 73.582

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1142.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x584x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x584x2048): 200.812
Elapsed time for attention_prob_times_values (96x2048x2048x584): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x584): 182.680

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 2809.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x585x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x585x2048): 81.847
Elapsed time for attention_prob_times_values (96x2048x2048x585): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x585): 73.865

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1142.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x586x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x586x2048): 125.155
Elapsed time for attention_prob_times_values (96x2048x2048x586): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x586): 125.592

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1847.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x587x2048): 0.0205
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x587x2048): 23.103
Elapsed time for attention_prob_times_values (96x2048x2048x587): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x587): 77.365

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 525.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x588x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x588x2048): 126.141
Elapsed time for attention_prob_times_values (96x2048x2048x588): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x588): 124.107

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 1849.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x589x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x589x2048): 82.118
Elapsed time for attention_prob_times_values (96x2048x2048x589): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x589): 76.840

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1175.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x590x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x590x2048): 113.391
Elapsed time for attention_prob_times_values (96x2048x2048x590): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x590): 122.096

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 1743.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x591x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x591x2048): 83.527
Elapsed time for attention_prob_times_values (96x2048x2048x591): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x591): 74.647

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1170.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x592x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x592x2048): 205.319
Elapsed time for attention_prob_times_values (96x2048x2048x592): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x592): 190.166

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 2937.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x593x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x593x2048): 83.849
Elapsed time for attention_prob_times_values (96x2048x2048x593): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x593): 75.453

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1183.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x594x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x594x2048): 123.407
Elapsed time for attention_prob_times_values (96x2048x2048x594): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x594): 126.432

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1863.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x595x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x595x2048): 84.421
Elapsed time for attention_prob_times_values (96x2048x2048x595): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x595): 79.445

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1223.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x596x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x596x2048): 126.834
Elapsed time for attention_prob_times_values (96x2048x2048x596): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x596): 125.326

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 1887.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x597x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x597x2048): 83.523
Elapsed time for attention_prob_times_values (96x2048x2048x597): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x597): 78.083

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1210.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x598x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x598x2048): 123.562
Elapsed time for attention_prob_times_values (96x2048x2048x598): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x598): 124.775

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1864.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x599x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x599x2048): 83.278
Elapsed time for attention_prob_times_values (96x2048x2048x599): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x599): 76.156

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1196.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x600x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x600x2048): 194.867
Elapsed time for attention_prob_times_values (96x2048x2048x600): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x600): 186.802

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 2873.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x601x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x601x2048): 83.289
Elapsed time for attention_prob_times_values (96x2048x2048x601): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x601): 76.293

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1201.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x602x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x602x2048): 125.781
Elapsed time for attention_prob_times_values (96x2048x2048x602): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x602): 125.370

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1897.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x603x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x603x2048): 82.240
Elapsed time for attention_prob_times_values (96x2048x2048x603): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x603): 78.436

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1215.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x604x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x604x2048): 129.302
Elapsed time for attention_prob_times_values (96x2048x2048x604): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x604): 128.682

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1955.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x605x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x605x2048): 82.867
Elapsed time for attention_prob_times_values (96x2048x2048x605): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x605): 78.017

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1219.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x606x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x606x2048): 129.198
Elapsed time for attention_prob_times_values (96x2048x2048x606): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x606): 129.722

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 1968.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x607x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x607x2048): 83.273
Elapsed time for attention_prob_times_values (96x2048x2048x607): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x607): 77.673

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1223.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x608x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x608x2048): 214.726
Elapsed time for attention_prob_times_values (96x2048x2048x608): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x608): 196.446

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 3128.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x609x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x609x2048): 83.905
Elapsed time for attention_prob_times_values (96x2048x2048x609): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x609): 66.715

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1135.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x610x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x610x2048): 125.336
Elapsed time for attention_prob_times_values (96x2048x2048x610): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x610): 130.636

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1956.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x611x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x611x2048): 82.445
Elapsed time for attention_prob_times_values (96x2048x2048x611): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x611): 80.324

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1246.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x612x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x612x2048): 130.206
Elapsed time for attention_prob_times_values (96x2048x2048x612): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x612): 127.147

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1974.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x613x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x613x2048): 82.518
Elapsed time for attention_prob_times_values (96x2048x2048x613): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x613): 78.980

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1240.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x614x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x614x2048): 129.806
Elapsed time for attention_prob_times_values (96x2048x2048x614): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x614): 126.566

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1972.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x615x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x615x2048): 81.725
Elapsed time for attention_prob_times_values (96x2048x2048x615): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x615): 76.766

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1220.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x616x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x616x2048): 196.846
Elapsed time for attention_prob_times_values (96x2048x2048x616): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x616): 192.529

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 3005.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x617x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x617x2048): 80.896
Elapsed time for attention_prob_times_values (96x2048x2048x617): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x617): 78.368

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1230.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x618x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x618x2048): 130.171
Elapsed time for attention_prob_times_values (96x2048x2048x618): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x618): 128.299

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 2001.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x619x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x619x2048): 81.351
Elapsed time for attention_prob_times_values (96x2048x2048x619): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x619): 78.578

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1239.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x620x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x620x2048): 129.534
Elapsed time for attention_prob_times_values (96x2048x2048x620): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x620): 117.882

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 1917.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x621x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x621x2048): 80.053
Elapsed time for attention_prob_times_values (96x2048x2048x621): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x621): 80.215

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1246.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x622x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x622x2048): 127.724
Elapsed time for attention_prob_times_values (96x2048x2048x622): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x622): 128.978

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1999.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x623x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x623x2048): 80.538
Elapsed time for attention_prob_times_values (96x2048x2048x623): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x623): 78.244

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1238.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x624x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x624x2048): 211.998
Elapsed time for attention_prob_times_values (96x2048x2048x624): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x624): 194.795

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 3172.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x625x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x625x2048): 80.324
Elapsed time for attention_prob_times_values (96x2048x2048x625): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x625): 78.092

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1239.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x626x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x626x2048): 130.432
Elapsed time for attention_prob_times_values (96x2048x2048x626): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x626): 128.389

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 2027.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x627x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x627x2048): 81.323
Elapsed time for attention_prob_times_values (96x2048x2048x627): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x627): 80.657

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1271.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x628x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x628x2048): 134.551
Elapsed time for attention_prob_times_values (96x2048x2048x628): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x628): 131.335

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 2089.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x629x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x629x2048): 82.087
Elapsed time for attention_prob_times_values (96x2048x2048x629): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x629): 80.061

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1276.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x630x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x630x2048): 128.321
Elapsed time for attention_prob_times_values (96x2048x2048x630): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x630): 132.731

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 2057.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x631x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x631x2048): 79.822
Elapsed time for attention_prob_times_values (96x2048x2048x631): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x631): 78.566

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1250.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x632x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x632x2048): 202.996
Elapsed time for attention_prob_times_values (96x2048x2048x632): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x632): 200.517

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 3190.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x633x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x633x2048): 81.277
Elapsed time for attention_prob_times_values (96x2048x2048x633): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x633): 80.951

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1284.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x634x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x634x2048): 134.076
Elapsed time for attention_prob_times_values (96x2048x2048x634): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x634): 133.833

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 2124.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x635x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x635x2048): 79.521
Elapsed time for attention_prob_times_values (96x2048x2048x635): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x635): 82.640

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1287.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x636x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x636x2048): 130.646
Elapsed time for attention_prob_times_values (96x2048x2048x636): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x636): 132.951

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 2096.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x637x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x637x2048): 79.869
Elapsed time for attention_prob_times_values (96x2048x2048x637): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x637): 81.500

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1285.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x638x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x638x2048): 133.924
Elapsed time for attention_prob_times_values (96x2048x2048x638): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x638): 135.918

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 2152.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x639x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x639x2048): 82.358
Elapsed time for attention_prob_times_values (96x2048x2048x639): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x639): 80.943

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1304.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x640x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x640x2048): 207.519
Elapsed time for attention_prob_times_values (96x2048x2048x640): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x640): 200.941

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 3266.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x641x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x641x2048): 81.312
Elapsed time for attention_prob_times_values (96x2048x2048x641): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x641): 81.081

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1301.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x642x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x642x2048): 127.851
Elapsed time for attention_prob_times_values (96x2048x2048x642): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x642): 134.806

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 2105.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x643x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x643x2048): 78.605
Elapsed time for attention_prob_times_values (96x2048x2048x643): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x643): 82.637

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1294.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x644x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x644x2048): 125.787
Elapsed time for attention_prob_times_values (96x2048x2048x644): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x644): 137.508

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 2114.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x645x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x645x2048): 79.552
Elapsed time for attention_prob_times_values (96x2048x2048x645): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x645): 82.035

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1301.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x646x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x646x2048): 126.956
Elapsed time for attention_prob_times_values (96x2048x2048x646): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x646): 133.447

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2100.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x647x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x647x2048): 78.920
Elapsed time for attention_prob_times_values (96x2048x2048x647): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x647): 80.832

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1290.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x648x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x648x2048): 203.567
Elapsed time for attention_prob_times_values (96x2048x2048x648): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x648): 203.378

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 3293.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x649x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x649x2048): 81.464
Elapsed time for attention_prob_times_values (96x2048x2048x649): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x649): 80.362

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1311.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x650x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x650x2048): 126.273
Elapsed time for attention_prob_times_values (96x2048x2048x650): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x650): 132.092

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2096.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x651x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x651x2048): 81.950
Elapsed time for attention_prob_times_values (96x2048x2048x651): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x651): 80.820

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1323.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x652x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x652x2048): 115.726
Elapsed time for attention_prob_times_values (96x2048x2048x652): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x652): 133.123

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2015.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x653x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x653x2048): 74.996
Elapsed time for attention_prob_times_values (96x2048x2048x653): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x653): 81.579

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1274.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x654x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x654x2048): 127.269
Elapsed time for attention_prob_times_values (96x2048x2048x654): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x654): 133.266

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2125.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x655x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x655x2048): 82.141
Elapsed time for attention_prob_times_values (96x2048x2048x655): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x655): 82.423

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1345.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x656x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x656x2048): 208.465
Elapsed time for attention_prob_times_values (96x2048x2048x656): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x656): 177.745

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 3142.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x657x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x657x2048): 83.966
Elapsed time for attention_prob_times_values (96x2048x2048x657): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x657): 80.641

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1349.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x658x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x658x2048): 130.937
Elapsed time for attention_prob_times_values (96x2048x2048x658): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x658): 133.726

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2172.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x659x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x659x2048): 83.193
Elapsed time for attention_prob_times_values (96x2048x2048x659): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x659): 80.439

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1345.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x660x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x660x2048): 129.791
Elapsed time for attention_prob_times_values (96x2048x2048x660): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x660): 132.577

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2160.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x661x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x661x2048): 83.608
Elapsed time for attention_prob_times_values (96x2048x2048x661): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x661): 83.218

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1375.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x662x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x662x2048): 131.850
Elapsed time for attention_prob_times_values (96x2048x2048x662): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x662): 132.302

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2181.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x663x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x663x2048): 83.817
Elapsed time for attention_prob_times_values (96x2048x2048x663): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x663): 82.547

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1375.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x664x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x664x2048): 205.874
Elapsed time for attention_prob_times_values (96x2048x2048x664): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x664): 209.842

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 3442.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x665x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x665x2048): 86.139
Elapsed time for attention_prob_times_values (96x2048x2048x665): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x665): 80.309

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1378.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x666x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x666x2048): 128.414
Elapsed time for attention_prob_times_values (96x2048x2048x666): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x666): 136.345

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2196.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x667x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x667x2048): 86.072
Elapsed time for attention_prob_times_values (96x2048x2048x667): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x667): 84.566

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1418.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x668x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x668x2048): 129.153
Elapsed time for attention_prob_times_values (96x2048x2048x668): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x668): 132.685

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 2180.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x669x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x669x2048): 85.815
Elapsed time for attention_prob_times_values (96x2048x2048x669): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x669): 84.034

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1416.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x670x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x670x2048): 133.207
Elapsed time for attention_prob_times_values (96x2048x2048x670): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x670): 134.482

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2235.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x671x2048): 0.0217
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x671x2048): 24.908
Elapsed time for attention_prob_times_values (96x2048x2048x671): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x671): 82.635

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 640.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x672x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x672x2048): 220.627
Elapsed time for attention_prob_times_values (96x2048x2048x672): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x672): 201.869

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 3531.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x673x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x673x2048): 85.349
Elapsed time for attention_prob_times_values (96x2048x2048x673): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x673): 82.494

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1407.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x674x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x674x2048): 130.973
Elapsed time for attention_prob_times_values (96x2048x2048x674): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x674): 134.351

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 2227.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x675x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x675x2048): 86.166
Elapsed time for attention_prob_times_values (96x2048x2048x675): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x675): 84.050

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1431.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x676x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x676x2048): 133.927
Elapsed time for attention_prob_times_values (96x2048x2048x676): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x676): 135.364

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2267.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x677x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x677x2048): 84.951
Elapsed time for attention_prob_times_values (96x2048x2048x677): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x677): 84.027

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1425.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x678x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x678x2048): 131.826
Elapsed time for attention_prob_times_values (96x2048x2048x678): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x678): 138.499

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2281.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x679x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x679x2048): 35.398
Elapsed time for attention_prob_times_values (96x2048x2048x679): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x679): 82.889

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 839.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x680x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x680x2048): 206.327
Elapsed time for attention_prob_times_values (96x2048x2048x680): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x680): 211.709

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 3539.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x681x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x681x2048): 86.925
Elapsed time for attention_prob_times_values (96x2048x2048x681): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x681): 83.154

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1441.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x682x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x682x2048): 128.370
Elapsed time for attention_prob_times_values (96x2048x2048x682): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x682): 135.267

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 2237.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x683x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x683x2048): 83.465
Elapsed time for attention_prob_times_values (96x2048x2048x683): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x683): 84.912

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1431.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x684x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x684x2048): 135.671
Elapsed time for attention_prob_times_values (96x2048x2048x684): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x684): 134.534

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 2300.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x685x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x685x2048): 87.143
Elapsed time for attention_prob_times_values (96x2048x2048x685): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x685): 85.709

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1473.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x686x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x686x2048): 133.960
Elapsed time for attention_prob_times_values (96x2048x2048x686): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x686): 133.182

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 2281.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x687x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x687x2048): 86.225
Elapsed time for attention_prob_times_values (96x2048x2048x687): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x687): 83.428

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1450.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x688x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x688x2048): 207.081
Elapsed time for attention_prob_times_values (96x2048x2048x688): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x688): 218.428

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 3640.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x689x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x689x2048): 84.976
Elapsed time for attention_prob_times_values (96x2048x2048x689): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x689): 85.013

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1457.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x690x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x690x2048): 133.361
Elapsed time for attention_prob_times_values (96x2048x2048x690): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x690): 137.079

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 2321.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x691x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x691x2048): 86.020
Elapsed time for attention_prob_times_values (96x2048x2048x691): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x691): 87.619

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1492.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x692x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x692x2048): 136.704
Elapsed time for attention_prob_times_values (96x2048x2048x692): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x692): 139.005

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2373.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x693x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x693x2048): 86.098
Elapsed time for attention_prob_times_values (96x2048x2048x693): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x693): 86.279

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1486.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x694x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x694x2048): 135.946
Elapsed time for attention_prob_times_values (96x2048x2048x694): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x694): 136.811

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 2354.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x695x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x695x2048): 86.932
Elapsed time for attention_prob_times_values (96x2048x2048x695): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x695): 85.249

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1488.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x696x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x696x2048): 206.453
Elapsed time for attention_prob_times_values (96x2048x2048x696): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x696): 214.790

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 3644.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x697x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x697x2048): 86.713
Elapsed time for attention_prob_times_values (96x2048x2048x697): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x697): 83.639

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1476.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x698x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x698x2048): 135.942
Elapsed time for attention_prob_times_values (96x2048x2048x698): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x698): 138.709

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 2383.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x699x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x699x2048): 88.078
Elapsed time for attention_prob_times_values (96x2048x2048x699): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x699): 88.467

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1534.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x700x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x700x2048): 135.838
Elapsed time for attention_prob_times_values (96x2048x2048x700): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x700): 135.534

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 2361.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x701x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x701x2048): 88.346
Elapsed time for attention_prob_times_values (96x2048x2048x701): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x701): 88.880

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1544.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x702x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x702x2048): 132.897
Elapsed time for attention_prob_times_values (96x2048x2048x702): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x702): 136.453

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 2350.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x703x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x703x2048): 86.121
Elapsed time for attention_prob_times_values (96x2048x2048x703): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x703): 87.553

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1517.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x704x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x704x2048): 223.130
Elapsed time for attention_prob_times_values (96x2048x2048x704): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x704): 223.760

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 3910.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x705x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x705x2048): 84.855
Elapsed time for attention_prob_times_values (96x2048x2048x705): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x705): 86.592

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1502.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x706x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x706x2048): 126.561
Elapsed time for attention_prob_times_values (96x2048x2048x706): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x706): 138.893

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 2323.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x707x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x707x2048): 84.428
Elapsed time for attention_prob_times_values (96x2048x2048x707): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x707): 88.006

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1514.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x708x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x708x2048): 126.859
Elapsed time for attention_prob_times_values (96x2048x2048x708): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x708): 137.044

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 2318.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x709x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x709x2048): 84.425
Elapsed time for attention_prob_times_values (96x2048x2048x709): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x709): 86.705

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1507.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x710x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x710x2048): 131.464
Elapsed time for attention_prob_times_values (96x2048x2048x710): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x710): 138.791

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2381.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x711x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x711x2048): 85.528
Elapsed time for attention_prob_times_values (96x2048x2048x711): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x711): 84.138

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1498.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x712x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x712x2048): 205.163
Elapsed time for attention_prob_times_values (96x2048x2048x712): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x712): 216.601

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 3727.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x713x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x713x2048): 83.537
Elapsed time for attention_prob_times_values (96x2048x2048x713): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x713): 84.153

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1484.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x714x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x714x2048): 131.764
Elapsed time for attention_prob_times_values (96x2048x2048x714): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x714): 138.039

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2391.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x715x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x715x2048): 85.668
Elapsed time for attention_prob_times_values (96x2048x2048x715): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x715): 86.150

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1525.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x716x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x716x2048): 132.444
Elapsed time for attention_prob_times_values (96x2048x2048x716): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x716): 139.463

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2415.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x717x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x717x2048): 85.983
Elapsed time for attention_prob_times_values (96x2048x2048x717): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x717): 88.037

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1548.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x718x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x718x2048): 132.235
Elapsed time for attention_prob_times_values (96x2048x2048x718): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x718): 138.148

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 2409.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x719x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x719x2048): 85.228
Elapsed time for attention_prob_times_values (96x2048x2048x719): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x719): 86.793

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1535.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x720x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x720x2048): 217.585
Elapsed time for attention_prob_times_values (96x2048x2048x720): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x720): 227.881

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 3979.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x721x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x721x2048): 84.014
Elapsed time for attention_prob_times_values (96x2048x2048x721): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x721): 86.784

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1528.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x722x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x722x2048): 133.211
Elapsed time for attention_prob_times_values (96x2048x2048x722): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x722): 139.673

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2443.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x723x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x723x2048): 83.684
Elapsed time for attention_prob_times_values (96x2048x2048x723): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x723): 87.794

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1537.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x724x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x724x2048): 134.380
Elapsed time for attention_prob_times_values (96x2048x2048x724): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x724): 141.078

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2473.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x725x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x725x2048): 83.384
Elapsed time for attention_prob_times_values (96x2048x2048x725): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x725): 86.005

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1523.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x726x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x726x2048): 133.716
Elapsed time for attention_prob_times_values (96x2048x2048x726): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x726): 140.255

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2466.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x727x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x727x2048): 83.934
Elapsed time for attention_prob_times_values (96x2048x2048x727): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x727): 81.774

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1494.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x728x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x728x2048): 206.705
Elapsed time for attention_prob_times_values (96x2048x2048x728): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x728): 219.865

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 3848.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x729x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x729x2048): 83.303
Elapsed time for attention_prob_times_values (96x2048x2048x729): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x729): 85.889

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1529.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x730x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x730x2048): 120.821
Elapsed time for attention_prob_times_values (96x2048x2048x730): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x730): 140.577

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 2353.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x731x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x731x2048): 85.127
Elapsed time for attention_prob_times_values (96x2048x2048x731): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x731): 87.149

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1561.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x732x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x732x2048): 129.524
Elapsed time for attention_prob_times_values (96x2048x2048x732): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x732): 141.998

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 2459.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x733x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x733x2048): 85.427
Elapsed time for attention_prob_times_values (96x2048x2048x733): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x733): 87.071

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1567.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x734x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x734x2048): 135.329
Elapsed time for attention_prob_times_values (96x2048x2048x734): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x734): 144.815

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 2546.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x735x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x735x2048): 85.903
Elapsed time for attention_prob_times_values (96x2048x2048x735): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x735): 88.484

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1588.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x736x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x736x2048): 211.393
Elapsed time for attention_prob_times_values (96x2048x2048x736): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x736): 222.519

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 3956.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x737x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x737x2048): 85.917
Elapsed time for attention_prob_times_values (96x2048x2048x737): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x737): 87.221

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1581.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x738x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x738x2048): 129.797
Elapsed time for attention_prob_times_values (96x2048x2048x738): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x738): 141.826

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 2480.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x739x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x739x2048): 83.729
Elapsed time for attention_prob_times_values (96x2048x2048x739): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x739): 88.496

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1576.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x740x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x740x2048): 131.969
Elapsed time for attention_prob_times_values (96x2048x2048x740): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x740): 142.373

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 2512.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x741x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x741x2048): 84.961
Elapsed time for attention_prob_times_values (96x2048x2048x741): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x741): 87.691

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1585.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x742x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x742x2048): 136.819
Elapsed time for attention_prob_times_values (96x2048x2048x742): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x742): 145.273

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2591.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x743x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x743x2048): 85.458
Elapsed time for attention_prob_times_values (96x2048x2048x743): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x743): 85.976

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1578.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x744x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x744x2048): 208.237
Elapsed time for attention_prob_times_values (96x2048x2048x744): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x744): 215.931

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 3909.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x745x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x745x2048): 85.333
Elapsed time for attention_prob_times_values (96x2048x2048x745): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x745): 86.581

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1586.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x746x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x746x2048): 133.398
Elapsed time for attention_prob_times_values (96x2048x2048x746): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x746): 143.055

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 2551.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x747x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x747x2048): 85.414
Elapsed time for attention_prob_times_values (96x2048x2048x747): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x747): 88.928

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1612.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x748x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x748x2048): 139.279
Elapsed time for attention_prob_times_values (96x2048x2048x748): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x748): 142.830

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2613.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x749x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x749x2048): 84.524
Elapsed time for attention_prob_times_values (96x2048x2048x749): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x749): 88.371

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1603.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x750x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x750x2048): 138.749
Elapsed time for attention_prob_times_values (96x2048x2048x750): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x750): 146.525

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2647.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x751x2048): 0.0177
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x751x2048): 34.182
Elapsed time for attention_prob_times_values (96x2048x2048x751): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x751): 83.071

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 900.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x752x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x752x2048): 216.081
Elapsed time for attention_prob_times_values (96x2048x2048x752): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x752): 233.587

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 4181.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x753x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x753x2048): 86.508
Elapsed time for attention_prob_times_values (96x2048x2048x753): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x753): 88.129

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1628.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x754x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x754x2048): 137.582
Elapsed time for attention_prob_times_values (96x2048x2048x754): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x754): 142.231

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 2611.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x755x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x755x2048): 85.674
Elapsed time for attention_prob_times_values (96x2048x2048x755): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x755): 88.834

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1630.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x756x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x756x2048): 135.560
Elapsed time for attention_prob_times_values (96x2048x2048x756): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x756): 140.412

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 2582.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x757x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x757x2048): 85.191
Elapsed time for attention_prob_times_values (96x2048x2048x757): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x757): 81.558

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1561.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x758x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x758x2048): 138.327
Elapsed time for attention_prob_times_values (96x2048x2048x758): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x758): 143.611

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 2644.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x759x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x759x2048): 85.243
Elapsed time for attention_prob_times_values (96x2048x2048x759): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x759): 88.696

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1633.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x760x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x760x2048): 200.113
Elapsed time for attention_prob_times_values (96x2048x2048x760): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x760): 230.250

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 4028.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x761x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x761x2048): 84.097
Elapsed time for attention_prob_times_values (96x2048x2048x761): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x761): 89.341

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1631.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x762x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x762x2048): 139.336
Elapsed time for attention_prob_times_values (96x2048x2048x762): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x762): 148.706

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2713.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x763x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x763x2048): 83.509
Elapsed time for attention_prob_times_values (96x2048x2048x763): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x763): 80.744

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1550.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x764x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x764x2048): 142.975
Elapsed time for attention_prob_times_values (96x2048x2048x764): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x764): 151.193

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 2778.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x765x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x765x2048): 86.538
Elapsed time for attention_prob_times_values (96x2048x2048x765): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x765): 92.558

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1693.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x766x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x766x2048): 142.340
Elapsed time for attention_prob_times_values (96x2048x2048x766): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x766): 152.115

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 2787.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x767x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x767x2048): 86.799
Elapsed time for attention_prob_times_values (96x2048x2048x767): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x767): 91.321

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1688.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x768x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x768x2048): 219.335
Elapsed time for attention_prob_times_values (96x2048x2048x768): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x768): 239.019

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 4346.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x769x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x769x2048): 84.470
Elapsed time for attention_prob_times_values (96x2048x2048x769): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x769): 76.250

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1524.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x770x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x770x2048): 133.513
Elapsed time for attention_prob_times_values (96x2048x2048x770): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x770): 126.432

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 2473.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x771x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x771x2048): 83.809
Elapsed time for attention_prob_times_values (96x2048x2048x771): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x771): 76.637

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1526.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x772x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x772x2048): 133.401
Elapsed time for attention_prob_times_values (96x2048x2048x772): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x772): 127.641

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 2490.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x773x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x773x2048): 83.948
Elapsed time for attention_prob_times_values (96x2048x2048x773): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x773): 76.306

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1528.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x774x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x774x2048): 135.937
Elapsed time for attention_prob_times_values (96x2048x2048x774): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x774): 124.285

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 2485.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x775x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x775x2048): 83.770
Elapsed time for attention_prob_times_values (96x2048x2048x775): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x775): 75.161

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1518.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x776x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x776x2048): 203.976
Elapsed time for attention_prob_times_values (96x2048x2048x776): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x776): 169.540

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 3552.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x777x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x777x2048): 84.307
Elapsed time for attention_prob_times_values (96x2048x2048x777): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x777): 75.124

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1526.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x778x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x778x2048): 133.360
Elapsed time for attention_prob_times_values (96x2048x2048x778): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x778): 123.594

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2467.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x779x2048): 0.0291
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x779x2048): 21.556
Elapsed time for attention_prob_times_values (96x2048x2048x779): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x779): 76.921

Attention duration (in seconds): 0.0373
Attention throughput (in TFLOP/s): 648.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0373
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x780x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x780x2048): 137.613
Elapsed time for attention_prob_times_values (96x2048x2048x780): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x780): 125.571

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 2531.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x781x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x781x2048): 84.610
Elapsed time for attention_prob_times_values (96x2048x2048x781): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x781): 77.098

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1557.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x782x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x782x2048): 134.134
Elapsed time for attention_prob_times_values (96x2048x2048x782): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x782): 126.037

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2511.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x783x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x783x2048): 84.405
Elapsed time for attention_prob_times_values (96x2048x2048x783): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x783): 76.182

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1549.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x784x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x784x2048): 219.779
Elapsed time for attention_prob_times_values (96x2048x2048x784): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x784): 64.074

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1922.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x785x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x785x2048): 85.967
Elapsed time for attention_prob_times_values (96x2048x2048x785): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x785): 76.009

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1565.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x786x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x786x2048): 136.575
Elapsed time for attention_prob_times_values (96x2048x2048x786): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x786): 125.206

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2537.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x787x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x787x2048): 87.417
Elapsed time for attention_prob_times_values (96x2048x2048x787): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x787): 78.154

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1604.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x788x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x788x2048): 138.281
Elapsed time for attention_prob_times_values (96x2048x2048x788): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x788): 126.660

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 2574.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x789x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x789x2048): 85.428
Elapsed time for attention_prob_times_values (96x2048x2048x789): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x789): 77.269

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1581.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x790x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x790x2048): 133.626
Elapsed time for attention_prob_times_values (96x2048x2048x790): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x790): 122.358

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2493.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x791x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x791x2048): 87.292
Elapsed time for attention_prob_times_values (96x2048x2048x791): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x791): 75.448

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1581.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x792x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x792x2048): 204.772
Elapsed time for attention_prob_times_values (96x2048x2048x792): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x792): 187.387

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 3828.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x793x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x793x2048): 85.189
Elapsed time for attention_prob_times_values (96x2048x2048x793): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x793): 76.042

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1573.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x794x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x794x2048): 133.860
Elapsed time for attention_prob_times_values (96x2048x2048x794): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x794): 123.068

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2514.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x795x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x795x2048): 83.430
Elapsed time for attention_prob_times_values (96x2048x2048x795): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x795): 77.537

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1578.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x796x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x796x2048): 135.047
Elapsed time for attention_prob_times_values (96x2048x2048x796): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x796): 125.053

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 2552.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x797x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x797x2048): 85.396
Elapsed time for attention_prob_times_values (96x2048x2048x797): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x797): 78.030

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1604.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x798x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x798x2048): 128.683
Elapsed time for attention_prob_times_values (96x2048x2048x798): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x798): 34.846

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1080.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x799x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x799x2048): 88.531
Elapsed time for attention_prob_times_values (96x2048x2048x799): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x799): 77.681

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1632.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x800x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x800x2048): 214.798
Elapsed time for attention_prob_times_values (96x2048x2048x800): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x800): 188.317

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 3963.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x801x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x801x2048): 85.370
Elapsed time for attention_prob_times_values (96x2048x2048x801): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x801): 78.021

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1612.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x802x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x802x2048): 137.760
Elapsed time for attention_prob_times_values (96x2048x2048x802): 0.0199
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x802): 32.439

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1039.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x803x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x803x2048): 86.464
Elapsed time for attention_prob_times_values (96x2048x2048x803): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x803): 79.216

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1638.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x804x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x804x2048): 139.623
Elapsed time for attention_prob_times_values (96x2048x2048x804): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x804): 128.140

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2651.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x805x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x805x2048): 85.279
Elapsed time for attention_prob_times_values (96x2048x2048x805): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x805): 79.398

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1633.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x806x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x806x2048): 136.812
Elapsed time for attention_prob_times_values (96x2048x2048x806): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x806): 127.299

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2623.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x807x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x807x2048): 85.515
Elapsed time for attention_prob_times_values (96x2048x2048x807): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x807): 77.011

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1613.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x808x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x808x2048): 205.248
Elapsed time for attention_prob_times_values (96x2048x2048x808): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x808): 193.586

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 3972.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x809x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x809x2048): 85.596
Elapsed time for attention_prob_times_values (96x2048x2048x809): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x809): 77.375

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1622.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x810x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x810x2048): 140.911
Elapsed time for attention_prob_times_values (96x2048x2048x810): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x810): 128.920

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2690.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x811x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x811x2048): 85.904
Elapsed time for attention_prob_times_values (96x2048x2048x811): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x811): 80.020

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1657.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x812x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x812x2048): 138.702
Elapsed time for attention_prob_times_values (96x2048x2048x812): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x812): 128.620

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2673.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x813x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x813x2048): 88.261
Elapsed time for attention_prob_times_values (96x2048x2048x813): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x813): 81.728

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1702.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x814x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x814x2048): 137.679
Elapsed time for attention_prob_times_values (96x2048x2048x814): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x814): 127.030

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 2653.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x815x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x815x2048): 86.578
Elapsed time for attention_prob_times_values (96x2048x2048x815): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x815): 78.354

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1653.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x816x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x816x2048): 221.684
Elapsed time for attention_prob_times_values (96x2048x2048x816): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x816): 196.171

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 4188.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x817x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x817x2048): 87.342
Elapsed time for attention_prob_times_values (96x2048x2048x817): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x817): 78.324

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1664.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x818x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x818x2048): 139.982
Elapsed time for attention_prob_times_values (96x2048x2048x818): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x818): 130.710

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2726.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x819x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x819x2048): 89.962
Elapsed time for attention_prob_times_values (96x2048x2048x819): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x819): 81.015

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1721.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x820x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x820x2048): 142.109
Elapsed time for attention_prob_times_values (96x2048x2048x820): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x820): 130.462

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2750.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x821x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x821x2048): 88.998
Elapsed time for attention_prob_times_values (96x2048x2048x821): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x821): 81.220

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1719.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x822x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x822x2048): 139.605
Elapsed time for attention_prob_times_values (96x2048x2048x822): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x822): 124.824

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2671.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x823x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x823x2048): 85.389
Elapsed time for attention_prob_times_values (96x2048x2048x823): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x823): 78.511

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1659.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x824x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x824x2048): 202.564
Elapsed time for attention_prob_times_values (96x2048x2048x824): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x824): 124.030

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 3125.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x825x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x825x2048): 88.276
Elapsed time for attention_prob_times_values (96x2048x2048x825): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x825): 77.354

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1676.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x826x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x826x2048): 138.461
Elapsed time for attention_prob_times_values (96x2048x2048x826): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x826): 131.374

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 2744.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x827x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x827x2048): 88.138
Elapsed time for attention_prob_times_values (96x2048x2048x827): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x827): 71.707

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1611.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x828x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x828x2048): 141.528
Elapsed time for attention_prob_times_values (96x2048x2048x828): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x828): 130.075

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 2766.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x829x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x829x2048): 89.022
Elapsed time for attention_prob_times_values (96x2048x2048x829): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x829): 82.453

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1749.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x830x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x830x2048): 143.416
Elapsed time for attention_prob_times_values (96x2048x2048x830): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x830): 132.190

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 2813.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x831x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x831x2048): 87.428
Elapsed time for attention_prob_times_values (96x2048x2048x831): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x831): 80.283

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1713.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x832x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x832x2048): 220.118
Elapsed time for attention_prob_times_values (96x2048x2048x832): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x832): 204.070

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 4341.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x833x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x833x2048): 88.091
Elapsed time for attention_prob_times_values (96x2048x2048x833): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x833): 80.149

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1722.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x834x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x834x2048): 132.721
Elapsed time for attention_prob_times_values (96x2048x2048x834): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x834): 111.662

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 2492.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x835x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x835x2048): 87.375
Elapsed time for attention_prob_times_values (96x2048x2048x835): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x835): 83.728

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1759.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x836x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x836x2048): 138.223
Elapsed time for attention_prob_times_values (96x2048x2048x836): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x836): 132.390

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2785.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x837x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x837x2048): 88.434
Elapsed time for attention_prob_times_values (96x2048x2048x837): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x837): 83.547

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1771.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x838x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x838x2048): 136.010
Elapsed time for attention_prob_times_values (96x2048x2048x838): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x838): 130.776

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2752.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x839x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x839x2048): 86.563
Elapsed time for attention_prob_times_values (96x2048x2048x839): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x839): 79.343

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1710.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x840x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x840x2048): 206.167
Elapsed time for attention_prob_times_values (96x2048x2048x840): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x840): 196.228

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 4159.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x841x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x841x2048): 86.080
Elapsed time for attention_prob_times_values (96x2048x2048x841): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x841): 80.889

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1727.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x842x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x842x2048): 139.324
Elapsed time for attention_prob_times_values (96x2048x2048x842): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x842): 131.230

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2802.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x843x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x843x2048): 87.229
Elapsed time for attention_prob_times_values (96x2048x2048x843): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x843): 82.942

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1765.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x844x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x844x2048): 137.480
Elapsed time for attention_prob_times_values (96x2048x2048x844): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x844): 133.276

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 2812.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x845x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x845x2048): 86.747
Elapsed time for attention_prob_times_values (96x2048x2048x845): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x845): 82.900

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1763.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x846x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x846x2048): 136.034
Elapsed time for attention_prob_times_values (96x2048x2048x846): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x846): 78.495

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 2073.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x847x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x847x2048): 86.401
Elapsed time for attention_prob_times_values (96x2048x2048x847): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x847): 80.171

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1734.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x848x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x848x2048): 214.666
Elapsed time for attention_prob_times_values (96x2048x2048x848): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x848): 179.579

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 4082.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x849x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x849x2048): 86.516
Elapsed time for attention_prob_times_values (96x2048x2048x849): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x849): 80.845

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1746.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x850x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x850x2048): 138.859
Elapsed time for attention_prob_times_values (96x2048x2048x850): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x850): 115.475

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 2638.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x851x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x851x2048): 87.922
Elapsed time for attention_prob_times_values (96x2048x2048x851): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x851): 82.987

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1788.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x852x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x852x2048): 139.747
Elapsed time for attention_prob_times_values (96x2048x2048x852): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x852): 91.729

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 2322.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x853x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x853x2048): 87.250
Elapsed time for attention_prob_times_values (96x2048x2048x853): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x853): 83.571

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1792.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x854x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x854x2048): 136.128
Elapsed time for attention_prob_times_values (96x2048x2048x854): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x854): 132.972

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 2827.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x855x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x855x2048): 87.442
Elapsed time for attention_prob_times_values (96x2048x2048x855): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x855): 81.403

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1773.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x856x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x856x2048): 207.993
Elapsed time for attention_prob_times_values (96x2048x2048x856): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x856): 192.357

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 4209.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x857x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x857x2048): 86.569
Elapsed time for attention_prob_times_values (96x2048x2048x857): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x857): 81.129

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1766.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x858x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x858x2048): 132.939
Elapsed time for attention_prob_times_values (96x2048x2048x858): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x858): 133.588

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 2813.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x859x2048): 0.0180
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x859x2048): 38.463
Elapsed time for attention_prob_times_values (96x2048x2048x859): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x859): 84.805

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1118.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x860x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x860x2048): 141.340
Elapsed time for attention_prob_times_values (96x2048x2048x860): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x860): 133.053

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2899.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x861x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x861x2048): 87.978
Elapsed time for attention_prob_times_values (96x2048x2048x861): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x861): 84.548

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1826.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x862x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x862x2048): 137.267
Elapsed time for attention_prob_times_values (96x2048x2048x862): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x862): 133.687

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 2872.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x863x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x863x2048): 88.340
Elapsed time for attention_prob_times_values (96x2048x2048x863): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x863): 79.194

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1772.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x864x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x864x2048): 217.867
Elapsed time for attention_prob_times_values (96x2048x2048x864): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x864): 201.598

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 4450.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x865x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x865x2048): 83.943
Elapsed time for attention_prob_times_values (96x2048x2048x865): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x865): 82.276

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1767.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x866x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x866x2048): 139.930
Elapsed time for attention_prob_times_values (96x2048x2048x866): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x866): 133.116

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 2905.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x867x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x867x2048): 86.898
Elapsed time for attention_prob_times_values (96x2048x2048x867): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x867): 85.264

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1835.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x868x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x868x2048): 140.253
Elapsed time for attention_prob_times_values (96x2048x2048x868): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x868): 133.984

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 2925.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x869x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x869x2048): 85.351
Elapsed time for attention_prob_times_values (96x2048x2048x869): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x869): 85.695

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1827.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x870x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x870x2048): 141.948
Elapsed time for attention_prob_times_values (96x2048x2048x870): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x870): 133.288

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 2940.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x871x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x871x2048): 84.937
Elapsed time for attention_prob_times_values (96x2048x2048x871): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x871): 83.643

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1804.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x872x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x872x2048): 214.613
Elapsed time for attention_prob_times_values (96x2048x2048x872): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x872): 205.681

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 4502.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x873x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x873x2048): 86.914
Elapsed time for attention_prob_times_values (96x2048x2048x873): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x873): 82.378

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1815.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x874x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x874x2048): 143.056
Elapsed time for attention_prob_times_values (96x2048x2048x874): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x874): 135.923

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 2994.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x875x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x875x2048): 87.565
Elapsed time for attention_prob_times_values (96x2048x2048x875): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x875): 85.865

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1864.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x876x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x876x2048): 139.740
Elapsed time for attention_prob_times_values (96x2048x2048x876): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x876): 138.239

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 2992.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x877x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x877x2048): 85.325
Elapsed time for attention_prob_times_values (96x2048x2048x877): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x877): 87.212

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1859.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x878x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x878x2048): 139.782
Elapsed time for attention_prob_times_values (96x2048x2048x878): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x878): 135.670

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 2971.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x879x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x879x2048): 87.916
Elapsed time for attention_prob_times_values (96x2048x2048x879): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x879): 83.549

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1850.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x880x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x880x2048): 216.220
Elapsed time for attention_prob_times_values (96x2048x2048x880): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x880): 212.161

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 4631.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x881x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x881x2048): 87.959
Elapsed time for attention_prob_times_values (96x2048x2048x881): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x881): 84.481

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1865.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x882x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x882x2048): 142.358
Elapsed time for attention_prob_times_values (96x2048x2048x882): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x882): 135.206

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 3005.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x883x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x883x2048): 89.662
Elapsed time for attention_prob_times_values (96x2048x2048x883): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x883): 87.866

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1925.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x884x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x884x2048): 143.988
Elapsed time for attention_prob_times_values (96x2048x2048x884): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x884): 138.396

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 3065.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x885x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x885x2048): 88.987
Elapsed time for attention_prob_times_values (96x2048x2048x885): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x885): 87.428

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1917.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x886x2048): 0.0202
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x886x2048): 35.294
Elapsed time for attention_prob_times_values (96x2048x2048x886): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x886): 137.504

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1222.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x887x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x887x2048): 88.222
Elapsed time for attention_prob_times_values (96x2048x2048x887): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x887): 85.408

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1891.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x888x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x888x2048): 214.611
Elapsed time for attention_prob_times_values (96x2048x2048x888): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x888): 206.684

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 4593.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x889x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x889x2048): 88.242
Elapsed time for attention_prob_times_values (96x2048x2048x889): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x889): 85.306

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1894.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x890x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x890x2048): 144.478
Elapsed time for attention_prob_times_values (96x2048x2048x890): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x890): 138.628

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 3092.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x891x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x891x2048): 88.690
Elapsed time for attention_prob_times_values (96x2048x2048x891): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x891): 89.591

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1950.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x892x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x892x2048): 147.244
Elapsed time for attention_prob_times_values (96x2048x2048x892): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x892): 144.553

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 3195.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x893x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x893x2048): 89.896
Elapsed time for attention_prob_times_values (96x2048x2048x893): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x893): 88.727

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1958.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x894x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x894x2048): 146.011
Elapsed time for attention_prob_times_values (96x2048x2048x894): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x894): 144.054

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 3183.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x895x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x895x2048): 90.938
Elapsed time for attention_prob_times_values (96x2048x2048x895): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x895): 87.321

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1957.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x896x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x896x2048): 226.410
Elapsed time for attention_prob_times_values (96x2048x2048x896): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x896): 217.650

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 4882.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x897x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x897x2048): 88.983
Elapsed time for attention_prob_times_values (96x2048x2048x897): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x897): 86.801

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1935.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x898x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x898x2048): 139.188
Elapsed time for attention_prob_times_values (96x2048x2048x898): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x898): 141.976

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 3099.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x899x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x899x2048): 88.970
Elapsed time for attention_prob_times_values (96x2048x2048x899): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x899): 87.716

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1949.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x900x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x900x2048): 138.734
Elapsed time for attention_prob_times_values (96x2048x2048x900): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x900): 140.901

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 3088.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x901x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x901x2048): 88.473
Elapsed time for attention_prob_times_values (96x2048x2048x901): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x901): 86.981

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1940.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x902x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x902x2048): 138.681
Elapsed time for attention_prob_times_values (96x2048x2048x902): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x902): 141.746

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 3104.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x903x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x903x2048): 87.475
Elapsed time for attention_prob_times_values (96x2048x2048x903): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x903): 84.854

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1909.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x904x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x904x2048): 207.069
Elapsed time for attention_prob_times_values (96x2048x2048x904): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x904): 195.758

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 4465.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x905x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x905x2048): 85.484
Elapsed time for attention_prob_times_values (96x2048x2048x905): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x905): 80.324

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1839.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x906x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x906x2048): 138.226
Elapsed time for attention_prob_times_values (96x2048x2048x906): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x906): 137.828

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 3068.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x907x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x907x2048): 87.516
Elapsed time for attention_prob_times_values (96x2048x2048x907): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x907): 50.089

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1418.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x908x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x908x2048): 141.198
Elapsed time for attention_prob_times_values (96x2048x2048x908): 0.0187
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x908): 39.049

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1363.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x909x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x909x2048): 87.277
Elapsed time for attention_prob_times_values (96x2048x2048x909): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x909): 81.686

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1882.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x910x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x910x2048): 139.536
Elapsed time for attention_prob_times_values (96x2048x2048x910): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x910): 138.412

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 3102.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x911x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x911x2048): 85.729
Elapsed time for attention_prob_times_values (96x2048x2048x911): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x911): 83.760

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1893.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x912x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x912x2048): 224.197
Elapsed time for attention_prob_times_values (96x2048x2048x912): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x912): 214.383

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 4904.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x913x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x913x2048): 87.482
Elapsed time for attention_prob_times_values (96x2048x2048x913): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x913): 84.507

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1925.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x914x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x914x2048): 137.804
Elapsed time for attention_prob_times_values (96x2048x2048x914): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x914): 138.677

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 3099.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x915x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x915x2048): 85.812
Elapsed time for attention_prob_times_values (96x2048x2048x915): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x915): 85.638

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1924.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x916x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x916x2048): 139.658
Elapsed time for attention_prob_times_values (96x2048x2048x916): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x916): 140.386

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 3146.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x917x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x917x2048): 88.221
Elapsed time for attention_prob_times_values (96x2048x2048x917): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x917): 85.804

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1956.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x918x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x918x2048): 138.872
Elapsed time for attention_prob_times_values (96x2048x2048x918): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x918): 137.025

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 3105.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x919x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x919x2048): 87.417
Elapsed time for attention_prob_times_values (96x2048x2048x919): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x919): 84.539

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1937.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x920x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x920x2048): 217.824
Elapsed time for attention_prob_times_values (96x2048x2048x920): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x920): 217.383

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 4909.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x921x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x921x2048): 87.664
Elapsed time for attention_prob_times_values (96x2048x2048x921): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x921): 85.108

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1950.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x922x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x922x2048): 139.715
Elapsed time for attention_prob_times_values (96x2048x2048x922): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x922): 138.171

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 3141.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x923x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x923x2048): 87.082
Elapsed time for attention_prob_times_values (96x2048x2048x923): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x923): 85.871

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1957.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x924x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x924x2048): 142.316
Elapsed time for attention_prob_times_values (96x2048x2048x924): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x924): 138.324

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 3178.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x925x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x925x2048): 87.926
Elapsed time for attention_prob_times_values (96x2048x2048x925): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x925): 86.390

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1976.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x926x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x926x2048): 139.806
Elapsed time for attention_prob_times_values (96x2048x2048x926): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x926): 138.451

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 3158.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x927x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x927x2048): 89.196
Elapsed time for attention_prob_times_values (96x2048x2048x927): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x927): 85.956

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1989.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x928x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x928x2048): 230.954
Elapsed time for attention_prob_times_values (96x2048x2048x928): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x928): 220.414

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 5131.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x929x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x929x2048): 83.408
Elapsed time for attention_prob_times_values (96x2048x2048x929): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x929): 85.368

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1921.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x930x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x930x2048): 141.479
Elapsed time for attention_prob_times_values (96x2048x2048x930): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x930): 141.696

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 3227.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x931x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x931x2048): 88.172
Elapsed time for attention_prob_times_values (96x2048x2048x931): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x931): 86.065

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1987.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x932x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x932x2048): 139.682
Elapsed time for attention_prob_times_values (96x2048x2048x932): 0.0203
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x932): 36.981

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1335.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x933x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x933x2048): 86.660
Elapsed time for attention_prob_times_values (96x2048x2048x933): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x933): 86.203

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1976.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x934x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x934x2048): 141.730
Elapsed time for attention_prob_times_values (96x2048x2048x934): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x934): 140.381

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 3228.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x935x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x935x2048): 86.274
Elapsed time for attention_prob_times_values (96x2048x2048x935): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x935): 84.648

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1958.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x936x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x936x2048): 210.979
Elapsed time for attention_prob_times_values (96x2048x2048x936): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x936): 209.609

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 4823.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x937x2048): 0.0276
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x937x2048): 27.355
Elapsed time for attention_prob_times_values (96x2048x2048x937): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x937): 84.564

Attention duration (in seconds): 0.0365
Attention throughput (in TFLOP/s): 949.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x938x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x938x2048): 144.282
Elapsed time for attention_prob_times_values (96x2048x2048x938): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x938): 136.741

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3227.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x939x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x939x2048): 87.911
Elapsed time for attention_prob_times_values (96x2048x2048x939): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x939): 87.663

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2019.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x940x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x940x2048): 141.597
Elapsed time for attention_prob_times_values (96x2048x2048x940): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x940): 140.969

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 3253.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x941x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x941x2048): 86.627
Elapsed time for attention_prob_times_values (96x2048x2048x941): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x941): 87.087

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2002.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x942x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x942x2048): 142.019
Elapsed time for attention_prob_times_values (96x2048x2048x942): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x942): 140.241

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3256.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x943x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x943x2048): 86.636
Elapsed time for attention_prob_times_values (96x2048x2048x943): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x943): 85.515

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1988.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x944x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x944x2048): 225.626
Elapsed time for attention_prob_times_values (96x2048x2048x944): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x944): 224.942

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 5209.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x945x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x945x2048): 86.387
Elapsed time for attention_prob_times_values (96x2048x2048x945): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x945): 84.806

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1981.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x946x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x946x2048): 140.731
Elapsed time for attention_prob_times_values (96x2048x2048x946): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x946): 140.342

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3256.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x947x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x947x2048): 86.341
Elapsed time for attention_prob_times_values (96x2048x2048x947): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x947): 86.662

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2006.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x948x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x948x2048): 142.105
Elapsed time for attention_prob_times_values (96x2048x2048x948): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x948): 140.509

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3280.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x949x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x949x2048): 86.871
Elapsed time for attention_prob_times_values (96x2048x2048x949): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x949): 87.513

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2026.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x950x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x950x2048): 144.379
Elapsed time for attention_prob_times_values (96x2048x2048x950): 0.0284
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x950): 26.944

Attention duration (in seconds): 0.0337
Attention throughput (in TFLOP/s): 1056.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x951x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x951x2048): 86.931
Elapsed time for attention_prob_times_values (96x2048x2048x951): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x951): 85.397

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2006.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x952x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x952x2048): 214.772
Elapsed time for attention_prob_times_values (96x2048x2048x952): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x952): 222.608

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 5096.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x953x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x953x2048): 86.633
Elapsed time for attention_prob_times_values (96x2048x2048x953): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x953): 84.915

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2001.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x954x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x954x2048): 144.746
Elapsed time for attention_prob_times_values (96x2048x2048x954): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x954): 141.101

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3338.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x955x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x955x2048): 85.692
Elapsed time for attention_prob_times_values (96x2048x2048x955): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x955): 88.069

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2031.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x956x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x956x2048): 144.925
Elapsed time for attention_prob_times_values (96x2048x2048x956): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x956): 139.999

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3333.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x957x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x957x2048): 86.402
Elapsed time for attention_prob_times_values (96x2048x2048x957): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x957): 86.759

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2028.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x958x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x958x2048): 140.878
Elapsed time for attention_prob_times_values (96x2048x2048x958): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x958): 143.077

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 3329.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x959x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x959x2048): 86.365
Elapsed time for attention_prob_times_values (96x2048x2048x959): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x959): 86.513

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2029.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x960x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x960x2048): 222.263
Elapsed time for attention_prob_times_values (96x2048x2048x960): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x960): 225.996

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 5266.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x961x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x961x2048): 85.745
Elapsed time for attention_prob_times_values (96x2048x2048x961): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x961): 85.791

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2017.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x962x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x962x2048): 138.423
Elapsed time for attention_prob_times_values (96x2048x2048x962): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x962): 142.891

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 3311.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x963x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x963x2048): 84.706
Elapsed time for attention_prob_times_values (96x2048x2048x963): 0.0224
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x963): 34.585

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 1157.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x964x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x964x2048): 138.617
Elapsed time for attention_prob_times_values (96x2048x2048x964): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x964): 141.459

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3303.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x965x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x965x2048): 85.371
Elapsed time for attention_prob_times_values (96x2048x2048x965): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x965): 87.585

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2042.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x966x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x966x2048): 139.665
Elapsed time for attention_prob_times_values (96x2048x2048x966): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x966): 143.555

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 3347.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x967x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x967x2048): 85.780
Elapsed time for attention_prob_times_values (96x2048x2048x967): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x967): 84.961

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2020.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x968x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x968x2048): 215.375
Elapsed time for attention_prob_times_values (96x2048x2048x968): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x968): 218.938

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 5143.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x969x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x969x2048): 85.993
Elapsed time for attention_prob_times_values (96x2048x2048x969): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x969): 85.159

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2029.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x970x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x970x2048): 137.472
Elapsed time for attention_prob_times_values (96x2048x2048x970): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x970): 140.974

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 3303.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x971x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x971x2048): 88.066
Elapsed time for attention_prob_times_values (96x2048x2048x971): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x971): 87.521

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2085.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x972x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x972x2048): 140.388
Elapsed time for attention_prob_times_values (96x2048x2048x972): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x972): 141.508

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3351.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x973x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x973x2048): 86.421
Elapsed time for attention_prob_times_values (96x2048x2048x973): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x973): 87.184

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2066.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x974x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x974x2048): 140.084
Elapsed time for attention_prob_times_values (96x2048x2048x974): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x974): 141.995

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3360.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x975x2048): 0.0221
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x975x2048): 35.543
Elapsed time for attention_prob_times_values (96x2048x2048x975): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x975): 87.354

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1205.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x976x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x976x2048): 222.957
Elapsed time for attention_prob_times_values (96x2048x2048x976): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x976): 228.267

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 5385.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x977x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x977x2048): 81.193
Elapsed time for attention_prob_times_values (96x2048x2048x977): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x977): 88.477

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2023.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x978x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x978x2048): 141.430
Elapsed time for attention_prob_times_values (96x2048x2048x978): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x978): 143.576

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3408.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x979x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x979x2048): 88.183
Elapsed time for attention_prob_times_values (96x2048x2048x979): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x979): 88.710

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2117.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x980x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x980x2048): 141.176
Elapsed time for attention_prob_times_values (96x2048x2048x980): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x980): 58.654

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1986.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x981x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x981x2048): 89.150
Elapsed time for attention_prob_times_values (96x2048x2048x981): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x981): 88.975

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2136.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x982x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x982x2048): 141.397
Elapsed time for attention_prob_times_values (96x2048x2048x982): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x982): 143.409

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3419.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x983x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x983x2048): 89.575
Elapsed time for attention_prob_times_values (96x2048x2048x983): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x983): 87.311

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2125.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x984x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x984x2048): 45.464
Elapsed time for attention_prob_times_values (96x2048x2048x984): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x984): 221.962

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1815.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x985x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x985x2048): 89.525
Elapsed time for attention_prob_times_values (96x2048x2048x985): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x985): 87.977

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2137.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x986x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x986x2048): 144.324
Elapsed time for attention_prob_times_values (96x2048x2048x986): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x986): 145.306

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 3491.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x987x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x987x2048): 89.339
Elapsed time for attention_prob_times_values (96x2048x2048x987): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x987): 90.132

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2165.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x988x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x988x2048): 145.632
Elapsed time for attention_prob_times_values (96x2048x2048x988): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x988): 144.956

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 3509.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x989x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x989x2048): 86.046
Elapsed time for attention_prob_times_values (96x2048x2048x989): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x989): 86.619

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2087.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x990x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x990x2048): 142.149
Elapsed time for attention_prob_times_values (96x2048x2048x990): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x990): 146.285

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3489.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x991x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x991x2048): 92.705
Elapsed time for attention_prob_times_values (96x2048x2048x991): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x991): 89.906

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2211.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x992x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x992x2048): 232.331
Elapsed time for attention_prob_times_values (96x2048x2048x992): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x992): 237.367

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 5694.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x993x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x993x2048): 92.434
Elapsed time for attention_prob_times_values (96x2048x2048x993): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x993): 89.480

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2207.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x994x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x994x2048): 142.210
Elapsed time for attention_prob_times_values (96x2048x2048x994): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x994): 146.670

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3508.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x995x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x995x2048): 89.419
Elapsed time for attention_prob_times_values (96x2048x2048x995): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x995): 90.833

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2191.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x996x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x996x2048): 143.597
Elapsed time for attention_prob_times_values (96x2048x2048x996): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x996): 147.623

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 3544.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x997x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x997x2048): 89.508
Elapsed time for attention_prob_times_values (96x2048x2048x997): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x997): 91.262

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2202.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x998x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x998x2048): 144.446
Elapsed time for attention_prob_times_values (96x2048x2048x998): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x998): 146.391

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3546.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x999x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x999x2048): 89.550
Elapsed time for attention_prob_times_values (96x2048x2048x999): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x999): 90.810

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2201.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1000x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1000x2048): 214.172
Elapsed time for attention_prob_times_values (96x2048x2048x1000): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1000): 230.434

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 5425.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1001x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1001x2048): 89.853
Elapsed time for attention_prob_times_values (96x2048x2048x1001): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1001): 89.288

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2190.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1002x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1002x2048): 142.810
Elapsed time for attention_prob_times_values (96x2048x2048x1002): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1002): 148.990

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3570.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1003x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1003x2048): 90.688
Elapsed time for attention_prob_times_values (96x2048x2048x1003): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1003): 92.183

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2240.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1004x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1004x2048): 145.546
Elapsed time for attention_prob_times_values (96x2048x2048x1004): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1004): 150.553

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 3630.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1005x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1005x2048): 91.132
Elapsed time for attention_prob_times_values (96x2048x2048x1005): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1005): 92.415

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2253.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1006x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1006x2048): 146.250
Elapsed time for attention_prob_times_values (96x2048x2048x1006): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1006): 152.355

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 3668.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1007x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1007x2048): 89.971
Elapsed time for attention_prob_times_values (96x2048x2048x1007): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1007): 90.377

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2218.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1008x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1008x2048): 227.057
Elapsed time for attention_prob_times_values (96x2048x2048x1008): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1008): 231.882

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 5650.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1009x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1009x2048): 90.733
Elapsed time for attention_prob_times_values (96x2048x2048x1009): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1009): 91.964

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2251.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1010x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1010x2048): 145.921
Elapsed time for attention_prob_times_values (96x2048x2048x1010): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1010): 150.259

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 3652.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1011x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1011x2048): 92.239
Elapsed time for attention_prob_times_values (96x2048x2048x1011): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1011): 94.369

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2303.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1012x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1012x2048): 150.358
Elapsed time for attention_prob_times_values (96x2048x2048x1012): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1012): 151.719

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 3733.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1013x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1013x2048): 92.696
Elapsed time for attention_prob_times_values (96x2048x2048x1013): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1013): 93.925

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2308.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1014x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1014x2048): 151.454
Elapsed time for attention_prob_times_values (96x2048x2048x1014): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1014): 154.313

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 3785.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1015x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1015x2048): 91.474
Elapsed time for attention_prob_times_values (96x2048x2048x1015): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1015): 93.758

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2295.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1016x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1016x2048): 196.819
Elapsed time for attention_prob_times_values (96x2048x2048x1016): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1016): 230.164

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 5264.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1017x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1017x2048): 92.886
Elapsed time for attention_prob_times_values (96x2048x2048x1017): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1017): 95.077

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2333.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1018x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1018x2048): 152.904
Elapsed time for attention_prob_times_values (96x2048x2048x1018): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1018): 155.020

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 3827.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1019x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1019x2048): 94.223
Elapsed time for attention_prob_times_values (96x2048x2048x1019): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1019): 96.444

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2371.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1020x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1020x2048): 152.871
Elapsed time for attention_prob_times_values (96x2048x2048x1020): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1020): 158.119

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 3871.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1021x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1021x2048): 95.548
Elapsed time for attention_prob_times_values (96x2048x2048x1021): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1021): 97.981

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2411.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1022x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1022x2048): 144.550
Elapsed time for attention_prob_times_values (96x2048x2048x1022): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1022): 157.712

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 3764.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1023x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1023x2048): 93.263
Elapsed time for attention_prob_times_values (96x2048x2048x1023): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1023): 97.305

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2378.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1024x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1024x2048): 225.356
Elapsed time for attention_prob_times_values (96x2048x2048x1024): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1024): 244.453

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 5862.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1025x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1025x2048): 92.869
Elapsed time for attention_prob_times_values (96x2048x2048x1025): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1025): 82.381

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2184.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1026x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1026x2048): 144.662
Elapsed time for attention_prob_times_values (96x2048x2048x1026): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1026): 136.982

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 3524.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1027x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1027x2048): 93.547
Elapsed time for attention_prob_times_values (96x2048x2048x1027): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1027): 84.389

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2224.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1028x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1028x2048): 143.684
Elapsed time for attention_prob_times_values (96x2048x2048x1028): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1028): 135.853

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 3504.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1029x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1029x2048): 93.797
Elapsed time for attention_prob_times_values (96x2048x2048x1029): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1029): 84.789

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2237.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1030x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1030x2048): 145.081
Elapsed time for attention_prob_times_values (96x2048x2048x1030): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1030): 136.182

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 3532.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1031x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1031x2048): 92.494
Elapsed time for attention_prob_times_values (96x2048x2048x1031): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1031): 81.632

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2182.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1032x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1032x2048): 199.701
Elapsed time for attention_prob_times_values (96x2048x2048x1032): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1032): 194.889

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 4968.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1033x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1033x2048): 90.644
Elapsed time for attention_prob_times_values (96x2048x2048x1033): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1033): 80.620

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2151.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1034x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1034x2048): 146.174
Elapsed time for attention_prob_times_values (96x2048x2048x1034): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1034): 136.866

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 3567.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1035x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1035x2048): 94.004
Elapsed time for attention_prob_times_values (96x2048x2048x1035): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1035): 82.781

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2223.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1036x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1036x2048): 145.350
Elapsed time for attention_prob_times_values (96x2048x2048x1036): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1036): 132.710

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 3507.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1037x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1037x2048): 92.497
Elapsed time for attention_prob_times_values (96x2048x2048x1037): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1037): 83.640

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2222.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1038x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1038x2048): 145.946
Elapsed time for attention_prob_times_values (96x2048x2048x1038): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1038): 132.679

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 3520.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1039x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1039x2048): 92.490
Elapsed time for attention_prob_times_values (96x2048x2048x1039): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1039): 80.636

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2184.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1040x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1040x2048): 217.538
Elapsed time for attention_prob_times_values (96x2048x2048x1040): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1040): 190.468

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 5153.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1041x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1041x2048): 92.240
Elapsed time for attention_prob_times_values (96x2048x2048x1041): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1041): 81.136

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2192.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1042x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1042x2048): 144.552
Elapsed time for attention_prob_times_values (96x2048x2048x1042): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1042): 133.778

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 3532.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1043x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1043x2048): 93.163
Elapsed time for attention_prob_times_values (96x2048x2048x1043): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1043): 81.879

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2217.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1044x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1044x2048): 142.691
Elapsed time for attention_prob_times_values (96x2048x2048x1044): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1044): 130.363

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 3470.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1045x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1045x2048): 88.657
Elapsed time for attention_prob_times_values (96x2048x2048x1045): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1045): 83.050

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2186.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1046x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1046x2048): 146.528
Elapsed time for attention_prob_times_values (96x2048x2048x1046): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1046): 132.723

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 3553.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1047x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1047x2048): 92.189
Elapsed time for attention_prob_times_values (96x2048x2048x1047): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1047): 79.767

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2184.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1048x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1048x2048): 202.589
Elapsed time for attention_prob_times_values (96x2048x2048x1048): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1048): 189.063

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 4999.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1049x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1049x2048): 90.966
Elapsed time for attention_prob_times_values (96x2048x2048x1049): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1049): 79.852

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2176.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1050x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1050x2048): 146.863
Elapsed time for attention_prob_times_values (96x2048x2048x1050): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1050): 134.351

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 3593.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1051x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1051x2048): 91.238
Elapsed time for attention_prob_times_values (96x2048x2048x1051): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1051): 82.326

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2218.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1052x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1052x2048): 144.509
Elapsed time for attention_prob_times_values (96x2048x2048x1052): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1052): 132.365

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 3544.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1053x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1053x2048): 91.456
Elapsed time for attention_prob_times_values (96x2048x2048x1053): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1053): 83.313

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2239.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1054x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1054x2048): 145.768
Elapsed time for attention_prob_times_values (96x2048x2048x1054): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1054): 133.068

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 3576.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1055x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1055x2048): 92.445
Elapsed time for attention_prob_times_values (96x2048x2048x1055): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1055): 81.010

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2221.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1056x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1056x2048): 216.619
Elapsed time for attention_prob_times_values (96x2048x2048x1056): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1056): 197.347

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 5318.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1057x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1057x2048): 91.068
Elapsed time for attention_prob_times_values (96x2048x2048x1057): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1057): 80.458

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2201.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1058x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1058x2048): 143.639
Elapsed time for attention_prob_times_values (96x2048x2048x1058): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1058): 133.968

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 3576.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1059x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1059x2048): 90.881
Elapsed time for attention_prob_times_values (96x2048x2048x1059): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1059): 82.751

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2236.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1060x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1060x2048): 147.790
Elapsed time for attention_prob_times_values (96x2048x2048x1060): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1060): 133.521

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 3625.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1061x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1061x2048): 89.589
Elapsed time for attention_prob_times_values (96x2048x2048x1061): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1061): 82.848

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2226.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1062x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1062x2048): 144.636
Elapsed time for attention_prob_times_values (96x2048x2048x1062): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1062): 133.024

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 3588.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1063x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1063x2048): 88.543
Elapsed time for attention_prob_times_values (96x2048x2048x1063): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1063): 80.339

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2183.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1064x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1064x2048): 200.551
Elapsed time for attention_prob_times_values (96x2048x2048x1064): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1064): 193.261

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 5105.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1065x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1065x2048): 88.771
Elapsed time for attention_prob_times_values (96x2048x2048x1065): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1065): 79.939

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2183.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1066x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1066x2048): 145.445
Elapsed time for attention_prob_times_values (96x2048x2048x1066): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1066): 133.159

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 3612.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1067x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1067x2048): 88.957
Elapsed time for attention_prob_times_values (96x2048x2048x1067): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1067): 82.375

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2224.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1068x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1068x2048): 145.788
Elapsed time for attention_prob_times_values (96x2048x2048x1068): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1068): 131.911

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 3605.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1069x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1069x2048): 90.231
Elapsed time for attention_prob_times_values (96x2048x2048x1069): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1069): 83.540

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2260.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1070x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1070x2048): 147.329
Elapsed time for attention_prob_times_values (96x2048x2048x1070): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1070): 135.742

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 3684.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1071x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1071x2048): 90.010
Elapsed time for attention_prob_times_values (96x2048x2048x1071): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1071): 80.522

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2218.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1072x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1072x2048): 223.484
Elapsed time for attention_prob_times_values (96x2048x2048x1072): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1072): 201.094

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 5530.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1073x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1073x2048): 88.859
Elapsed time for attention_prob_times_values (96x2048x2048x1073): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1073): 79.944

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2200.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1074x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1074x2048): 148.723
Elapsed time for attention_prob_times_values (96x2048x2048x1074): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1074): 134.178

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 3692.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1075x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1075x2048): 88.452
Elapsed time for attention_prob_times_values (96x2048x2048x1075): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1075): 82.146

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2231.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1076x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1076x2048): 144.855
Elapsed time for attention_prob_times_values (96x2048x2048x1076): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1076): 134.613

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 3658.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1077x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1077x2048): 89.273
Elapsed time for attention_prob_times_values (96x2048x2048x1077): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1077): 82.510

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2250.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1078x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1078x2048): 145.909
Elapsed time for attention_prob_times_values (96x2048x2048x1078): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1078): 132.454

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 3647.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1079x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1079x2048): 89.388
Elapsed time for attention_prob_times_values (96x2048x2048x1079): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1079): 79.945

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2218.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1080x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1080x2048): 202.392
Elapsed time for attention_prob_times_values (96x2048x2048x1080): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1080): 192.644

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 5194.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1081x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1081x2048): 88.017
Elapsed time for attention_prob_times_values (96x2048x2048x1081): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1081): 79.911

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2206.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1082x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1082x2048): 149.049
Elapsed time for attention_prob_times_values (96x2048x2048x1082): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1082): 134.487

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 3727.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1083x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1083x2048): 88.594
Elapsed time for attention_prob_times_values (96x2048x2048x1083): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1083): 82.814

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2258.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1084x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1084x2048): 146.804
Elapsed time for attention_prob_times_values (96x2048x2048x1084): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1084): 133.648

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 3694.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1085x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1085x2048): 89.029
Elapsed time for attention_prob_times_values (96x2048x2048x1085): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1085): 82.753

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2267.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1086x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1086x2048): 146.242
Elapsed time for attention_prob_times_values (96x2048x2048x1086): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1086): 134.741

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 3710.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1087x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1087x2048): 89.965
Elapsed time for attention_prob_times_values (96x2048x2048x1087): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1087): 81.969

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2271.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1088x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1088x2048): 219.951
Elapsed time for attention_prob_times_values (96x2048x2048x1088): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1088): 198.463

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 5529.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1089x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1089x2048): 88.653
Elapsed time for attention_prob_times_values (96x2048x2048x1089): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1089): 81.456

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2251.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1090x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1090x2048): 141.153
Elapsed time for attention_prob_times_values (96x2048x2048x1090): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1090): 136.278

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3681.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1091x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1091x2048): 88.705
Elapsed time for attention_prob_times_values (96x2048x2048x1091): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1091): 82.588

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2272.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1092x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1092x2048): 141.738
Elapsed time for attention_prob_times_values (96x2048x2048x1092): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1092): 136.493

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 3698.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1093x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1093x2048): 85.818
Elapsed time for attention_prob_times_values (96x2048x2048x1093): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1093): 82.563

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2240.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1094x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1094x2048): 141.524
Elapsed time for attention_prob_times_values (96x2048x2048x1094): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1094): 133.031

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3653.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1095x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1095x2048): 85.724
Elapsed time for attention_prob_times_values (96x2048x2048x1095): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1095): 77.514

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2170.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1096x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1096x2048): 200.899
Elapsed time for attention_prob_times_values (96x2048x2048x1096): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1096): 192.090

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 5241.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1097x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1097x2048): 86.766
Elapsed time for attention_prob_times_values (96x2048x2048x1097): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1097): 80.435

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2229.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1098x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1098x2048): 141.673
Elapsed time for attention_prob_times_values (96x2048x2048x1098): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1098): 132.322

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 3658.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1099x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1099x2048): 87.962
Elapsed time for attention_prob_times_values (96x2048x2048x1099): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1099): 83.594

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2293.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1100x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1100x2048): 145.579
Elapsed time for attention_prob_times_values (96x2048x2048x1100): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1100): 135.581

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 3760.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1101x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1101x2048): 88.639
Elapsed time for attention_prob_times_values (96x2048x2048x1101): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1101): 83.071

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2298.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1102x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1102x2048): 143.815
Elapsed time for attention_prob_times_values (96x2048x2048x1102): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1102): 137.544

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 3772.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1103x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1103x2048): 88.247
Elapsed time for attention_prob_times_values (96x2048x2048x1103): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1103): 82.065

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2283.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1104x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1104x2048): 205.953
Elapsed time for attention_prob_times_values (96x2048x2048x1104): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1104): 209.599

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 5583.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1105x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1105x2048): 90.453
Elapsed time for attention_prob_times_values (96x2048x2048x1105): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1105): 81.585

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2307.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1106x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1106x2048): 142.973
Elapsed time for attention_prob_times_values (96x2048x2048x1106): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1106): 138.386

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3786.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1107x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1107x2048): 89.099
Elapsed time for attention_prob_times_values (96x2048x2048x1107): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1107): 83.703

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2325.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1108x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1108x2048): 146.644
Elapsed time for attention_prob_times_values (96x2048x2048x1108): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1108): 138.120

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 3836.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1109x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1109x2048): 90.849
Elapsed time for attention_prob_times_values (96x2048x2048x1109): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1109): 83.292

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2345.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1110x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1110x2048): 144.457
Elapsed time for attention_prob_times_values (96x2048x2048x1110): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1110): 137.441

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3805.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1111x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1111x2048): 89.780
Elapsed time for attention_prob_times_values (96x2048x2048x1111): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1111): 83.148

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2334.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1112x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1112x2048): 203.710
Elapsed time for attention_prob_times_values (96x2048x2048x1112): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1112): 194.119

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 5379.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1113x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1113x2048): 89.686
Elapsed time for attention_prob_times_values (96x2048x2048x1113): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1113): 81.727

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2316.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1114x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1114x2048): 145.308
Elapsed time for attention_prob_times_values (96x2048x2048x1114): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1114): 137.452

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3829.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1115x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1115x2048): 90.634
Elapsed time for attention_prob_times_values (96x2048x2048x1115): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1115): 85.530

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2387.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1116x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1116x2048): 145.862
Elapsed time for attention_prob_times_values (96x2048x2048x1116): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1116): 136.216

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3825.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1117x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1117x2048): 91.342
Elapsed time for attention_prob_times_values (96x2048x2048x1117): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1117): 85.528

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2401.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1118x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1118x2048): 146.634
Elapsed time for attention_prob_times_values (96x2048x2048x1118): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1118): 137.025

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3853.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1119x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1119x2048): 91.817
Elapsed time for attention_prob_times_values (96x2048x2048x1119): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1119): 83.787

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2385.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1120x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1120x2048): 220.778
Elapsed time for attention_prob_times_values (96x2048x2048x1120): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1120): 203.684

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 5773.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1121x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1121x2048): 90.510
Elapsed time for attention_prob_times_values (96x2048x2048x1121): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1121): 83.134

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2363.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1122x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1122x2048): 139.417
Elapsed time for attention_prob_times_values (96x2048x2048x1122): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1122): 137.605

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 3780.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1123x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1123x2048): 89.108
Elapsed time for attention_prob_times_values (96x2048x2048x1123): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1123): 84.546

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2370.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1124x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1124x2048): 148.668
Elapsed time for attention_prob_times_values (96x2048x2048x1124): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1124): 138.555

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 3922.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1125x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1125x2048): 89.847
Elapsed time for attention_prob_times_values (96x2048x2048x1125): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1125): 84.546

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2384.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1126x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1126x2048): 148.080
Elapsed time for attention_prob_times_values (96x2048x2048x1126): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1126): 138.236

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3916.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1127x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1127x2048): 89.623
Elapsed time for attention_prob_times_values (96x2048x2048x1127): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1127): 83.248

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2366.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1128x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1128x2048): 208.815
Elapsed time for attention_prob_times_values (96x2048x2048x1128): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1128): 200.118

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 5607.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1129x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1129x2048): 89.055
Elapsed time for attention_prob_times_values (96x2048x2048x1129): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1129): 84.029

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2374.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1130x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1130x2048): 148.884
Elapsed time for attention_prob_times_values (96x2048x2048x1130): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1130): 140.380

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 3971.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1131x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1131x2048): 90.065
Elapsed time for attention_prob_times_values (96x2048x2048x1131): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1131): 85.864

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2418.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1132x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1132x2048): 148.372
Elapsed time for attention_prob_times_values (96x2048x2048x1132): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1132): 138.560

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3945.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1133x2048): 0.0224
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1133x2048): 40.767
Elapsed time for attention_prob_times_values (96x2048x2048x1133): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1133): 85.683

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 1522.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1134x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1134x2048): 146.061
Elapsed time for attention_prob_times_values (96x2048x2048x1134): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1134): 139.619

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 3937.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1135x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1135x2048): 89.795
Elapsed time for attention_prob_times_values (96x2048x2048x1135): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1135): 84.543

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2403.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1136x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1136x2048): 210.043
Elapsed time for attention_prob_times_values (96x2048x2048x1136): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1136): 207.544

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 5767.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1137x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1137x2048): 90.276
Elapsed time for attention_prob_times_values (96x2048x2048x1137): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1137): 84.761

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2417.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1138x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1138x2048): 148.789
Elapsed time for attention_prob_times_values (96x2048x2048x1138): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1138): 140.219

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3995.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1139x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1139x2048): 90.048
Elapsed time for attention_prob_times_values (96x2048x2048x1139): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1139): 88.043

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2465.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1140x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1140x2048): 148.786
Elapsed time for attention_prob_times_values (96x2048x2048x1140): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1140): 139.731

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 3994.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1141x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1141x2048): 90.502
Elapsed time for attention_prob_times_values (96x2048x2048x1141): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1141): 86.869

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2459.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1142x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1142x2048): 149.058
Elapsed time for attention_prob_times_values (96x2048x2048x1142): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1142): 145.236

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 4084.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1143x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1143x2048): 89.947
Elapsed time for attention_prob_times_values (96x2048x2048x1143): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1143): 85.484

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2435.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1144x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1144x2048): 205.788
Elapsed time for attention_prob_times_values (96x2048x2048x1144): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1144): 203.716

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 5694.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1145x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1145x2048): 91.653
Elapsed time for attention_prob_times_values (96x2048x2048x1145): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1145): 86.254

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2473.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1146x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1146x2048): 149.800
Elapsed time for attention_prob_times_values (96x2048x2048x1146): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1146): 145.226

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 4108.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1147x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1147x2048): 91.259
Elapsed time for attention_prob_times_values (96x2048x2048x1147): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1147): 88.016

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2498.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1148x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1148x2048): 150.120
Elapsed time for attention_prob_times_values (96x2048x2048x1148): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1148): 139.758

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 4039.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1149x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1149x2048): 91.520
Elapsed time for attention_prob_times_values (96x2048x2048x1149): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1149): 89.028

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2520.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1150x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1150x2048): 150.364
Elapsed time for attention_prob_times_values (96x2048x2048x1150): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1150): 142.707

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 4093.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1151x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1151x2048): 91.416
Elapsed time for attention_prob_times_values (96x2048x2048x1151): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1151): 88.432

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2515.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1152x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1152x2048): 218.283
Elapsed time for attention_prob_times_values (96x2048x2048x1152): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1152): 216.317

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 6084.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1153x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1153x2048): 91.057
Elapsed time for attention_prob_times_values (96x2048x2048x1153): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1153): 87.197

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2496.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1154x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1154x2048): 146.168
Elapsed time for attention_prob_times_values (96x2048x2048x1154): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1154): 143.531

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 4062.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1155x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1155x2048): 90.550
Elapsed time for attention_prob_times_values (96x2048x2048x1155): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1155): 88.176

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2507.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1156x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1156x2048): 148.459
Elapsed time for attention_prob_times_values (96x2048x2048x1156): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1156): 141.216

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 4066.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1157x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1157x2048): 90.742
Elapsed time for attention_prob_times_values (96x2048x2048x1157): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1157): 89.602

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2535.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1158x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1158x2048): 143.711
Elapsed time for attention_prob_times_values (96x2048x2048x1158): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1158): 143.790

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 4045.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1159x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1159x2048): 89.652
Elapsed time for attention_prob_times_values (96x2048x2048x1159): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1159): 85.929

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2471.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1160x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1160x2048): 206.163
Elapsed time for attention_prob_times_values (96x2048x2048x1160): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1160): 206.533

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 5816.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1161x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1161x2048): 89.858
Elapsed time for attention_prob_times_values (96x2048x2048x1161): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1161): 86.721

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2489.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1162x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1162x2048): 148.473
Elapsed time for attention_prob_times_values (96x2048x2048x1162): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1162): 141.596

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 4092.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1163x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1163x2048): 91.519
Elapsed time for attention_prob_times_values (96x2048x2048x1163): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1163): 87.663

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2530.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1164x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1164x2048): 147.324
Elapsed time for attention_prob_times_values (96x2048x2048x1164): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1164): 142.157

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 4092.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1165x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1165x2048): 91.423
Elapsed time for attention_prob_times_values (96x2048x2048x1165): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1165): 87.912

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2537.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1166x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1166x2048): 147.910
Elapsed time for attention_prob_times_values (96x2048x2048x1166): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1166): 141.756

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 4100.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1167x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1167x2048): 90.979
Elapsed time for attention_prob_times_values (96x2048x2048x1167): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1167): 86.450

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2513.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1168x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1168x2048): 223.243
Elapsed time for attention_prob_times_values (96x2048x2048x1168): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1168): 212.151

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 6173.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1169x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1169x2048): 92.988
Elapsed time for attention_prob_times_values (96x2048x2048x1169): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1169): 86.199

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2540.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1170x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1170x2048): 148.306
Elapsed time for attention_prob_times_values (96x2048x2048x1170): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1170): 141.884

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 4121.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1171x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1171x2048): 91.945
Elapsed time for attention_prob_times_values (96x2048x2048x1171): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1171): 88.201

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2561.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1172x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1172x2048): 149.108
Elapsed time for attention_prob_times_values (96x2048x2048x1172): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1172): 141.436

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 4132.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1173x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1173x2048): 93.425
Elapsed time for attention_prob_times_values (96x2048x2048x1173): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1173): 89.558

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2605.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1174x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1174x2048): 147.627
Elapsed time for attention_prob_times_values (96x2048x2048x1174): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1174): 141.410

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 4119.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1175x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1175x2048): 91.985
Elapsed time for attention_prob_times_values (96x2048x2048x1175): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1175): 86.065

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2537.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1176x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1176x2048): 206.165
Elapsed time for attention_prob_times_values (96x2048x2048x1176): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1176): 208.480

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 5921.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1177x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1177x2048): 91.914
Elapsed time for attention_prob_times_values (96x2048x2048x1177): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1177): 85.980

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2539.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1178x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1178x2048): 145.738
Elapsed time for attention_prob_times_values (96x2048x2048x1178): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1178): 141.473

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 4107.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1179x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1179x2048): 92.442
Elapsed time for attention_prob_times_values (96x2048x2048x1179): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1179): 89.914

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2610.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1180x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1180x2048): 149.450
Elapsed time for attention_prob_times_values (96x2048x2048x1180): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1180): 142.260

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 4177.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1181x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1181x2048): 93.022
Elapsed time for attention_prob_times_values (96x2048x2048x1181): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1181): 90.609

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2632.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1182x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1182x2048): 147.486
Elapsed time for attention_prob_times_values (96x2048x2048x1182): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1182): 141.006

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 4138.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1183x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1183x2048): 94.757
Elapsed time for attention_prob_times_values (96x2048x2048x1183): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1183): 88.662

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2631.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1184x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1184x2048): 220.475
Elapsed time for attention_prob_times_values (96x2048x2048x1184): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1184): 216.770

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 6284.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1185x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1185x2048): 93.120
Elapsed time for attention_prob_times_values (96x2048x2048x1185): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1185): 87.210

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2591.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1186x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1186x2048): 147.381
Elapsed time for attention_prob_times_values (96x2048x2048x1186): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1186): 142.922

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 4178.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1187x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1187x2048): 91.911
Elapsed time for attention_prob_times_values (96x2048x2048x1187): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1187): 88.857

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2604.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1188x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1188x2048): 147.645
Elapsed time for attention_prob_times_values (96x2048x2048x1188): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1188): 142.239

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 4179.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1189x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1189x2048): 93.075
Elapsed time for attention_prob_times_values (96x2048x2048x1189): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1189): 89.151

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2628.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1190x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1190x2048): 146.274
Elapsed time for attention_prob_times_values (96x2048x2048x1190): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1190): 141.675

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 4158.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1191x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1191x2048): 91.347
Elapsed time for attention_prob_times_values (96x2048x2048x1191): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1191): 87.256

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2580.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1192x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1192x2048): 203.715
Elapsed time for attention_prob_times_values (96x2048x2048x1192): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1192): 204.397

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 5904.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1193x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1193x2048): 92.692
Elapsed time for attention_prob_times_values (96x2048x2048x1193): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1193): 87.421

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2605.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1194x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1194x2048): 148.626
Elapsed time for attention_prob_times_values (96x2048x2048x1194): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1194): 143.089

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 4226.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1195x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1195x2048): 91.927
Elapsed time for attention_prob_times_values (96x2048x2048x1195): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1195): 89.685

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2633.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1196x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1196x2048): 147.215
Elapsed time for attention_prob_times_values (96x2048x2048x1196): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1196): 113.495

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 3721.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1197x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1197x2048): 92.083
Elapsed time for attention_prob_times_values (96x2048x2048x1197): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1197): 90.883

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2657.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1198x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1198x2048): 150.248
Elapsed time for attention_prob_times_values (96x2048x2048x1198): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1198): 144.491

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 4283.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1199x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1199x2048): 93.222
Elapsed time for attention_prob_times_values (96x2048x2048x1199): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1199): 87.436

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2626.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1200x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1200x2048): 211.205
Elapsed time for attention_prob_times_values (96x2048x2048x1200): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1200): 217.390

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 6240.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1201x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1201x2048): 92.116
Elapsed time for attention_prob_times_values (96x2048x2048x1201): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1201): 87.280

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2612.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1202x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1202x2048): 150.137
Elapsed time for attention_prob_times_values (96x2048x2048x1202): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1202): 143.793

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 4285.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1203x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1203x2048): 93.594
Elapsed time for attention_prob_times_values (96x2048x2048x1203): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1203): 90.607

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2688.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1204x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1204x2048): 149.332
Elapsed time for attention_prob_times_values (96x2048x2048x1204): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1204): 144.621

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 4293.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1205x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1205x2048): 94.450
Elapsed time for attention_prob_times_values (96x2048x2048x1205): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1205): 89.729

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2691.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1206x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1206x2048): 145.925
Elapsed time for attention_prob_times_values (96x2048x2048x1206): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1206): 139.868

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4180.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1207x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1207x2048): 91.898
Elapsed time for attention_prob_times_values (96x2048x2048x1207): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1207): 88.409

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2639.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1208x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1208x2048): 201.306
Elapsed time for attention_prob_times_values (96x2048x2048x1208): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1208): 215.636

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 6103.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1209x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1209x2048): 92.306
Elapsed time for attention_prob_times_values (96x2048x2048x1209): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1209): 86.657

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2622.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1210x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1210x2048): 147.887
Elapsed time for attention_prob_times_values (96x2048x2048x1210): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1210): 141.248

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 4242.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1211x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1211x2048): 93.642
Elapsed time for attention_prob_times_values (96x2048x2048x1211): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1211): 89.826

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2694.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1212x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1212x2048): 149.648
Elapsed time for attention_prob_times_values (96x2048x2048x1212): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1212): 143.899

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 4314.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1213x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1213x2048): 92.870
Elapsed time for attention_prob_times_values (96x2048x2048x1213): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1213): 89.806

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2687.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1214x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1214x2048): 150.087
Elapsed time for attention_prob_times_values (96x2048x2048x1214): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1214): 145.533

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 4352.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1215x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1215x2048): 92.430
Elapsed time for attention_prob_times_values (96x2048x2048x1215): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1215): 88.439

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2664.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1216x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1216x2048): 222.147
Elapsed time for attention_prob_times_values (96x2048x2048x1216): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1216): 225.168

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 6597.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1217x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1217x2048): 93.447
Elapsed time for attention_prob_times_values (96x2048x2048x1217): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1217): 86.884

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2658.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1218x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1218x2048): 146.329
Elapsed time for attention_prob_times_values (96x2048x2048x1218): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1218): 145.275

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 4307.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1219x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1219x2048): 91.659
Elapsed time for attention_prob_times_values (96x2048x2048x1219): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1219): 89.336

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2675.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1220x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1220x2048): 147.768
Elapsed time for attention_prob_times_values (96x2048x2048x1220): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1220): 144.813

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 4328.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1221x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1221x2048): 92.088
Elapsed time for attention_prob_times_values (96x2048x2048x1221): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1221): 86.733

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2645.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1222x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1222x2048): 147.729
Elapsed time for attention_prob_times_values (96x2048x2048x1222): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1222): 141.079

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4277.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1223x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1223x2048): 91.060
Elapsed time for attention_prob_times_values (96x2048x2048x1223): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1223): 86.830

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2636.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1224x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1224x2048): 207.379
Elapsed time for attention_prob_times_values (96x2048x2048x1224): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1224): 224.252

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 6397.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1225x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1225x2048): 89.084
Elapsed time for attention_prob_times_values (96x2048x2048x1225): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1225): 85.472

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2591.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1226x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1226x2048): 144.243
Elapsed time for attention_prob_times_values (96x2048x2048x1226): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1226): 142.148

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 4257.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1227x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1227x2048): 91.059
Elapsed time for attention_prob_times_values (96x2048x2048x1227): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1227): 89.424

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2685.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1228x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1228x2048): 146.163
Elapsed time for attention_prob_times_values (96x2048x2048x1228): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1228): 143.911

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4319.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1229x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1229x2048): 90.507
Elapsed time for attention_prob_times_values (96x2048x2048x1229): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1229): 87.984

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2659.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1230x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1230x2048): 146.411
Elapsed time for attention_prob_times_values (96x2048x2048x1230): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1230): 139.715

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 4264.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1231x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1231x2048): 90.057
Elapsed time for attention_prob_times_values (96x2048x2048x1231): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1231): 87.499

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2649.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1232x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1232x2048): 219.560
Elapsed time for attention_prob_times_values (96x2048x2048x1232): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1232): 219.676

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 6561.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1233x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1233x2048): 91.819
Elapsed time for attention_prob_times_values (96x2048x2048x1233): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1233): 86.108

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2657.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1234x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1234x2048): 148.468
Elapsed time for attention_prob_times_values (96x2048x2048x1234): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1234): 145.133

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 4391.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1235x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1235x2048): 90.311
Elapsed time for attention_prob_times_values (96x2048x2048x1235): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1235): 89.254

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2688.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1236x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1236x2048): 148.703
Elapsed time for attention_prob_times_values (96x2048x2048x1236): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1236): 143.160

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4371.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1237x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1237x2048): 90.485
Elapsed time for attention_prob_times_values (96x2048x2048x1237): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1237): 87.914

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2674.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1238x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1238x2048): 147.120
Elapsed time for attention_prob_times_values (96x2048x2048x1238): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1238): 142.401

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 4343.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1239x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1239x2048): 90.019
Elapsed time for attention_prob_times_values (96x2048x2048x1239): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1239): 87.873

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2671.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1240x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1240x2048): 205.927
Elapsed time for attention_prob_times_values (96x2048x2048x1240): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1240): 217.769

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 6363.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1241x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1241x2048): 90.382
Elapsed time for attention_prob_times_values (96x2048x2048x1241): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1241): 86.383

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2657.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1242x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1242x2048): 147.621
Elapsed time for attention_prob_times_values (96x2048x2048x1242): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1242): 143.638

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 4384.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1243x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1243x2048): 90.309
Elapsed time for attention_prob_times_values (96x2048x2048x1243): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1243): 88.804

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2698.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1244x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1244x2048): 149.762
Elapsed time for attention_prob_times_values (96x2048x2048x1244): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1244): 145.062

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4444.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1245x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1245x2048): 91.204
Elapsed time for attention_prob_times_values (96x2048x2048x1245): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1245): 88.388

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2709.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1246x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1246x2048): 148.683
Elapsed time for attention_prob_times_values (96x2048x2048x1246): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1246): 143.904

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 4417.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1247x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1247x2048): 92.348
Elapsed time for attention_prob_times_values (96x2048x2048x1247): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1247): 88.808

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2736.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1248x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1248x2048): 213.978
Elapsed time for attention_prob_times_values (96x2048x2048x1248): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1248): 222.388

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 6597.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1249x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1249x2048): 87.455
Elapsed time for attention_prob_times_values (96x2048x2048x1249): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1249): 86.395

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2631.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1250x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1250x2048): 148.374
Elapsed time for attention_prob_times_values (96x2048x2048x1250): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1250): 144.287

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 4432.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1251x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1251x2048): 90.511
Elapsed time for attention_prob_times_values (96x2048x2048x1251): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1251): 88.738

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2717.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1252x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1252x2048): 148.522
Elapsed time for attention_prob_times_values (96x2048x2048x1252): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1252): 140.841

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 4387.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1253x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1253x2048): 90.539
Elapsed time for attention_prob_times_values (96x2048x2048x1253): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1253): 88.969

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2725.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1254x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1254x2048): 150.226
Elapsed time for attention_prob_times_values (96x2048x2048x1254): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1254): 146.248

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4504.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1255x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1255x2048): 90.496
Elapsed time for attention_prob_times_values (96x2048x2048x1255): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1255): 86.243

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2686.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1256x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1256x2048): 209.853
Elapsed time for attention_prob_times_values (96x2048x2048x1256): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1256): 221.477

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 6559.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1257x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1257x2048): 89.706
Elapsed time for attention_prob_times_values (96x2048x2048x1257): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1257): 86.296

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2679.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1258x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1258x2048): 148.707
Elapsed time for attention_prob_times_values (96x2048x2048x1258): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1258): 145.379

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 4481.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1259x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1259x2048): 89.738
Elapsed time for attention_prob_times_values (96x2048x2048x1259): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1259): 88.051

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2711.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1260x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1260x2048): 149.540
Elapsed time for attention_prob_times_values (96x2048x2048x1260): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1260): 147.391

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 4532.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1261x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1261x2048): 90.919
Elapsed time for attention_prob_times_values (96x2048x2048x1261): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1261): 86.802

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2713.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1262x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1262x2048): 148.364
Elapsed time for attention_prob_times_values (96x2048x2048x1262): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1262): 145.488

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 4492.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1263x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1263x2048): 89.422
Elapsed time for attention_prob_times_values (96x2048x2048x1263): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1263): 83.970

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2650.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1264x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1264x2048): 211.860
Elapsed time for attention_prob_times_values (96x2048x2048x1264): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1264): 220.630

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 6619.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1265x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1265x2048): 89.122
Elapsed time for attention_prob_times_values (96x2048x2048x1265): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1265): 87.512

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2706.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1266x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1266x2048): 151.524
Elapsed time for attention_prob_times_values (96x2048x2048x1266): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1266): 132.111

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 4329.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1267x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1267x2048): 88.355
Elapsed time for attention_prob_times_values (96x2048x2048x1267): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1267): 88.216

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2709.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1268x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1268x2048): 150.357
Elapsed time for attention_prob_times_values (96x2048x2048x1268): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1268): 149.174

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4600.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1269x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1269x2048): 89.892
Elapsed time for attention_prob_times_values (96x2048x2048x1269): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1269): 88.305

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2738.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1270x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1270x2048): 153.548
Elapsed time for attention_prob_times_values (96x2048x2048x1270): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1270): 152.165

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 4702.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1271x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1271x2048): 87.665
Elapsed time for attention_prob_times_values (96x2048x2048x1271): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1271): 89.301

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2724.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1272x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1272x2048): 194.462
Elapsed time for attention_prob_times_values (96x2048x2048x1272): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1272): 221.260

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 6378.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1273x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1273x2048): 87.159
Elapsed time for attention_prob_times_values (96x2048x2048x1273): 0.0236
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1273): 43.388

Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 1786.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1274x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1274x2048): 148.735
Elapsed time for attention_prob_times_values (96x2048x2048x1274): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1274): 151.676

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 4634.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1275x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1275x2048): 87.188
Elapsed time for attention_prob_times_values (96x2048x2048x1275): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1275): 89.522

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2728.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1276x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1276x2048): 153.718
Elapsed time for attention_prob_times_values (96x2048x2048x1276): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1276): 147.910

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4659.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1277x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1277x2048): 87.896
Elapsed time for attention_prob_times_values (96x2048x2048x1277): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1277): 89.520

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2743.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1278x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1278x2048): 152.647
Elapsed time for attention_prob_times_values (96x2048x2048x1278): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1278): 150.397

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 4689.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1279x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1279x2048): 88.592
Elapsed time for attention_prob_times_values (96x2048x2048x1279): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1279): 89.565

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2759.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1280x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1280x2048): 221.205
Elapsed time for attention_prob_times_values (96x2048x2048x1280): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1280): 229.179

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 6978.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1281x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1281x2048): 88.346
Elapsed time for attention_prob_times_values (96x2048x2048x1281): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1281): 82.959

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2654.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1282x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1282x2048): 149.030
Elapsed time for attention_prob_times_values (96x2048x2048x1282): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1282): 138.063

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 4450.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1283x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1283x2048): 88.372
Elapsed time for attention_prob_times_values (96x2048x2048x1283): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1283): 83.409

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2666.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1284x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1284x2048): 148.501
Elapsed time for attention_prob_times_values (96x2048x2048x1284): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1284): 136.194

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 4417.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1285x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1285x2048): 87.742
Elapsed time for attention_prob_times_values (96x2048x2048x1285): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1285): 83.083

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2655.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1286x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1286x2048): 147.922
Elapsed time for attention_prob_times_values (96x2048x2048x1286): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1286): 136.535

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 4421.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1287x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1287x2048): 86.972
Elapsed time for attention_prob_times_values (96x2048x2048x1287): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1287): 81.923

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2629.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1288x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1288x2048): 208.396
Elapsed time for attention_prob_times_values (96x2048x2048x1288): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1288): 195.457

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 6291.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1289x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1289x2048): 87.657
Elapsed time for attention_prob_times_values (96x2048x2048x1289): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1289): 81.766

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2640.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1290x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1290x2048): 150.073
Elapsed time for attention_prob_times_values (96x2048x2048x1290): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1290): 138.356

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 4497.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1291x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1291x2048): 90.226
Elapsed time for attention_prob_times_values (96x2048x2048x1291): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1291): 83.184

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2705.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1292x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1292x2048): 147.477
Elapsed time for attention_prob_times_values (96x2048x2048x1292): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1292): 136.439

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4433.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1293x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1293x2048): 89.226
Elapsed time for attention_prob_times_values (96x2048x2048x1293): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1293): 83.096

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2693.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1294x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1294x2048): 148.741
Elapsed time for attention_prob_times_values (96x2048x2048x1294): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1294): 136.701

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 4463.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1295x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1295x2048): 89.988
Elapsed time for attention_prob_times_values (96x2048x2048x1295): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1295): 82.613

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2700.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1296x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1296x2048): 217.364
Elapsed time for attention_prob_times_values (96x2048x2048x1296): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1296): 198.364

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 6508.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1297x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1297x2048): 90.703
Elapsed time for attention_prob_times_values (96x2048x2048x1297): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1297): 81.390

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2693.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1298x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1298x2048): 147.752
Elapsed time for attention_prob_times_values (96x2048x2048x1298): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1298): 136.892

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4465.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1299x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1299x2048): 90.885
Elapsed time for attention_prob_times_values (96x2048x2048x1299): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1299): 83.189

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2731.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1300x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1300x2048): 147.112
Elapsed time for attention_prob_times_values (96x2048x2048x1300): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1300): 136.439

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 4455.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1301x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1301x2048): 90.709
Elapsed time for attention_prob_times_values (96x2048x2048x1301): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1301): 83.366

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2736.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1302x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1302x2048): 147.151
Elapsed time for attention_prob_times_values (96x2048x2048x1302): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1302): 135.013

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4438.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1303x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1303x2048): 91.449
Elapsed time for attention_prob_times_values (96x2048x2048x1303): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1303): 83.180

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2747.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1304x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1304x2048): 201.996
Elapsed time for attention_prob_times_values (96x2048x2048x1304): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1304): 195.487

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 6271.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1305x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1305x2048): 91.299
Elapsed time for attention_prob_times_values (96x2048x2048x1305): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1305): 83.157

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2749.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1306x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1306x2048): 147.204
Elapsed time for attention_prob_times_values (96x2048x2048x1306): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1306): 137.137

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 4488.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1307x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1307x2048): 91.693
Elapsed time for attention_prob_times_values (96x2048x2048x1307): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1307): 84.557

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2783.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1308x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1308x2048): 148.306
Elapsed time for attention_prob_times_values (96x2048x2048x1308): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1308): 134.561

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4466.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1309x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1309x2048): 91.782
Elapsed time for attention_prob_times_values (96x2048x2048x1309): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1309): 86.211

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2816.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1310x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1310x2048): 149.805
Elapsed time for attention_prob_times_values (96x2048x2048x1310): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1310): 136.939

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4536.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1311x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1311x2048): 94.347
Elapsed time for attention_prob_times_values (96x2048x2048x1311): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1311): 84.136

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2822.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1312x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1312x2048): 221.234
Elapsed time for attention_prob_times_values (96x2048x2048x1312): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1312): 207.596

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 6800.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1313x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1313x2048): 92.917
Elapsed time for attention_prob_times_values (96x2048x2048x1313): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1313): 83.662

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2797.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1314x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1314x2048): 150.003
Elapsed time for attention_prob_times_values (96x2048x2048x1314): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1314): 137.754

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 4566.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1315x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1315x2048): 91.347
Elapsed time for attention_prob_times_values (96x2048x2048x1315): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1315): 86.589

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2828.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1316x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1316x2048): 146.813
Elapsed time for attention_prob_times_values (96x2048x2048x1316): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1316): 135.570

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 4488.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1317x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1317x2048): 91.879
Elapsed time for attention_prob_times_values (96x2048x2048x1317): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1317): 85.447

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2821.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1318x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1318x2048): 148.785
Elapsed time for attention_prob_times_values (96x2048x2048x1318): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1318): 137.355

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4555.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1319x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1319x2048): 91.179
Elapsed time for attention_prob_times_values (96x2048x2048x1319): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1319): 84.289

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2795.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1320x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1320x2048): 205.464
Elapsed time for attention_prob_times_values (96x2048x2048x1320): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1320): 196.136

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 6409.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1321x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1321x2048): 91.312
Elapsed time for attention_prob_times_values (96x2048x2048x1321): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1321): 83.468

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2787.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1322x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1322x2048): 149.122
Elapsed time for attention_prob_times_values (96x2048x2048x1322): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1322): 137.972

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4584.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1323x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1323x2048): 92.927
Elapsed time for attention_prob_times_values (96x2048x2048x1323): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1323): 86.029

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2859.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1324x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1324x2048): 140.990
Elapsed time for attention_prob_times_values (96x2048x2048x1324): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1324): 135.797

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 4431.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1325x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1325x2048): 91.802
Elapsed time for attention_prob_times_values (96x2048x2048x1325): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1325): 87.236

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2867.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1326x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1326x2048): 150.231
Elapsed time for attention_prob_times_values (96x2048x2048x1326): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1326): 137.887

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4612.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1327x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1327x2048): 92.830
Elapsed time for attention_prob_times_values (96x2048x2048x1327): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1327): 84.274

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2836.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1328x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1328x2048): 218.489
Elapsed time for attention_prob_times_values (96x2048x2048x1328): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1328): 210.334

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 6885.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1329x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1329x2048): 91.557
Elapsed time for attention_prob_times_values (96x2048x2048x1329): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1329): 85.149

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2836.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1330x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1330x2048): 149.520
Elapsed time for attention_prob_times_values (96x2048x2048x1330): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1330): 135.413

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 4572.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1331x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1331x2048): 92.777
Elapsed time for attention_prob_times_values (96x2048x2048x1331): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1331): 86.454

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2881.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1332x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1332x2048): 152.120
Elapsed time for attention_prob_times_values (96x2048x2048x1332): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1332): 138.119

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 4664.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1333x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1333x2048): 92.083
Elapsed time for attention_prob_times_values (96x2048x2048x1333): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1333): 85.004

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2850.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1334x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1334x2048): 150.985
Elapsed time for attention_prob_times_values (96x2048x2048x1334): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1334): 138.242

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4656.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1335x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1335x2048): 92.050
Elapsed time for attention_prob_times_values (96x2048x2048x1335): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1335): 84.610

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2847.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1336x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1336x2048): 201.771
Elapsed time for attention_prob_times_values (96x2048x2048x1336): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1336): 203.097

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 6541.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1337x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1337x2048): 92.084
Elapsed time for attention_prob_times_values (96x2048x2048x1337): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1337): 84.914

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2857.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1338x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1338x2048): 151.018
Elapsed time for attention_prob_times_values (96x2048x2048x1338): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1338): 139.079

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4685.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1339x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1339x2048): 93.166
Elapsed time for attention_prob_times_values (96x2048x2048x1339): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1339): 88.003

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2930.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1340x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1340x2048): 149.926
Elapsed time for attention_prob_times_values (96x2048x2048x1340): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1340): 139.282

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4679.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1341x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1341x2048): 91.322
Elapsed time for attention_prob_times_values (96x2048x2048x1341): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1341): 88.389

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2913.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1342x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1342x2048): 149.926
Elapsed time for attention_prob_times_values (96x2048x2048x1342): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1342): 140.020

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 4699.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1343x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1343x2048): 93.046
Elapsed time for attention_prob_times_values (96x2048x2048x1343): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1343): 86.866

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2918.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1344x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1344x2048): 219.253
Elapsed time for attention_prob_times_values (96x2048x2048x1344): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1344): 214.660

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 7050.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1345x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1345x2048): 91.745
Elapsed time for attention_prob_times_values (96x2048x2048x1345): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1345): 86.449

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2895.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1346x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1346x2048): 148.216
Elapsed time for attention_prob_times_values (96x2048x2048x1346): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1346): 139.468

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 4677.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1347x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1347x2048): 92.080
Elapsed time for attention_prob_times_values (96x2048x2048x1347): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1347): 87.390

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2920.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1348x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1348x2048): 149.034
Elapsed time for attention_prob_times_values (96x2048x2048x1348): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1348): 131.914

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 4561.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1349x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1349x2048): 92.464
Elapsed time for attention_prob_times_values (96x2048x2048x1349): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1349): 88.981

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2958.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1350x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1350x2048): 145.609
Elapsed time for attention_prob_times_values (96x2048x2048x1350): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1350): 140.020

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 4659.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1351x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1351x2048): 90.392
Elapsed time for attention_prob_times_values (96x2048x2048x1351): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1351): 84.852

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2859.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1352x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1352x2048): 209.460
Elapsed time for attention_prob_times_values (96x2048x2048x1352): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1352): 205.455

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 6780.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1353x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1353x2048): 90.645
Elapsed time for attention_prob_times_values (96x2048x2048x1353): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1353): 85.354

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2875.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1354x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1354x2048): 148.367
Elapsed time for attention_prob_times_values (96x2048x2048x1354): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1354): 133.764

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 4605.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1355x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1355x2048): 94.266
Elapsed time for attention_prob_times_values (96x2048x2048x1355): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1355): 89.750

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 3012.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1356x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1356x2048): 148.939
Elapsed time for attention_prob_times_values (96x2048x2048x1356): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1356): 140.444

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 4739.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1357x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1357x2048): 93.700
Elapsed time for attention_prob_times_values (96x2048x2048x1357): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1357): 88.595

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2987.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1358x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1358x2048): 146.425
Elapsed time for attention_prob_times_values (96x2048x2048x1358): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1358): 139.683

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 4693.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1359x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1359x2048): 94.671
Elapsed time for attention_prob_times_values (96x2048x2048x1359): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1359): 87.411

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2986.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1360x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1360x2048): 221.254
Elapsed time for attention_prob_times_values (96x2048x2048x1360): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1360): 210.535

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 7093.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1361x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1361x2048): 94.837
Elapsed time for attention_prob_times_values (96x2048x2048x1361): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1361): 87.950

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 3002.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1362x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1362x2048): 149.301
Elapsed time for attention_prob_times_values (96x2048x2048x1362): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1362): 140.957

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 4773.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1363x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1363x2048): 96.457
Elapsed time for attention_prob_times_values (96x2048x2048x1363): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1363): 90.961

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 3084.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1364x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1364x2048): 148.628
Elapsed time for attention_prob_times_values (96x2048x2048x1364): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1364): 139.887

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 4751.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1365x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1365x2048): 96.694
Elapsed time for attention_prob_times_values (96x2048x2048x1365): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1365): 90.908

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 3091.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1366x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1366x2048): 150.360
Elapsed time for attention_prob_times_values (96x2048x2048x1366): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1366): 140.267

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 4791.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
