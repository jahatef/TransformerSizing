
num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 254.747
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 110.454
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 142.794
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26290487296, 42481549312)
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 257.033
(26290487296, 42481549312)
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 253.623
Elapsed time for mlp_fused_gelu (2048x4x32768): 0.0009
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 243.650
Elapsed time for transformer_add_bias_dropout (2048x4x8192): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8192): 0.0003

Attention duration (in seconds): 0.0570
Attention throughput (in TFLOP/s): 86.745
MLP duration (in seconds): 0.0363
MLP throughput (in TFLOP/s): 242.058
Transformer duration (in seconds): 0.0952
Transformer throughput (in TFLOP/s): 144.316
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 189.809
MLP duration (in seconds): 0.0368
MLP throughput (in TFLOP/s): 239.153
Transformer duration (in seconds): 0.0655
Transformer throughput (in TFLOP/s): 209.973
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 64, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8256x24768, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8256x24768, b=2048): 254.148
Elapsed time for attention_key_query_prob (256x2048x129x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x129x2048): 31.802
Elapsed time for attention_prob_times_values (256x2048x2048x129): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x129): 52.177
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8256x8256, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x8256x8256, b=2048): 249.961
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8256x33024, b=2048): 0.0177
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8256x33024, b=2048): 252.736
Elapsed time for mlp_fused_gelu (2048x4x33024): 0.0010
Elapsed time for mlp_4h_to_h (4x33024x8256, b=2048): 0.0186
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33024x8256, b=2048): 239.592
Elapsed time for transformer_add_bias_dropout (2048x4x8256): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8256): 0.0003

Attention duration (in seconds): 0.0671
Attention throughput (in TFLOP/s): 74.862
MLP duration (in seconds): 0.0373
MLP throughput (in TFLOP/s): 239.701
Transformer duration (in seconds): 0.1062
Transformer throughput (in TFLOP/s): 131.359
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 126.105
MLP duration (in seconds): 0.0374
MLP throughput (in TFLOP/s): 238.955
Transformer duration (in seconds): 0.0802
Transformer throughput (in TFLOP/s): 173.911
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 252.027
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 67.610
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 93.079
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 251.996
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0180
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 251.589
Elapsed time for mlp_fused_gelu (2048x4x33280): 0.0010
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0188
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 241.389
Elapsed time for transformer_add_bias_dropout (2048x4x8320): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8320): 0.0003

Attention duration (in seconds): 0.0605
Attention throughput (in TFLOP/s): 84.174
MLP duration (in seconds): 0.0378
MLP throughput (in TFLOP/s): 240.117
Transformer duration (in seconds): 0.1002
Transformer throughput (in TFLOP/s): 141.379
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0296
Attention throughput (in TFLOP/s): 172.083
MLP duration (in seconds): 0.0379
MLP throughput (in TFLOP/s): 239.183
Transformer duration (in seconds): 0.0705
Transformer throughput (in TFLOP/s): 200.975
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 64, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8384x25152, b=2048): 0.0136
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8384x25152, b=2048): 253.826
Elapsed time for attention_key_query_prob (256x2048x131x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x131x2048): 32.428
Elapsed time for attention_prob_times_values (256x2048x2048x131): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x131): 55.094
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8384x8384, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x8384x8384, b=2048): 253.697
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8384x33536, b=2048): 0.0180
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8384x33536, b=2048): 255.330
Elapsed time for mlp_fused_gelu (2048x4x33536): 0.0010
Elapsed time for mlp_4h_to_h (4x33536x8384, b=2048): 0.0182
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33536x8384, b=2048): 252.451
Elapsed time for transformer_add_bias_dropout (2048x4x8384): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8384): 0.0003

Attention duration (in seconds): 0.0673
Attention throughput (in TFLOP/s): 76.762
MLP duration (in seconds): 0.0373
MLP throughput (in TFLOP/s): 247.256
Transformer duration (in seconds): 0.1065
Transformer throughput (in TFLOP/s): 134.995
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0408
Attention throughput (in TFLOP/s): 126.596
MLP duration (in seconds): 0.0379
MLP throughput (in TFLOP/s): 243.306
Transformer duration (in seconds): 0.0810
Transformer throughput (in TFLOP/s): 177.525
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 255.674
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 68.264
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 92.295
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 253.708
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 252.317
Elapsed time for mlp_fused_gelu (2048x4x33792): 0.0010
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0194
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 241.143
Elapsed time for transformer_add_bias_dropout (2048x4x8448): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8448): 0.0003

Attention duration (in seconds): 0.0610
Attention throughput (in TFLOP/s): 86.015
MLP duration (in seconds): 0.0389
MLP throughput (in TFLOP/s): 240.401
Transformer duration (in seconds): 0.1018
Transformer throughput (in TFLOP/s): 143.383
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 173.901
MLP duration (in seconds): 0.0393
MLP throughput (in TFLOP/s): 237.830
Transformer duration (in seconds): 0.0724
Transformer throughput (in TFLOP/s): 201.652
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8512x25536, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8512x25536, b=2048): 252.332
Elapsed time for attention_key_query_prob (256x2048x133x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x133x2048): 32.140
Elapsed time for attention_prob_times_values (256x2048x2048x133): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x133): 55.600
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8512x8512, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x8512x8512, b=2048): 254.395
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8512x34048, b=2048): 0.0187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8512x34048, b=2048): 253.714
Elapsed time for mlp_fused_gelu (2048x4x34048): 0.0010
Elapsed time for mlp_4h_to_h (4x34048x8512, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34048x8512, b=2048): 242.111
Elapsed time for transformer_add_bias_dropout (2048x4x8512): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8512): 0.0003

Attention duration (in seconds): 0.0682
Attention throughput (in TFLOP/s): 77.984
MLP duration (in seconds): 0.0393
MLP throughput (in TFLOP/s): 241.557
Transformer duration (in seconds): 0.1095
Transformer throughput (in TFLOP/s): 135.319
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 127.126
MLP duration (in seconds): 0.0399
MLP throughput (in TFLOP/s): 238.237
Transformer duration (in seconds): 0.0844
Transformer throughput (in TFLOP/s): 175.503
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 248.802
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 68.867
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 91.057
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 256.491
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0191
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 252.207
Elapsed time for mlp_fused_gelu (2048x4x34304): 0.0010
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 243.586
Elapsed time for transformer_add_bias_dropout (2048x4x8576): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8576): 0.0003

Attention duration (in seconds): 0.0620
Attention throughput (in TFLOP/s): 87.058
MLP duration (in seconds): 0.0399
MLP throughput (in TFLOP/s): 241.651
Transformer duration (in seconds): 0.1038
Transformer throughput (in TFLOP/s): 144.798
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 175.343
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 239.711
Transformer duration (in seconds): 0.0743
Transformer throughput (in TFLOP/s): 202.318
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8640x25920, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8640x25920, b=2048): 253.048
Elapsed time for attention_key_query_prob (256x2048x135x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x135x2048): 31.108
Elapsed time for attention_prob_times_values (256x2048x2048x135): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x135): 54.638
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8640x8640, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x8640x8640, b=2048): 250.067
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8640x34560, b=2048): 0.0191
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8640x34560, b=2048): 256.366
Elapsed time for mlp_fused_gelu (2048x4x34560): 0.0010
Elapsed time for mlp_4h_to_h (4x34560x8640, b=2048): 0.0194
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34560x8640, b=2048): 252.411
Elapsed time for transformer_add_bias_dropout (2048x4x8640): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8640): 0.0003

Attention duration (in seconds): 0.0694
Attention throughput (in TFLOP/s): 78.828
MLP duration (in seconds): 0.0395
MLP throughput (in TFLOP/s): 247.945
Transformer duration (in seconds): 0.1109
Transformer throughput (in TFLOP/s): 137.621
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0431
Attention throughput (in TFLOP/s): 126.932
MLP duration (in seconds): 0.0399
MLP throughput (in TFLOP/s): 245.245
Transformer duration (in seconds): 0.0853
Transformer throughput (in TFLOP/s): 178.807
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 252.196
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 98.140
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 125.768
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 249.242
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0194
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 256.354
Elapsed time for mlp_fused_gelu (2048x4x34816): 0.0010
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 243.937
Elapsed time for transformer_add_bias_dropout (2048x4x8704): 0.0007
Elapsed time for transformer_layer_norm (2048x4x8704): 0.0003

Attention duration (in seconds): 0.0604
Attention throughput (in TFLOP/s): 91.804
MLP duration (in seconds): 0.0407
MLP throughput (in TFLOP/s): 243.831
Transformer duration (in seconds): 0.1032
Transformer throughput (in TFLOP/s): 150.048
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 185.776
MLP duration (in seconds): 0.0413
MLP throughput (in TFLOP/s): 240.342
Transformer duration (in seconds): 0.0743
Transformer throughput (in TFLOP/s): 208.327
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8768x26304, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8768x26304, b=2048): 252.666
Elapsed time for attention_key_query_prob (256x2048x137x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x137x2048): 31.362
Elapsed time for attention_prob_times_values (256x2048x2048x137): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x137): 55.140
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8768x8768, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_linear_projection (4x8768x8768, b=2048): 250.534
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8768x35072, b=2048): 0.0199
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8768x35072, b=2048): 253.388
Elapsed time for mlp_fused_gelu (2048x4x35072): 0.0010
Elapsed time for mlp_4h_to_h (4x35072x8768, b=2048): 0.0211
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35072x8768, b=2048): 239.223
Elapsed time for transformer_add_bias_dropout (2048x4x8768): 0.0007
Elapsed time for transformer_layer_norm (2048x4x8768): 0.0004

Attention duration (in seconds): 0.0701
Attention throughput (in TFLOP/s): 80.263
MLP duration (in seconds): 0.0420
MLP throughput (in TFLOP/s): 240.172
Transformer duration (in seconds): 0.1141
Transformer throughput (in TFLOP/s): 137.661
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 128.264
MLP duration (in seconds): 0.0424
MLP throughput (in TFLOP/s): 237.806
Transformer duration (in seconds): 0.0889
Transformer throughput (in TFLOP/s): 176.661
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 252.794
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 70.305
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 91.195
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 252.445
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0199
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 256.946
Elapsed time for mlp_fused_gelu (2048x4x35328): 0.0010
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 240.885
Elapsed time for transformer_add_bias_dropout (2048x4x8832): 0.0007
Elapsed time for transformer_layer_norm (2048x4x8832): 0.0004

Attention duration (in seconds): 0.0631
Attention throughput (in TFLOP/s): 90.414
MLP duration (in seconds): 0.0421
MLP throughput (in TFLOP/s): 242.650
Transformer duration (in seconds): 0.1073
Transformer throughput (in TFLOP/s): 148.511
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 176.038
MLP duration (in seconds): 0.0427
MLP throughput (in TFLOP/s): 239.446
Transformer duration (in seconds): 0.0782
Transformer throughput (in TFLOP/s): 203.779
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8896x26688, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8896x26688, b=2048): 250.990
Elapsed time for attention_key_query_prob (256x2048x139x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x139x2048): 32.958
Elapsed time for attention_prob_times_values (256x2048x2048x139): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x139): 56.935
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8896x8896, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x8896x8896, b=2048): 254.296
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8896x35584, b=2048): 0.0203
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8896x35584, b=2048): 256.026
Elapsed time for mlp_fused_gelu (2048x4x35584): 0.0010
Elapsed time for mlp_4h_to_h (4x35584x8896, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35584x8896, b=2048): 251.917
Elapsed time for transformer_add_bias_dropout (2048x4x8896): 0.0007
Elapsed time for transformer_layer_norm (2048x4x8896): 0.0004

Attention duration (in seconds): 0.0703
Attention throughput (in TFLOP/s): 82.270
MLP duration (in seconds): 0.0419
MLP throughput (in TFLOP/s): 247.738
Transformer duration (in seconds): 0.1142
Transformer throughput (in TFLOP/s): 141.461
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0443
Attention throughput (in TFLOP/s): 130.601
MLP duration (in seconds): 0.0424
MLP throughput (in TFLOP/s): 244.636
Transformer duration (in seconds): 0.0892
Transformer throughput (in TFLOP/s): 181.068
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 251.688
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 71.288
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 92.551
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 256.219
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0207
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 254.432
Elapsed time for mlp_fused_gelu (2048x4x35840): 0.0010
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0209
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 251.300
Elapsed time for transformer_add_bias_dropout (2048x4x8960): 0.0007
Elapsed time for transformer_layer_norm (2048x4x8960): 0.0004

Attention duration (in seconds): 0.0637
Attention throughput (in TFLOP/s): 92.070
MLP duration (in seconds): 0.0426
MLP throughput (in TFLOP/s): 246.741
Transformer duration (in seconds): 0.1084
Transformer throughput (in TFLOP/s): 151.201
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 178.407
MLP duration (in seconds): 0.0427
MLP throughput (in TFLOP/s): 246.296
Transformer duration (in seconds): 0.0785
Transformer throughput (in TFLOP/s): 208.789
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9024x27072, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9024x27072, b=2048): 253.220
Elapsed time for attention_key_query_prob (256x2048x141x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x141x2048): 32.636
Elapsed time for attention_prob_times_values (256x2048x2048x141): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x141): 55.744
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9024x9024, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_linear_projection (4x9024x9024, b=2048): 249.666
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9024x36096, b=2048): 0.0210
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9024x36096, b=2048): 254.375
Elapsed time for mlp_fused_gelu (2048x4x36096): 0.0010
Elapsed time for mlp_4h_to_h (4x36096x9024, b=2048): 0.0213
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36096x9024, b=2048): 250.768
Elapsed time for transformer_add_bias_dropout (2048x4x9024): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9024): 0.0004

Attention duration (in seconds): 0.0713
Attention throughput (in TFLOP/s): 83.386
MLP duration (in seconds): 0.0433
MLP throughput (in TFLOP/s): 246.500
Transformer duration (in seconds): 0.1166
Transformer throughput (in TFLOP/s): 142.460
Transformer - MLP - Attention (in seconds): 0.0021


Actual
------
Attention duration (in seconds): 0.0450
Attention throughput (in TFLOP/s): 131.969
MLP duration (in seconds): 0.0437
MLP throughput (in TFLOP/s): 244.451
Transformer duration (in seconds): 0.0915
Transformer throughput (in TFLOP/s): 181.598
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 251.265
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 71.993
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 92.993
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 248.660
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 255.327
Elapsed time for mlp_fused_gelu (2048x4x36352): 0.0010
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 250.855
Elapsed time for transformer_add_bias_dropout (2048x4x9088): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9088): 0.0004

Attention duration (in seconds): 0.0645
Attention throughput (in TFLOP/s): 93.353
MLP duration (in seconds): 0.0438
MLP throughput (in TFLOP/s): 247.034
Transformer duration (in seconds): 0.1104
Transformer throughput (in TFLOP/s): 152.594
Transformer - MLP - Attention (in seconds): 0.0021


Actual
------
Attention duration (in seconds): 0.0336
Attention throughput (in TFLOP/s): 179.184
MLP duration (in seconds): 0.0442
MLP throughput (in TFLOP/s): 244.750
Transformer duration (in seconds): 0.0809
Transformer throughput (in TFLOP/s): 208.326
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 64, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9152x27456, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9152x27456, b=2048): 252.602
Elapsed time for attention_key_query_prob (256x2048x143x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x143x2048): 32.683
Elapsed time for attention_prob_times_values (256x2048x2048x143): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x143): 55.456
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9152x9152, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x9152x9152, b=2048): 250.169
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9152x36608, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9152x36608, b=2048): 254.081
Elapsed time for mlp_fused_gelu (2048x4x36608): 0.0011
Elapsed time for mlp_4h_to_h (4x36608x9152, b=2048): 0.0229
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36608x9152, b=2048): 239.866
Elapsed time for transformer_add_bias_dropout (2048x4x9152): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9152): 0.0004

Attention duration (in seconds): 0.0721
Attention throughput (in TFLOP/s): 84.616
MLP duration (in seconds): 0.0455
MLP throughput (in TFLOP/s): 241.042
Transformer duration (in seconds): 0.1198
Transformer throughput (in TFLOP/s): 142.593
Transformer - MLP - Attention (in seconds): 0.0021


Actual
------
Attention duration (in seconds): 0.0456
Attention throughput (in TFLOP/s): 133.769
MLP duration (in seconds): 0.0458
MLP throughput (in TFLOP/s): 239.514
Transformer duration (in seconds): 0.0950
Transformer throughput (in TFLOP/s): 179.896
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 254.410
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 109.593
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 135.432
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 251.906
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 253.548
Elapsed time for mlp_fused_gelu (2048x4x36864): 0.0011
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0230
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 242.444
Elapsed time for transformer_add_bias_dropout (2048x4x9216): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9216): 0.0004

Attention duration (in seconds): 0.0625
Attention throughput (in TFLOP/s): 99.033
MLP duration (in seconds): 0.0460
MLP throughput (in TFLOP/s): 242.125
Transformer duration (in seconds): 0.1106
Transformer throughput (in TFLOP/s): 156.626
Transformer - MLP - Attention (in seconds): 0.0021


Actual
------
Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 192.283
MLP duration (in seconds): 0.0464
MLP throughput (in TFLOP/s): 239.962
Transformer duration (in seconds): 0.0813
Transformer throughput (in TFLOP/s): 212.963
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 64, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9280x27840, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9280x27840, b=2048): 252.637
Elapsed time for attention_key_query_prob (256x2048x145x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x145x2048): 53.195
Elapsed time for attention_prob_times_values (256x2048x2048x145): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x145): 55.390
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9280x9280, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_linear_projection (4x9280x9280, b=2048): 253.058
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9280x37120, b=2048): 0.0221
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9280x37120, b=2048): 255.354
Elapsed time for mlp_fused_gelu (2048x4x37120): 0.0011
Elapsed time for mlp_4h_to_h (4x37120x9280, b=2048): 0.0234
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37120x9280, b=2048): 240.910
Elapsed time for transformer_add_bias_dropout (2048x4x9280): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9280): 0.0004

Attention duration (in seconds): 0.0692
Attention throughput (in TFLOP/s): 90.535
MLP duration (in seconds): 0.0466
MLP throughput (in TFLOP/s): 242.219
Transformer duration (in seconds): 0.1180
Transformer throughput (in TFLOP/s): 148.809
Transformer - MLP - Attention (in seconds): 0.0021


Actual
------
Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 157.300
MLP duration (in seconds): 0.0473
MLP throughput (in TFLOP/s): 238.602
Transformer duration (in seconds): 0.0905
Transformer throughput (in TFLOP/s): 193.916
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 250.766
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 73.561
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 94.404
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 254.904
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0225
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 253.747
Elapsed time for mlp_fused_gelu (2048x4x37376): 0.0011
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0236
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 242.257
Elapsed time for transformer_add_bias_dropout (2048x4x9344): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9344): 0.0004

Attention duration (in seconds): 0.0657
Attention throughput (in TFLOP/s): 96.608
MLP duration (in seconds): 0.0472
MLP throughput (in TFLOP/s): 242.202
Transformer duration (in seconds): 0.1151
Transformer throughput (in TFLOP/s): 154.546
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 183.214
MLP duration (in seconds): 0.0476
MLP throughput (in TFLOP/s): 240.235
Transformer duration (in seconds): 0.0856
Transformer throughput (in TFLOP/s): 207.973
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9408x28224, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9408x28224, b=2048): 252.266
Elapsed time for attention_key_query_prob (256x2048x147x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x147x2048): 53.784
Elapsed time for attention_prob_times_values (256x2048x2048x147): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x147): 56.016
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9408x9408, b=2048): 0.0057
Throughput (in TFLOP/s) for attention_linear_projection (4x9408x9408, b=2048): 256.533
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9408x37632, b=2048): 0.0227
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9408x37632, b=2048): 255.370
Elapsed time for mlp_fused_gelu (2048x4x37632): 0.0011
Elapsed time for mlp_4h_to_h (4x37632x9408, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37632x9408, b=2048): 254.023
Elapsed time for transformer_add_bias_dropout (2048x4x9408): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9408): 0.0004

Attention duration (in seconds): 0.0698
Attention throughput (in TFLOP/s): 92.122
MLP duration (in seconds): 0.0466
MLP throughput (in TFLOP/s): 248.761
Transformer duration (in seconds): 0.1186
Transformer throughput (in TFLOP/s): 151.995
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0404
Attention throughput (in TFLOP/s): 159.399
MLP duration (in seconds): 0.0472
MLP throughput (in TFLOP/s): 245.931
Transformer duration (in seconds): 0.0907
Transformer throughput (in TFLOP/s): 198.893
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0179
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 246.505
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 74.498
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 95.492
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0061
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 242.409
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0238
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 246.768
Elapsed time for mlp_fused_gelu (2048x4x37888): 0.0011
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0243
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 242.399
Elapsed time for transformer_add_bias_dropout (2048x4x9472): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9472): 0.0004

Attention duration (in seconds): 0.0670
Attention throughput (in TFLOP/s): 97.297
MLP duration (in seconds): 0.0492
MLP throughput (in TFLOP/s): 239.124
Transformer duration (in seconds): 0.1183
Transformer throughput (in TFLOP/s): 154.460
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0354
Attention throughput (in TFLOP/s): 183.959
MLP duration (in seconds): 0.0498
MLP throughput (in TFLOP/s): 236.110
Transformer duration (in seconds): 0.0885
Transformer throughput (in TFLOP/s): 206.578
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9536x28608, b=2048): 0.0182
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9536x28608, b=2048): 246.231
Elapsed time for attention_key_query_prob (256x2048x149x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x149x2048): 54.105
Elapsed time for attention_prob_times_values (256x2048x2048x149): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x149): 56.241
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9536x9536, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x9536x9536, b=2048): 239.785
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9536x38144, b=2048): 0.0241
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9536x38144, b=2048): 247.058
Elapsed time for mlp_fused_gelu (2048x4x38144): 0.0011
Elapsed time for mlp_4h_to_h (4x38144x9536, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38144x9536, b=2048): 240.715
Elapsed time for transformer_add_bias_dropout (2048x4x9536): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9536): 0.0004

Attention duration (in seconds): 0.0714
Attention throughput (in TFLOP/s): 92.448
MLP duration (in seconds): 0.0500
MLP throughput (in TFLOP/s): 238.470
Transformer duration (in seconds): 0.1236
Transformer throughput (in TFLOP/s): 149.858
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0414
Attention throughput (in TFLOP/s): 159.551
MLP duration (in seconds): 0.0506
MLP throughput (in TFLOP/s): 235.551
Transformer duration (in seconds): 0.0953
Transformer throughput (in TFLOP/s): 194.225
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 246.613
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 75.814
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 96.354
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 244.232
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0244
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 247.088
Elapsed time for mlp_fused_gelu (2048x4x38400): 0.0011
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 242.433
Elapsed time for transformer_add_bias_dropout (2048x4x9600): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9600): 0.0004

Attention duration (in seconds): 0.0676
Attention throughput (in TFLOP/s): 98.949
MLP duration (in seconds): 0.0505
MLP throughput (in TFLOP/s): 239.387
Transformer duration (in seconds): 0.1202
Transformer throughput (in TFLOP/s): 156.076
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0361
Attention throughput (in TFLOP/s): 185.219
MLP duration (in seconds): 0.0511
MLP throughput (in TFLOP/s): 236.344
Transformer duration (in seconds): 0.0906
Transformer throughput (in TFLOP/s): 207.024
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9664x28992, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9664x28992, b=2048): 246.331
Elapsed time for attention_key_query_prob (256x2048x151x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x151x2048): 54.310
Elapsed time for attention_prob_times_values (256x2048x2048x151): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x151): 55.755
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9664x9664, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9664x9664, b=2048): 241.565
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9664x38656, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9664x38656, b=2048): 247.218
Elapsed time for mlp_fused_gelu (2048x4x38656): 0.0011
Elapsed time for mlp_4h_to_h (4x38656x9664, b=2048): 0.0244
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38656x9664, b=2048): 251.004
Elapsed time for transformer_add_bias_dropout (2048x4x9664): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9664): 0.0004

Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 93.798
MLP duration (in seconds): 0.0503
MLP throughput (in TFLOP/s): 243.596
Transformer duration (in seconds): 0.1246
Transformer throughput (in TFLOP/s): 152.517
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0420
Attention throughput (in TFLOP/s): 161.122
MLP duration (in seconds): 0.0506
MLP throughput (in TFLOP/s): 242.025
Transformer duration (in seconds): 0.0955
Transformer throughput (in TFLOP/s): 199.102
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 246.998
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 103.940
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 140.018
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 243.065
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0251
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 247.331
Elapsed time for mlp_fused_gelu (2048x4x38912): 0.0011
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0245
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 252.713
Elapsed time for transformer_add_bias_dropout (2048x4x9728): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9728): 0.0004

Attention duration (in seconds): 0.0661
Attention throughput (in TFLOP/s): 103.726
MLP duration (in seconds): 0.0507
MLP throughput (in TFLOP/s): 244.488
Transformer duration (in seconds): 0.1190
Transformer throughput (in TFLOP/s): 161.774
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 195.266
MLP duration (in seconds): 0.0511
MLP throughput (in TFLOP/s): 242.783
Transformer duration (in seconds): 0.0891
Transformer throughput (in TFLOP/s): 216.174
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9792x29376, b=2048): 0.0191
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9792x29376, b=2048): 246.747
Elapsed time for attention_key_query_prob (256x2048x153x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x153x2048): 54.850
Elapsed time for attention_prob_times_values (256x2048x2048x153): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x153): 56.121
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9792x9792, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9792x9792, b=2048): 241.278
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9792x39168, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9792x39168, b=2048): 247.455
Elapsed time for mlp_fused_gelu (2048x4x39168): 0.0011
Elapsed time for mlp_4h_to_h (4x39168x9792, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39168x9792, b=2048): 240.590
Elapsed time for transformer_add_bias_dropout (2048x4x9792): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9792): 0.0004

Attention duration (in seconds): 0.0729
Attention throughput (in TFLOP/s): 95.265
MLP duration (in seconds): 0.0526
MLP throughput (in TFLOP/s): 238.766
Transformer duration (in seconds): 0.1277
Transformer throughput (in TFLOP/s): 152.713
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 162.627
MLP duration (in seconds): 0.0533
MLP throughput (in TFLOP/s): 235.787
Transformer duration (in seconds): 0.0995
Transformer throughput (in TFLOP/s): 196.029
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 247.349
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 77.535
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 98.300
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 242.885
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 245.704
Elapsed time for mlp_fused_gelu (2048x4x39424): 0.0011
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0266
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 239.605
Elapsed time for transformer_add_bias_dropout (2048x4x9856): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9856): 0.0004

Attention duration (in seconds): 0.0689
Attention throughput (in TFLOP/s): 102.018
MLP duration (in seconds): 0.0536
MLP throughput (in TFLOP/s): 237.492
Transformer duration (in seconds): 0.1248
Transformer throughput (in TFLOP/s): 158.342
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0376
Attention throughput (in TFLOP/s): 186.658
MLP duration (in seconds): 0.0541
MLP throughput (in TFLOP/s): 235.312
Transformer duration (in seconds): 0.0950
Transformer throughput (in TFLOP/s): 207.959
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9920x29760, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9920x29760, b=2048): 246.456
Elapsed time for attention_key_query_prob (256x2048x155x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x155x2048): 55.336
Elapsed time for attention_prob_times_values (256x2048x2048x155): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x155): 58.670
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9920x9920, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x9920x9920, b=2048): 242.286
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9920x39680, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9920x39680, b=2048): 247.188
Elapsed time for mlp_fused_gelu (2048x4x39680): 0.0011
Elapsed time for mlp_4h_to_h (4x39680x9920, b=2048): 0.0255
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39680x9920, b=2048): 253.261
Elapsed time for transformer_add_bias_dropout (2048x4x9920): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9920): 0.0004

Attention duration (in seconds): 0.0734
Attention throughput (in TFLOP/s): 96.954
MLP duration (in seconds): 0.0527
MLP throughput (in TFLOP/s): 244.758
Transformer duration (in seconds): 0.1284
Transformer throughput (in TFLOP/s): 155.885
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0434
Attention throughput (in TFLOP/s): 164.110
MLP duration (in seconds): 0.0533
MLP throughput (in TFLOP/s): 242.159
Transformer duration (in seconds): 0.0995
Transformer throughput (in TFLOP/s): 201.108
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 247.589
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 77.950
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 99.365
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 244.520
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 247.609
Elapsed time for mlp_fused_gelu (2048x4x39936): 0.0011
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 252.139
Elapsed time for transformer_add_bias_dropout (2048x4x9984): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9984): 0.0004

Attention duration (in seconds): 0.0695
Attention throughput (in TFLOP/s): 103.574
MLP duration (in seconds): 0.0534
MLP throughput (in TFLOP/s): 244.497
Transformer duration (in seconds): 0.1253
Transformer throughput (in TFLOP/s): 161.800
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0382
Attention throughput (in TFLOP/s): 188.785
MLP duration (in seconds): 0.0538
MLP throughput (in TFLOP/s): 243.005
Transformer duration (in seconds): 0.0949
Transformer throughput (in TFLOP/s): 213.666
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10048x30144, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10048x30144, b=2048): 247.259
Elapsed time for attention_key_query_prob (256x2048x157x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x157x2048): 55.830
Elapsed time for attention_prob_times_values (256x2048x2048x157): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x157): 59.355
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10048x10048, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x10048x10048, b=2048): 242.412
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10048x40192, b=2048): 0.0268
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10048x40192, b=2048): 246.801
Elapsed time for mlp_fused_gelu (2048x4x40192): 0.0012
Elapsed time for mlp_4h_to_h (4x40192x10048, b=2048): 0.0275
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40192x10048, b=2048): 240.378
Elapsed time for transformer_add_bias_dropout (2048x4x10048): 0.0007
Elapsed time for transformer_layer_norm (2048x4x10048): 0.0004

Attention duration (in seconds): 0.0740
Attention throughput (in TFLOP/s): 98.505
MLP duration (in seconds): 0.0555
MLP throughput (in TFLOP/s): 238.484
Transformer duration (in seconds): 0.1318
Transformer throughput (in TFLOP/s): 155.704
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 165.978
MLP duration (in seconds): 0.0563
MLP throughput (in TFLOP/s): 235.074
Transformer duration (in seconds): 0.1035
Transformer throughput (in TFLOP/s): 198.332
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 247.673
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 79.178
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 100.604
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 244.043
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 247.846
Elapsed time for mlp_fused_gelu (2048x4x40448): 0.0012
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0277
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 241.905
Elapsed time for transformer_add_bias_dropout (2048x4x10112): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10112): 0.0004

Attention duration (in seconds): 0.0702
Attention throughput (in TFLOP/s): 105.099
MLP duration (in seconds): 0.0559
MLP throughput (in TFLOP/s): 239.751
Transformer duration (in seconds): 0.1284
Transformer throughput (in TFLOP/s): 161.816
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0389
Attention throughput (in TFLOP/s): 189.576
MLP duration (in seconds): 0.0568
MLP throughput (in TFLOP/s): 236.033
Transformer duration (in seconds): 0.0993
Transformer throughput (in TFLOP/s): 209.269
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10176x30528, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10176x30528, b=2048): 247.051
Elapsed time for attention_key_query_prob (256x2048x159x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x159x2048): 56.633
Elapsed time for attention_prob_times_values (256x2048x2048x159): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x159): 58.997
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10176x10176, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_linear_projection (4x10176x10176, b=2048): 243.906
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10176x40704, b=2048): 0.0275
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10176x40704, b=2048): 246.842
Elapsed time for mlp_fused_gelu (2048x4x40704): 0.0012
Elapsed time for mlp_4h_to_h (4x40704x10176, b=2048): 0.0271
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40704x10176, b=2048): 250.089
Elapsed time for transformer_add_bias_dropout (2048x4x10176): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10176): 0.0004

Attention duration (in seconds): 0.0748
Attention throughput (in TFLOP/s): 99.879
MLP duration (in seconds): 0.0558
MLP throughput (in TFLOP/s): 243.256
Transformer duration (in seconds): 0.1329
Transformer throughput (in TFLOP/s): 158.299
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0446
Attention throughput (in TFLOP/s): 167.308
MLP duration (in seconds): 0.0560
MLP throughput (in TFLOP/s): 242.175
Transformer duration (in seconds): 0.1041
Transformer throughput (in TFLOP/s): 202.137
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0208
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 247.703
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 124.377
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 149.002
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 242.806
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0278
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 247.553
Elapsed time for mlp_fused_gelu (2048x4x40960): 0.0012
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0273
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 251.672
Elapsed time for transformer_add_bias_dropout (2048x4x10240): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10240): 0.0004

Attention duration (in seconds): 0.0684
Attention throughput (in TFLOP/s): 110.581
MLP duration (in seconds): 0.0562
MLP throughput (in TFLOP/s): 244.375
Transformer duration (in seconds): 0.1270
Transformer throughput (in TFLOP/s): 167.798
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0378
Attention throughput (in TFLOP/s): 199.840
MLP duration (in seconds): 0.0566
MLP throughput (in TFLOP/s): 242.627
Transformer duration (in seconds): 0.0977
Transformer throughput (in TFLOP/s): 218.139
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10304x30912, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10304x30912, b=2048): 244.554
Elapsed time for attention_key_query_prob (256x2048x161x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x161x2048): 54.114
Elapsed time for attention_prob_times_values (256x2048x2048x161): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x161): 59.272
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10304x10304, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x10304x10304, b=2048): 242.589
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10304x41216, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10304x41216, b=2048): 248.176
Elapsed time for mlp_fused_gelu (2048x4x41216): 0.0012
Elapsed time for mlp_4h_to_h (4x41216x10304, b=2048): 0.0289
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41216x10304, b=2048): 241.023
Elapsed time for transformer_add_bias_dropout (2048x4x10304): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10304): 0.0004

Attention duration (in seconds): 0.0761
Attention throughput (in TFLOP/s): 100.473
MLP duration (in seconds): 0.0581
MLP throughput (in TFLOP/s): 239.567
Transformer duration (in seconds): 0.1366
Transformer throughput (in TFLOP/s): 157.869
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0458
Attention throughput (in TFLOP/s): 167.020
MLP duration (in seconds): 0.0590
MLP throughput (in TFLOP/s): 235.968
Transformer duration (in seconds): 0.1085
Transformer throughput (in TFLOP/s): 198.836
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 247.917
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 75.281
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 102.715
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 244.127
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0284
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 248.195
Elapsed time for mlp_fused_gelu (2048x4x41472): 0.0012
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0292
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 241.010
Elapsed time for transformer_add_bias_dropout (2048x4x10368): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10368): 0.0004

Attention duration (in seconds): 0.0719
Attention throughput (in TFLOP/s): 107.596
MLP duration (in seconds): 0.0588
MLP throughput (in TFLOP/s): 239.600
Transformer duration (in seconds): 0.1331
Transformer throughput (in TFLOP/s): 163.967
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0406
Attention throughput (in TFLOP/s): 190.467
MLP duration (in seconds): 0.0596
MLP throughput (in TFLOP/s): 236.326
Transformer duration (in seconds): 0.1039
Transformer throughput (in TFLOP/s): 210.071
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10432x31296, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10432x31296, b=2048): 247.581
Elapsed time for attention_key_query_prob (256x2048x163x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x163x2048): 54.174
Elapsed time for attention_prob_times_values (256x2048x2048x163): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x163): 61.115
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10432x10432, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10432x10432, b=2048): 242.156
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10432x41728, b=2048): 0.0289
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10432x41728, b=2048): 246.370
Elapsed time for mlp_fused_gelu (2048x4x41728): 0.0012
Elapsed time for mlp_4h_to_h (4x41728x10432, b=2048): 0.0283
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41728x10432, b=2048): 252.192
Elapsed time for transformer_add_bias_dropout (2048x4x10432): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10432): 0.0004

Attention duration (in seconds): 0.0766
Attention throughput (in TFLOP/s): 102.295
MLP duration (in seconds): 0.0584
MLP throughput (in TFLOP/s): 244.140
Transformer duration (in seconds): 0.1374
Transformer throughput (in TFLOP/s): 160.815
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0468
Attention throughput (in TFLOP/s): 167.472
MLP duration (in seconds): 0.0586
MLP throughput (in TFLOP/s): 243.380
Transformer duration (in seconds): 0.1089
Transformer throughput (in TFLOP/s): 202.950
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0219
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 247.159
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 75.928
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 103.524
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 243.810
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0294
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 245.701
Elapsed time for mlp_fused_gelu (2048x4x41984): 0.0012
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0299
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 241.187
Elapsed time for transformer_add_bias_dropout (2048x4x10496): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10496): 0.0004

Attention duration (in seconds): 0.0728
Attention throughput (in TFLOP/s): 108.903
MLP duration (in seconds): 0.0605
MLP throughput (in TFLOP/s): 238.566
Transformer duration (in seconds): 0.1357
Transformer throughput (in TFLOP/s): 164.799
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 192.559
MLP duration (in seconds): 0.0609
MLP throughput (in TFLOP/s): 237.029
Transformer duration (in seconds): 0.1061
Transformer throughput (in TFLOP/s): 210.734
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10560x31680, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10560x31680, b=2048): 247.665
Elapsed time for attention_key_query_prob (256x2048x165x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x165x2048): 54.692
Elapsed time for attention_prob_times_values (256x2048x2048x165): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x165): 61.570
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10560x10560, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_linear_projection (4x10560x10560, b=2048): 241.367
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10560x42240, b=2048): 0.0297
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10560x42240, b=2048): 246.016
Elapsed time for mlp_fused_gelu (2048x4x42240): 0.0012
Elapsed time for mlp_4h_to_h (4x42240x10560, b=2048): 0.0306
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42240x10560, b=2048): 238.973
Elapsed time for transformer_add_bias_dropout (2048x4x10560): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10560): 0.0004

Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 103.646
MLP duration (in seconds): 0.0615
MLP throughput (in TFLOP/s): 237.653
Transformer duration (in seconds): 0.1413
Transformer throughput (in TFLOP/s): 160.163
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0477
Attention throughput (in TFLOP/s): 168.128
MLP duration (in seconds): 0.0620
MLP throughput (in TFLOP/s): 235.936
Transformer duration (in seconds): 0.1136
Transformer throughput (in TFLOP/s): 199.229
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 245.497
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 76.854
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 104.764
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 242.886
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0301
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 245.725
Elapsed time for mlp_fused_gelu (2048x4x42496): 0.0012
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 240.510
Elapsed time for transformer_add_bias_dropout (2048x4x10624): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10624): 0.0005

Attention duration (in seconds): 0.0737
Attention throughput (in TFLOP/s): 110.095
MLP duration (in seconds): 0.0621
MLP throughput (in TFLOP/s): 238.306
Transformer duration (in seconds): 0.1382
Transformer throughput (in TFLOP/s): 165.685
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0420
Attention throughput (in TFLOP/s): 192.934
MLP duration (in seconds): 0.0625
MLP throughput (in TFLOP/s): 236.688
Transformer duration (in seconds): 0.1084
Transformer throughput (in TFLOP/s): 211.335
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10688x32064, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10688x32064, b=2048): 248.103
Elapsed time for attention_key_query_prob (256x2048x167x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x167x2048): 55.119
Elapsed time for attention_prob_times_values (256x2048x2048x167): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x167): 60.461
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10688x10688, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x10688x10688, b=2048): 243.420
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10688x42752, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10688x42752, b=2048): 245.725
Elapsed time for mlp_fused_gelu (2048x4x42752): 0.0012
Elapsed time for mlp_4h_to_h (4x42752x10688, b=2048): 0.0297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42752x10688, b=2048): 251.992
Elapsed time for transformer_add_bias_dropout (2048x4x10688): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10688): 0.0004

Attention duration (in seconds): 0.0782
Attention throughput (in TFLOP/s): 104.947
MLP duration (in seconds): 0.0614
MLP throughput (in TFLOP/s): 243.841
Transformer duration (in seconds): 0.1421
Transformer throughput (in TFLOP/s): 163.151
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0485
Attention throughput (in TFLOP/s): 169.005
MLP duration (in seconds): 0.0616
MLP throughput (in TFLOP/s): 243.206
Transformer duration (in seconds): 0.1138
Transformer throughput (in TFLOP/s): 203.606
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 245.699
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 110.228
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 152.464
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 244.939
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 245.742
Elapsed time for mlp_fused_gelu (2048x4x43008): 0.0012
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0299
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 253.606
Elapsed time for transformer_add_bias_dropout (2048x4x10752): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10752): 0.0004

Attention duration (in seconds): 0.0719
Attention throughput (in TFLOP/s): 115.392
MLP duration (in seconds): 0.0619
MLP throughput (in TFLOP/s): 244.629
Transformer duration (in seconds): 0.1363
Transformer throughput (in TFLOP/s): 172.009
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0414
Attention throughput (in TFLOP/s): 200.663
MLP duration (in seconds): 0.0622
MLP throughput (in TFLOP/s): 243.418
Transformer duration (in seconds): 0.1071
Transformer throughput (in TFLOP/s): 218.888
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10816x32448, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10816x32448, b=2048): 245.207
Elapsed time for attention_key_query_prob (256x2048x169x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x169x2048): 55.681
Elapsed time for attention_prob_times_values (256x2048x2048x169): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x169): 61.042
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10816x10816, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10816x10816, b=2048): 242.569
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10816x43264, b=2048): 0.0312
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10816x43264, b=2048): 245.885
Elapsed time for mlp_fused_gelu (2048x4x43264): 0.0012
Elapsed time for mlp_4h_to_h (4x43264x10816, b=2048): 0.0320
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43264x10816, b=2048): 239.216
Elapsed time for transformer_add_bias_dropout (2048x4x10816): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10816): 0.0004

Attention duration (in seconds): 0.0792
Attention throughput (in TFLOP/s): 105.945
MLP duration (in seconds): 0.0645
MLP throughput (in TFLOP/s): 237.845
Transformer duration (in seconds): 0.1462
Transformer throughput (in TFLOP/s): 162.311
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0493
Attention throughput (in TFLOP/s): 170.172
MLP duration (in seconds): 0.0651
MLP throughput (in TFLOP/s): 235.660
Transformer duration (in seconds): 0.1182
Transformer throughput (in TFLOP/s): 200.754
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 245.752
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 78.864
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 107.492
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 243.355
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0316
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 245.861
Elapsed time for mlp_fused_gelu (2048x4x43520): 0.0012
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 240.374
Elapsed time for transformer_add_bias_dropout (2048x4x10880): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10880): 0.0004

Attention duration (in seconds): 0.0751
Attention throughput (in TFLOP/s): 113.057
MLP duration (in seconds): 0.0651
MLP throughput (in TFLOP/s): 238.433
Transformer duration (in seconds): 0.1426
Transformer throughput (in TFLOP/s): 168.275
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0438
Attention throughput (in TFLOP/s): 193.638
MLP duration (in seconds): 0.0658
MLP throughput (in TFLOP/s): 235.968
Transformer duration (in seconds): 0.1137
Transformer throughput (in TFLOP/s): 211.172
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10944x32832, b=2048): 0.0240
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10944x32832, b=2048): 245.351
Elapsed time for attention_key_query_prob (256x2048x171x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x171x2048): 56.237
Elapsed time for attention_prob_times_values (256x2048x2048x171): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x171): 64.070
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x10944x10944, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10944x10944, b=2048): 244.043
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x10944x43776, b=2048): 0.0319
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10944x43776, b=2048): 246.002
Elapsed time for mlp_fused_gelu (2048x4x43776): 0.0013
Elapsed time for mlp_4h_to_h (4x43776x10944, b=2048): 0.0312
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43776x10944, b=2048): 251.710
Elapsed time for transformer_add_bias_dropout (2048x4x10944): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10944): 0.0004

Attention duration (in seconds): 0.0797
Attention throughput (in TFLOP/s): 107.697
MLP duration (in seconds): 0.0643
MLP throughput (in TFLOP/s): 243.977
Transformer duration (in seconds): 0.1466
Transformer throughput (in TFLOP/s): 165.671
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0500
Attention throughput (in TFLOP/s): 171.762
MLP duration (in seconds): 0.0646
MLP throughput (in TFLOP/s): 242.949
Transformer duration (in seconds): 0.1182
Transformer throughput (in TFLOP/s): 205.490
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 246.526
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 79.241
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 108.558
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 245.407
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 246.036
Elapsed time for mlp_fused_gelu (2048x4x44032): 0.0013
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0314
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 253.261
Elapsed time for transformer_add_bias_dropout (2048x4x11008): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11008): 0.0004

Attention duration (in seconds): 0.0757
Attention throughput (in TFLOP/s): 114.638
MLP duration (in seconds): 0.0649
MLP throughput (in TFLOP/s): 244.747
Transformer duration (in seconds): 0.1431
Transformer throughput (in TFLOP/s): 171.606
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0443
Attention throughput (in TFLOP/s): 196.141
MLP duration (in seconds): 0.0652
MLP throughput (in TFLOP/s): 243.620
Transformer duration (in seconds): 0.1132
Transformer throughput (in TFLOP/s): 217.048
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11072x33216, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11072x33216, b=2048): 245.405
Elapsed time for attention_key_query_prob (256x2048x173x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x173x2048): 56.602
Elapsed time for attention_prob_times_values (256x2048x2048x173): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x173): 64.358
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11072x11072, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x11072x11072, b=2048): 243.638
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11072x44288, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11072x44288, b=2048): 246.038
Elapsed time for mlp_fused_gelu (2048x4x44288): 0.0013
Elapsed time for mlp_4h_to_h (4x44288x11072, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44288x11072, b=2048): 238.747
Elapsed time for transformer_add_bias_dropout (2048x4x11072): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11072): 0.0005

Attention duration (in seconds): 0.0805
Attention throughput (in TFLOP/s): 108.979
MLP duration (in seconds): 0.0676
MLP throughput (in TFLOP/s): 237.788
Transformer duration (in seconds): 0.1507
Transformer throughput (in TFLOP/s): 164.890
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0509
Attention throughput (in TFLOP/s): 172.313
MLP duration (in seconds): 0.0684
MLP throughput (in TFLOP/s): 234.783
Transformer duration (in seconds): 0.1231
Transformer throughput (in TFLOP/s): 201.794
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0248
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 246.268
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 79.872
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 109.193
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 245.172
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 246.220
Elapsed time for mlp_fused_gelu (2048x4x44544): 0.0013
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0338
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 240.149
Elapsed time for transformer_add_bias_dropout (2048x4x11136): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11136): 0.0005

Attention duration (in seconds): 0.0765
Attention throughput (in TFLOP/s): 115.932
MLP duration (in seconds): 0.0681
MLP throughput (in TFLOP/s): 238.595
Transformer duration (in seconds): 0.1472
Transformer throughput (in TFLOP/s): 170.665
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0452
Attention throughput (in TFLOP/s): 196.516
MLP duration (in seconds): 0.0690
MLP throughput (in TFLOP/s): 235.556
Transformer duration (in seconds): 0.1182
Transformer throughput (in TFLOP/s): 212.589
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11200x33600, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11200x33600, b=2048): 245.937
Elapsed time for attention_key_query_prob (256x2048x175x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x175x2048): 56.880
Elapsed time for attention_prob_times_values (256x2048x2048x175): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x175): 63.151
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11200x11200, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11200x11200, b=2048): 242.863
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11200x44800, b=2048): 0.0334
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11200x44800, b=2048): 246.303
Elapsed time for mlp_fused_gelu (2048x4x44800): 0.0013
Elapsed time for mlp_4h_to_h (4x44800x11200, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44800x11200, b=2048): 251.520
Elapsed time for transformer_add_bias_dropout (2048x4x11200): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11200): 0.0005

Attention duration (in seconds): 0.0815
Attention throughput (in TFLOP/s): 110.101
MLP duration (in seconds): 0.0673
MLP throughput (in TFLOP/s): 244.146
Transformer duration (in seconds): 0.1514
Transformer throughput (in TFLOP/s): 167.825
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0515
Attention throughput (in TFLOP/s): 174.067
MLP duration (in seconds): 0.0676
MLP throughput (in TFLOP/s): 243.160
Transformer duration (in seconds): 0.1231
Transformer throughput (in TFLOP/s): 206.517
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0253
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 246.449
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 123.118
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 161.531
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 244.195
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 246.438
Elapsed time for mlp_fused_gelu (2048x4x45056): 0.0013
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 254.277
Elapsed time for transformer_add_bias_dropout (2048x4x11264): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11264): 0.0005

Attention duration (in seconds): 0.0746
Attention throughput (in TFLOP/s): 121.550
MLP duration (in seconds): 0.0677
MLP throughput (in TFLOP/s): 245.529
Transformer duration (in seconds): 0.1450
Transformer throughput (in TFLOP/s): 177.302
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0445
Attention throughput (in TFLOP/s): 204.030
MLP duration (in seconds): 0.0682
MLP throughput (in TFLOP/s): 243.905
Transformer duration (in seconds): 0.1163
Transformer throughput (in TFLOP/s): 221.063
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11328x33984, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11328x33984, b=2048): 246.040
Elapsed time for attention_key_query_prob (256x2048x177x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x177x2048): 57.183
Elapsed time for attention_prob_times_values (256x2048x2048x177): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x177): 63.487
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11328x11328, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_linear_projection (4x11328x11328, b=2048): 244.512
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11328x45312, b=2048): 0.0341
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11328x45312, b=2048): 246.639
Elapsed time for mlp_fused_gelu (2048x4x45312): 0.0013
Elapsed time for mlp_4h_to_h (4x45312x11328, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45312x11328, b=2048): 249.850
Elapsed time for transformer_add_bias_dropout (2048x4x11328): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11328): 0.0005

Attention duration (in seconds): 0.0823
Attention throughput (in TFLOP/s): 111.460
MLP duration (in seconds): 0.0691
MLP throughput (in TFLOP/s): 243.570
Transformer duration (in seconds): 0.1539
Transformer throughput (in TFLOP/s): 168.823
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0522
Attention throughput (in TFLOP/s): 175.606
MLP duration (in seconds): 0.0695
MLP throughput (in TFLOP/s): 242.130
Transformer duration (in seconds): 0.1258
Transformer throughput (in TFLOP/s): 206.563
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0259
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 246.435
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 81.130
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 110.969
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 245.905
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0345
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 246.498
Elapsed time for mlp_fused_gelu (2048x4x45568): 0.0013
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0338
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 251.651
Elapsed time for transformer_add_bias_dropout (2048x4x11392): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11392): 0.0005

Attention duration (in seconds): 0.0781
Attention throughput (in TFLOP/s): 118.703
MLP duration (in seconds): 0.0696
MLP throughput (in TFLOP/s): 244.384
Transformer duration (in seconds): 0.1503
Transformer throughput (in TFLOP/s): 174.838
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0467
Attention throughput (in TFLOP/s): 198.373
MLP duration (in seconds): 0.0700
MLP throughput (in TFLOP/s): 243.038
Transformer duration (in seconds): 0.1204
Transformer throughput (in TFLOP/s): 218.228
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11456x34368, b=2048): 0.0262
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11456x34368, b=2048): 246.176
Elapsed time for attention_key_query_prob (256x2048x179x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x179x2048): 57.297
Elapsed time for attention_prob_times_values (256x2048x2048x179): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x179): 66.058
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11456x11456, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_linear_projection (4x11456x11456, b=2048): 243.618
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11456x45824, b=2048): 0.0349
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11456x45824, b=2048): 246.633
Elapsed time for mlp_fused_gelu (2048x4x45824): 0.0013
Elapsed time for mlp_4h_to_h (4x45824x11456, b=2048): 0.0340
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45824x11456, b=2048): 253.111
Elapsed time for transformer_add_bias_dropout (2048x4x11456): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11456): 0.0005

Attention duration (in seconds): 0.0830
Attention throughput (in TFLOP/s): 112.942
MLP duration (in seconds): 0.0702
MLP throughput (in TFLOP/s): 245.164
Transformer duration (in seconds): 0.1558
Transformer throughput (in TFLOP/s): 170.580
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0530
Attention throughput (in TFLOP/s): 176.771
MLP duration (in seconds): 0.0706
MLP throughput (in TFLOP/s): 243.517
Transformer duration (in seconds): 0.1278
Transformer throughput (in TFLOP/s): 207.982
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 246.734
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 82.216
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 111.922
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 245.090
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0353
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 246.677
Elapsed time for mlp_fused_gelu (2048x4x46080): 0.0013
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0360
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 241.370
Elapsed time for transformer_add_bias_dropout (2048x4x11520): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11520): 0.0005

Attention duration (in seconds): 0.0789
Attention throughput (in TFLOP/s): 120.079
MLP duration (in seconds): 0.0726
MLP throughput (in TFLOP/s): 239.564
Transformer duration (in seconds): 0.1541
Transformer throughput (in TFLOP/s): 174.312
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0473
Attention throughput (in TFLOP/s): 200.080
MLP duration (in seconds): 0.0735
MLP throughput (in TFLOP/s): 236.641
Transformer duration (in seconds): 0.1249
Transformer throughput (in TFLOP/s): 215.069
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11584x34752, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11584x34752, b=2048): 246.252
Elapsed time for attention_key_query_prob (256x2048x181x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x181x2048): 57.859
Elapsed time for attention_prob_times_values (256x2048x2048x181): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x181): 66.695
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11584x11584, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11584x11584, b=2048): 245.322
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11584x46336, b=2048): 0.0357
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11584x46336, b=2048): 246.639
Elapsed time for mlp_fused_gelu (2048x4x46336): 0.0013
Elapsed time for mlp_4h_to_h (4x46336x11584, b=2048): 0.0366
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46336x11584, b=2048): 240.016
Elapsed time for transformer_add_bias_dropout (2048x4x11584): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11584): 0.0005

Attention duration (in seconds): 0.0837
Attention throughput (in TFLOP/s): 114.364
MLP duration (in seconds): 0.0736
MLP throughput (in TFLOP/s): 238.902
Transformer duration (in seconds): 0.1600
Transformer throughput (in TFLOP/s): 169.751
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0538
Attention throughput (in TFLOP/s): 178.047
MLP duration (in seconds): 0.0745
MLP throughput (in TFLOP/s): 236.010
Transformer duration (in seconds): 0.1325
Transformer throughput (in TFLOP/s): 204.961
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 246.860
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 83.032
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 112.830
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 246.721
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0360
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 246.700
Elapsed time for mlp_fused_gelu (2048x4x46592): 0.0013
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0368
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 241.320
Elapsed time for transformer_add_bias_dropout (2048x4x11648): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11648): 0.0005

Attention duration (in seconds): 0.0796
Attention throughput (in TFLOP/s): 121.530
MLP duration (in seconds): 0.0742
MLP throughput (in TFLOP/s): 239.600
Transformer duration (in seconds): 0.1565
Transformer throughput (in TFLOP/s): 175.455
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0480
Attention throughput (in TFLOP/s): 201.477
MLP duration (in seconds): 0.0751
MLP throughput (in TFLOP/s): 236.886
Transformer duration (in seconds): 0.1273
Transformer throughput (in TFLOP/s): 215.647
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11712x35136, b=2048): 0.0274
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11712x35136, b=2048): 246.455
Elapsed time for attention_key_query_prob (256x2048x183x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x183x2048): 57.757
Elapsed time for attention_prob_times_values (256x2048x2048x183): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x183): 64.963
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11712x11712, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x11712x11712, b=2048): 244.655
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11712x46848, b=2048): 0.0364
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11712x46848, b=2048): 246.767
Elapsed time for mlp_fused_gelu (2048x4x46848): 0.0013
Elapsed time for mlp_4h_to_h (4x46848x11712, b=2048): 0.0355
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46848x11712, b=2048): 252.909
Elapsed time for transformer_add_bias_dropout (2048x4x11712): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11712): 0.0005

Attention duration (in seconds): 0.0848
Attention throughput (in TFLOP/s): 115.277
MLP duration (in seconds): 0.0733
MLP throughput (in TFLOP/s): 245.239
Transformer duration (in seconds): 0.1608
Transformer throughput (in TFLOP/s): 172.579
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0546
Attention throughput (in TFLOP/s): 178.946
MLP duration (in seconds): 0.0738
MLP throughput (in TFLOP/s): 243.631
Transformer duration (in seconds): 0.1323
Transformer throughput (in TFLOP/s): 209.750
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 246.660
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 113.984
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 166.398
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 245.926
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0368
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 246.693
Elapsed time for mlp_fused_gelu (2048x4x47104): 0.0013
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0357
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 254.223
Elapsed time for transformer_add_bias_dropout (2048x4x11776): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11776): 0.0005

Attention duration (in seconds): 0.0781
Attention throughput (in TFLOP/s): 126.453
MLP duration (in seconds): 0.0739
MLP throughput (in TFLOP/s): 245.840
Transformer duration (in seconds): 0.1548
Transformer throughput (in TFLOP/s): 181.274
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0476
Attention throughput (in TFLOP/s): 207.614
MLP duration (in seconds): 0.0744
MLP throughput (in TFLOP/s): 244.316
Transformer duration (in seconds): 0.1255
Transformer throughput (in TFLOP/s): 223.475
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11840x35520, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11840x35520, b=2048): 246.452
Elapsed time for attention_key_query_prob (256x2048x185x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x185x2048): 58.269
Elapsed time for attention_prob_times_values (256x2048x2048x185): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x185): 65.351
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11840x11840, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_linear_projection (4x11840x11840, b=2048): 243.898
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11840x47360, b=2048): 0.0372
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11840x47360, b=2048): 246.979
Elapsed time for mlp_fused_gelu (2048x4x47360): 0.0014
Elapsed time for mlp_4h_to_h (4x47360x11840, b=2048): 0.0383
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47360x11840, b=2048): 239.872
Elapsed time for transformer_add_bias_dropout (2048x4x11840): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11840): 0.0005

Attention duration (in seconds): 0.0857
Attention throughput (in TFLOP/s): 116.505
MLP duration (in seconds): 0.0769
MLP throughput (in TFLOP/s): 239.086
Transformer duration (in seconds): 0.1653
Transformer throughput (in TFLOP/s): 171.583
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0555
Attention throughput (in TFLOP/s): 179.984
MLP duration (in seconds): 0.0777
MLP throughput (in TFLOP/s): 236.455
Transformer duration (in seconds): 0.1372
Transformer throughput (in TFLOP/s): 206.653
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0282
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 247.015
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 84.630
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 115.057
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 245.213
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0376
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 246.980
Elapsed time for mlp_fused_gelu (2048x4x47616): 0.0014
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 241.142
Elapsed time for transformer_add_bias_dropout (2048x4x11904): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11904): 0.0005

Attention duration (in seconds): 0.0813
Attention throughput (in TFLOP/s): 124.119
MLP duration (in seconds): 0.0775
MLP throughput (in TFLOP/s): 239.744
Transformer duration (in seconds): 0.1615
Transformer throughput (in TFLOP/s): 177.499
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0498
Attention throughput (in TFLOP/s): 202.354
MLP duration (in seconds): 0.0785
MLP throughput (in TFLOP/s): 236.667
Transformer duration (in seconds): 0.1328
Transformer throughput (in TFLOP/s): 215.858
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11968x35904, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11968x35904, b=2048): 253.886
Elapsed time for attention_key_query_prob (256x2048x187x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x187x2048): 58.922
Elapsed time for attention_prob_times_values (256x2048x2048x187): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x187): 69.164
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x11968x11968, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11968x11968, b=2048): 258.738
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x11968x47872, b=2048): 0.0367
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11968x47872, b=2048): 255.567
Elapsed time for mlp_fused_gelu (2048x4x47872): 0.0014
Elapsed time for mlp_4h_to_h (4x47872x11968, b=2048): 0.0372
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47872x11968, b=2048): 252.584
Elapsed time for transformer_add_bias_dropout (2048x4x11968): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11968): 0.0005

Attention duration (in seconds): 0.0848
Attention throughput (in TFLOP/s): 120.134
MLP duration (in seconds): 0.0753
MLP throughput (in TFLOP/s): 249.450
Transformer duration (in seconds): 0.1628
Transformer throughput (in TFLOP/s): 177.859
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0559
Attention throughput (in TFLOP/s): 182.389
MLP duration (in seconds): 0.0758
MLP throughput (in TFLOP/s): 247.724
Transformer duration (in seconds): 0.1359
Transformer throughput (in TFLOP/s): 213.148
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 254.990
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 85.575
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 116.390
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 260.015
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0372
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 255.075
Elapsed time for mlp_fused_gelu (2048x4x48128): 0.0014
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0378
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 251.119
Elapsed time for transformer_add_bias_dropout (2048x4x12032): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12032): 0.0005

Attention duration (in seconds): 0.0806
Attention throughput (in TFLOP/s): 127.699
MLP duration (in seconds): 0.0764
MLP throughput (in TFLOP/s): 248.524
Transformer duration (in seconds): 0.1597
Transformer throughput (in TFLOP/s): 183.245
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 205.464
MLP duration (in seconds): 0.0768
MLP throughput (in TFLOP/s): 247.025
Transformer duration (in seconds): 0.1306
Transformer throughput (in TFLOP/s): 224.078
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12096x36288, b=2048): 0.0283
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12096x36288, b=2048): 253.741
Elapsed time for attention_key_query_prob (256x2048x189x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x189x2048): 59.465
Elapsed time for attention_prob_times_values (256x2048x2048x189): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x189): 69.927
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12096x12096, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x12096x12096, b=2048): 253.385
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12096x48384, b=2048): 0.0375
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12096x48384, b=2048): 255.952
Elapsed time for mlp_fused_gelu (2048x4x48384): 0.0014
Elapsed time for mlp_4h_to_h (4x48384x12096, b=2048): 0.0400
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48384x12096, b=2048): 239.557
Elapsed time for transformer_add_bias_dropout (2048x4x12096): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12096): 0.0005

Attention duration (in seconds): 0.0858
Attention throughput (in TFLOP/s): 121.171
MLP duration (in seconds): 0.0789
MLP throughput (in TFLOP/s): 243.144
Transformer duration (in seconds): 0.1675
Transformer throughput (in TFLOP/s): 176.588
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0567
Attention throughput (in TFLOP/s): 183.284
MLP duration (in seconds): 0.0799
MLP throughput (in TFLOP/s): 239.975
Transformer duration (in seconds): 0.1409
Transformer throughput (in TFLOP/s): 209.881
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 254.847
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 86.717
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 117.757
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 247.714
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0379
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 255.531
Elapsed time for mlp_fused_gelu (2048x4x48640): 0.0014
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0402
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 240.915
Elapsed time for transformer_add_bias_dropout (2048x4x12160): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12160): 0.0005

Attention duration (in seconds): 0.0819
Attention throughput (in TFLOP/s): 128.334
MLP duration (in seconds): 0.0795
MLP throughput (in TFLOP/s): 243.676
Transformer duration (in seconds): 0.1642
Transformer throughput (in TFLOP/s): 182.021
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0513
Attention throughput (in TFLOP/s): 204.991
MLP duration (in seconds): 0.0806
MLP throughput (in TFLOP/s): 240.400
Transformer duration (in seconds): 0.1364
Transformer throughput (in TFLOP/s): 219.076
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 64, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12224x36672, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12224x36672, b=2048): 253.320
Elapsed time for attention_key_query_prob (256x2048x191x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x191x2048): 60.061
Elapsed time for attention_prob_times_values (256x2048x2048x191): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x191): 68.793
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12224x12224, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x12224x12224, b=2048): 255.689
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12224x48896, b=2048): 0.0384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12224x48896, b=2048): 254.864
Elapsed time for mlp_fused_gelu (2048x4x48896): 0.0014
Elapsed time for mlp_4h_to_h (4x48896x12224, b=2048): 0.0390
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48896x12224, b=2048): 250.971
Elapsed time for transformer_add_bias_dropout (2048x4x12224): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12224): 0.0005

Attention duration (in seconds): 0.0868
Attention throughput (in TFLOP/s): 122.328
MLP duration (in seconds): 0.0788
MLP throughput (in TFLOP/s): 248.425
Transformer duration (in seconds): 0.1684
Transformer throughput (in TFLOP/s): 179.307
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0575
Attention throughput (in TFLOP/s): 184.452
MLP duration (in seconds): 0.0796
MLP throughput (in TFLOP/s): 246.180
Transformer duration (in seconds): 0.1407
Transformer throughput (in TFLOP/s): 214.573
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 254.652
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 136.150
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 175.287
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 257.030
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0387
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 255.572
Elapsed time for mlp_fused_gelu (2048x4x49152): 0.0014
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0390
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 253.484
Elapsed time for transformer_add_bias_dropout (2048x4x12288): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12288): 0.0005

Attention duration (in seconds): 0.0796
Attention throughput (in TFLOP/s): 134.754
MLP duration (in seconds): 0.0792
MLP throughput (in TFLOP/s): 250.009
Transformer duration (in seconds): 0.1615
Transformer throughput (in TFLOP/s): 188.886
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0504
Attention throughput (in TFLOP/s): 212.704
MLP duration (in seconds): 0.0801
MLP throughput (in TFLOP/s): 247.072
Transformer duration (in seconds): 0.1344
Transformer throughput (in TFLOP/s): 227.039
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12352x37056, b=2048): 0.0294
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12352x37056, b=2048): 255.159
Elapsed time for attention_key_query_prob (256x2048x193x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x193x2048): 57.794
Elapsed time for attention_prob_times_values (256x2048x2048x193): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x193): 68.915
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12352x12352, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x12352x12352, b=2048): 258.014
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12352x49408, b=2048): 0.0392
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12352x49408, b=2048): 255.246
Elapsed time for mlp_fused_gelu (2048x4x49408): 0.0014
Elapsed time for mlp_4h_to_h (4x49408x12352, b=2048): 0.0412
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49408x12352, b=2048): 242.430
Elapsed time for transformer_add_bias_dropout (2048x4x12352): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12352): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 123.500
MLP duration (in seconds): 0.0818
MLP throughput (in TFLOP/s): 244.381
Transformer duration (in seconds): 0.1724
Transformer throughput (in TFLOP/s): 178.852
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0586
Attention throughput (in TFLOP/s): 184.747
MLP duration (in seconds): 0.0833
MLP throughput (in TFLOP/s): 240.125
Transformer duration (in seconds): 0.1464
Transformer throughput (in TFLOP/s): 210.627
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 254.334
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 82.467
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 117.480
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 259.272
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0395
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 255.978
Elapsed time for mlp_fused_gelu (2048x4x49664): 0.0014
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0420
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 240.718
Elapsed time for transformer_add_bias_dropout (2048x4x12416): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12416): 0.0005

Attention duration (in seconds): 0.0835
Attention throughput (in TFLOP/s): 130.900
MLP duration (in seconds): 0.0829
MLP throughput (in TFLOP/s): 243.854
Transformer duration (in seconds): 0.1693
Transformer throughput (in TFLOP/s): 183.968
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0532
Attention throughput (in TFLOP/s): 205.455
MLP duration (in seconds): 0.0839
MLP throughput (in TFLOP/s): 240.746
Transformer duration (in seconds): 0.1418
Transformer throughput (in TFLOP/s): 219.621
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 64, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12480x37440, b=2048): 0.0300
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12480x37440, b=2048): 255.161
Elapsed time for attention_key_query_prob (256x2048x195x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x195x2048): 57.763
Elapsed time for attention_prob_times_values (256x2048x2048x195): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x195): 71.294
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12480x12480, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x12480x12480, b=2048): 252.723
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12480x49920, b=2048): 0.0399
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12480x49920, b=2048): 255.668
Elapsed time for mlp_fused_gelu (2048x4x49920): 0.0014
Elapsed time for mlp_4h_to_h (4x49920x12480, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49920x12480, b=2048): 251.901
Elapsed time for transformer_add_bias_dropout (2048x4x12480): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12480): 0.0005

Attention duration (in seconds): 0.0886
Attention throughput (in TFLOP/s): 124.608
MLP duration (in seconds): 0.0819
MLP throughput (in TFLOP/s): 249.337
Transformer duration (in seconds): 0.1734
Transformer throughput (in TFLOP/s): 181.411
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0599
Attention throughput (in TFLOP/s): 184.517
MLP duration (in seconds): 0.0824
MLP throughput (in TFLOP/s): 247.728
Transformer duration (in seconds): 0.1464
Transformer throughput (in TFLOP/s): 214.944
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0303
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 255.113
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 82.907
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 116.667
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 247.305
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 254.910
Elapsed time for mlp_fused_gelu (2048x4x50176): 0.0014
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0407
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 253.324
Elapsed time for transformer_add_bias_dropout (2048x4x12544): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12544): 0.0005

Attention duration (in seconds): 0.0848
Attention throughput (in TFLOP/s): 131.472
MLP duration (in seconds): 0.0826
MLP throughput (in TFLOP/s): 249.693
Transformer duration (in seconds): 0.1703
Transformer throughput (in TFLOP/s): 186.574
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0543
Attention throughput (in TFLOP/s): 205.416
MLP duration (in seconds): 0.0834
MLP throughput (in TFLOP/s): 247.404
Transformer duration (in seconds): 0.1416
Transformer throughput (in TFLOP/s): 224.383
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12608x37824, b=2048): 0.0306
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12608x37824, b=2048): 255.262
Elapsed time for attention_key_query_prob (256x2048x197x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x197x2048): 58.187
Elapsed time for attention_prob_times_values (256x2048x2048x197): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x197): 71.259
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12608x12608, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12608x12608, b=2048): 253.669
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12608x50432, b=2048): 0.0408
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12608x50432, b=2048): 255.619
Elapsed time for mlp_fused_gelu (2048x4x50432): 0.0014
Elapsed time for mlp_4h_to_h (4x50432x12608, b=2048): 0.0411
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50432x12608, b=2048): 253.233
Elapsed time for transformer_add_bias_dropout (2048x4x12608): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12608): 0.0005

Attention duration (in seconds): 0.0895
Attention throughput (in TFLOP/s): 125.854
MLP duration (in seconds): 0.0833
MLP throughput (in TFLOP/s): 250.012
Transformer duration (in seconds): 0.1758
Transformer throughput (in TFLOP/s): 182.630
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 185.285
MLP duration (in seconds): 0.0842
MLP throughput (in TFLOP/s): 247.577
Transformer duration (in seconds): 0.1493
Transformer throughput (in TFLOP/s): 214.958
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0308
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 256.118
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 83.525
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 115.082
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 256.483
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0412
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 255.337
Elapsed time for mlp_fused_gelu (2048x4x50688): 0.0014
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 241.440
Elapsed time for transformer_add_bias_dropout (2048x4x12672): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12672): 0.0005

Attention duration (in seconds): 0.0853
Attention throughput (in TFLOP/s): 133.385
MLP duration (in seconds): 0.0862
MLP throughput (in TFLOP/s): 244.032
Transformer duration (in seconds): 0.1744
Transformer throughput (in TFLOP/s): 185.872
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0548
Attention throughput (in TFLOP/s): 207.426
MLP duration (in seconds): 0.0876
MLP throughput (in TFLOP/s): 240.380
Transformer duration (in seconds): 0.1470
Transformer throughput (in TFLOP/s): 220.495
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 64, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12736x38208, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12736x38208, b=2048): 258.113
Elapsed time for attention_key_query_prob (256x2048x199x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x199x2048): 58.202
Elapsed time for attention_prob_times_values (256x2048x2048x199): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x199): 69.038
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12736x12736, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12736x12736, b=2048): 254.950
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12736x50944, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12736x50944, b=2048): 256.015
Elapsed time for mlp_fused_gelu (2048x4x50944): 0.0015
Elapsed time for mlp_4h_to_h (4x50944x12736, b=2048): 0.0422
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50944x12736, b=2048): 251.974
Elapsed time for transformer_add_bias_dropout (2048x4x12736): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12736): 0.0005

Attention duration (in seconds): 0.0903
Attention throughput (in TFLOP/s): 127.256
MLP duration (in seconds): 0.0852
MLP throughput (in TFLOP/s): 249.645
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 183.599
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0619
Attention throughput (in TFLOP/s): 185.473
MLP duration (in seconds): 0.0858
MLP throughput (in TFLOP/s): 247.717
Transformer duration (in seconds): 0.1520
Transformer throughput (in TFLOP/s): 215.391
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 255.910
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 121.040
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 176.820
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 258.786
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0420
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 255.638
Elapsed time for mlp_fused_gelu (2048x4x51200): 0.0015
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0422
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 254.737
Elapsed time for transformer_add_bias_dropout (2048x4x12800): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12800): 0.0005

Attention duration (in seconds): 0.0832
Attention throughput (in TFLOP/s): 139.327
MLP duration (in seconds): 0.0856
MLP throughput (in TFLOP/s): 250.831
Transformer duration (in seconds): 0.1718
Transformer throughput (in TFLOP/s): 192.526
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0545
Attention throughput (in TFLOP/s): 212.796
MLP duration (in seconds): 0.0866
MLP throughput (in TFLOP/s): 248.089
Transformer duration (in seconds): 0.1448
Transformer throughput (in TFLOP/s): 228.457
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12864x38592, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12864x38592, b=2048): 254.792
Elapsed time for attention_key_query_prob (256x2048x201x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x201x2048): 58.617
Elapsed time for attention_prob_times_values (256x2048x2048x201): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x201): 68.864
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12864x12864, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12864x12864, b=2048): 259.685
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12864x51456, b=2048): 0.0425
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12864x51456, b=2048): 255.036
Elapsed time for mlp_fused_gelu (2048x4x51456): 0.0015
Elapsed time for mlp_4h_to_h (4x51456x12864, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51456x12864, b=2048): 249.658
Elapsed time for transformer_add_bias_dropout (2048x4x12864): 0.0010
Elapsed time for transformer_layer_norm (2048x4x12864): 0.0005

Attention duration (in seconds): 0.0914
Attention throughput (in TFLOP/s): 128.100
MLP duration (in seconds): 0.0874
MLP throughput (in TFLOP/s): 248.081
Transformer duration (in seconds): 0.1818
Transformer throughput (in TFLOP/s): 183.701
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0628
Attention throughput (in TFLOP/s): 186.474
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 246.941
Transformer duration (in seconds): 0.1544
Transformer throughput (in TFLOP/s): 216.324
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 256.005
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 84.630
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 115.316
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 261.006
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0428
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 255.968
Elapsed time for mlp_fused_gelu (2048x4x51712): 0.0015
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 250.574
Elapsed time for transformer_add_bias_dropout (2048x4x12928): 0.0010
Elapsed time for transformer_layer_norm (2048x4x12928): 0.0005

Attention duration (in seconds): 0.0869
Attention throughput (in TFLOP/s): 136.076
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 248.998
Transformer duration (in seconds): 0.1778
Transformer throughput (in TFLOP/s): 189.678
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0569
Attention throughput (in TFLOP/s): 207.680
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 248.067
Transformer duration (in seconds): 0.1495
Transformer throughput (in TFLOP/s): 225.544
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12992x38976, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12992x38976, b=2048): 254.646
Elapsed time for attention_key_query_prob (256x2048x203x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x203x2048): 59.555
Elapsed time for attention_prob_times_values (256x2048x2048x203): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x203): 71.679
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x12992x12992, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x12992x12992, b=2048): 254.795
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x12992x51968, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12992x51968, b=2048): 255.349
Elapsed time for mlp_fused_gelu (2048x4x51968): 0.0015
Elapsed time for mlp_4h_to_h (4x51968x12992, b=2048): 0.0440
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51968x12992, b=2048): 251.545
Elapsed time for transformer_add_bias_dropout (2048x4x12992): 0.0010
Elapsed time for transformer_layer_norm (2048x4x12992): 0.0005

Attention duration (in seconds): 0.0922
Attention throughput (in TFLOP/s): 129.383
MLP duration (in seconds): 0.0888
MLP throughput (in TFLOP/s): 249.200
Transformer duration (in seconds): 0.1840
Transformer throughput (in TFLOP/s): 185.087
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0639
Attention throughput (in TFLOP/s): 186.880
MLP duration (in seconds): 0.0897
MLP throughput (in TFLOP/s): 246.765
Transformer duration (in seconds): 0.1578
Transformer throughput (in TFLOP/s): 215.777
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0328
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 255.815
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.563
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.546
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.063
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 256.123
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0444
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 251.846
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0879
Attention throughput (in TFLOP/s): 136.998
MLP duration (in seconds): 0.0895
MLP throughput (in TFLOP/s): 249.739
Transformer duration (in seconds): 0.1804
Transformer throughput (in TFLOP/s): 190.638
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0578
Attention throughput (in TFLOP/s): 208.409
MLP duration (in seconds): 0.0900
MLP throughput (in TFLOP/s): 248.292
Transformer duration (in seconds): 0.1527
Transformer throughput (in TFLOP/s): 225.158
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 64, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13120x39360, b=2048): 0.0332
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13120x39360, b=2048): 254.465
Elapsed time for attention_key_query_prob (256x2048x205x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x205x2048): 59.902
Elapsed time for attention_prob_times_values (256x2048x2048x205): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x205): 71.881
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x13120x13120, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13120x13120, b=2048): 257.124
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x13120x52480, b=2048): 0.0441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13120x52480, b=2048): 255.812
Elapsed time for mlp_fused_gelu (2048x4x52480): 0.0015
Elapsed time for mlp_4h_to_h (4x52480x13120, b=2048): 0.0451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52480x13120, b=2048): 250.389
Elapsed time for transformer_add_bias_dropout (2048x4x13120): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13120): 0.0005

Attention duration (in seconds): 0.0931
Attention throughput (in TFLOP/s): 130.639
MLP duration (in seconds): 0.0906
MLP throughput (in TFLOP/s): 248.894
Transformer duration (in seconds): 0.1868
Transformer throughput (in TFLOP/s): 185.922
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0647
Attention throughput (in TFLOP/s): 187.873
MLP duration (in seconds): 0.0912
MLP throughput (in TFLOP/s): 247.408
Transformer duration (in seconds): 0.1609
Transformer throughput (in TFLOP/s): 215.811
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 255.594
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 86.010
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 117.146
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 258.369
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 255.142
Elapsed time for mlp_fused_gelu (2048x4x52736): 0.0015
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0454
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 251.008
Elapsed time for transformer_add_bias_dropout (2048x4x13184): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13184): 0.0005

Attention duration (in seconds): 0.0888
Attention throughput (in TFLOP/s): 138.290
MLP duration (in seconds): 0.0915
MLP throughput (in TFLOP/s): 248.899
Transformer duration (in seconds): 0.1833
Transformer throughput (in TFLOP/s): 191.236
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0587
Attention throughput (in TFLOP/s): 209.219
MLP duration (in seconds): 0.0920
MLP throughput (in TFLOP/s): 247.581
Transformer duration (in seconds): 0.1551
Transformer throughput (in TFLOP/s): 226.008
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13248x39744, b=2048): 0.0339
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13248x39744, b=2048): 254.368
Elapsed time for attention_key_query_prob (256x2048x207x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x207x2048): 60.254
Elapsed time for attention_prob_times_values (256x2048x2048x207): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x207): 71.180
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(26288390144, 42481549312)
Elapsed time for attention_linear_projection (4x13248x13248, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_linear_projection (4x13248x13248, b=2048): 256.714
(26288390144, 42481549312)
Elapsed time for mlp_h_to_4h (4x13248x52992, b=2048): 0.0449
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13248x52992, b=2048): 256.138
Elapsed time for mlp_fused_gelu (2048x4x52992): 0.0015
Elapsed time for mlp_4h_to_h (4x52992x13248, b=2048): 0.0460
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52992x13248, b=2048): 249.996
Elapsed time for transformer_add_bias_dropout (2048x4x13248): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13248): 0.0005

Attention duration (in seconds): 0.0941
Attention throughput (in TFLOP/s): 131.624
MLP duration (in seconds): 0.0924
MLP throughput (in TFLOP/s): 248.892
Transformer duration (in seconds): 0.1896
Transformer throughput (in TFLOP/s): 186.666
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0654
Attention throughput (in TFLOP/s): 189.343
MLP duration (in seconds): 0.0929
MLP throughput (in TFLOP/s): 247.545
Transformer duration (in seconds): 0.1627
Transformer throughput (in TFLOP/s): 217.498
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 255.414
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 133.516
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 187.294
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 260.509
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0454
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 255.604
Elapsed time for mlp_fused_gelu (2048x4x53248): 0.0015
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 251.236
Elapsed time for transformer_add_bias_dropout (2048x4x13312): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13312): 0.0005

Attention duration (in seconds): 0.0864
Attention throughput (in TFLOP/s): 144.769
MLP duration (in seconds): 0.0932
MLP throughput (in TFLOP/s): 249.262
Transformer duration (in seconds): 0.1826
Transformer throughput (in TFLOP/s): 195.667
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0582
Attention throughput (in TFLOP/s): 214.847
MLP duration (in seconds): 0.0937
MLP throughput (in TFLOP/s): 247.875
Transformer duration (in seconds): 0.1567
Transformer throughput (in TFLOP/s): 228.011
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13376x40128, b=2048): 0.0346
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13376x40128, b=2048): 254.134
Elapsed time for attention_key_query_prob (256x2048x209x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x209x2048): 60.515
Elapsed time for attention_prob_times_values (256x2048x2048x209): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x209): 71.067
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13376x13376, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x13376x13376, b=2048): 254.626
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13376x53504, b=2048): 0.0458
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13376x53504, b=2048): 256.011
Elapsed time for mlp_fused_gelu (2048x4x53504): 0.0015
Elapsed time for mlp_4h_to_h (4x53504x13376, b=2048): 0.0473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53504x13376, b=2048): 247.865
Elapsed time for transformer_add_bias_dropout (2048x4x13376): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13376): 0.0006

Attention duration (in seconds): 0.0953
Attention throughput (in TFLOP/s): 132.512
MLP duration (in seconds): 0.0946
MLP throughput (in TFLOP/s): 247.805
Transformer duration (in seconds): 0.1930
Transformer throughput (in TFLOP/s): 186.922
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0667
Attention throughput (in TFLOP/s): 189.290
MLP duration (in seconds): 0.0949
MLP throughput (in TFLOP/s): 247.191
Transformer duration (in seconds): 0.1660
Transformer throughput (in TFLOP/s): 217.262
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0348
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 255.239
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 87.130
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 118.657
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 255.776
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0463
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 255.791
Elapsed time for mlp_fused_gelu (2048x4x53760): 0.0015
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 250.370
Elapsed time for transformer_add_bias_dropout (2048x4x13440): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13440): 0.0005

Attention duration (in seconds): 0.0907
Attention throughput (in TFLOP/s): 140.386
MLP duration (in seconds): 0.0951
MLP throughput (in TFLOP/s): 248.966
Transformer duration (in seconds): 0.1889
Transformer throughput (in TFLOP/s): 192.752
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 209.521
MLP duration (in seconds): 0.0956
MLP throughput (in TFLOP/s): 247.614
Transformer duration (in seconds): 0.1615
Transformer throughput (in TFLOP/s): 225.451
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13504x40512, b=2048): 0.0350
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13504x40512, b=2048): 255.936
Elapsed time for attention_key_query_prob (256x2048x211x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x211x2048): 60.625
Elapsed time for attention_prob_times_values (256x2048x2048x211): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x211): 73.004
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13504x13504, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13504x13504, b=2048): 256.610
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13504x54016, b=2048): 0.0470
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13504x54016, b=2048): 254.155
Elapsed time for mlp_fused_gelu (2048x4x54016): 0.0015
Elapsed time for mlp_4h_to_h (4x54016x13504, b=2048): 0.0477
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54016x13504, b=2048): 250.648
Elapsed time for transformer_add_bias_dropout (2048x4x13504): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13504): 0.0006

Attention duration (in seconds): 0.0958
Attention throughput (in TFLOP/s): 134.265
MLP duration (in seconds): 0.0962
MLP throughput (in TFLOP/s): 248.343
Transformer duration (in seconds): 0.1951
Transformer throughput (in TFLOP/s): 188.392
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0674
Attention throughput (in TFLOP/s): 190.857
MLP duration (in seconds): 0.0967
MLP throughput (in TFLOP/s): 247.076
Transformer duration (in seconds): 0.1692
Transformer throughput (in TFLOP/s): 217.276
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 254.875
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 87.997
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 119.457
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 257.671
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 255.947
Elapsed time for mlp_fused_gelu (2048x4x54272): 0.0015
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0481
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 250.703
Elapsed time for transformer_add_bias_dropout (2048x4x13568): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13568): 0.0006

Attention duration (in seconds): 0.0916
Attention throughput (in TFLOP/s): 141.636
MLP duration (in seconds): 0.0968
MLP throughput (in TFLOP/s): 249.243
Transformer duration (in seconds): 0.1915
Transformer throughput (in TFLOP/s): 193.724
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0612
Attention throughput (in TFLOP/s): 212.150
MLP duration (in seconds): 0.0973
MLP throughput (in TFLOP/s): 247.901
Transformer duration (in seconds): 0.1630
Transformer throughput (in TFLOP/s): 227.658
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13632x40896, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13632x40896, b=2048): 255.652
Elapsed time for attention_key_query_prob (256x2048x213x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x213x2048): 60.946
Elapsed time for attention_prob_times_values (256x2048x2048x213): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x213): 73.614
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13632x13632, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x13632x13632, b=2048): 255.829
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13632x54528, b=2048): 0.0477
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13632x54528, b=2048): 255.280
Elapsed time for mlp_fused_gelu (2048x4x54528): 0.0016
Elapsed time for mlp_4h_to_h (4x54528x13632, b=2048): 0.0486
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54528x13632, b=2048): 250.768
Elapsed time for transformer_add_bias_dropout (2048x4x13632): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13632): 0.0006

Attention duration (in seconds): 0.0968
Attention throughput (in TFLOP/s): 135.315
MLP duration (in seconds): 0.0978
MLP throughput (in TFLOP/s): 248.979
Transformer duration (in seconds): 0.1977
Transformer throughput (in TFLOP/s): 189.400
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0683
Attention throughput (in TFLOP/s): 191.727
MLP duration (in seconds): 0.0995
MLP throughput (in TFLOP/s): 244.819
Transformer duration (in seconds): 0.1716
Transformer throughput (in TFLOP/s): 218.195
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 254.800
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 89.255
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 120.471
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 260.024
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0480
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 256.220
Elapsed time for mlp_fused_gelu (2048x4x54784): 0.0016
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0491
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 250.323
Elapsed time for transformer_add_bias_dropout (2048x4x13696): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13696): 0.0006

Attention duration (in seconds): 0.0924
Attention throughput (in TFLOP/s): 143.017
MLP duration (in seconds): 0.0987
MLP throughput (in TFLOP/s): 249.222
Transformer duration (in seconds): 0.1942
Transformer throughput (in TFLOP/s): 194.667
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0620
Attention throughput (in TFLOP/s): 213.019
MLP duration (in seconds): 0.0992
MLP throughput (in TFLOP/s): 247.855
Transformer duration (in seconds): 0.1656
Transformer throughput (in TFLOP/s): 228.220
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13760x41280, b=2048): 0.0364
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13760x41280, b=2048): 255.604
Elapsed time for attention_key_query_prob (256x2048x215x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x215x2048): 61.301
Elapsed time for attention_prob_times_values (256x2048x2048x215): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x215): 72.503
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13760x13760, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x13760x13760, b=2048): 260.761
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13760x55040, b=2048): 0.0485
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13760x55040, b=2048): 255.737
Elapsed time for mlp_fused_gelu (2048x4x55040): 0.0016
Elapsed time for mlp_4h_to_h (4x55040x13760, b=2048): 0.0495
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55040x13760, b=2048): 250.852
Elapsed time for transformer_add_bias_dropout (2048x4x13760): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13760): 0.0006

Attention duration (in seconds): 0.0976
Attention throughput (in TFLOP/s): 136.571
MLP duration (in seconds): 0.0996
MLP throughput (in TFLOP/s): 249.274
Transformer duration (in seconds): 0.2003
Transformer throughput (in TFLOP/s): 190.416
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0691
Attention throughput (in TFLOP/s): 192.964
MLP duration (in seconds): 0.1005
MLP throughput (in TFLOP/s): 247.019
Transformer duration (in seconds): 0.1743
Transformer throughput (in TFLOP/s): 218.872
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 256.416
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 123.574
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 189.921
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 258.978
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0494
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 253.714
Elapsed time for mlp_fused_gelu (2048x4x55296): 0.0016
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0498
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 251.515
Elapsed time for transformer_add_bias_dropout (2048x4x13824): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13824): 0.0006

Attention duration (in seconds): 0.0903
Attention throughput (in TFLOP/s): 148.911
MLP duration (in seconds): 0.1007
MLP throughput (in TFLOP/s): 248.649
Transformer duration (in seconds): 0.1942
Transformer throughput (in TFLOP/s): 198.210
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0616
Attention throughput (in TFLOP/s): 218.407
MLP duration (in seconds): 0.1006
MLP throughput (in TFLOP/s): 248.912
Transformer duration (in seconds): 0.1675
Transformer throughput (in TFLOP/s): 229.854
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13888x41664, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13888x41664, b=2048): 255.628
Elapsed time for attention_key_query_prob (256x2048x217x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x217x2048): 61.793
Elapsed time for attention_prob_times_values (256x2048x2048x217): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x217): 72.760
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13888x13888, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13888x13888, b=2048): 256.207
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13888x55552, b=2048): 0.0496
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13888x55552, b=2048): 254.728
Elapsed time for mlp_fused_gelu (2048x4x55552): 0.0016
Elapsed time for mlp_4h_to_h (4x55552x13888, b=2048): 0.0508
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55552x13888, b=2048): 248.697
Elapsed time for transformer_add_bias_dropout (2048x4x13888): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13888): 0.0006

Attention duration (in seconds): 0.0988
Attention throughput (in TFLOP/s): 137.401
MLP duration (in seconds): 0.1020
MLP throughput (in TFLOP/s): 247.764
Transformer duration (in seconds): 0.2040
Transformer throughput (in TFLOP/s): 190.440
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0703
Attention throughput (in TFLOP/s): 193.160
MLP duration (in seconds): 0.1022
MLP throughput (in TFLOP/s): 247.269
Transformer duration (in seconds): 0.1777
Transformer throughput (in TFLOP/s): 218.692
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 256.269
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 90.475
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 122.158
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 257.370
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0504
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 253.331
Elapsed time for mlp_fused_gelu (2048x4x55808): 0.0016
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0512
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 249.032
Elapsed time for transformer_add_bias_dropout (2048x4x13952): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13952): 0.0006

Attention duration (in seconds): 0.0941
Attention throughput (in TFLOP/s): 145.450
MLP duration (in seconds): 0.1032
MLP throughput (in TFLOP/s): 247.285
Transformer duration (in seconds): 0.2005
Transformer throughput (in TFLOP/s): 195.526
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0645
Attention throughput (in TFLOP/s): 212.273
MLP duration (in seconds): 0.1040
MLP throughput (in TFLOP/s): 245.404
Transformer duration (in seconds): 0.1731
Transformer throughput (in TFLOP/s): 226.505
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 64, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14016x42048, b=2048): 0.0378
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14016x42048, b=2048): 255.435
Elapsed time for attention_key_query_prob (256x2048x219x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x219x2048): 62.479
Elapsed time for attention_prob_times_values (256x2048x2048x219): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x219): 76.003
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24140906496, 42481549312)
Elapsed time for attention_linear_projection (4x14016x14016, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x14016x14016, b=2048): 258.241
(24140906496, 42481549312)
Elapsed time for mlp_h_to_4h (4x14016x56064, b=2048): 0.0502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14016x56064, b=2048): 256.244
Elapsed time for mlp_fused_gelu (2048x4x56064): 0.0016
Elapsed time for mlp_4h_to_h (4x56064x14016, b=2048): 0.0514
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56064x14016, b=2048): 250.304
Elapsed time for transformer_add_bias_dropout (2048x4x14016): 0.0010
Elapsed time for transformer_layer_norm (2048x4x14016): 0.0006

Attention duration (in seconds): 0.0994
Attention throughput (in TFLOP/s): 138.995
MLP duration (in seconds): 0.1033
MLP throughput (in TFLOP/s): 249.316
Transformer duration (in seconds): 0.2059
Transformer throughput (in TFLOP/s): 192.147
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0712
Attention throughput (in TFLOP/s): 193.984
MLP duration (in seconds): 0.1037
MLP throughput (in TFLOP/s): 248.210
Transformer duration (in seconds): 0.1809
Transformer throughput (in TFLOP/s): 218.713
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 256.108
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 90.894
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 123.409
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 259.468
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0511
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 254.260
Elapsed time for mlp_fused_gelu (2048x4x56320): 0.0016
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0520
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 249.992
Elapsed time for transformer_add_bias_dropout (2048x4x14080): 0.0010
Elapsed time for transformer_layer_norm (2048x4x14080): 0.0006

Attention duration (in seconds): 0.0950
Attention throughput (in TFLOP/s): 146.709
MLP duration (in seconds): 0.1047
MLP throughput (in TFLOP/s): 248.232
Transformer duration (in seconds): 0.2029
Transformer throughput (in TFLOP/s): 196.746
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0651
Attention throughput (in TFLOP/s): 214.230
MLP duration (in seconds): 0.1048
MLP throughput (in TFLOP/s): 247.932
Transformer duration (in seconds): 0.1751
Transformer throughput (in TFLOP/s): 227.957
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14144x42432, b=2048): 0.0385
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14144x42432, b=2048): 255.195
Elapsed time for attention_key_query_prob (256x2048x221x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x221x2048): 63.240
Elapsed time for attention_prob_times_values (256x2048x2048x221): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x221): 76.851
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14144x14144, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x14144x14144, b=2048): 260.232
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14144x56576, b=2048): 0.0515
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14144x56576, b=2048): 254.466
Elapsed time for mlp_fused_gelu (2048x4x56576): 0.0016
Elapsed time for mlp_4h_to_h (4x56576x14144, b=2048): 0.0528
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56576x14144, b=2048): 248.272
Elapsed time for transformer_add_bias_dropout (2048x4x14144): 0.0010
Elapsed time for transformer_layer_norm (2048x4x14144): 0.0006

Attention duration (in seconds): 0.1002
Attention throughput (in TFLOP/s): 140.296
MLP duration (in seconds): 0.1059
MLP throughput (in TFLOP/s): 247.498
Transformer duration (in seconds): 0.2094
Transformer throughput (in TFLOP/s): 192.339
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0721
Attention throughput (in TFLOP/s): 195.135
MLP duration (in seconds): 0.1060
MLP throughput (in TFLOP/s): 247.427
Transformer duration (in seconds): 0.1838
Transformer throughput (in TFLOP/s): 219.212
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0387
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 256.227
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 92.369
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 124.819
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 261.257
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 256.065
Elapsed time for mlp_fused_gelu (2048x4x56832): 0.0016
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0530
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 249.542
Elapsed time for transformer_add_bias_dropout (2048x4x14208): 0.0010
Elapsed time for transformer_layer_norm (2048x4x14208): 0.0006

Attention duration (in seconds): 0.0958
Attention throughput (in TFLOP/s): 148.089
MLP duration (in seconds): 0.1063
MLP throughput (in TFLOP/s): 248.902
Transformer duration (in seconds): 0.2053
Transformer throughput (in TFLOP/s): 197.936
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0659
Attention throughput (in TFLOP/s): 215.334
MLP duration (in seconds): 0.1073
MLP throughput (in TFLOP/s): 246.566
Transformer duration (in seconds): 0.1765
Transformer throughput (in TFLOP/s): 230.224
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14272x42816, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14272x42816, b=2048): 255.044
Elapsed time for attention_key_query_prob (256x2048x223x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x223x2048): 64.352
Elapsed time for attention_prob_times_values (256x2048x2048x223): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x223): 76.130
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14272x14272, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14272x14272, b=2048): 253.587
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14272x57088, b=2048): 0.0524
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14272x57088, b=2048): 254.510
Elapsed time for mlp_fused_gelu (2048x4x57088): 0.0016
Elapsed time for mlp_4h_to_h (4x57088x14272, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57088x14272, b=2048): 250.442
Elapsed time for transformer_add_bias_dropout (2048x4x14272): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14272): 0.0006

Attention duration (in seconds): 0.1016
Attention throughput (in TFLOP/s): 140.879
MLP duration (in seconds): 0.1074
MLP throughput (in TFLOP/s): 248.628
Transformer duration (in seconds): 0.2122
Transformer throughput (in TFLOP/s): 193.219
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0732
Attention throughput (in TFLOP/s): 195.547
MLP duration (in seconds): 0.1076
MLP throughput (in TFLOP/s): 248.050
Transformer duration (in seconds): 0.1864
Transformer throughput (in TFLOP/s): 219.937
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0395
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 256.022
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 147.627
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 197.379
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 256.400
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0527
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 255.780
Elapsed time for mlp_fused_gelu (2048x4x57344): 0.0016
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0536
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 251.278
Elapsed time for transformer_add_bias_dropout (2048x4x14336): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14336): 0.0006

Attention duration (in seconds): 0.0937
Attention throughput (in TFLOP/s): 154.028
MLP duration (in seconds): 0.1079
MLP throughput (in TFLOP/s): 249.665
Transformer duration (in seconds): 0.2049
Transformer throughput (in TFLOP/s): 201.923
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0654
Attention throughput (in TFLOP/s): 220.496
MLP duration (in seconds): 0.1082
MLP throughput (in TFLOP/s): 249.072
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 231.627
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 64, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14400x43200, b=2048): 0.0400
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14400x43200, b=2048): 254.906
Elapsed time for attention_key_query_prob (256x2048x225x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x225x2048): 62.258
Elapsed time for attention_prob_times_values (256x2048x2048x225): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x225): 76.592
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14400x14400, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14400x14400, b=2048): 256.598
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14400x57600, b=2048): 0.0531
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14400x57600, b=2048): 255.896
Elapsed time for mlp_fused_gelu (2048x4x57600): 0.0016
Elapsed time for mlp_4h_to_h (4x57600x14400, b=2048): 0.0547
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57600x14400, b=2048): 248.454
Elapsed time for transformer_add_bias_dropout (2048x4x14400): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14400): 0.0006

Attention duration (in seconds): 0.1027
Attention throughput (in TFLOP/s): 141.727
MLP duration (in seconds): 0.1094
MLP throughput (in TFLOP/s): 248.332
Transformer duration (in seconds): 0.2155
Transformer throughput (in TFLOP/s): 193.689
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0745
Attention throughput (in TFLOP/s): 195.428
MLP duration (in seconds): 0.1103
MLP throughput (in TFLOP/s): 246.496
Transformer duration (in seconds): 0.1899
Transformer throughput (in TFLOP/s): 219.772
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0402
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 255.913
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 88.594
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 126.909
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 258.880
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0539
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 254.158
Elapsed time for mlp_fused_gelu (2048x4x57856): 0.0017
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0551
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 249.052
Elapsed time for transformer_add_bias_dropout (2048x4x14464): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14464): 0.0006

Attention duration (in seconds): 0.0981
Attention throughput (in TFLOP/s): 149.605
MLP duration (in seconds): 0.1106
MLP throughput (in TFLOP/s): 247.821
Transformer duration (in seconds): 0.2121
Transformer throughput (in TFLOP/s): 198.510
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0686
Attention throughput (in TFLOP/s): 214.023
MLP duration (in seconds): 0.1113
MLP throughput (in TFLOP/s): 246.339
Transformer duration (in seconds): 0.1847
Transformer throughput (in TFLOP/s): 227.919
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14528x43584, b=2048): 0.0407
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14528x43584, b=2048): 254.735
Elapsed time for attention_key_query_prob (256x2048x227x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x227x2048): 62.128
Elapsed time for attention_prob_times_values (256x2048x2048x227): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x227): 78.700
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14528x14528, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x14528x14528, b=2048): 259.735
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14528x58112, b=2048): 0.0540
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14528x58112, b=2048): 256.239
Elapsed time for mlp_fused_gelu (2048x4x58112): 0.0017
Elapsed time for mlp_4h_to_h (4x58112x14528, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58112x14528, b=2048): 250.143
Elapsed time for transformer_add_bias_dropout (2048x4x14528): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14528): 0.0006

Attention duration (in seconds): 0.1035
Attention throughput (in TFLOP/s): 143.082
MLP duration (in seconds): 0.1109
MLP throughput (in TFLOP/s): 249.369
Transformer duration (in seconds): 0.2178
Transformer throughput (in TFLOP/s): 195.031
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0755
Attention throughput (in TFLOP/s): 196.174
MLP duration (in seconds): 0.1125
MLP throughput (in TFLOP/s): 245.921
Transformer duration (in seconds): 0.1922
Transformer throughput (in TFLOP/s): 221.006
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0409
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 255.727
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 88.910
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 127.722
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 260.698
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0548
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 254.646
Elapsed time for mlp_fused_gelu (2048x4x58368): 0.0017
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0556
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 250.799
Elapsed time for transformer_add_bias_dropout (2048x4x14592): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14592): 0.0006

Attention duration (in seconds): 0.0991
Attention throughput (in TFLOP/s): 150.766
MLP duration (in seconds): 0.1121
MLP throughput (in TFLOP/s): 248.954
Transformer duration (in seconds): 0.2145
Transformer throughput (in TFLOP/s): 199.707
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0692
Attention throughput (in TFLOP/s): 215.855
MLP duration (in seconds): 0.1124
MLP throughput (in TFLOP/s): 248.319
Transformer duration (in seconds): 0.1867
Transformer throughput (in TFLOP/s): 229.454
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14656x43968, b=2048): 0.0412
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14656x43968, b=2048): 256.313
Elapsed time for attention_key_query_prob (256x2048x229x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x229x2048): 62.605
Elapsed time for attention_prob_times_values (256x2048x2048x229): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x229): 79.189
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14656x14656, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x14656x14656, b=2048): 254.545
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14656x58624, b=2048): 0.0549
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14656x58624, b=2048): 256.240
Elapsed time for mlp_fused_gelu (2048x4x58624): 0.0017
Elapsed time for mlp_4h_to_h (4x58624x14656, b=2048): 0.0569
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58624x14656, b=2048): 247.609
Elapsed time for transformer_add_bias_dropout (2048x4x14656): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14656): 0.0006

Attention duration (in seconds): 0.1045
Attention throughput (in TFLOP/s): 144.139
MLP duration (in seconds): 0.1135
MLP throughput (in TFLOP/s): 248.140
Transformer duration (in seconds): 0.2213
Transformer throughput (in TFLOP/s): 195.260
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0768
Attention throughput (in TFLOP/s): 196.012
MLP duration (in seconds): 0.1142
MLP throughput (in TFLOP/s): 246.551
Transformer duration (in seconds): 0.1952
Transformer throughput (in TFLOP/s): 221.424
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 255.564
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 89.935
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 128.459
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 255.728
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0555
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 255.994
Elapsed time for mlp_fused_gelu (2048x4x58880): 0.0017
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0570
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 249.015
Elapsed time for transformer_add_bias_dropout (2048x4x14720): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14720): 0.0006

Attention duration (in seconds): 0.1003
Attention throughput (in TFLOP/s): 151.433
MLP duration (in seconds): 0.1142
MLP throughput (in TFLOP/s): 248.741
Transformer duration (in seconds): 0.2178
Transformer throughput (in TFLOP/s): 200.093
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0704
Attention throughput (in TFLOP/s): 215.635
MLP duration (in seconds): 0.1149
MLP throughput (in TFLOP/s): 247.156
Transformer duration (in seconds): 0.1903
Transformer throughput (in TFLOP/s): 229.031
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 64, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14784x44352, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14784x44352, b=2048): 256.144
Elapsed time for attention_key_query_prob (256x2048x231x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x231x2048): 62.695
Elapsed time for attention_prob_times_values (256x2048x2048x231): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x231): 77.671
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14784x14784, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_linear_projection (4x14784x14784, b=2048): 257.223
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14784x59136, b=2048): 0.0560
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14784x59136, b=2048): 255.890
Elapsed time for mlp_fused_gelu (2048x4x59136): 0.0017
Elapsed time for mlp_4h_to_h (4x59136x14784, b=2048): 0.0578
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59136x14784, b=2048): 247.843
Elapsed time for transformer_add_bias_dropout (2048x4x14784): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14784): 0.0006

Attention duration (in seconds): 0.1056
Attention throughput (in TFLOP/s): 145.088
MLP duration (in seconds): 0.1155
MLP throughput (in TFLOP/s): 248.124
Transformer duration (in seconds): 0.2244
Transformer throughput (in TFLOP/s): 195.899
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0779
Attention throughput (in TFLOP/s): 196.642
MLP duration (in seconds): 0.1160
MLP throughput (in TFLOP/s): 246.874
Transformer duration (in seconds): 0.1988
Transformer throughput (in TFLOP/s): 221.133
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 255.372
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 128.992
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 200.506
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 255.577
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 256.471
Elapsed time for mlp_fused_gelu (2048x4x59392): 0.0017
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0579
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 249.577
Elapsed time for transformer_add_bias_dropout (2048x4x14848): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14848): 0.0006

Attention duration (in seconds): 0.0983
Attention throughput (in TFLOP/s): 157.089
MLP duration (in seconds): 0.1159
MLP throughput (in TFLOP/s): 249.281
Transformer duration (in seconds): 0.2176
Transformer throughput (in TFLOP/s): 203.743
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0705
Attention throughput (in TFLOP/s): 219.117
MLP duration (in seconds): 0.1173
MLP throughput (in TFLOP/s): 246.320
Transformer duration (in seconds): 0.1922
Transformer throughput (in TFLOP/s): 230.708
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14912x44736, b=2048): 0.0427
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14912x44736, b=2048): 256.256
Elapsed time for attention_key_query_prob (256x2048x233x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x233x2048): 63.016
Elapsed time for attention_prob_times_values (256x2048x2048x233): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x233): 78.181
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14912x14912, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x14912x14912, b=2048): 256.358
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14912x59648, b=2048): 0.0570
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14912x59648, b=2048): 255.890
Elapsed time for mlp_fused_gelu (2048x4x59648): 0.0017
Elapsed time for mlp_4h_to_h (4x59648x14912, b=2048): 0.0588
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59648x14912, b=2048): 247.978
Elapsed time for transformer_add_bias_dropout (2048x4x14912): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14912): 0.0006

Attention duration (in seconds): 0.1066
Attention throughput (in TFLOP/s): 146.079
MLP duration (in seconds): 0.1174
MLP throughput (in TFLOP/s): 248.223
Transformer duration (in seconds): 0.2275
Transformer throughput (in TFLOP/s): 196.603
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0789
Attention throughput (in TFLOP/s): 197.400
MLP duration (in seconds): 0.1182
MLP throughput (in TFLOP/s): 246.484
Transformer duration (in seconds): 0.2021
Transformer throughput (in TFLOP/s): 221.240
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0429
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 256.908
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 91.822
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 131.148
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 258.074
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0579
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 253.893
Elapsed time for mlp_fused_gelu (2048x4x59904): 0.0017
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0589
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 249.394
Elapsed time for transformer_add_bias_dropout (2048x4x14976): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14976): 0.0006

Attention duration (in seconds): 0.1019
Attention throughput (in TFLOP/s): 154.172
MLP duration (in seconds): 0.1185
MLP throughput (in TFLOP/s): 247.996
Transformer duration (in seconds): 0.2238
Transformer throughput (in TFLOP/s): 201.504
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0725
Attention throughput (in TFLOP/s): 216.704
MLP duration (in seconds): 0.1196
MLP throughput (in TFLOP/s): 245.727
Transformer duration (in seconds): 0.1965
Transformer throughput (in TFLOP/s): 229.473
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15040x45120, b=2048): 0.0434
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15040x45120, b=2048): 255.990
Elapsed time for attention_key_query_prob (256x2048x235x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x235x2048): 63.831
Elapsed time for attention_prob_times_values (256x2048x2048x235): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x235): 81.669
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15040x15040, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_linear_projection (4x15040x15040, b=2048): 256.019
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15040x60160, b=2048): 0.0582
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15040x60160, b=2048): 254.542
Elapsed time for mlp_fused_gelu (2048x4x60160): 0.0017
Elapsed time for mlp_4h_to_h (4x60160x15040, b=2048): 0.0595
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60160x15040, b=2048): 248.978
Elapsed time for transformer_add_bias_dropout (2048x4x15040): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15040): 0.0006

Attention duration (in seconds): 0.1074
Attention throughput (in TFLOP/s): 147.426
MLP duration (in seconds): 0.1195
MLP throughput (in TFLOP/s): 248.114
Transformer duration (in seconds): 0.2304
Transformer throughput (in TFLOP/s): 197.439
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0800
Attention throughput (in TFLOP/s): 198.028
MLP duration (in seconds): 0.1202
MLP throughput (in TFLOP/s): 246.580
Transformer duration (in seconds): 0.2049
Transformer throughput (in TFLOP/s): 221.947
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0437
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 256.691
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 92.277
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 132.923
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 259.202
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0583
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 256.445
Elapsed time for mlp_fused_gelu (2048x4x60416): 0.0017
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0599
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 249.716
Elapsed time for transformer_add_bias_dropout (2048x4x15104): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15104): 0.0006

Attention duration (in seconds): 0.1028
Attention throughput (in TFLOP/s): 155.272
MLP duration (in seconds): 0.1199
MLP throughput (in TFLOP/s): 249.398
Transformer duration (in seconds): 0.2262
Transformer throughput (in TFLOP/s): 202.796
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0732
Attention throughput (in TFLOP/s): 217.948
MLP duration (in seconds): 0.1208
MLP throughput (in TFLOP/s): 247.468
Transformer duration (in seconds): 0.1996
Transformer throughput (in TFLOP/s): 229.820
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 64, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15168x45504, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15168x45504, b=2048): 255.869
Elapsed time for attention_key_query_prob (256x2048x237x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x237x2048): 64.376
Elapsed time for attention_prob_times_values (256x2048x2048x237): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x237): 82.174
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15168x15168, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x15168x15168, b=2048): 254.486
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15168x60672, b=2048): 0.0589
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15168x60672, b=2048): 255.946
Elapsed time for mlp_fused_gelu (2048x4x60672): 0.0017
Elapsed time for mlp_4h_to_h (4x60672x15168, b=2048): 0.0605
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60672x15168, b=2048): 249.060
Elapsed time for transformer_add_bias_dropout (2048x4x15168): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15168): 0.0006

Attention duration (in seconds): 0.1085
Attention throughput (in TFLOP/s): 148.327
MLP duration (in seconds): 0.1212
MLP throughput (in TFLOP/s): 248.853
Transformer duration (in seconds): 0.2332
Transformer throughput (in TFLOP/s): 198.345
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0812
Attention throughput (in TFLOP/s): 198.326
MLP duration (in seconds): 0.1222
MLP throughput (in TFLOP/s): 246.733
Transformer duration (in seconds): 0.2077
Transformer throughput (in TFLOP/s): 222.676
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0445
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 256.500
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 92.785
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 133.665
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 257.883
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0597
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 254.905
Elapsed time for mlp_fused_gelu (2048x4x60928): 0.0017
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0610
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 249.238
Elapsed time for transformer_add_bias_dropout (2048x4x15232): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15232): 0.0006

Attention duration (in seconds): 0.1039
Attention throughput (in TFLOP/s): 156.124
MLP duration (in seconds): 0.1224
MLP throughput (in TFLOP/s): 248.460
Transformer duration (in seconds): 0.2298
Transformer throughput (in TFLOP/s): 202.934
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0744
Attention throughput (in TFLOP/s): 218.015
MLP duration (in seconds): 0.1228
MLP throughput (in TFLOP/s): 247.649
Transformer duration (in seconds): 0.2021
Transformer throughput (in TFLOP/s): 230.788
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 64, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15296x45888, b=2048): 0.0450
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15296x45888, b=2048): 255.632
Elapsed time for attention_key_query_prob (256x2048x239x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x239x2048): 64.383
Elapsed time for attention_prob_times_values (256x2048x2048x239): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x239): 81.167
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15296x15296, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_linear_projection (4x15296x15296, b=2048): 255.838
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15296x61184, b=2048): 0.0597
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15296x61184, b=2048): 256.629
Elapsed time for mlp_fused_gelu (2048x4x61184): 0.0017
Elapsed time for mlp_4h_to_h (4x61184x15296, b=2048): 0.0615
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61184x15296, b=2048): 249.434
Elapsed time for transformer_add_bias_dropout (2048x4x15296): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15296): 0.0006

Attention duration (in seconds): 0.1097
Attention throughput (in TFLOP/s): 149.173
MLP duration (in seconds): 0.1230
MLP throughput (in TFLOP/s): 249.392
Transformer duration (in seconds): 0.2361
Transformer throughput (in TFLOP/s): 199.139
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0819
Attention throughput (in TFLOP/s): 199.751
MLP duration (in seconds): 0.1244
MLP throughput (in TFLOP/s): 246.553
Transformer duration (in seconds): 0.2111
Transformer throughput (in TFLOP/s): 222.729
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0452
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 256.705
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 143.885
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 208.500
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 256.912
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0603
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 256.205
Elapsed time for mlp_fused_gelu (2048x4x61440): 0.0018
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0615
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 251.408
Elapsed time for transformer_add_bias_dropout (2048x4x15360): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15360): 0.0006

Attention duration (in seconds): 0.1017
Attention throughput (in TFLOP/s): 162.206
MLP duration (in seconds): 0.1236
MLP throughput (in TFLOP/s): 250.189
Transformer duration (in seconds): 0.2288
Transformer throughput (in TFLOP/s): 207.255
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0743
Attention throughput (in TFLOP/s): 222.052
MLP duration (in seconds): 0.1250
MLP throughput (in TFLOP/s): 247.476
Transformer duration (in seconds): 0.2046
Transformer throughput (in TFLOP/s): 231.774
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 64, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15424x46272, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15424x46272, b=2048): 255.556
Elapsed time for attention_key_query_prob (256x2048x241x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x241x2048): 64.945
Elapsed time for attention_prob_times_values (256x2048x2048x241): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x241): 81.930
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15424x15424, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_linear_projection (4x15424x15424, b=2048): 257.877
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15424x61696, b=2048): 0.0607
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15424x61696, b=2048): 256.909
Elapsed time for mlp_fused_gelu (2048x4x61696): 0.0018
Elapsed time for mlp_4h_to_h (4x61696x15424, b=2048): 0.0624
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61696x15424, b=2048): 249.778
Elapsed time for transformer_add_bias_dropout (2048x4x15424): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15424): 0.0006

Attention duration (in seconds): 0.1106
Attention throughput (in TFLOP/s): 150.380
MLP duration (in seconds): 0.1249
MLP throughput (in TFLOP/s): 249.727
Transformer duration (in seconds): 0.2390
Transformer throughput (in TFLOP/s): 200.055
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0829
Attention throughput (in TFLOP/s): 200.597
MLP duration (in seconds): 0.1258
MLP throughput (in TFLOP/s): 247.843
Transformer duration (in seconds): 0.2139
Transformer throughput (in TFLOP/s): 223.463
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0460
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 256.467
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 94.259
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 136.910
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 256.581
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0614
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 255.840
Elapsed time for mlp_fused_gelu (2048x4x61952): 0.0018
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0623
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 252.385
Elapsed time for transformer_add_bias_dropout (2048x4x15488): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15488): 0.0006

Attention duration (in seconds): 0.1060
Attention throughput (in TFLOP/s): 158.106
MLP duration (in seconds): 0.1255
MLP throughput (in TFLOP/s): 250.525
Transformer duration (in seconds): 0.2350
Transformer throughput (in TFLOP/s): 205.071
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 219.247
MLP duration (in seconds): 0.1268
MLP throughput (in TFLOP/s): 248.056
Transformer duration (in seconds): 0.2081
Transformer throughput (in TFLOP/s): 231.653
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 64, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15552x46656, b=2048): 0.0465
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15552x46656, b=2048): 255.616
Elapsed time for attention_key_query_prob (256x2048x243x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x243x2048): 65.803
Elapsed time for attention_prob_times_values (256x2048x2048x243): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x243): 84.632
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15552x15552, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x15552x15552, b=2048): 255.910
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15552x62208, b=2048): 0.0617
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15552x62208, b=2048): 257.010
Elapsed time for mlp_fused_gelu (2048x4x62208): 0.0018
Elapsed time for mlp_4h_to_h (4x62208x15552, b=2048): 0.0637
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62208x15552, b=2048): 248.924
Elapsed time for transformer_add_bias_dropout (2048x4x15552): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15552): 0.0007

Attention duration (in seconds): 0.1115
Attention throughput (in TFLOP/s): 151.529
MLP duration (in seconds): 0.1271
MLP throughput (in TFLOP/s): 249.374
Transformer duration (in seconds): 0.2422
Transformer throughput (in TFLOP/s): 200.633
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0841
Attention throughput (in TFLOP/s): 200.893
MLP duration (in seconds): 0.1286
MLP throughput (in TFLOP/s): 246.551
Transformer duration (in seconds): 0.2175
Transformer throughput (in TFLOP/s): 223.403
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 256.297
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 95.696
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 138.529
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 257.521
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.0623
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 256.411
Elapsed time for mlp_fused_gelu (2048x4x62464): 0.0018
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0641
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 249.405
Elapsed time for transformer_add_bias_dropout (2048x4x15616): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15616): 0.0006

Attention duration (in seconds): 0.1069
Attention throughput (in TFLOP/s): 159.236
MLP duration (in seconds): 0.1282
MLP throughput (in TFLOP/s): 249.347
Transformer duration (in seconds): 0.2387
Transformer throughput (in TFLOP/s): 205.255
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 220.256
MLP duration (in seconds): 0.1299
MLP throughput (in TFLOP/s): 246.066
Transformer duration (in seconds): 0.2116
Transformer throughput (in TFLOP/s): 231.498
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15680x47040, b=2048): 0.0474
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15680x47040, b=2048): 255.168
Elapsed time for attention_key_query_prob (256x2048x245x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x245x2048): 66.543
Elapsed time for attention_prob_times_values (256x2048x2048x245): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x245): 85.292
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15680x15680, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_linear_projection (4x15680x15680, b=2048): 255.355
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15680x62720, b=2048): 0.0631
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15680x62720, b=2048): 255.489
Elapsed time for mlp_fused_gelu (2048x4x62720): 0.0018
Elapsed time for mlp_4h_to_h (4x62720x15680, b=2048): 0.0649
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62720x15680, b=2048): 248.453
Elapsed time for transformer_add_bias_dropout (2048x4x15680): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15680): 0.0006

Attention duration (in seconds): 0.1126
Attention throughput (in TFLOP/s): 152.418
MLP duration (in seconds): 0.1297
MLP throughput (in TFLOP/s): 248.450
Transformer duration (in seconds): 0.2459
Transformer throughput (in TFLOP/s): 200.837
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0851
Attention throughput (in TFLOP/s): 201.662
MLP duration (in seconds): 0.1311
MLP throughput (in TFLOP/s): 245.890
Transformer duration (in seconds): 0.2205
Transformer throughput (in TFLOP/s): 223.993
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 256.109
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 96.058
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 140.088
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 256.892
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0635
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 255.673
Elapsed time for mlp_fused_gelu (2048x4x62976): 0.0018
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0652
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 248.997
Elapsed time for transformer_add_bias_dropout (2048x4x15744): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15744): 0.0006

Attention duration (in seconds): 0.1081
Attention throughput (in TFLOP/s): 160.108
MLP duration (in seconds): 0.1306
MLP throughput (in TFLOP/s): 248.821
Transformer duration (in seconds): 0.2422
Transformer throughput (in TFLOP/s): 205.555
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0785
Attention throughput (in TFLOP/s): 220.394
MLP duration (in seconds): 0.1321
MLP throughput (in TFLOP/s): 246.030
Transformer duration (in seconds): 0.2155
Transformer throughput (in TFLOP/s): 231.068
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 64, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15808x47424, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15808x47424, b=2048): 256.712
Elapsed time for attention_key_query_prob (256x2048x247x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x247x2048): 65.882
Elapsed time for attention_prob_times_values (256x2048x2048x247): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x247): 84.712
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15808x15808, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x15808x15808, b=2048): 257.224
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15808x63232, b=2048): 0.0644
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15808x63232, b=2048): 254.332
Elapsed time for mlp_fused_gelu (2048x4x63232): 0.0018
Elapsed time for mlp_4h_to_h (4x63232x15808, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63232x15808, b=2048): 246.817
Elapsed time for transformer_add_bias_dropout (2048x4x15808): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15808): 0.0007

Attention duration (in seconds): 0.1135
Attention throughput (in TFLOP/s): 153.663
MLP duration (in seconds): 0.1325
MLP throughput (in TFLOP/s): 247.112
Transformer duration (in seconds): 0.2497
Transformer throughput (in TFLOP/s): 201.041
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0859
Attention throughput (in TFLOP/s): 202.975
MLP duration (in seconds): 0.1337
MLP throughput (in TFLOP/s): 245.070
Transformer duration (in seconds): 0.2236
Transformer throughput (in TFLOP/s): 224.513
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0484
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 255.815
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 130.000
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 211.453
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(21993422848, 42481549312)
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 259.435
(21993422848, 42481549312)
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0651
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 253.661
Elapsed time for mlp_fused_gelu (2048x4x63488): 0.0018
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 248.588
Elapsed time for transformer_add_bias_dropout (2048x4x15872): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15872): 0.0006

Attention duration (in seconds): 0.1063
Attention throughput (in TFLOP/s): 165.280
MLP duration (in seconds): 0.1333
MLP throughput (in TFLOP/s): 247.693
Transformer duration (in seconds): 0.2433
Transformer throughput (in TFLOP/s): 207.983
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0787
Attention throughput (in TFLOP/s): 223.343
MLP duration (in seconds): 0.1341
MLP throughput (in TFLOP/s): 246.317
Transformer duration (in seconds): 0.2190
Transformer throughput (in TFLOP/s): 230.975
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15936x47808, b=2048): 0.0487
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15936x47808, b=2048): 256.294
Elapsed time for attention_key_query_prob (256x2048x249x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x249x2048): 66.487
Elapsed time for attention_prob_times_values (256x2048x2048x249): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x249): 85.515
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19845939200, 42481549312)
Elapsed time for attention_linear_projection (4x15936x15936, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x15936x15936, b=2048): 255.988
(19845939200, 42481549312)
Elapsed time for mlp_h_to_4h (4x15936x63744, b=2048): 0.0656
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15936x63744, b=2048): 253.699
Elapsed time for mlp_fused_gelu (2048x4x63744): 0.0018
Elapsed time for mlp_4h_to_h (4x63744x15936, b=2048): 0.0670
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63744x15936, b=2048): 248.386
Elapsed time for transformer_add_bias_dropout (2048x4x15936): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15936): 0.0007

Attention duration (in seconds): 0.1147
Attention throughput (in TFLOP/s): 154.474
MLP duration (in seconds): 0.1344
MLP throughput (in TFLOP/s): 247.626
Transformer duration (in seconds): 0.2528
Transformer throughput (in TFLOP/s): 201.771
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0874
Attention throughput (in TFLOP/s): 202.727
MLP duration (in seconds): 0.1363
MLP throughput (in TFLOP/s): 244.274
Transformer duration (in seconds): 0.2267
Transformer throughput (in TFLOP/s): 224.974
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0492
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 255.719
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 98.165
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 143.317
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19845939200, 42481549312)
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 257.282
(19845939200, 42481549312)
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 254.182
Elapsed time for mlp_fused_gelu (2048x4x64000): 0.0018
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0672
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 249.697
Elapsed time for transformer_add_bias_dropout (2048x4x16000): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16000): 0.0007

Attention duration (in seconds): 0.1101
Attention throughput (in TFLOP/s): 162.086
MLP duration (in seconds): 0.1350
MLP throughput (in TFLOP/s): 248.519
Transformer duration (in seconds): 0.2488
Transformer throughput (in TFLOP/s): 206.599
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0810
Attention throughput (in TFLOP/s): 220.462
MLP duration (in seconds): 0.1362
MLP throughput (in TFLOP/s): 246.435
Transformer duration (in seconds): 0.2225
Transformer throughput (in TFLOP/s): 231.016
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 64, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16064x48192, b=2048): 0.0494
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16064x48192, b=2048): 256.548
Elapsed time for attention_key_query_prob (256x2048x251x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x251x2048): 67.825
Elapsed time for attention_prob_times_values (256x2048x2048x251): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x251): 88.649
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19845939200, 42481549312)
Elapsed time for attention_linear_projection (4x16064x16064, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x16064x16064, b=2048): 254.938
(19845939200, 42481549312)
Elapsed time for mlp_h_to_4h (4x16064x64256, b=2048): 0.0669
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16064x64256, b=2048): 252.942
Elapsed time for mlp_fused_gelu (2048x4x64256): 0.0018
Elapsed time for mlp_4h_to_h (4x64256x16064, b=2048): 0.0682
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64256x16064, b=2048): 247.841
Elapsed time for transformer_add_bias_dropout (2048x4x16064): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16064): 0.0007

Attention duration (in seconds): 0.1155
Attention throughput (in TFLOP/s): 155.812
MLP duration (in seconds): 0.1369
MLP throughput (in TFLOP/s): 247.023
Transformer duration (in seconds): 0.2561
Transformer throughput (in TFLOP/s): 202.334
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0884
Attention throughput (in TFLOP/s): 203.565
MLP duration (in seconds): 0.1378
MLP throughput (in TFLOP/s): 245.378
Transformer duration (in seconds): 0.2302
Transformer throughput (in TFLOP/s): 225.100
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0497
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 257.059
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 99.140
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 145.370
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19845939200, 42481549312)
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 256.630
(19845939200, 42481549312)
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0675
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 252.712
Elapsed time for mlp_fused_gelu (2048x4x64512): 0.0018
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0685
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 248.857
Elapsed time for transformer_add_bias_dropout (2048x4x16128): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16128): 0.0006

Attention duration (in seconds): 0.1109
Attention throughput (in TFLOP/s): 163.417
MLP duration (in seconds): 0.1378
MLP throughput (in TFLOP/s): 247.430
Transformer duration (in seconds): 0.2524
Transformer throughput (in TFLOP/s): 206.903
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0817
Attention throughput (in TFLOP/s): 222.014
MLP duration (in seconds): 0.1393
MLP throughput (in TFLOP/s): 244.702
Transformer duration (in seconds): 0.2252
Transformer throughput (in TFLOP/s): 231.907
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16192x48576, b=2048): 0.0505
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16192x48576, b=2048): 255.369
Elapsed time for attention_key_query_prob (256x2048x253x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x253x2048): 68.567
Elapsed time for attention_prob_times_values (256x2048x2048x253): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x253): 89.464
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19845939200, 42481549312)
Elapsed time for attention_linear_projection (4x16192x16192, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x16192x16192, b=2048): 258.749
(19845939200, 42481549312)
Elapsed time for mlp_h_to_4h (4x16192x64768, b=2048): 0.0677
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16192x64768, b=2048): 253.853
Elapsed time for mlp_fused_gelu (2048x4x64768): 0.0018
Elapsed time for mlp_4h_to_h (4x64768x16192, b=2048): 0.0695
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64768x16192, b=2048): 247.087
Elapsed time for transformer_add_bias_dropout (2048x4x16192): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16192): 0.0007

Attention duration (in seconds): 0.1165
Attention throughput (in TFLOP/s): 156.853
MLP duration (in seconds): 0.1391
MLP throughput (in TFLOP/s): 247.106
Transformer duration (in seconds): 0.2593
Transformer throughput (in TFLOP/s): 203.020
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0896
Attention throughput (in TFLOP/s): 203.866
MLP duration (in seconds): 0.1401
MLP throughput (in TFLOP/s): 245.367
Transformer duration (in seconds): 0.2343
Transformer throughput (in TFLOP/s): 224.612
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0505
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 257.146
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 100.644
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 146.929
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19845939200, 42481549312)
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 257.799
(19845939200, 42481549312)
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0682
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 254.096
Elapsed time for mlp_fused_gelu (2048x4x65024): 0.0019
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0696
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 248.721
Elapsed time for transformer_add_bias_dropout (2048x4x16256): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16256): 0.0007

Attention duration (in seconds): 0.1118
Attention throughput (in TFLOP/s): 164.592
MLP duration (in seconds): 0.1396
MLP throughput (in TFLOP/s): 248.049
Transformer duration (in seconds): 0.2552
Transformer throughput (in TFLOP/s): 207.866
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0828
Attention throughput (in TFLOP/s): 222.379
MLP duration (in seconds): 0.1415
MLP throughput (in TFLOP/s): 244.705
Transformer duration (in seconds): 0.2287
Transformer throughput (in TFLOP/s): 231.956
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16320x48960, b=2048): 0.0511
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16320x48960, b=2048): 256.218
Elapsed time for attention_key_query_prob (256x2048x255x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x255x2048): 68.517
Elapsed time for attention_prob_times_values (256x2048x2048x255): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x255): 89.056
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19845939200, 42481549312)
Elapsed time for attention_linear_projection (4x16320x16320, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x16320x16320, b=2048): 258.446
(19845939200, 42481549312)
Elapsed time for mlp_h_to_4h (4x16320x65280, b=2048): 0.0685
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16320x65280, b=2048): 254.703
Elapsed time for mlp_fused_gelu (2048x4x65280): 0.0019
Elapsed time for mlp_4h_to_h (4x65280x16320, b=2048): 0.0703
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65280x16320, b=2048): 248.173
Elapsed time for transformer_add_bias_dropout (2048x4x16320): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16320): 0.0007

Attention duration (in seconds): 0.1175
Attention throughput (in TFLOP/s): 157.831
MLP duration (in seconds): 0.1407
MLP throughput (in TFLOP/s): 248.078
Transformer duration (in seconds): 0.2620
Transformer throughput (in TFLOP/s): 204.047
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0906
Attention throughput (in TFLOP/s): 204.855
MLP duration (in seconds): 0.1422
MLP throughput (in TFLOP/s): 245.423
Transformer duration (in seconds): 0.2362
Transformer throughput (in TFLOP/s): 226.336
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0514
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 256.902
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 158.466
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 221.078
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19845939200, 42481549312)
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 259.525
(19845939200, 42481549312)
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.0691
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 254.416
Elapsed time for mlp_fused_gelu (2048x4x65536): 0.0019
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0706
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 249.319
Elapsed time for transformer_add_bias_dropout (2048x4x16384): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16384): 0.0007

Attention duration (in seconds): 0.1097
Attention throughput (in TFLOP/s): 170.434
MLP duration (in seconds): 0.1416
MLP throughput (in TFLOP/s): 248.522
Transformer duration (in seconds): 0.2550
Transformer throughput (in TFLOP/s): 211.298
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0824
Attention throughput (in TFLOP/s): 226.715
MLP duration (in seconds): 0.1427
MLP throughput (in TFLOP/s): 246.494
Transformer duration (in seconds): 0.2305
Transformer throughput (in TFLOP/s): 233.702
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16448x49344, b=2048): 0.0522
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16448x49344, b=2048): 254.845
Elapsed time for attention_key_query_prob (256x2048x257x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x257x2048): 66.171
Elapsed time for attention_prob_times_values (256x2048x2048x257): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x257): 52.173
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19845939200, 42481549312)
Elapsed time for attention_linear_projection (4x16448x16448, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_linear_projection (4x16448x16448, b=2048): 254.984
(19845939200, 42481549312)
Elapsed time for mlp_h_to_4h (4x16448x65792, b=2048): 0.0698
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16448x65792, b=2048): 254.112
Elapsed time for mlp_fused_gelu (2048x4x65792): 0.0019
Elapsed time for mlp_4h_to_h (4x65792x16448, b=2048): 0.0713
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65792x16448, b=2048): 248.705
Elapsed time for transformer_add_bias_dropout (2048x4x16448): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16448): 0.0007

Attention duration (in seconds): 0.1239
Attention throughput (in TFLOP/s): 152.020
MLP duration (in seconds): 0.1429
MLP throughput (in TFLOP/s): 248.088
Transformer duration (in seconds): 0.2706
Transformer throughput (in TFLOP/s): 200.643
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0968
Attention throughput (in TFLOP/s): 194.538
MLP duration (in seconds): 0.1446
MLP throughput (in TFLOP/s): 245.182
Transformer duration (in seconds): 0.2450
Transformer throughput (in TFLOP/s): 221.602
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0522
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 256.948
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 96.023
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 92.543
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15513223168, 42481549312)
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 258.611
(15513223168, 42481549312)
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.0702
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 254.537
Elapsed time for mlp_fused_gelu (2048x4x66048): 0.0019
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0717
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 249.062
Elapsed time for transformer_add_bias_dropout (2048x4x16512): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16512): 0.0007

Attention duration (in seconds): 0.1166
Attention throughput (in TFLOP/s): 162.754
MLP duration (in seconds): 0.1438
MLP throughput (in TFLOP/s): 248.482
Transformer duration (in seconds): 0.2642
Transformer throughput (in TFLOP/s): 207.101
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0879
Attention throughput (in TFLOP/s): 215.794
MLP duration (in seconds): 0.1452
MLP throughput (in TFLOP/s): 246.128
Transformer duration (in seconds): 0.2371
Transformer throughput (in TFLOP/s): 230.755
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16576x49728, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16576x49728, b=2048): 255.645
Elapsed time for attention_key_query_prob (256x2048x259x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x259x2048): 66.477
Elapsed time for attention_prob_times_values (256x2048x2048x259): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x259): 53.227
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11146952704, 42481549312)
Elapsed time for attention_linear_projection (4x16576x16576, b=2048): 0.0176
Throughput (in TFLOP/s) for attention_linear_projection (4x16576x16576, b=2048): 256.485
(11146952704, 42481549312)
Elapsed time for mlp_h_to_4h (4x16576x66304, b=2048): 0.0709
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16576x66304, b=2048): 254.056
Elapsed time for mlp_fused_gelu (2048x4x66304): 0.0019
Elapsed time for mlp_4h_to_h (4x66304x16576, b=2048): 0.0727
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66304x16576, b=2048): 247.795
Elapsed time for transformer_add_bias_dropout (2048x4x16576): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16576): 0.0007

Attention duration (in seconds): 0.1246
Attention throughput (in TFLOP/s): 153.439
MLP duration (in seconds): 0.1454
MLP throughput (in TFLOP/s): 247.635
Transformer duration (in seconds): 0.2738
Transformer throughput (in TFLOP/s): 201.336
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0977
Attention throughput (in TFLOP/s): 195.766
MLP duration (in seconds): 0.1472
MLP throughput (in TFLOP/s): 244.665
Transformer duration (in seconds): 0.2487
Transformer throughput (in TFLOP/s): 221.657
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0530
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 256.703
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 96.389
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 92.725
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6747127808, 42481549312)
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 260.325
(6747127808, 42481549312)
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.0711
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 255.274
Elapsed time for mlp_fused_gelu (2048x4x66560): 0.0019
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0726
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 250.040
Elapsed time for transformer_add_bias_dropout (2048x4x16640): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16640): 0.0007

Attention duration (in seconds): 0.1177
Attention throughput (in TFLOP/s): 163.708
MLP duration (in seconds): 0.1456
MLP throughput (in TFLOP/s): 249.344
Transformer duration (in seconds): 0.2670
Transformer throughput (in TFLOP/s): 208.066
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0888
Attention throughput (in TFLOP/s): 216.996
MLP duration (in seconds): 0.1478
MLP throughput (in TFLOP/s): 245.599
Transformer duration (in seconds): 0.2410
Transformer throughput (in TFLOP/s): 230.517
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16704x50112, b=2048): 0.0536
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16704x50112, b=2048): 255.750
Elapsed time for attention_key_query_prob (256x2048x261x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x261x2048): 66.447
Elapsed time for attention_prob_times_values (256x2048x2048x261): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x261): 53.245
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2313748480, 42481549312)
Elapsed time for attention_linear_projection (4x16704x16704, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_linear_projection (4x16704x16704, b=2048): 260.805
(2313748480, 42481549312)
Elapsed time for mlp_h_to_4h (4x16704x66816, b=2048): 0.0720
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16704x66816, b=2048): 253.973
Elapsed time for mlp_fused_gelu (2048x4x66816): 0.0019
Elapsed time for mlp_4h_to_h (4x66816x16704, b=2048): 0.0740
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66816x16704, b=2048): 247.176
Elapsed time for transformer_add_bias_dropout (2048x4x16704): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16704): 0.0007

Attention duration (in seconds): 0.1255
Attention throughput (in TFLOP/s): 154.609
MLP duration (in seconds): 0.1479
MLP throughput (in TFLOP/s): 247.313
Transformer duration (in seconds): 0.2772
Transformer throughput (in TFLOP/s): 201.920
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0992
Attention throughput (in TFLOP/s): 195.732
MLP duration (in seconds): 0.1496
MLP throughput (in TFLOP/s): 244.447
Transformer duration (in seconds): 0.2517
Transformer throughput (in TFLOP/s): 222.424
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0539
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 256.476
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 96.543
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 92.632
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18466013184, 42481549312)
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 258.735
(18466013184, 42481549312)
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.0727
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 253.444
Elapsed time for mlp_fused_gelu (2048x4x67072): 0.0019
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0743
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 248.097
Elapsed time for transformer_add_bias_dropout (2048x4x16768): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16768): 0.0007

Attention duration (in seconds): 0.1190
Attention throughput (in TFLOP/s): 164.302
MLP duration (in seconds): 0.1489
MLP throughput (in TFLOP/s): 247.528
Transformer duration (in seconds): 0.2717
Transformer throughput (in TFLOP/s): 207.582
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0903
Attention throughput (in TFLOP/s): 216.413
MLP duration (in seconds): 0.1509
MLP throughput (in TFLOP/s): 244.244
Transformer duration (in seconds): 0.2445
Transformer throughput (in TFLOP/s): 230.665
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16832x50496, b=2048): 0.0547
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16832x50496, b=2048): 254.468
Elapsed time for attention_key_query_prob (256x2048x263x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x263x2048): 65.597
Elapsed time for attention_prob_times_values (256x2048x2048x263): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x263): 52.708
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(13965524992, 42481549312)
Elapsed time for attention_linear_projection (4x16832x16832, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x16832x16832, b=2048): 257.320
(13965524992, 42481549312)
Elapsed time for mlp_h_to_4h (4x16832x67328, b=2048): 0.0732
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16832x67328, b=2048): 253.777
Elapsed time for mlp_fused_gelu (2048x4x67328): 0.0019
Elapsed time for mlp_4h_to_h (4x67328x16832, b=2048): 0.0752
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67328x16832, b=2048): 246.841
Elapsed time for transformer_add_bias_dropout (2048x4x16832): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16832): 0.0007

Attention duration (in seconds): 0.1275
Attention throughput (in TFLOP/s): 154.492
MLP duration (in seconds): 0.1503
MLP throughput (in TFLOP/s): 247.073
Transformer duration (in seconds): 0.2817
Transformer throughput (in TFLOP/s): 201.760
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1008
Attention throughput (in TFLOP/s): 195.339
MLP duration (in seconds): 0.1514
MLP throughput (in TFLOP/s): 245.323
Transformer duration (in seconds): 0.2569
Transformer throughput (in TFLOP/s): 221.255
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0548
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 256.217
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 134.558
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 130.898
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(9431482368, 42481549312)
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 258.070
(9431482368, 42481549312)
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.0738
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 253.609
Elapsed time for mlp_fused_gelu (2048x4x67584): 0.0019
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0753
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 248.339
Elapsed time for transformer_add_bias_dropout (2048x4x16896): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16896): 0.0007

Attention duration (in seconds): 0.1168
Attention throughput (in TFLOP/s): 169.830
MLP duration (in seconds): 0.1510
MLP throughput (in TFLOP/s): 247.754
Transformer duration (in seconds): 0.2717
Transformer throughput (in TFLOP/s): 210.732
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.0909
Attention throughput (in TFLOP/s): 218.410
MLP duration (in seconds): 0.1530
MLP throughput (in TFLOP/s): 244.637
Transformer duration (in seconds): 0.2483
Transformer throughput (in TFLOP/s): 230.627
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16960x50880, b=2048): 0.0551
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16960x50880, b=2048): 256.691
Elapsed time for attention_key_query_prob (256x2048x265x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x265x2048): 65.605
Elapsed time for attention_prob_times_values (256x2048x2048x265): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x265): 52.809
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4863885312, 42481549312)
Elapsed time for attention_linear_projection (4x16960x16960, b=2048): 0.0183
Throughput (in TFLOP/s) for attention_linear_projection (4x16960x16960, b=2048): 257.831
(4863885312, 42481549312)
Elapsed time for mlp_h_to_4h (4x16960x67840, b=2048): 0.0742
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16960x67840, b=2048): 253.927
Elapsed time for mlp_fused_gelu (2048x4x67840): 0.0019
Elapsed time for mlp_4h_to_h (4x67840x16960, b=2048): 0.0762
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67840x16960, b=2048): 247.520
Elapsed time for transformer_add_bias_dropout (2048x4x16960): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16960): 0.0007

Attention duration (in seconds): 0.1282
Attention throughput (in TFLOP/s): 155.900
MLP duration (in seconds): 0.1523
MLP throughput (in TFLOP/s): 247.506
Transformer duration (in seconds): 0.2844
Transformer throughput (in TFLOP/s): 202.825
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1017
Attention throughput (in TFLOP/s): 196.519
MLP duration (in seconds): 0.1545
MLP throughput (in TFLOP/s): 244.019
Transformer duration (in seconds): 0.2598
Transformer throughput (in TFLOP/s): 222.060
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0556
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 256.201
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 96.942
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 92.951
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(258539520, 42481549312)
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 257.300
(258539520, 42481549312)
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.0749
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 253.539
Elapsed time for mlp_fused_gelu (2048x4x68096): 0.0019
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0765
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 248.131
Elapsed time for transformer_add_bias_dropout (2048x4x17024): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17024): 0.0007

Attention duration (in seconds): 0.1215
Attention throughput (in TFLOP/s): 165.724
MLP duration (in seconds): 0.1534
MLP throughput (in TFLOP/s): 247.639
Transformer duration (in seconds): 0.2788
Transformer throughput (in TFLOP/s): 208.486
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.0930
Attention throughput (in TFLOP/s): 216.543
MLP duration (in seconds): 0.1554
MLP throughput (in TFLOP/s): 244.474
Transformer duration (in seconds): 0.2512
Transformer throughput (in TFLOP/s): 231.334
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17088x51264, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17088x51264, b=2048): 255.264
Elapsed time for attention_key_query_prob (256x2048x267x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x267x2048): 66.887
Elapsed time for attention_prob_times_values (256x2048x2048x267): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x267): 54.180
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18294046720, 42481549312)
Elapsed time for attention_linear_projection (4x17088x17088, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_linear_projection (4x17088x17088, b=2048): 260.477
(18294046720, 42481549312)
Elapsed time for mlp_h_to_4h (4x17088x68352, b=2048): 0.0754
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17088x68352, b=2048): 253.670
Elapsed time for mlp_fused_gelu (2048x4x68352): 0.0019
Elapsed time for mlp_4h_to_h (4x68352x17088, b=2048): 0.0774
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68352x17088, b=2048): 247.367
Elapsed time for transformer_add_bias_dropout (2048x4x17088): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17088): 0.0007

Attention duration (in seconds): 0.1292
Attention throughput (in TFLOP/s): 157.040
MLP duration (in seconds): 0.1547
MLP throughput (in TFLOP/s): 247.336
Transformer duration (in seconds): 0.2878
Transformer throughput (in TFLOP/s): 203.443
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1029
Attention throughput (in TFLOP/s): 197.140
MLP duration (in seconds): 0.1563
MLP throughput (in TFLOP/s): 244.877
Transformer duration (in seconds): 0.2632
Transformer throughput (in TFLOP/s): 222.438
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0567
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 255.174
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 97.888
Elapsed time for attention_prob_times_values (256x2048x2048x268): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 92.983
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(13621592064, 42481549312)
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 261.450
(13621592064, 42481549312)
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.0760
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 253.762
Elapsed time for mlp_fused_gelu (2048x4x68608): 0.0020
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0779
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 247.643
Elapsed time for transformer_add_bias_dropout (2048x4x17152): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17152): 0.0007

Attention duration (in seconds): 0.1226
Attention throughput (in TFLOP/s): 166.672
MLP duration (in seconds): 0.1558
MLP throughput (in TFLOP/s): 247.528
Transformer duration (in seconds): 0.2823
Transformer throughput (in TFLOP/s): 208.990
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.0939
Attention throughput (in TFLOP/s): 217.674
MLP duration (in seconds): 0.1577
MLP throughput (in TFLOP/s): 244.546
Transformer duration (in seconds): 0.2551
Transformer throughput (in TFLOP/s): 231.207
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17216x51648, b=2048): 0.0570
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17216x51648, b=2048): 255.524
Elapsed time for attention_key_query_prob (256x2048x269x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x269x2048): 66.902
Elapsed time for attention_prob_times_values (256x2048x2048x269): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x269): 54.348
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8911388672, 42481549312)
Elapsed time for attention_linear_projection (4x17216x17216, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_linear_projection (4x17216x17216, b=2048): 258.964
(8911388672, 42481549312)
Elapsed time for mlp_h_to_4h (4x17216x68864, b=2048): 0.0767
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17216x68864, b=2048): 253.338
Elapsed time for mlp_fused_gelu (2048x4x68864): 0.0020
Elapsed time for mlp_4h_to_h (4x68864x17216, b=2048): 0.0786
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68864x17216, b=2048): 247.171
Elapsed time for transformer_add_bias_dropout (2048x4x17216): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17216): 0.0007

Attention duration (in seconds): 0.1304
Attention throughput (in TFLOP/s): 157.771
MLP duration (in seconds): 0.1572
MLP throughput (in TFLOP/s): 247.099
Transformer duration (in seconds): 0.2916
Transformer throughput (in TFLOP/s): 203.797
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1039
Attention throughput (in TFLOP/s): 198.120
MLP duration (in seconds): 0.1592
MLP throughput (in TFLOP/s): 244.042
Transformer duration (in seconds): 0.2660
Transformer throughput (in TFLOP/s): 223.454
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0572
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 256.417
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 97.822
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 93.221
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4167630848, 42481549312)
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 262.650
(4167630848, 42481549312)
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.0775
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 252.418
Elapsed time for mlp_fused_gelu (2048x4x69120): 0.0020
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0788
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 248.219
Elapsed time for transformer_add_bias_dropout (2048x4x17280): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17280): 0.0007

Attention duration (in seconds): 0.1234
Attention throughput (in TFLOP/s): 167.943
MLP duration (in seconds): 0.1583
MLP throughput (in TFLOP/s): 247.194
Transformer duration (in seconds): 0.2857
Transformer throughput (in TFLOP/s): 209.547
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.0949
Attention throughput (in TFLOP/s): 218.375
MLP duration (in seconds): 0.1599
MLP throughput (in TFLOP/s): 244.769
Transformer duration (in seconds): 0.2584
Transformer throughput (in TFLOP/s): 231.700
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17344x52032, b=2048): 0.0584
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17344x52032, b=2048): 253.280
Elapsed time for attention_key_query_prob (256x2048x271x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x271x2048): 66.711
Elapsed time for attention_prob_times_values (256x2048x2048x271): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x271): 53.723
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17868324864, 42481549312)
Elapsed time for attention_linear_projection (4x17344x17344, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_linear_projection (4x17344x17344, b=2048): 255.570
(17868324864, 42481549312)
Elapsed time for mlp_h_to_4h (4x17344x69376, b=2048): 0.0779
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17344x69376, b=2048): 253.162
Elapsed time for mlp_fused_gelu (2048x4x69376): 0.0020
Elapsed time for mlp_4h_to_h (4x69376x17344, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69376x17344, b=2048): 247.388
Elapsed time for transformer_add_bias_dropout (2048x4x17344): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17344): 0.0007

Attention duration (in seconds): 0.1326
Attention throughput (in TFLOP/s): 157.413
MLP duration (in seconds): 0.1595
MLP throughput (in TFLOP/s): 247.149
Transformer duration (in seconds): 0.2962
Transformer throughput (in TFLOP/s): 203.607
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1054
Attention throughput (in TFLOP/s): 198.025
MLP duration (in seconds): 0.1615
MLP throughput (in TFLOP/s): 244.100
Transformer duration (in seconds): 0.2693
Transformer throughput (in TFLOP/s): 223.960
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 256.317
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 151.353
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 135.290
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(13053263872, 42481549312)
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0192
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 259.223
(13053263872, 42481549312)
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.0783
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 253.572
Elapsed time for mlp_fused_gelu (2048x4x69632): 0.0020
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0802
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 247.512
Elapsed time for transformer_add_bias_dropout (2048x4x17408): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17408): 0.0007

Attention duration (in seconds): 0.1209
Attention throughput (in TFLOP/s): 173.998
MLP duration (in seconds): 0.1605
MLP throughput (in TFLOP/s): 247.419
Transformer duration (in seconds): 0.2854
Transformer throughput (in TFLOP/s): 212.883
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.0952
Attention throughput (in TFLOP/s): 220.901
MLP duration (in seconds): 0.1622
MLP throughput (in TFLOP/s): 244.815
Transformer duration (in seconds): 0.2606
Transformer throughput (in TFLOP/s): 233.133
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17472x52416, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17472x52416, b=2048): 255.427
Elapsed time for attention_key_query_prob (256x2048x273x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x273x2048): 67.159
Elapsed time for attention_prob_times_values (256x2048x2048x273): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x273): 53.757
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8204648448, 42481549312)
Elapsed time for attention_linear_projection (4x17472x17472, b=2048): 0.0192
Throughput (in TFLOP/s) for attention_linear_projection (4x17472x17472, b=2048): 260.183
(8204648448, 42481549312)
Elapsed time for mlp_h_to_4h (4x17472x69888, b=2048): 0.0789
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17472x69888, b=2048): 253.505
Elapsed time for mlp_fused_gelu (2048x4x69888): 0.0020
Elapsed time for mlp_4h_to_h (4x69888x17472, b=2048): 0.0811
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69888x17472, b=2048): 246.594
Elapsed time for transformer_add_bias_dropout (2048x4x17472): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17472): 0.0007

Attention duration (in seconds): 0.1330
Attention throughput (in TFLOP/s): 159.227
MLP duration (in seconds): 0.1620
MLP throughput (in TFLOP/s): 246.937
Transformer duration (in seconds): 0.2991
Transformer throughput (in TFLOP/s): 204.612
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1066
Attention throughput (in TFLOP/s): 198.623
MLP duration (in seconds): 0.1636
MLP throughput (in TFLOP/s): 244.556
Transformer duration (in seconds): 0.2740
Transformer throughput (in TFLOP/s): 223.320
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0588
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 257.147
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 98.790
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 93.608
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3318284288, 42481549312)
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 258.142
(3318284288, 42481549312)
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.0799
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 252.169
Elapsed time for mlp_fused_gelu (2048x4x70144): 0.0020
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0813
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 247.994
Elapsed time for transformer_add_bias_dropout (2048x4x17536): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17536): 0.0007

Attention duration (in seconds): 0.1259
Attention throughput (in TFLOP/s): 169.356
MLP duration (in seconds): 0.1632
MLP throughput (in TFLOP/s): 247.008
Transformer duration (in seconds): 0.2931
Transformer throughput (in TFLOP/s): 210.274
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.0977
Attention throughput (in TFLOP/s): 218.377
MLP duration (in seconds): 0.1642
MLP throughput (in TFLOP/s): 245.412
Transformer duration (in seconds): 0.2658
Transformer throughput (in TFLOP/s): 231.863
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17600x52800, b=2048): 0.0594
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17600x52800, b=2048): 256.293
Elapsed time for attention_key_query_prob (256x2048x275x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x275x2048): 67.768
Elapsed time for attention_prob_times_values (256x2048x2048x275): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x275): 54.983
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18008834048, 42481549312)
Elapsed time for attention_linear_projection (4x17600x17600, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_linear_projection (4x17600x17600, b=2048): 258.683
(18008834048, 42481549312)
Elapsed time for mlp_h_to_4h (4x17600x70400, b=2048): 0.0798
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17600x70400, b=2048): 254.246
Elapsed time for mlp_fused_gelu (2048x4x70400): 0.0020
Elapsed time for mlp_4h_to_h (4x70400x17600, b=2048): 0.0819
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70400x17600, b=2048): 247.976
Elapsed time for transformer_add_bias_dropout (2048x4x17600): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17600): 0.0007

Attention duration (in seconds): 0.1339
Attention throughput (in TFLOP/s): 160.428
MLP duration (in seconds): 0.1637
MLP throughput (in TFLOP/s): 247.999
Transformer duration (in seconds): 0.3017
Transformer throughput (in TFLOP/s): 205.805
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1076
Attention throughput (in TFLOP/s): 199.726
MLP duration (in seconds): 0.1664
MLP throughput (in TFLOP/s): 243.946
Transformer duration (in seconds): 0.2780
Transformer throughput (in TFLOP/s): 223.336
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0601
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 255.306
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 99.527
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 93.722
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(13051166720, 42481549312)
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 259.512
(13051166720, 42481549312)
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.0809
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 252.777
Elapsed time for mlp_fused_gelu (2048x4x70656): 0.0020
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0823
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 248.586
Elapsed time for transformer_add_bias_dropout (2048x4x17664): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17664): 0.0007

Attention duration (in seconds): 0.1275
Attention throughput (in TFLOP/s): 169.713
MLP duration (in seconds): 0.1652
MLP throughput (in TFLOP/s): 247.614
Transformer duration (in seconds): 0.2967
Transformer throughput (in TFLOP/s): 210.776
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.0981
Attention throughput (in TFLOP/s): 220.477
MLP duration (in seconds): 0.1672
MLP throughput (in TFLOP/s): 244.638
Transformer duration (in seconds): 0.2692
Transformer throughput (in TFLOP/s): 232.315
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17728x53184, b=2048): 0.0609
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17728x53184, b=2048): 253.680
Elapsed time for attention_key_query_prob (256x2048x277x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x277x2048): 68.115
Elapsed time for attention_prob_times_values (256x2048x2048x277): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x277): 55.139
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8055750656, 42481549312)
Elapsed time for attention_linear_projection (4x17728x17728, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x17728x17728, b=2048): 255.378
(8055750656, 42481549312)
Elapsed time for mlp_h_to_4h (4x17728x70912, b=2048): 0.0817
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17728x70912, b=2048): 252.018
Elapsed time for mlp_fused_gelu (2048x4x70912): 0.0020
Elapsed time for mlp_4h_to_h (4x70912x17728, b=2048): 0.0835
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70912x17728, b=2048): 246.802
Elapsed time for transformer_add_bias_dropout (2048x4x17728): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17728): 0.0007

Attention duration (in seconds): 0.1360
Attention throughput (in TFLOP/s): 160.191
MLP duration (in seconds): 0.1672
MLP throughput (in TFLOP/s): 246.373
Transformer duration (in seconds): 0.3073
Transformer throughput (in TFLOP/s): 204.967
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1095
Attention throughput (in TFLOP/s): 198.970
MLP duration (in seconds): 0.1695
MLP throughput (in TFLOP/s): 243.021
Transformer duration (in seconds): 0.2821
Transformer throughput (in TFLOP/s): 223.225
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0613
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 253.856
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 100.891
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 94.104
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3026780160, 42481549312)
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 259.063
(3026780160, 42481549312)
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.0824
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 251.668
Elapsed time for mlp_fused_gelu (2048x4x71168): 0.0020
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0837
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 247.737
Elapsed time for transformer_add_bias_dropout (2048x4x17792): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17792): 0.0007

Attention duration (in seconds): 0.1290
Attention throughput (in TFLOP/s): 170.081
MLP duration (in seconds): 0.1682
MLP throughput (in TFLOP/s): 246.680
Transformer duration (in seconds): 0.3013
Transformer throughput (in TFLOP/s): 210.559
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.0999
Attention throughput (in TFLOP/s): 219.695
MLP duration (in seconds): 0.1701
MLP throughput (in TFLOP/s): 243.856
Transformer duration (in seconds): 0.2736
Transformer throughput (in TFLOP/s): 231.824
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17856x53568, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17856x53568, b=2048): 255.895
Elapsed time for attention_key_query_prob (256x2048x279x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x279x2048): 68.072
Elapsed time for attention_prob_times_values (256x2048x2048x279): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x279): 54.438
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17866227712, 42481549312)
Elapsed time for attention_linear_projection (4x17856x17856, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_linear_projection (4x17856x17856, b=2048): 259.349
(17866227712, 42481549312)
Elapsed time for mlp_h_to_4h (4x17856x71424, b=2048): 0.0824
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17856x71424, b=2048): 253.566
Elapsed time for mlp_fused_gelu (2048x4x71424): 0.0020
Elapsed time for mlp_4h_to_h (4x71424x17856, b=2048): 0.0846
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71424x17856, b=2048): 247.024
Elapsed time for transformer_add_bias_dropout (2048x4x17856): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17856): 0.0007

Attention duration (in seconds): 0.1366
Attention throughput (in TFLOP/s): 161.722
MLP duration (in seconds): 0.1690
MLP throughput (in TFLOP/s): 247.242
Transformer duration (in seconds): 0.3098
Transformer throughput (in TFLOP/s): 206.242
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1098
Attention throughput (in TFLOP/s): 201.289
MLP duration (in seconds): 0.1717
MLP throughput (in TFLOP/s): 243.432
Transformer duration (in seconds): 0.2845
Transformer throughput (in TFLOP/s): 224.585
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0620
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 254.670
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 134.838
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 138.679
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12761759744, 42481549312)
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 259.999
(12761759744, 42481549312)
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.0831
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 253.148
Elapsed time for mlp_fused_gelu (2048x4x71680): 0.0020
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0848
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 248.048
Elapsed time for transformer_add_bias_dropout (2048x4x17920): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17920): 0.0007

Attention duration (in seconds): 0.1264
Attention throughput (in TFLOP/s): 175.967
MLP duration (in seconds): 0.1700
MLP throughput (in TFLOP/s): 247.567
Transformer duration (in seconds): 0.3005
Transformer throughput (in TFLOP/s): 214.078
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1001
Attention throughput (in TFLOP/s): 222.221
MLP duration (in seconds): 0.1723
MLP throughput (in TFLOP/s): 244.234
Transformer duration (in seconds): 0.2759
Transformer throughput (in TFLOP/s): 233.207
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17984x53952, b=2048): 0.0628
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17984x53952, b=2048): 252.968
Elapsed time for attention_key_query_prob (256x2048x281x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x281x2048): 68.519
Elapsed time for attention_prob_times_values (256x2048x2048x281): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x281): 54.588
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7623737344, 42481549312)
Elapsed time for attention_linear_projection (4x17984x17984, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_linear_projection (4x17984x17984, b=2048): 258.085
(7623737344, 42481549312)
Elapsed time for mlp_h_to_4h (4x17984x71936, b=2048): 0.0837
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17984x71936, b=2048): 253.142
Elapsed time for mlp_fused_gelu (2048x4x71936): 0.0020
Elapsed time for mlp_4h_to_h (4x71936x17984, b=2048): 0.0858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71936x17984, b=2048): 246.935
Elapsed time for transformer_add_bias_dropout (2048x4x17984): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17984): 0.0007

Attention duration (in seconds): 0.1387
Attention throughput (in TFLOP/s): 161.570
MLP duration (in seconds): 0.1716
MLP throughput (in TFLOP/s): 247.016
Transformer duration (in seconds): 0.3144
Transformer throughput (in TFLOP/s): 206.083
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1112
Attention throughput (in TFLOP/s): 201.518
MLP duration (in seconds): 0.1732
MLP throughput (in TFLOP/s): 244.715
Transformer duration (in seconds): 0.2887
Transformer throughput (in TFLOP/s): 224.425
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0625
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 256.074
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 101.678
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 94.830
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2447966208, 42481549312)
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 259.155
(2447966208, 42481549312)
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.0842
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 253.529
Elapsed time for mlp_fused_gelu (2048x4x72192): 0.0021
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0860
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 248.335
Elapsed time for transformer_add_bias_dropout (2048x4x18048): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18048): 0.0007

Attention duration (in seconds): 0.1309
Attention throughput (in TFLOP/s): 172.367
MLP duration (in seconds): 0.1722
MLP throughput (in TFLOP/s): 247.913
Transformer duration (in seconds): 0.3072
Transformer throughput (in TFLOP/s): 212.410
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1020
Attention throughput (in TFLOP/s): 221.105
MLP duration (in seconds): 0.1741
MLP throughput (in TFLOP/s): 245.272
Transformer duration (in seconds): 0.2801
Transformer throughput (in TFLOP/s): 232.973
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18112x54336, b=2048): 0.0632
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18112x54336, b=2048): 254.976
Elapsed time for attention_key_query_prob (256x2048x283x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x283x2048): 69.007
Elapsed time for attention_prob_times_values (256x2048x2048x283): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x283): 56.155
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15571943424, 42481549312)
Elapsed time for attention_linear_projection (4x18112x18112, b=2048): 0.0209
Throughput (in TFLOP/s) for attention_linear_projection (4x18112x18112, b=2048): 257.679
(15571943424, 42481549312)
Elapsed time for mlp_h_to_4h (4x18112x72448, b=2048): 0.0851
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18112x72448, b=2048): 252.547
Elapsed time for mlp_fused_gelu (2048x4x72448): 0.0021
Elapsed time for mlp_4h_to_h (4x72448x18112, b=2048): 0.0870
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72448x18112, b=2048): 247.101
Elapsed time for transformer_add_bias_dropout (2048x4x18112): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18112): 0.0007

Attention duration (in seconds): 0.1391
Attention throughput (in TFLOP/s): 163.248
MLP duration (in seconds): 0.1742
MLP throughput (in TFLOP/s): 246.840
Transformer duration (in seconds): 0.3175
Transformer throughput (in TFLOP/s): 206.975
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1125
Attention throughput (in TFLOP/s): 201.863
MLP duration (in seconds): 0.1765
MLP throughput (in TFLOP/s): 243.669
Transformer duration (in seconds): 0.2918
Transformer throughput (in TFLOP/s): 225.173
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0637
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 254.847
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 101.699
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 95.215
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(10320674816, 42481549312)
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0209
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 258.844
(10320674816, 42481549312)
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.0860
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 251.629
Elapsed time for mlp_fused_gelu (2048x4x72704): 0.0021
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0876
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 247.174
Elapsed time for transformer_add_bias_dropout (2048x4x18176): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18176): 0.0007

Attention duration (in seconds): 0.1325
Attention throughput (in TFLOP/s): 172.667
MLP duration (in seconds): 0.1757
MLP throughput (in TFLOP/s): 246.440
Transformer duration (in seconds): 0.3123
Transformer throughput (in TFLOP/s): 211.872
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1036
Attention throughput (in TFLOP/s): 220.671
MLP duration (in seconds): 0.1769
MLP throughput (in TFLOP/s): 244.726
Transformer duration (in seconds): 0.2847
Transformer throughput (in TFLOP/s): 232.454
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18240x54720, b=2048): 0.0641
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18240x54720, b=2048): 255.030
Elapsed time for attention_key_query_prob (256x2048x285x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x285x2048): 69.382
Elapsed time for attention_prob_times_values (256x2048x2048x285): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x285): 56.497
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5031657472, 42481549312)
Elapsed time for attention_linear_projection (4x18240x18240, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_linear_projection (4x18240x18240, b=2048): 256.232
(5031657472, 42481549312)
Elapsed time for mlp_h_to_4h (4x18240x72960, b=2048): 0.0863
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18240x72960, b=2048): 252.605
Elapsed time for mlp_fused_gelu (2048x4x72960): 0.0021
Elapsed time for mlp_4h_to_h (4x72960x18240, b=2048): 0.0884
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72960x18240, b=2048): 246.623
Elapsed time for transformer_add_bias_dropout (2048x4x18240): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18240): 0.0008

Attention duration (in seconds): 0.1405
Attention throughput (in TFLOP/s): 163.930
MLP duration (in seconds): 0.1768
MLP throughput (in TFLOP/s): 246.643
Transformer duration (in seconds): 0.3215
Transformer throughput (in TFLOP/s): 207.278
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1140
Attention throughput (in TFLOP/s): 202.001
MLP duration (in seconds): 0.1789
MLP throughput (in TFLOP/s): 243.705
Transformer duration (in seconds): 0.2967
Transformer throughput (in TFLOP/s): 224.618
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0643
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 256.023
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 103.026
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 96.021
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15458697216, 42481549312)
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 259.011
(15458697216, 42481549312)
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.0863
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 254.504
Elapsed time for mlp_fused_gelu (2048x4x73216): 0.0021
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 249.174
Elapsed time for transformer_add_bias_dropout (2048x4x18304): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18304): 0.0007

Attention duration (in seconds): 0.1333
Attention throughput (in TFLOP/s): 173.935
MLP duration (in seconds): 0.1765
MLP throughput (in TFLOP/s): 248.834
Transformer duration (in seconds): 0.3140
Transformer throughput (in TFLOP/s): 213.710
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1045
Attention throughput (in TFLOP/s): 221.914
MLP duration (in seconds): 0.1781
MLP throughput (in TFLOP/s): 246.602
Transformer duration (in seconds): 0.2863
Transformer throughput (in TFLOP/s): 234.380
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18368x55104, b=2048): 0.0653
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18368x55104, b=2048): 254.085
Elapsed time for attention_key_query_prob (256x2048x287x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x287x2048): 70.346
Elapsed time for attention_prob_times_values (256x2048x2048x287): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x287): 55.967
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(10094182400, 42481549312)
Elapsed time for attention_linear_projection (4x18368x18368, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_linear_projection (4x18368x18368, b=2048): 258.802
(10094182400, 42481549312)
Elapsed time for mlp_h_to_4h (4x18368x73472, b=2048): 0.0872
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18368x73472, b=2048): 253.419
Elapsed time for mlp_fused_gelu (2048x4x73472): 0.0021
Elapsed time for mlp_4h_to_h (4x73472x18368, b=2048): 0.0894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73472x18368, b=2048): 247.291
Elapsed time for transformer_add_bias_dropout (2048x4x18368): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18368): 0.0008

Attention duration (in seconds): 0.1418
Attention throughput (in TFLOP/s): 164.593
MLP duration (in seconds): 0.1788
MLP throughput (in TFLOP/s): 247.391
Transformer duration (in seconds): 0.3248
Transformer throughput (in TFLOP/s): 208.027
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1152
Attention throughput (in TFLOP/s): 202.595
MLP duration (in seconds): 0.1814
MLP throughput (in TFLOP/s): 243.818
Transformer duration (in seconds): 0.2995
Transformer throughput (in TFLOP/s): 225.614
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0656
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 254.521
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 163.160
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 143.279
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4691918848, 42481549312)
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 259.235
(4691918848, 42481549312)
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.0882
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 252.334
Elapsed time for mlp_fused_gelu (2048x4x73728): 0.0021
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0895
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 248.769
Elapsed time for transformer_add_bias_dropout (2048x4x18432): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18432): 0.0008

Attention duration (in seconds): 0.1306
Attention throughput (in TFLOP/s): 179.947
MLP duration (in seconds): 0.1798
MLP throughput (in TFLOP/s): 247.617
Transformer duration (in seconds): 0.3147
Transformer throughput (in TFLOP/s): 216.215
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1044
Attention throughput (in TFLOP/s): 225.207
MLP duration (in seconds): 0.1816
MLP throughput (in TFLOP/s): 245.251
Transformer duration (in seconds): 0.2901
Transformer throughput (in TFLOP/s): 234.533
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18496x55488, b=2048): 0.0663
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18496x55488, b=2048): 253.742
Elapsed time for attention_key_query_prob (256x2048x289x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x289x2048): 68.069
Elapsed time for attention_prob_times_values (256x2048x2048x289): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x289): 55.937
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15349645312, 42481549312)
Elapsed time for attention_linear_projection (4x18496x18496, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_linear_projection (4x18496x18496, b=2048): 261.695
(15349645312, 42481549312)
Elapsed time for mlp_h_to_4h (4x18496x73984, b=2048): 0.0889
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18496x73984, b=2048): 252.107
Elapsed time for mlp_fused_gelu (2048x4x73984): 0.0021
Elapsed time for mlp_4h_to_h (4x73984x18496, b=2048): 0.0908
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73984x18496, b=2048): 246.935
Elapsed time for transformer_add_bias_dropout (2048x4x18496): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18496): 0.0008

Attention duration (in seconds): 0.1433
Attention throughput (in TFLOP/s): 165.093
MLP duration (in seconds): 0.1818
MLP throughput (in TFLOP/s): 246.608
Transformer duration (in seconds): 0.3294
Transformer throughput (in TFLOP/s): 207.957
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1169
Attention throughput (in TFLOP/s): 202.394
MLP duration (in seconds): 0.1831
MLP throughput (in TFLOP/s): 244.864
Transformer duration (in seconds): 0.3048
Transformer throughput (in TFLOP/s): 224.751
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.0662
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 255.947
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 98.838
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 96.963
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(9871884288, 42481549312)
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0217
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 260.252
(9871884288, 42481549312)
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.0894
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 252.544
Elapsed time for mlp_fused_gelu (2048x4x74240): 0.0021
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.0910
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 248.139
Elapsed time for transformer_add_bias_dropout (2048x4x18560): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18560): 0.0008

Attention duration (in seconds): 0.1360
Attention throughput (in TFLOP/s): 175.183
MLP duration (in seconds): 0.1825
MLP throughput (in TFLOP/s): 247.426
Transformer duration (in seconds): 0.3227
Transformer throughput (in TFLOP/s): 213.742
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1075
Attention throughput (in TFLOP/s): 221.499
MLP duration (in seconds): 0.1841
MLP throughput (in TFLOP/s): 245.282
Transformer duration (in seconds): 0.2965
Transformer throughput (in TFLOP/s): 232.588
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 64, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18624x55872, b=2048): 0.0670
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18624x55872, b=2048): 254.492
Elapsed time for attention_key_query_prob (256x2048x291x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x291x2048): 67.321
Elapsed time for attention_prob_times_values (256x2048x2048x291): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x291): 57.106
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4356374528, 42481549312)
Elapsed time for attention_linear_projection (4x18624x18624, b=2048): 0.0222
Throughput (in TFLOP/s) for attention_linear_projection (4x18624x18624, b=2048): 255.914
(4356374528, 42481549312)
Elapsed time for mlp_h_to_4h (4x18624x74496, b=2048): 0.0901
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18624x74496, b=2048): 252.366
Elapsed time for mlp_fused_gelu (2048x4x74496): 0.0021
Elapsed time for mlp_4h_to_h (4x74496x18624, b=2048): 0.0918
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74496x18624, b=2048): 247.540
Elapsed time for transformer_add_bias_dropout (2048x4x18624): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18624): 0.0008

Attention duration (in seconds): 0.1448
Attention throughput (in TFLOP/s): 165.572
MLP duration (in seconds): 0.1840
MLP throughput (in TFLOP/s): 247.050
Transformer duration (in seconds): 0.3331
Transformer throughput (in TFLOP/s): 208.456
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1185
Attention throughput (in TFLOP/s): 202.334
MLP duration (in seconds): 0.1861
MLP throughput (in TFLOP/s): 244.237
Transformer duration (in seconds): 0.3077
Transformer throughput (in TFLOP/s): 225.719
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.0674
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 254.594
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 98.950
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 96.830
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15232204800, 42481549312)
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 256.744
(15232204800, 42481549312)
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.0908
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 252.059
Elapsed time for mlp_fused_gelu (2048x4x74752): 0.0021
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0924
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 247.723
Elapsed time for transformer_add_bias_dropout (2048x4x18688): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18688): 0.0008

Attention duration (in seconds): 0.1379
Attention throughput (in TFLOP/s): 175.015
MLP duration (in seconds): 0.1853
MLP throughput (in TFLOP/s): 247.008
Transformer duration (in seconds): 0.3275
Transformer throughput (in TFLOP/s): 213.474
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1087
Attention throughput (in TFLOP/s): 222.099
MLP duration (in seconds): 0.1865
MLP throughput (in TFLOP/s): 245.498
Transformer duration (in seconds): 0.3017
Transformer throughput (in TFLOP/s): 231.714
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18752x56256, b=2048): 0.0679
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18752x56256, b=2048): 254.424
Elapsed time for attention_key_query_prob (256x2048x293x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x293x2048): 67.250
Elapsed time for attention_prob_times_values (256x2048x2048x293): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x293): 57.195
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(9641197568, 42481549312)
Elapsed time for attention_linear_projection (4x18752x18752, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_linear_projection (4x18752x18752, b=2048): 257.487
(9641197568, 42481549312)
Elapsed time for mlp_h_to_4h (4x18752x75008, b=2048): 0.0912
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18752x75008, b=2048): 252.722
Elapsed time for mlp_fused_gelu (2048x4x75008): 0.0021
Elapsed time for mlp_4h_to_h (4x75008x18752, b=2048): 0.0932
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75008x18752, b=2048): 247.289
Elapsed time for transformer_add_bias_dropout (2048x4x18752): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18752): 0.0008

Attention duration (in seconds): 0.1461
Attention throughput (in TFLOP/s): 166.371
MLP duration (in seconds): 0.1865
MLP throughput (in TFLOP/s): 247.118
Transformer duration (in seconds): 0.3369
Transformer throughput (in TFLOP/s): 208.921
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1199
Attention throughput (in TFLOP/s): 202.698
MLP duration (in seconds): 0.1885
MLP throughput (in TFLOP/s): 244.454
Transformer duration (in seconds): 0.3128
Transformer throughput (in TFLOP/s): 225.070
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.0680
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 255.927
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 99.594
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 97.774
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4012441600, 42481549312)
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 258.366
(4012441600, 42481549312)
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.0915
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 253.450
Elapsed time for mlp_fused_gelu (2048x4x75264): 0.0021
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0937
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 247.605
Elapsed time for transformer_add_bias_dropout (2048x4x18816): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18816): 0.0008

Attention duration (in seconds): 0.1387
Attention throughput (in TFLOP/s): 176.446
MLP duration (in seconds): 0.1874
MLP throughput (in TFLOP/s): 247.636
Transformer duration (in seconds): 0.3303
Transformer throughput (in TFLOP/s): 214.546
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1102
Attention throughput (in TFLOP/s): 222.080
MLP duration (in seconds): 0.1893
MLP throughput (in TFLOP/s): 245.163
Transformer duration (in seconds): 0.3039
Transformer throughput (in TFLOP/s): 233.211
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18880x56640, b=2048): 0.0689
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18880x56640, b=2048): 254.286
Elapsed time for attention_key_query_prob (256x2048x295x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x295x2048): 67.009
Elapsed time for attention_prob_times_values (256x2048x2048x295): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x295): 56.288
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12971474944, 42481549312)
Elapsed time for attention_linear_projection (4x18880x18880, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x18880x18880, b=2048): 258.901
(12971474944, 42481549312)
Elapsed time for mlp_h_to_4h (4x18880x75520, b=2048): 0.0926
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18880x75520, b=2048): 252.403
Elapsed time for mlp_fused_gelu (2048x4x75520): 0.0021
Elapsed time for mlp_4h_to_h (4x75520x18880, b=2048): 0.0936
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75520x18880, b=2048): 249.683
Elapsed time for transformer_add_bias_dropout (2048x4x18880): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18880): 0.0008

Attention duration (in seconds): 0.1476
Attention throughput (in TFLOP/s): 166.875
MLP duration (in seconds): 0.1883
MLP throughput (in TFLOP/s): 248.171
Transformer duration (in seconds): 0.3402
Transformer throughput (in TFLOP/s): 209.744
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1208
Attention throughput (in TFLOP/s): 203.942
MLP duration (in seconds): 0.1886
MLP throughput (in TFLOP/s): 247.703
Transformer duration (in seconds): 0.3164
Transformer throughput (in TFLOP/s): 225.473
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.0694
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 254.270
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 138.027
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 146.595
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7267221504, 42481549312)
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 259.673
(7267221504, 42481549312)
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.0932
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 252.280
Elapsed time for mlp_fused_gelu (2048x4x75776): 0.0022
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.0938
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 250.629
Elapsed time for transformer_add_bias_dropout (2048x4x18944): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18944): 0.0008

Attention duration (in seconds): 0.1364
Attention throughput (in TFLOP/s): 181.788
MLP duration (in seconds): 0.1892
MLP throughput (in TFLOP/s): 248.589
Transformer duration (in seconds): 0.3299
Transformer throughput (in TFLOP/s): 217.724
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1112
Attention throughput (in TFLOP/s): 223.009
MLP duration (in seconds): 0.1900
MLP throughput (in TFLOP/s): 247.521
Transformer duration (in seconds): 0.3062
Transformer throughput (in TFLOP/s): 234.549
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 64, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19008x57024, b=2048): 0.0700
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19008x57024, b=2048): 253.572
Elapsed time for attention_key_query_prob (256x2048x297x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x297x2048): 66.909
Elapsed time for attention_prob_times_values (256x2048x2048x297): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x297): 56.496
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1525219328, 42481549312)
Elapsed time for attention_linear_projection (4x19008x19008, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_linear_projection (4x19008x19008, b=2048): 257.150
(1525219328, 42481549312)
Elapsed time for mlp_h_to_4h (4x19008x76032, b=2048): 0.0939
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19008x76032, b=2048): 252.173
Elapsed time for mlp_fused_gelu (2048x4x76032): 0.0022
Elapsed time for mlp_4h_to_h (4x76032x19008, b=2048): 0.0951
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76032x19008, b=2048): 248.863
Elapsed time for transformer_add_bias_dropout (2048x4x19008): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19008): 0.0008

Attention duration (in seconds): 0.1493
Attention throughput (in TFLOP/s): 167.150
MLP duration (in seconds): 0.1912
MLP throughput (in TFLOP/s): 247.675
Transformer duration (in seconds): 0.3449
Transformer throughput (in TFLOP/s): 209.675
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1223
Attention throughput (in TFLOP/s): 204.015
MLP duration (in seconds): 0.1931
MLP throughput (in TFLOP/s): 245.235
Transformer duration (in seconds): 0.3191
Transformer throughput (in TFLOP/s): 226.588
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.0697
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 256.368
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 100.730
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 99.061
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15777464320, 42481549312)
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 256.444
(15777464320, 42481549312)
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.0943
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 252.785
Elapsed time for mlp_fused_gelu (2048x4x76288): 0.0022
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0954
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 249.826
Elapsed time for transformer_add_bias_dropout (2048x4x19072): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19072): 0.0008

Attention duration (in seconds): 0.1412
Attention throughput (in TFLOP/s): 177.880
MLP duration (in seconds): 0.1919
MLP throughput (in TFLOP/s): 248.459
Transformer duration (in seconds): 0.3374
Transformer throughput (in TFLOP/s): 215.723
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1131
Attention throughput (in TFLOP/s): 222.148
MLP duration (in seconds): 0.1928
MLP throughput (in TFLOP/s): 247.283
Transformer duration (in seconds): 0.3117
Transformer throughput (in TFLOP/s): 233.562
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19136x57408, b=2048): 0.0707
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19136x57408, b=2048): 254.586
Elapsed time for attention_key_query_prob (256x2048x299x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x299x2048): 66.606
Elapsed time for attention_prob_times_values (256x2048x2048x299): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x299): 58.028
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(9955770368, 42481549312)
Elapsed time for attention_linear_projection (4x19136x19136, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_linear_projection (4x19136x19136, b=2048): 259.772
(9955770368, 42481549312)
Elapsed time for mlp_h_to_4h (4x19136x76544, b=2048): 0.0951
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19136x76544, b=2048): 252.341
Elapsed time for mlp_fused_gelu (2048x4x76544): 0.0022
Elapsed time for mlp_4h_to_h (4x76544x19136, b=2048): 0.0960
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76544x19136, b=2048): 249.920
Elapsed time for transformer_add_bias_dropout (2048x4x19136): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19136): 0.0008

Attention duration (in seconds): 0.1499
Attention throughput (in TFLOP/s): 168.632
MLP duration (in seconds): 0.1933
MLP throughput (in TFLOP/s): 248.292
Transformer duration (in seconds): 0.3476
Transformer throughput (in TFLOP/s): 210.786
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1237
Attention throughput (in TFLOP/s): 204.434
MLP duration (in seconds): 0.1950
MLP throughput (in TFLOP/s): 246.144
Transformer duration (in seconds): 0.3244
Transformer throughput (in TFLOP/s): 225.892
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0710
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 255.226
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 100.789
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 99.209
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4096327680, 42481549312)
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 257.758
(4096327680, 42481549312)
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.0956
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 252.587
Elapsed time for mlp_fused_gelu (2048x4x76800): 0.0022
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0967
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 249.817
Elapsed time for transformer_add_bias_dropout (2048x4x19200): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19200): 0.0008

Attention duration (in seconds): 0.1427
Attention throughput (in TFLOP/s): 178.283
MLP duration (in seconds): 0.1945
MLP throughput (in TFLOP/s): 248.371
Transformer duration (in seconds): 0.3417
Transformer throughput (in TFLOP/s): 215.899
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1148
Attention throughput (in TFLOP/s): 221.669
MLP duration (in seconds): 0.1966
MLP throughput (in TFLOP/s): 245.778
Transformer duration (in seconds): 0.3146
Transformer throughput (in TFLOP/s): 234.503
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19264x57792, b=2048): 0.0718
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19264x57792, b=2048): 253.965
Elapsed time for attention_key_query_prob (256x2048x301x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x301x2048): 66.658
Elapsed time for attention_prob_times_values (256x2048x2048x301): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x301): 58.157
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(13512540160, 42481549312)
Elapsed time for attention_linear_projection (4x19264x19264, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_linear_projection (4x19264x19264, b=2048): 258.130
(13512540160, 42481549312)
Elapsed time for mlp_h_to_4h (4x19264x77056, b=2048): 0.0971
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19264x77056, b=2048): 250.416
Elapsed time for mlp_fused_gelu (2048x4x77056): 0.0022
Elapsed time for mlp_4h_to_h (4x77056x19264, b=2048): 0.0981
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77056x19264, b=2048): 247.940
Elapsed time for transformer_add_bias_dropout (2048x4x19264): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19264): 0.0008

Attention duration (in seconds): 0.1516
Attention throughput (in TFLOP/s): 168.943
MLP duration (in seconds): 0.1974
MLP throughput (in TFLOP/s): 246.411
Transformer duration (in seconds): 0.3534
Transformer throughput (in TFLOP/s): 210.091
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1253
Attention throughput (in TFLOP/s): 204.356
MLP duration (in seconds): 0.1994
MLP throughput (in TFLOP/s): 243.972
Transformer duration (in seconds): 0.3285
Transformer throughput (in TFLOP/s): 226.015
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.0724
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 253.775
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 100.856
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 100.006
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7573405696, 42481549312)
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 259.275
(7573405696, 42481549312)
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.0972
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 251.799
Elapsed time for mlp_fused_gelu (2048x4x77312): 0.0022
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.0981
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 249.572
Elapsed time for transformer_add_bias_dropout (2048x4x19328): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19328): 0.0008

Attention duration (in seconds): 0.1443
Attention throughput (in TFLOP/s): 178.665
MLP duration (in seconds): 0.1975
MLP throughput (in TFLOP/s): 247.897
Transformer duration (in seconds): 0.3462
Transformer throughput (in TFLOP/s): 215.888
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1157
Attention throughput (in TFLOP/s): 222.781
MLP duration (in seconds): 0.1996
MLP throughput (in TFLOP/s): 245.339
Transformer duration (in seconds): 0.3204
Transformer throughput (in TFLOP/s): 233.316
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19392x58176, b=2048): 0.0729
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19392x58176, b=2048): 253.375
Elapsed time for attention_key_query_prob (256x2048x303x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x303x2048): 66.746
Elapsed time for attention_prob_times_values (256x2048x2048x303): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x303): 57.614
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1592328192, 42481549312)
Elapsed time for attention_linear_projection (4x19392x19392, b=2048): 0.0239
Throughput (in TFLOP/s) for attention_linear_projection (4x19392x19392, b=2048): 257.274
(1592328192, 42481549312)
Elapsed time for mlp_h_to_4h (4x19392x77568, b=2048): 0.0980
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19392x77568, b=2048): 251.450
Elapsed time for mlp_fused_gelu (2048x4x77568): 0.0022
Elapsed time for mlp_4h_to_h (4x77568x19392, b=2048): 0.0995
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77568x19392, b=2048): 247.662
Elapsed time for transformer_add_bias_dropout (2048x4x19392): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19392): 0.0008

Attention duration (in seconds): 0.1534
Attention throughput (in TFLOP/s): 169.195
MLP duration (in seconds): 0.1997
MLP throughput (in TFLOP/s): 246.791
Transformer duration (in seconds): 0.3575
Transformer throughput (in TFLOP/s): 210.442
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1266
Attention throughput (in TFLOP/s): 204.991
MLP duration (in seconds): 0.2023
MLP throughput (in TFLOP/s): 243.601
Transformer duration (in seconds): 0.3323
Transformer throughput (in TFLOP/s): 226.393
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.0733
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 253.674
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 153.503
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 151.173
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(13395099648, 42481549312)
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 256.016
(13395099648, 42481549312)
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.0986
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 251.473
Elapsed time for mlp_fused_gelu (2048x4x77824): 0.0022
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.0997
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 248.811
Elapsed time for transformer_add_bias_dropout (2048x4x19456): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19456): 0.0008

Attention duration (in seconds): 0.1415
Attention throughput (in TFLOP/s): 184.485
MLP duration (in seconds): 0.2006
MLP throughput (in TFLOP/s): 247.382
Transformer duration (in seconds): 0.3465
Transformer throughput (in TFLOP/s): 218.531
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1167
Attention throughput (in TFLOP/s): 223.825
MLP duration (in seconds): 0.2022
MLP throughput (in TFLOP/s): 245.349
Transformer duration (in seconds): 0.3224
Transformer throughput (in TFLOP/s): 234.904
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19520x58560, b=2048): 0.0738
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19520x58560, b=2048): 253.941
Elapsed time for attention_key_query_prob (256x2048x305x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x305x2048): 66.860
Elapsed time for attention_prob_times_values (256x2048x2048x305): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x305): 57.797
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7338524672, 42481549312)
Elapsed time for attention_linear_projection (4x19520x19520, b=2048): 0.0241
Throughput (in TFLOP/s) for attention_linear_projection (4x19520x19520, b=2048): 259.568
(7338524672, 42481549312)
Elapsed time for mlp_h_to_4h (4x19520x78080, b=2048): 0.0992
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19520x78080, b=2048): 251.825
Elapsed time for mlp_fused_gelu (2048x4x78080): 0.0022
Elapsed time for mlp_4h_to_h (4x78080x19520, b=2048): 0.1005
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78080x19520, b=2048): 248.575
Elapsed time for transformer_add_bias_dropout (2048x4x19520): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19520): 0.0008

Attention duration (in seconds): 0.1543
Attention throughput (in TFLOP/s): 170.286
MLP duration (in seconds): 0.2018
MLP throughput (in TFLOP/s): 247.441
Transformer duration (in seconds): 0.3607
Transformer throughput (in TFLOP/s): 211.338
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1276
Attention throughput (in TFLOP/s): 206.002
MLP duration (in seconds): 0.2036
MLP throughput (in TFLOP/s): 245.293
Transformer duration (in seconds): 0.3343
Transformer throughput (in TFLOP/s): 228.001
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.0744
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 253.288
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 101.219
Elapsed time for attention_prob_times_values (256x2048x2048x306): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 101.052
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1240006656, 42481549312)
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0241
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 260.280
(1240006656, 42481549312)
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.1000
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 251.308
Elapsed time for mlp_fused_gelu (2048x4x78336): 0.0022
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.1007
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 249.637
Elapsed time for transformer_add_bias_dropout (2048x4x19584): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19584): 0.0008

Attention duration (in seconds): 0.1470
Attention throughput (in TFLOP/s): 179.959
MLP duration (in seconds): 0.2029
MLP throughput (in TFLOP/s): 247.727
Transformer duration (in seconds): 0.3544
Transformer throughput (in TFLOP/s): 216.502
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1182
Attention throughput (in TFLOP/s): 223.694
MLP duration (in seconds): 0.2052
MLP throughput (in TFLOP/s): 244.949
Transformer duration (in seconds): 0.3268
Transformer throughput (in TFLOP/s): 234.739
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19648x58944, b=2048): 0.0749
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19648x58944, b=2048): 253.370
Elapsed time for attention_key_query_prob (256x2048x307x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x307x2048): 66.363
Elapsed time for attention_prob_times_values (256x2048x2048x307): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x307): 59.001
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12952600576, 42481549312)
Elapsed time for attention_linear_projection (4x19648x19648, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_linear_projection (4x19648x19648, b=2048): 258.104
(12952600576, 42481549312)
Elapsed time for mlp_h_to_4h (4x19648x78592, b=2048): 0.1005
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19648x78592, b=2048): 251.852
Elapsed time for mlp_fused_gelu (2048x4x78592): 0.0022
Elapsed time for mlp_4h_to_h (4x78592x19648, b=2048): 0.1018
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78592x19648, b=2048): 248.421
Elapsed time for transformer_add_bias_dropout (2048x4x19648): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19648): 0.0008

Attention duration (in seconds): 0.1559
Attention throughput (in TFLOP/s): 170.724
MLP duration (in seconds): 0.2045
MLP throughput (in TFLOP/s): 247.397
Transformer duration (in seconds): 0.3650
Transformer throughput (in TFLOP/s): 211.584
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1289
Attention throughput (in TFLOP/s): 206.472
MLP duration (in seconds): 0.2061
MLP throughput (in TFLOP/s): 245.478
Transformer duration (in seconds): 0.3384
Transformer throughput (in TFLOP/s): 228.186
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.0752
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 253.966
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 102.453
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 101.241
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6774390784, 42481549312)
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 258.894
(6774390784, 42481549312)
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.1008
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 252.687
Elapsed time for mlp_fused_gelu (2048x4x78848): 0.0022
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.1022
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 249.234
Elapsed time for transformer_add_bias_dropout (2048x4x19712): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19712): 0.0008

Attention duration (in seconds): 0.1482
Attention throughput (in TFLOP/s): 180.768
MLP duration (in seconds): 0.2052
MLP throughput (in TFLOP/s): 248.212
Transformer duration (in seconds): 0.3579
Transformer throughput (in TFLOP/s): 217.146
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1190
Attention throughput (in TFLOP/s): 225.166
MLP duration (in seconds): 0.2078
MLP throughput (in TFLOP/s): 245.123
Transformer duration (in seconds): 0.3325
Transformer throughput (in TFLOP/s): 233.748
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19776x59328, b=2048): 0.0757
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19776x59328, b=2048): 254.022
Elapsed time for attention_key_query_prob (256x2048x309x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x309x2048): 66.802
Elapsed time for attention_prob_times_values (256x2048x2048x309): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x309): 59.175
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(554237952, 42481549312)
Elapsed time for attention_linear_projection (4x19776x19776, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_linear_projection (4x19776x19776, b=2048): 259.288
(554237952, 42481549312)
Elapsed time for mlp_h_to_4h (4x19776x79104, b=2048): 0.1017
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19776x79104, b=2048): 252.105
Elapsed time for mlp_fused_gelu (2048x4x79104): 0.0022
Elapsed time for mlp_4h_to_h (4x79104x19776, b=2048): 0.1030
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79104x19776, b=2048): 248.817
Elapsed time for transformer_add_bias_dropout (2048x4x19776): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19776): 0.0008

Attention duration (in seconds): 0.1569
Attention throughput (in TFLOP/s): 171.765
MLP duration (in seconds): 0.2069
MLP throughput (in TFLOP/s): 247.731
Transformer duration (in seconds): 0.3684
Transformer throughput (in TFLOP/s): 212.322
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1301
Attention throughput (in TFLOP/s): 207.255
MLP duration (in seconds): 0.2093
MLP throughput (in TFLOP/s): 244.884
Transformer duration (in seconds): 0.3420
Transformer throughput (in TFLOP/s): 228.703
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.0762
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 253.775
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 102.852
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 101.929
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12830965760, 42481549312)
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0248
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 260.279
(12830965760, 42481549312)
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.1019
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 253.162
Elapsed time for mlp_fused_gelu (2048x4x79360): 0.0023
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.1031
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 250.201
Elapsed time for transformer_add_bias_dropout (2048x4x19840): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19840): 0.0008

Attention duration (in seconds): 0.1494
Attention throughput (in TFLOP/s): 181.540
MLP duration (in seconds): 0.2073
MLP throughput (in TFLOP/s): 248.937
Transformer duration (in seconds): 0.3612
Transformer throughput (in TFLOP/s): 217.938
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1204
Attention throughput (in TFLOP/s): 225.356
MLP duration (in seconds): 0.2102
MLP throughput (in TFLOP/s): 245.435
Transformer duration (in seconds): 0.3351
Transformer throughput (in TFLOP/s): 234.915
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19904x59712, b=2048): 0.0767
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19904x59712, b=2048): 254.010
Elapsed time for attention_key_query_prob (256x2048x311x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x311x2048): 66.254
Elapsed time for attention_prob_times_values (256x2048x2048x311): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x311): 58.507
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6531121152, 42481549312)
Elapsed time for attention_linear_projection (4x19904x19904, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_linear_projection (4x19904x19904, b=2048): 259.146
(6531121152, 42481549312)
Elapsed time for mlp_h_to_4h (4x19904x79616, b=2048): 0.1032
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19904x79616, b=2048): 251.645
Elapsed time for mlp_fused_gelu (2048x4x79616): 0.0023
Elapsed time for mlp_4h_to_h (4x79616x19904, b=2048): 0.1044
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79616x19904, b=2048): 248.742
Elapsed time for transformer_add_bias_dropout (2048x4x19904): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19904): 0.0008

Attention duration (in seconds): 0.1586
Attention throughput (in TFLOP/s): 172.106
MLP duration (in seconds): 0.2098
MLP throughput (in TFLOP/s): 247.488
Transformer duration (in seconds): 0.3730
Transformer throughput (in TFLOP/s): 212.397
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1318
Attention throughput (in TFLOP/s): 207.079
MLP duration (in seconds): 0.2118
MLP throughput (in TFLOP/s): 245.143
Transformer duration (in seconds): 0.3464
Transformer throughput (in TFLOP/s): 228.706
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.0772
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 253.736
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 136.632
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 154.316
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(189333504, 42481549312)
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 259.867
(189333504, 42481549312)
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.1036
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 252.119
Elapsed time for mlp_fused_gelu (2048x4x79872): 0.0023
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.1048
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 249.315
Elapsed time for transformer_add_bias_dropout (2048x4x19968): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19968): 0.0008

Attention duration (in seconds): 0.1470
Attention throughput (in TFLOP/s): 186.823
MLP duration (in seconds): 0.2107
MLP throughput (in TFLOP/s): 248.013
Transformer duration (in seconds): 0.3623
Transformer throughput (in TFLOP/s): 220.067
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1218
Attention throughput (in TFLOP/s): 225.590
MLP duration (in seconds): 0.2136
MLP throughput (in TFLOP/s): 244.678
Transformer duration (in seconds): 0.3411
Transformer throughput (in TFLOP/s): 233.758
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20032x60096, b=2048): 0.0780
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20032x60096, b=2048): 252.784
Elapsed time for attention_key_query_prob (256x2048x313x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x313x2048): 66.420
Elapsed time for attention_prob_times_values (256x2048x2048x313): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x313): 58.769
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12380078080, 42481549312)
Elapsed time for attention_linear_projection (4x20032x20032, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_linear_projection (4x20032x20032, b=2048): 257.566
(12380078080, 42481549312)
Elapsed time for mlp_h_to_4h (4x20032x80128, b=2048): 0.1045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20032x80128, b=2048): 251.758
Elapsed time for mlp_fused_gelu (2048x4x80128): 0.0023
Elapsed time for mlp_4h_to_h (4x80128x20032, b=2048): 0.1059
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80128x20032, b=2048): 248.254
Elapsed time for transformer_add_bias_dropout (2048x4x20032): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20032): 0.0008

Attention duration (in seconds): 0.1605
Attention throughput (in TFLOP/s): 172.197
MLP duration (in seconds): 0.2127
MLP throughput (in TFLOP/s): 247.320
Transformer duration (in seconds): 0.3778
Transformer throughput (in TFLOP/s): 212.394
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1330
Attention throughput (in TFLOP/s): 207.916
MLP duration (in seconds): 0.2143
MLP throughput (in TFLOP/s): 245.407
Transformer duration (in seconds): 0.3528
Transformer throughput (in TFLOP/s): 227.444
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.0780
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 254.478
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 103.919
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 103.482
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5958598656, 42481549312)
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 258.534
(5958598656, 42481549312)
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1052
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 251.585
Elapsed time for mlp_fused_gelu (2048x4x80384): 0.0023
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.1065
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 248.422
Elapsed time for transformer_add_bias_dropout (2048x4x20096): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20096): 0.0008

Attention duration (in seconds): 0.1520
Attention throughput (in TFLOP/s): 182.982
MLP duration (in seconds): 0.2140
MLP throughput (in TFLOP/s): 247.327
Transformer duration (in seconds): 0.3706
Transformer throughput (in TFLOP/s): 217.885
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1233
Attention throughput (in TFLOP/s): 225.673
MLP duration (in seconds): 0.2157
MLP throughput (in TFLOP/s): 245.440
Transformer duration (in seconds): 0.3440
Transformer throughput (in TFLOP/s): 234.747
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20160x60480, b=2048): 0.0793
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20160x60480, b=2048): 252.068
Elapsed time for attention_key_query_prob (256x2048x315x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x315x2048): 66.781
Elapsed time for attention_prob_times_values (256x2048x2048x315): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x315): 60.211
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12291997696, 42481549312)
Elapsed time for attention_linear_projection (4x20160x20160, b=2048): 0.0259
Throughput (in TFLOP/s) for attention_linear_projection (4x20160x20160, b=2048): 257.485
(12291997696, 42481549312)
Elapsed time for mlp_h_to_4h (4x20160x80640, b=2048): 0.1058
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20160x80640, b=2048): 251.692
Elapsed time for mlp_fused_gelu (2048x4x80640): 0.0023
Elapsed time for mlp_4h_to_h (4x80640x20160, b=2048): 0.1069
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80640x20160, b=2048): 249.156
Elapsed time for transformer_add_bias_dropout (2048x4x20160): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20160): 0.0008

Attention duration (in seconds): 0.1619
Attention throughput (in TFLOP/s): 172.886
MLP duration (in seconds): 0.2150
MLP throughput (in TFLOP/s): 247.754
Transformer duration (in seconds): 0.3815
Transformer throughput (in TFLOP/s): 212.987
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1348
Attention throughput (in TFLOP/s): 207.702
MLP duration (in seconds): 0.2175
MLP throughput (in TFLOP/s): 244.935
Transformer duration (in seconds): 0.3565
Transformer throughput (in TFLOP/s): 227.920
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.0789
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 254.791
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 104.397
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 103.776
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5786632192, 42481549312)
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0258
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 259.721
(5786632192, 42481549312)
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1069
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 250.829
Elapsed time for mlp_fused_gelu (2048x4x80896): 0.0023
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.1076
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 249.032
Elapsed time for transformer_add_bias_dropout (2048x4x20224): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20224): 0.0008

Attention duration (in seconds): 0.1532
Attention throughput (in TFLOP/s): 183.875
MLP duration (in seconds): 0.2168
MLP throughput (in TFLOP/s): 247.280
Transformer duration (in seconds): 0.3746
Transformer throughput (in TFLOP/s): 218.318
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1248
Attention throughput (in TFLOP/s): 225.637
MLP duration (in seconds): 0.2197
MLP throughput (in TFLOP/s): 244.015
Transformer duration (in seconds): 0.3502
Transformer throughput (in TFLOP/s): 233.470
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20288x60864, b=2048): 0.0798
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20288x60864, b=2048): 253.499
Elapsed time for attention_key_query_prob (256x2048x317x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x317x2048): 67.201
Elapsed time for attention_prob_times_values (256x2048x2048x317): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x317): 60.583
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12203917312, 42481549312)
Elapsed time for attention_linear_projection (4x20288x20288, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x20288x20288, b=2048): 255.858
(12203917312, 42481549312)
Elapsed time for mlp_h_to_4h (4x20288x81152, b=2048): 0.1071
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20288x81152, b=2048): 251.965
Elapsed time for mlp_fused_gelu (2048x4x81152): 0.0023
Elapsed time for mlp_4h_to_h (4x81152x20288, b=2048): 0.1089
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81152x20288, b=2048): 247.643
Elapsed time for transformer_add_bias_dropout (2048x4x20288): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20288): 0.0008

Attention duration (in seconds): 0.1629
Attention throughput (in TFLOP/s): 173.901
MLP duration (in seconds): 0.2183
MLP throughput (in TFLOP/s): 247.148
Transformer duration (in seconds): 0.3859
Transformer throughput (in TFLOP/s): 213.242
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1360
Attention throughput (in TFLOP/s): 208.313
MLP duration (in seconds): 0.2207
MLP throughput (in TFLOP/s): 244.395
Transformer duration (in seconds): 0.3624
Transformer throughput (in TFLOP/s): 227.061
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.0804
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 253.370
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 105.857
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 104.731
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5614665728, 42481549312)
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 256.695
(5614665728, 42481549312)
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1078
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 251.825
Elapsed time for mlp_fused_gelu (2048x4x81408): 0.0023
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1092
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 248.498
Elapsed time for transformer_add_bias_dropout (2048x4x20352): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20352): 0.0008

Attention duration (in seconds): 0.1552
Attention throughput (in TFLOP/s): 183.735
MLP duration (in seconds): 0.2193
MLP throughput (in TFLOP/s): 247.516
Transformer duration (in seconds): 0.3792
Transformer throughput (in TFLOP/s): 218.387
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1265
Attention throughput (in TFLOP/s): 225.312
MLP duration (in seconds): 0.2218
MLP throughput (in TFLOP/s): 244.827
Transformer duration (in seconds): 0.3554
Transformer throughput (in TFLOP/s): 232.958
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20416x61248, b=2048): 0.0805
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20416x61248, b=2048): 254.655
Elapsed time for attention_key_query_prob (256x2048x319x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x319x2048): 67.906
Elapsed time for attention_prob_times_values (256x2048x2048x319): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x319): 60.553
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(9634906112, 42481549312)
Elapsed time for attention_linear_projection (4x20416x20416, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_linear_projection (4x20416x20416, b=2048): 257.309
(9634906112, 42481549312)
Elapsed time for mlp_h_to_4h (4x20416x81664, b=2048): 0.1086
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20416x81664, b=2048): 251.644
Elapsed time for mlp_fused_gelu (2048x4x81664): 0.0023
Elapsed time for mlp_4h_to_h (4x81664x20416, b=2048): 0.1098
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81664x20416, b=2048): 248.676
Elapsed time for transformer_add_bias_dropout (2048x4x20416): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20416): 0.0008

Attention duration (in seconds): 0.1638
Attention throughput (in TFLOP/s): 175.138
MLP duration (in seconds): 0.2207
MLP throughput (in TFLOP/s): 247.527
Transformer duration (in seconds): 0.3892
Transformer throughput (in TFLOP/s): 214.088
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1372
Attention throughput (in TFLOP/s): 209.075
MLP duration (in seconds): 0.2238
MLP throughput (in TFLOP/s): 244.158
Transformer duration (in seconds): 0.3654
Transformer throughput (in TFLOP/s): 228.043
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.0807
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 255.455
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 166.300
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 159.252
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2961768448, 42481549312)
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 258.035
(2961768448, 42481549312)
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1092
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 251.828
Elapsed time for mlp_fused_gelu (2048x4x81920): 0.0023
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1100
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 249.901
Elapsed time for transformer_add_bias_dropout (2048x4x20480): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20480): 0.0008

Attention duration (in seconds): 0.1512
Attention throughput (in TFLOP/s): 190.917
MLP duration (in seconds): 0.2215
MLP throughput (in TFLOP/s): 248.231
Transformer duration (in seconds): 0.3773
Transformer throughput (in TFLOP/s): 222.191
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1270
Attention throughput (in TFLOP/s): 227.292
MLP duration (in seconds): 0.2231
MLP throughput (in TFLOP/s): 246.374
Transformer duration (in seconds): 0.3548
Transformer throughput (in TFLOP/s): 236.263
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 64, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20544x61632, b=2048): 0.0819
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20544x61632, b=2048): 253.363
Elapsed time for attention_key_query_prob (256x2048x321x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x321x2048): 66.228
Elapsed time for attention_prob_times_values (256x2048x2048x321): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x321): 60.413
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11098718208, 42481549312)
Elapsed time for attention_linear_projection (4x20544x20544, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x20544x20544, b=2048): 258.655
(11098718208, 42481549312)
Elapsed time for mlp_h_to_4h (4x20544x82176, b=2048): 0.1101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20544x82176, b=2048): 251.167
Elapsed time for mlp_fused_gelu (2048x4x82176): 0.0023
Elapsed time for mlp_4h_to_h (4x82176x20544, b=2048): 0.1115
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82176x20544, b=2048): 248.056
Elapsed time for transformer_add_bias_dropout (2048x4x20544): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20544): 0.0009

Attention duration (in seconds): 0.1659
Attention throughput (in TFLOP/s): 175.088
MLP duration (in seconds): 0.2240
MLP throughput (in TFLOP/s): 247.003
Transformer duration (in seconds): 0.3945
Transformer throughput (in TFLOP/s): 213.822
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1394
Attention throughput (in TFLOP/s): 208.327
MLP duration (in seconds): 0.2258
MLP throughput (in TFLOP/s): 244.968
Transformer duration (in seconds): 0.3708
Transformer throughput (in TFLOP/s): 227.501
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.0822
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 253.885
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 101.944
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 105.681
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4341694464, 42481549312)
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 259.443
(4341694464, 42481549312)
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1105
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 251.770
Elapsed time for mlp_fused_gelu (2048x4x82432): 0.0023
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1748
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 159.185
Elapsed time for transformer_add_bias_dropout (2048x4x20608): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20608): 0.0008

Attention duration (in seconds): 0.1578
Attention throughput (in TFLOP/s): 185.159
MLP duration (in seconds): 0.2877
MLP throughput (in TFLOP/s): 193.462
Transformer duration (in seconds): 0.4502
Transformer throughput (in TFLOP/s): 188.539
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1291
Attention throughput (in TFLOP/s): 226.224
MLP duration (in seconds): 0.2823
MLP throughput (in TFLOP/s): 197.162
Transformer duration (in seconds): 0.4194
Transformer throughput (in TFLOP/s): 202.378
Transformer - MLP - Attention (in seconds): 0.0079
========================================================================================================================
num_attention_heads: 64, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20672x62016, b=2048): 0.0829
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20672x62016, b=2048): 253.219
Elapsed time for attention_key_query_prob (256x2048x323x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x323x2048): 65.716
Elapsed time for attention_prob_times_values (256x2048x2048x323): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x323): 61.024
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11000152064, 42481549312)
Elapsed time for attention_linear_projection (4x20672x20672, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_linear_projection (4x20672x20672, b=2048): 259.614
(11000152064, 42481549312)
Elapsed time for mlp_h_to_4h (4x20672x82688, b=2048): 0.1114
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20672x82688, b=2048): 251.472
Elapsed time for mlp_fused_gelu (2048x4x82688): 0.0023
Elapsed time for mlp_4h_to_h (4x82688x20672, b=2048): 0.1773
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82688x20672, b=2048): 158.000
Elapsed time for transformer_add_bias_dropout (2048x4x20672): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20672): 0.0009

Attention duration (in seconds): 0.1673
Attention throughput (in TFLOP/s): 175.730
MLP duration (in seconds): 0.2910
MLP throughput (in TFLOP/s): 192.501
Transformer duration (in seconds): 0.4630
Transformer throughput (in TFLOP/s): 184.461
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1405
Attention throughput (in TFLOP/s): 209.191
MLP duration (in seconds): 0.2873
MLP throughput (in TFLOP/s): 194.979
Transformer duration (in seconds): 0.4359
Transformer throughput (in TFLOP/s): 195.932
Transformer - MLP - Attention (in seconds): 0.0081
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.0835
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 253.151
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 102.168
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 105.515
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4159242240, 42481549312)
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 260.584
(4159242240, 42481549312)
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1122
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 251.051
Elapsed time for mlp_fused_gelu (2048x4x82944): 0.0024
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1745
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 161.498
Elapsed time for transformer_add_bias_dropout (2048x4x20736): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20736): 0.0008

Attention duration (in seconds): 0.1593
Attention throughput (in TFLOP/s): 185.579
MLP duration (in seconds): 0.2891
MLP throughput (in TFLOP/s): 194.954
Transformer duration (in seconds): 0.4532
Transformer throughput (in TFLOP/s): 189.624
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1301
Attention throughput (in TFLOP/s): 227.367
MLP duration (in seconds): 0.2875
MLP throughput (in TFLOP/s): 196.046
Transformer duration (in seconds): 0.4213
Transformer throughput (in TFLOP/s): 203.986
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20800x62400, b=2048): 0.0841
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20800x62400, b=2048): 252.769
Elapsed time for attention_key_query_prob (256x2048x325x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x325x2048): 66.297
Elapsed time for attention_prob_times_values (256x2048x2048x325): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x325): 61.093
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(10572333056, 42481549312)
Elapsed time for attention_linear_projection (4x20800x20800, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_linear_projection (4x20800x20800, b=2048): 256.929
(10572333056, 42481549312)
Elapsed time for mlp_h_to_4h (4x20800x83200, b=2048): 0.1127
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20800x83200, b=2048): 251.663
Elapsed time for mlp_fused_gelu (2048x4x83200): 0.0024
Elapsed time for mlp_4h_to_h (4x83200x20800, b=2048): 0.1817
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83200x20800, b=2048): 156.015
Elapsed time for transformer_add_bias_dropout (2048x4x20800): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20800): 0.0009

Attention duration (in seconds): 0.1691
Attention throughput (in TFLOP/s): 175.939
MLP duration (in seconds): 0.2968
MLP throughput (in TFLOP/s): 191.086
Transformer duration (in seconds): 0.4706
Transformer throughput (in TFLOP/s): 183.709
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1419
Attention throughput (in TFLOP/s): 209.579
MLP duration (in seconds): 0.2962
MLP throughput (in TFLOP/s): 191.464
Transformer duration (in seconds): 0.4434
Transformer throughput (in TFLOP/s): 194.971
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.0847
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 252.553
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 102.666
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 105.757
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3647537152, 42481549312)
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0276
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 258.736
(3647537152, 42481549312)
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1138
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 250.722
Elapsed time for mlp_fused_gelu (2048x4x83456): 0.0024
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1800
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 158.456
Elapsed time for transformer_add_bias_dropout (2048x4x20864): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20864): 0.0008

Attention duration (in seconds): 0.1611
Attention throughput (in TFLOP/s): 185.725
MLP duration (in seconds): 0.2962
MLP throughput (in TFLOP/s): 192.634
Transformer duration (in seconds): 0.4621
Transformer throughput (in TFLOP/s): 188.245
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1319
Attention throughput (in TFLOP/s): 226.903
MLP duration (in seconds): 0.2935
MLP throughput (in TFLOP/s): 194.410
Transformer duration (in seconds): 0.4273
Transformer throughput (in TFLOP/s): 203.569
Transformer - MLP - Attention (in seconds): 0.0019
========================================================================================================================
num_attention_heads: 64, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20928x62784, b=2048): 0.0855
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20928x62784, b=2048): 251.672
Elapsed time for attention_key_query_prob (256x2048x327x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x327x2048): 66.298
Elapsed time for attention_prob_times_values (256x2048x2048x327): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x327): 60.256
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(10482155520, 42481549312)
Elapsed time for attention_linear_projection (4x20928x20928, b=2048): 0.0278
Throughput (in TFLOP/s) for attention_linear_projection (4x20928x20928, b=2048): 258.406
(10482155520, 42481549312)
Elapsed time for mlp_h_to_4h (4x20928x83712, b=2048): 0.1142
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20928x83712, b=2048): 251.270
Elapsed time for mlp_fused_gelu (2048x4x83712): 0.0024
Elapsed time for mlp_4h_to_h (4x83712x20928, b=2048): 0.1856
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83712x20928, b=2048): 154.639
Elapsed time for transformer_add_bias_dropout (2048x4x20928): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20928): 0.0009

Attention duration (in seconds): 0.1710
Attention throughput (in TFLOP/s): 176.098
MLP duration (in seconds): 0.3022
MLP throughput (in TFLOP/s): 189.947
Transformer duration (in seconds): 0.4780
Transformer throughput (in TFLOP/s): 183.089
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1438
Attention throughput (in TFLOP/s): 209.368
MLP duration (in seconds): 0.2998
MLP throughput (in TFLOP/s): 191.491
Transformer duration (in seconds): 0.4511
Transformer throughput (in TFLOP/s): 193.989
Transformer - MLP - Attention (in seconds): 0.0075
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.0854
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 253.490
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 140.704
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 160.458
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3473473536, 42481549312)
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 259.090
(3473473536, 42481549312)
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1152
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 250.626
Elapsed time for mlp_fused_gelu (2048x4x83968): 0.0024
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1876
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 153.977
Elapsed time for transformer_add_bias_dropout (2048x4x20992): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20992): 0.0008

Attention duration (in seconds): 0.1581
Attention throughput (in TFLOP/s): 191.547
MLP duration (in seconds): 0.3052
MLP throughput (in TFLOP/s): 189.268
Transformer duration (in seconds): 0.4681
Transformer throughput (in TFLOP/s): 188.109
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1336
Attention throughput (in TFLOP/s): 226.739
MLP duration (in seconds): 0.3016
MLP throughput (in TFLOP/s): 191.513
Transformer duration (in seconds): 0.4432
Transformer throughput (in TFLOP/s): 198.657
Transformer - MLP - Attention (in seconds): 0.0080
========================================================================================================================
num_attention_heads: 64, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21056x63168, b=2048): 0.0860
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21056x63168, b=2048): 253.403
Elapsed time for attention_key_query_prob (256x2048x329x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x329x2048): 66.934
Elapsed time for attention_prob_times_values (256x2048x2048x329): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x329): 60.946
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(10391977984, 42481549312)
Elapsed time for attention_linear_projection (4x21056x21056, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_linear_projection (4x21056x21056, b=2048): 259.458
(10391977984, 42481549312)
Elapsed time for mlp_h_to_4h (4x21056x84224, b=2048): 0.1160
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21056x84224, b=2048): 250.500
Elapsed time for mlp_fused_gelu (2048x4x84224): 0.0024
Elapsed time for mlp_4h_to_h (4x84224x21056, b=2048): 0.1886
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84224x21056, b=2048): 154.087
Elapsed time for transformer_add_bias_dropout (2048x4x21056): 0.0015
Elapsed time for transformer_layer_norm (2048x4x21056): 0.0009

Attention duration (in seconds): 0.1716
Attention throughput (in TFLOP/s): 177.592
MLP duration (in seconds): 0.3069
MLP throughput (in TFLOP/s): 189.322
Transformer duration (in seconds): 0.4834
Transformer throughput (in TFLOP/s): 183.259
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1456
Attention throughput (in TFLOP/s): 209.297
MLP duration (in seconds): 0.3035
MLP throughput (in TFLOP/s): 191.463
Transformer duration (in seconds): 0.4526
Transformer throughput (in TFLOP/s): 195.693
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.0868
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 252.680
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 103.524
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 105.647
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3295215616, 42481549312)
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0281
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 260.167
(3295215616, 42481549312)
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1168
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 250.269
Elapsed time for mlp_fused_gelu (2048x4x84480): 0.0024
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1939
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 150.775
Elapsed time for transformer_add_bias_dropout (2048x4x21120): 0.0015
Elapsed time for transformer_layer_norm (2048x4x21120): 0.0009

Attention duration (in seconds): 0.1638
Attention throughput (in TFLOP/s): 187.081
MLP duration (in seconds): 0.3131
MLP throughput (in TFLOP/s): 186.741
Transformer duration (in seconds): 0.4817
Transformer throughput (in TFLOP/s): 184.990
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1351
Attention throughput (in TFLOP/s): 226.924
MLP duration (in seconds): 0.3078
MLP throughput (in TFLOP/s): 189.927
Transformer duration (in seconds): 0.4481
Transformer throughput (in TFLOP/s): 198.885
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 64, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21184x63552, b=2048): 0.0876
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21184x63552, b=2048): 251.674
Elapsed time for attention_key_query_prob (256x2048x331x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x331x2048): 68.068
Elapsed time for attention_prob_times_values (256x2048x2048x331): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x331): 62.553
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8150122496, 42481549312)
Elapsed time for attention_linear_projection (4x21184x21184, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_linear_projection (4x21184x21184, b=2048): 256.386
(8150122496, 42481549312)
Elapsed time for mlp_h_to_4h (4x21184x84736, b=2048): 0.1174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21184x84736, b=2048): 250.593
Elapsed time for mlp_fused_gelu (2048x4x84736): 0.0024
Elapsed time for mlp_4h_to_h (4x84736x21184, b=2048): 0.1803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84736x21184, b=2048): 163.081
Elapsed time for transformer_add_bias_dropout (2048x4x21184): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21184): 0.0009

Attention duration (in seconds): 0.1735
Attention throughput (in TFLOP/s): 177.654
MLP duration (in seconds): 0.3001
MLP throughput (in TFLOP/s): 195.996
Transformer duration (in seconds): 0.4785
Transformer throughput (in TFLOP/s): 187.349
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1476
Attention throughput (in TFLOP/s): 208.839
MLP duration (in seconds): 0.3008
MLP throughput (in TFLOP/s): 195.527
Transformer duration (in seconds): 0.4562
Transformer throughput (in TFLOP/s): 196.531
Transformer - MLP - Attention (in seconds): 0.0077
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.0872
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 254.590
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 104.566
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 106.306
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(969474048, 42481549312)
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 257.347
(969474048, 42481549312)
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1181
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 250.569
Elapsed time for mlp_fused_gelu (2048x4x84992): 0.0024
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1873
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 157.971
Elapsed time for transformer_add_bias_dropout (2048x4x21248): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21248): 0.0009

Attention duration (in seconds): 0.1649
Attention throughput (in TFLOP/s): 188.130
MLP duration (in seconds): 0.3078
MLP throughput (in TFLOP/s): 192.256
Transformer duration (in seconds): 0.4775
Transformer throughput (in TFLOP/s): 188.862
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1365
Attention throughput (in TFLOP/s): 227.231
MLP duration (in seconds): 0.3074
MLP throughput (in TFLOP/s): 192.508
Transformer duration (in seconds): 0.4517
Transformer throughput (in TFLOP/s): 199.658
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 64, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21312x63936, b=2048): 0.0886
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21312x63936, b=2048): 252.085
Elapsed time for attention_key_query_prob (256x2048x333x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x333x2048): 68.522
Elapsed time for attention_prob_times_values (256x2048x2048x333): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x333): 62.583
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7359496192, 42481549312)
Elapsed time for attention_linear_projection (4x21312x21312, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x21312x21312, b=2048): 257.882
(7359496192, 42481549312)
Elapsed time for mlp_h_to_4h (4x21312x85248, b=2048): 0.1182
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21312x85248, b=2048): 251.809
Elapsed time for mlp_fused_gelu (2048x4x85248): 0.0024
Elapsed time for mlp_4h_to_h (4x85248x21312, b=2048): 0.1882
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85248x21312, b=2048): 158.149
Elapsed time for transformer_add_bias_dropout (2048x4x21312): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21312): 0.0009

Attention duration (in seconds): 0.1747
Attention throughput (in TFLOP/s): 178.570
MLP duration (in seconds): 0.3088
MLP throughput (in TFLOP/s): 192.758
Transformer duration (in seconds): 0.4884
Transformer throughput (in TFLOP/s): 185.753
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1483
Attention throughput (in TFLOP/s): 210.303
MLP duration (in seconds): 0.3078
MLP throughput (in TFLOP/s): 193.398
Transformer duration (in seconds): 0.4602
Transformer throughput (in TFLOP/s): 197.135
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.0887
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 253.229
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 104.615
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 107.068
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(90767360, 42481549312)
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 258.636
(90767360, 42481549312)
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 251.598
Elapsed time for mlp_fused_gelu (2048x4x85504): 0.0024
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1878
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 159.431
Elapsed time for transformer_add_bias_dropout (2048x4x21376): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21376): 0.0009

Attention duration (in seconds): 0.1666
Attention throughput (in TFLOP/s): 188.347
MLP duration (in seconds): 0.3093
MLP throughput (in TFLOP/s): 193.650
Transformer duration (in seconds): 0.4807
Transformer throughput (in TFLOP/s): 189.854
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1381
Attention throughput (in TFLOP/s): 227.202
MLP duration (in seconds): 0.3115
MLP throughput (in TFLOP/s): 192.244
Transformer duration (in seconds): 0.4490
Transformer throughput (in TFLOP/s): 203.298
Transformer - MLP - Attention (in seconds): -0.0007
========================================================================================================================
num_attention_heads: 64, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21440x64320, b=2048): 0.0897
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21440x64320, b=2048): 251.954
Elapsed time for attention_key_query_prob (256x2048x335x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x335x2048): 69.566
Elapsed time for attention_prob_times_values (256x2048x2048x335): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x335): 62.156
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7260930048, 42481549312)
Elapsed time for attention_linear_projection (4x21440x21440, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_linear_projection (4x21440x21440, b=2048): 258.884
(7260930048, 42481549312)
Elapsed time for mlp_h_to_4h (4x21440x85760, b=2048): 0.1203
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21440x85760, b=2048): 250.521
Elapsed time for mlp_fused_gelu (2048x4x85760): 0.0024
Elapsed time for mlp_4h_to_h (4x85760x21440, b=2048): 0.1910
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85760x21440, b=2048): 157.729
Elapsed time for transformer_add_bias_dropout (2048x4x21440): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21440): 0.0009

Attention duration (in seconds): 0.1761
Attention throughput (in TFLOP/s): 179.251
MLP duration (in seconds): 0.3137
MLP throughput (in TFLOP/s): 192.078
Transformer duration (in seconds): 0.4947
Transformer throughput (in TFLOP/s): 185.609
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1497
Attention throughput (in TFLOP/s): 210.794
MLP duration (in seconds): 0.3112
MLP throughput (in TFLOP/s): 193.627
Transformer duration (in seconds): 0.4641
Transformer throughput (in TFLOP/s): 197.823
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.0898
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 253.021
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 154.638
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 165.216
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7210598400, 42481549312)
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 259.836
(7210598400, 42481549312)
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1201
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 252.349
Elapsed time for mlp_fused_gelu (2048x4x86016): 0.0024
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1851
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 163.694
Elapsed time for transformer_add_bias_dropout (2048x4x21504): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21504): 0.0009

Attention duration (in seconds): 0.1634
Attention throughput (in TFLOP/s): 194.252
MLP duration (in seconds): 0.3077
MLP throughput (in TFLOP/s): 197.002
Transformer duration (in seconds): 0.4760
Transformer throughput (in TFLOP/s): 194.033
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1388
Attention throughput (in TFLOP/s): 228.766
MLP duration (in seconds): 0.3114
MLP throughput (in TFLOP/s): 194.640
Transformer duration (in seconds): 0.4579
Transformer throughput (in TFLOP/s): 201.717
Transformer - MLP - Attention (in seconds): 0.0077
========================================================================================================================
num_attention_heads: 64, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21568x64704, b=2048): 0.0904
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21568x64704, b=2048): 252.823
Elapsed time for attention_key_query_prob (256x2048x337x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x337x2048): 70.410
Elapsed time for attention_prob_times_values (256x2048x2048x337): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x337): 62.977
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7166558208, 42481549312)
Elapsed time for attention_linear_projection (4x21568x21568, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_linear_projection (4x21568x21568, b=2048): 256.264
(7166558208, 42481549312)
Elapsed time for mlp_h_to_4h (4x21568x86272, b=2048): 0.1215
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21568x86272, b=2048): 250.904
Elapsed time for mlp_fused_gelu (2048x4x86272): 0.0024
Elapsed time for mlp_4h_to_h (4x86272x21568, b=2048): 0.1967
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86272x21568, b=2048): 155.025
Elapsed time for transformer_add_bias_dropout (2048x4x21568): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21568): 0.0009

Attention duration (in seconds): 0.1774
Attention throughput (in TFLOP/s): 180.043
MLP duration (in seconds): 0.3206
MLP throughput (in TFLOP/s): 190.178
Transformer duration (in seconds): 0.5029
Transformer throughput (in TFLOP/s): 184.729
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1516
Attention throughput (in TFLOP/s): 210.582
MLP duration (in seconds): 0.3189
MLP throughput (in TFLOP/s): 191.220
Transformer duration (in seconds): 0.4774
Transformer throughput (in TFLOP/s): 194.605
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.0908
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 253.222
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 105.713
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 108.501
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7112032256, 42481549312)
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 257.045
(7112032256, 42481549312)
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1225
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 250.329
Elapsed time for mlp_fused_gelu (2048x4x86528): 0.0025
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1906
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 160.927
Elapsed time for transformer_add_bias_dropout (2048x4x21632): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21632): 0.0009

Attention duration (in seconds): 0.1696
Attention throughput (in TFLOP/s): 189.347
MLP duration (in seconds): 0.3155
MLP throughput (in TFLOP/s): 194.387
Transformer duration (in seconds): 0.4901
Transformer throughput (in TFLOP/s): 190.695
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1413
Attention throughput (in TFLOP/s): 227.337
MLP duration (in seconds): 0.3147
MLP throughput (in TFLOP/s): 194.915
Transformer duration (in seconds): 0.4600
Transformer throughput (in TFLOP/s): 203.139
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21696x65088, b=2048): 0.0914
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21696x65088, b=2048): 253.249
Elapsed time for attention_key_query_prob (256x2048x339x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x339x2048): 70.632
Elapsed time for attention_prob_times_values (256x2048x2048x339): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x339): 64.102
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7063797760, 42481549312)
Elapsed time for attention_linear_projection (4x21696x21696, b=2048): 0.0299
Throughput (in TFLOP/s) for attention_linear_projection (4x21696x21696, b=2048): 257.506
(7063797760, 42481549312)
Elapsed time for mlp_h_to_4h (4x21696x86784, b=2048): 0.1221
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21696x86784, b=2048): 252.563
Elapsed time for mlp_fused_gelu (2048x4x86784): 0.0025
Elapsed time for mlp_4h_to_h (4x86784x21696, b=2048): 0.1955
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86784x21696, b=2048): 157.797
Elapsed time for transformer_add_bias_dropout (2048x4x21696): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21696): 0.0009

Attention duration (in seconds): 0.1784
Attention throughput (in TFLOP/s): 181.092
MLP duration (in seconds): 0.3201
MLP throughput (in TFLOP/s): 192.743
Transformer duration (in seconds): 0.5035
Transformer throughput (in TFLOP/s): 186.711
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1518
Attention throughput (in TFLOP/s): 212.795
MLP duration (in seconds): 0.3175
MLP throughput (in TFLOP/s): 194.339
Transformer duration (in seconds): 0.4811
Transformer throughput (in TFLOP/s): 195.380
Transformer - MLP - Attention (in seconds): 0.0118
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.0920
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 253.002
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 106.799
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 108.798
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7013466112, 42481549312)
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0300
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 258.326
(7013466112, 42481549312)
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1236
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 250.991
Elapsed time for mlp_fused_gelu (2048x4x87040): 0.0025
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1934
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 160.411
Elapsed time for transformer_add_bias_dropout (2048x4x21760): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21760): 0.0009

Attention duration (in seconds): 0.1710
Attention throughput (in TFLOP/s): 190.024
MLP duration (in seconds): 0.3195
MLP throughput (in TFLOP/s): 194.219
Transformer duration (in seconds): 0.4955
Transformer throughput (in TFLOP/s): 190.830
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1423
Attention throughput (in TFLOP/s): 228.406
MLP duration (in seconds): 0.3134
MLP throughput (in TFLOP/s): 198.048
Transformer duration (in seconds): 0.4650
Transformer throughput (in TFLOP/s): 203.333
Transformer - MLP - Attention (in seconds): 0.0094
========================================================================================================================
num_attention_heads: 64, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21824x65472, b=2048): 0.0931
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21824x65472, b=2048): 251.457
Elapsed time for attention_key_query_prob (256x2048x341x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x341x2048): 71.252
Elapsed time for attention_prob_times_values (256x2048x2048x341): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x341): 64.244
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7959281664, 42481549312)
Elapsed time for attention_linear_projection (4x21824x21824, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_linear_projection (4x21824x21824, b=2048): 258.769
(7959281664, 42481549312)
Elapsed time for mlp_h_to_4h (4x21824x87296, b=2048): 0.1248
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21824x87296, b=2048): 250.083
Elapsed time for mlp_fused_gelu (2048x4x87296): 0.0025
Elapsed time for mlp_4h_to_h (4x87296x21824, b=2048): 0.2051
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87296x21824, b=2048): 152.195
Elapsed time for transformer_add_bias_dropout (2048x4x21824): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21824): 0.0009

Attention duration (in seconds): 0.1804
Attention throughput (in TFLOP/s): 181.191
MLP duration (in seconds): 0.3324
MLP throughput (in TFLOP/s): 187.819
Transformer duration (in seconds): 0.5177
Transformer throughput (in TFLOP/s): 183.695
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1545
Attention throughput (in TFLOP/s): 211.529
MLP duration (in seconds): 0.3304
MLP throughput (in TFLOP/s): 188.961
Transformer duration (in seconds): 0.4940
Transformer throughput (in TFLOP/s): 192.538
Transformer - MLP - Attention (in seconds): 0.0091
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.0924
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 254.870
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 108.353
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 109.621
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(338231296, 42481549312)
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 259.743
(338231296, 42481549312)
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.1247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 251.693
Elapsed time for mlp_fused_gelu (2048x4x87552): 0.0025
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.2021
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 155.342
Elapsed time for transformer_add_bias_dropout (2048x4x21888): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21888): 0.0009

Attention duration (in seconds): 0.1715
Attention throughput (in TFLOP/s): 191.630
MLP duration (in seconds): 0.3293
MLP throughput (in TFLOP/s): 190.665
Transformer duration (in seconds): 0.5058
Transformer throughput (in TFLOP/s): 189.110
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1440
Attention throughput (in TFLOP/s): 228.244
MLP duration (in seconds): 0.3263
MLP throughput (in TFLOP/s): 192.428
Transformer duration (in seconds): 0.4785
Transformer throughput (in TFLOP/s): 199.927
Transformer - MLP - Attention (in seconds): 0.0082
========================================================================================================================
num_attention_heads: 64, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21952x65856, b=2048): 0.0940
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21952x65856, b=2048): 251.893
Elapsed time for attention_key_query_prob (256x2048x343x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x343x2048): 71.575
Elapsed time for attention_prob_times_values (256x2048x2048x343): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x343): 63.406
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7864909824, 42481549312)
Elapsed time for attention_linear_projection (4x21952x21952, b=2048): 0.0304
Throughput (in TFLOP/s) for attention_linear_projection (4x21952x21952, b=2048): 259.880
(7864909824, 42481549312)
Elapsed time for mlp_h_to_4h (4x21952x87808, b=2048): 0.1255
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21952x87808, b=2048): 251.675
Elapsed time for mlp_fused_gelu (2048x4x87808): 0.0025
Elapsed time for mlp_4h_to_h (4x87808x21952, b=2048): 0.1987
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87808x21952, b=2048): 158.962
Elapsed time for transformer_add_bias_dropout (2048x4x21952): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21952): 0.0009

Attention duration (in seconds): 0.1817
Attention throughput (in TFLOP/s): 181.875
MLP duration (in seconds): 0.3266
MLP throughput (in TFLOP/s): 193.367
Transformer duration (in seconds): 0.5134
Transformer throughput (in TFLOP/s): 187.406
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1538
Attention throughput (in TFLOP/s): 214.969
MLP duration (in seconds): 0.3228
MLP throughput (in TFLOP/s): 195.687
Transformer duration (in seconds): 0.4865
Transformer throughput (in TFLOP/s): 197.762
Transformer - MLP - Attention (in seconds): 0.0100
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.0939
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 253.721
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 179.601
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 167.939
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(151584768, 42481549312)
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 260.627
(151584768, 42481549312)
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.1262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 251.656
Elapsed time for mlp_fused_gelu (2048x4x88064): 0.0025
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.2055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 154.577
Elapsed time for transformer_add_bias_dropout (2048x4x22016): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22016): 0.0009

Attention duration (in seconds): 0.1683
Attention throughput (in TFLOP/s): 197.521
MLP duration (in seconds): 0.3342
MLP throughput (in TFLOP/s): 190.087
Transformer duration (in seconds): 0.5075
Transformer throughput (in TFLOP/s): 190.678
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1441
Attention throughput (in TFLOP/s): 230.745
MLP duration (in seconds): 0.3328
MLP throughput (in TFLOP/s): 190.884
Transformer duration (in seconds): 0.4838
Transformer throughput (in TFLOP/s): 200.030
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22080x66240, b=2048): 0.0946
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22080x66240, b=2048): 253.271
Elapsed time for attention_key_query_prob (256x2048x345x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x345x2048): 71.588
Elapsed time for attention_prob_times_values (256x2048x2048x345): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x345): 64.211
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7770537984, 42481549312)
Elapsed time for attention_linear_projection (4x22080x22080, b=2048): 0.0310
Throughput (in TFLOP/s) for attention_linear_projection (4x22080x22080, b=2048): 257.294
(7770537984, 42481549312)
Elapsed time for mlp_h_to_4h (4x22080x88320, b=2048): 0.1271
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22080x88320, b=2048): 251.361
Elapsed time for mlp_fused_gelu (2048x4x88320): 0.0025
Elapsed time for mlp_4h_to_h (4x88320x22080, b=2048): 0.2041
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88320x22080, b=2048): 156.550
Elapsed time for transformer_add_bias_dropout (2048x4x22080): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22080): 0.0009

Attention duration (in seconds): 0.1830
Attention throughput (in TFLOP/s): 182.733
MLP duration (in seconds): 0.3337
MLP throughput (in TFLOP/s): 191.488
Transformer duration (in seconds): 0.5217
Transformer throughput (in TFLOP/s): 186.564
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1569
Attention throughput (in TFLOP/s): 213.124
MLP duration (in seconds): 0.3319
MLP throughput (in TFLOP/s): 192.527
Transformer duration (in seconds): 0.4926
Transformer throughput (in TFLOP/s): 197.594
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.0954
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 252.556
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 108.539
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 110.451
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7720206336, 42481549312)
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 257.954
(7720206336, 42481549312)
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.1276
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 251.869
Elapsed time for mlp_fused_gelu (2048x4x88576): 0.0025
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.2087
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 153.979
Elapsed time for transformer_add_bias_dropout (2048x4x22144): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22144): 0.0009

Attention duration (in seconds): 0.1756
Attention throughput (in TFLOP/s): 191.504
MLP duration (in seconds): 0.3388
MLP throughput (in TFLOP/s): 189.700
Transformer duration (in seconds): 0.5194
Transformer throughput (in TFLOP/s): 188.464
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1472
Attention throughput (in TFLOP/s): 228.407
MLP duration (in seconds): 0.3371
MLP throughput (in TFLOP/s): 190.659
Transformer duration (in seconds): 0.4916
Transformer throughput (in TFLOP/s): 199.116
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22208x66624, b=2048): 0.0959
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22208x66624, b=2048): 252.718
Elapsed time for attention_key_query_prob (256x2048x347x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x347x2048): 71.306
Elapsed time for attention_prob_times_values (256x2048x2048x347): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x347): 65.547
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7671971840, 42481549312)
Elapsed time for attention_linear_projection (4x22208x22208, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_linear_projection (4x22208x22208, b=2048): 258.383
(7671971840, 42481549312)
Elapsed time for mlp_h_to_4h (4x22208x88832, b=2048): 0.1287
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22208x88832, b=2048): 251.136
Elapsed time for mlp_fused_gelu (2048x4x88832): 0.0025
Elapsed time for mlp_4h_to_h (4x88832x22208, b=2048): 0.2050
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88832x22208, b=2048): 157.700
Elapsed time for transformer_add_bias_dropout (2048x4x22208): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22208): 0.0009

Attention duration (in seconds): 0.1844
Attention throughput (in TFLOP/s): 183.329
MLP duration (in seconds): 0.3362
MLP throughput (in TFLOP/s): 192.288
Transformer duration (in seconds): 0.5257
Transformer throughput (in TFLOP/s): 187.286
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1575
Attention throughput (in TFLOP/s): 214.672
MLP duration (in seconds): 0.3317
MLP throughput (in TFLOP/s): 194.896
Transformer duration (in seconds): 0.4983
Transformer throughput (in TFLOP/s): 197.591
Transformer - MLP - Attention (in seconds): 0.0091
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.0966
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 252.326
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 107.767
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 110.858
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7621640192, 42481549312)
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 259.238
(7621640192, 42481549312)
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.1296
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 250.743
Elapsed time for mlp_fused_gelu (2048x4x89088): 0.0025
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.2049
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 158.627
Elapsed time for transformer_add_bias_dropout (2048x4x22272): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22272): 0.0009

Attention duration (in seconds): 0.1771
Attention throughput (in TFLOP/s): 192.037
MLP duration (in seconds): 0.3371
MLP throughput (in TFLOP/s): 192.866
Transformer duration (in seconds): 0.5192
Transformer throughput (in TFLOP/s): 190.700
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1482
Attention throughput (in TFLOP/s): 229.478
MLP duration (in seconds): 0.3335
MLP throughput (in TFLOP/s): 194.932
Transformer duration (in seconds): 0.4907
Transformer throughput (in TFLOP/s): 201.801
Transformer - MLP - Attention (in seconds): 0.0090
========================================================================================================================
num_attention_heads: 64, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22336x67008, b=2048): 0.0967
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22336x67008, b=2048): 253.467
Elapsed time for attention_key_query_prob (256x2048x349x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x349x2048): 71.634
Elapsed time for attention_prob_times_values (256x2048x2048x349): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x349): 65.801
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7573405696, 42481549312)
Elapsed time for attention_linear_projection (4x22336x22336, b=2048): 0.0315
Throughput (in TFLOP/s) for attention_linear_projection (4x22336x22336, b=2048): 259.640
(7573405696, 42481549312)
Elapsed time for mlp_h_to_4h (4x22336x89344, b=2048): 0.1304
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22336x89344, b=2048): 250.725
Elapsed time for mlp_fused_gelu (2048x4x89344): 0.0025
Elapsed time for mlp_4h_to_h (4x89344x22336, b=2048): 0.1359
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89344x22336, b=2048): 240.546
Elapsed time for transformer_add_bias_dropout (2048x4x22336): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22336): 0.0009

Attention duration (in seconds): 0.1855
Attention throughput (in TFLOP/s): 184.342
MLP duration (in seconds): 0.2689
MLP throughput (in TFLOP/s): 243.215
Transformer duration (in seconds): 0.4595
Transformer throughput (in TFLOP/s): 216.743
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1592
Attention throughput (in TFLOP/s): 214.841
MLP duration (in seconds): 0.2723
MLP throughput (in TFLOP/s): 240.114
Transformer duration (in seconds): 0.4383
Transformer throughput (in TFLOP/s): 227.197
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.0975
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 252.900
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 109.413
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 111.911
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7523074048, 42481549312)
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0316
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 260.381
(7523074048, 42481549312)
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.1314
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 250.223
Elapsed time for mlp_fused_gelu (2048x4x89600): 0.0025
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1365
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 240.860
Elapsed time for transformer_add_bias_dropout (2048x4x22400): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22400): 0.0009

Attention duration (in seconds): 0.1781
Attention throughput (in TFLOP/s): 193.077
MLP duration (in seconds): 0.2705
MLP throughput (in TFLOP/s): 243.146
Transformer duration (in seconds): 0.4537
Transformer throughput (in TFLOP/s): 220.752
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1498
Attention throughput (in TFLOP/s): 229.504
MLP duration (in seconds): 0.2744
MLP throughput (in TFLOP/s): 239.650
Transformer duration (in seconds): 0.4338
Transformer throughput (in TFLOP/s): 230.893
Transformer - MLP - Attention (in seconds): 0.0095
========================================================================================================================
num_attention_heads: 64, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22464x67392, b=2048): 0.0980
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22464x67392, b=2048): 253.174
Elapsed time for attention_key_query_prob (256x2048x351x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x351x2048): 72.348
Elapsed time for attention_prob_times_values (256x2048x2048x351): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x351): 65.655
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7376273408, 42481549312)
Elapsed time for attention_linear_projection (4x22464x22464, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_linear_projection (4x22464x22464, b=2048): 256.711
(7376273408, 42481549312)
Elapsed time for mlp_h_to_4h (4x22464x89856, b=2048): 0.1315
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22464x89856, b=2048): 251.579
Elapsed time for mlp_fused_gelu (2048x4x89856): 0.0025
Elapsed time for mlp_4h_to_h (4x89856x22464, b=2048): 0.1376
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89856x22464, b=2048): 240.266
Elapsed time for transformer_add_bias_dropout (2048x4x22464): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22464): 0.0009

Attention duration (in seconds): 0.1875
Attention throughput (in TFLOP/s): 184.431
MLP duration (in seconds): 0.2716
MLP throughput (in TFLOP/s): 243.489
Transformer duration (in seconds): 0.4643
Transformer throughput (in TFLOP/s): 216.942
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1603
Attention throughput (in TFLOP/s): 215.729
MLP duration (in seconds): 0.2744
MLP throughput (in TFLOP/s): 241.052
Transformer duration (in seconds): 0.4410
Transformer throughput (in TFLOP/s): 228.409
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.0989
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 252.165
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 196.683
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 173.036
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7325941760, 42481549312)
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 256.648
(7325941760, 42481549312)
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.1327
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 250.578
Elapsed time for mlp_fused_gelu (2048x4x90112): 0.0026
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1381
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 240.918
Elapsed time for transformer_add_bias_dropout (2048x4x22528): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22528): 0.0009

Attention duration (in seconds): 0.1750
Attention throughput (in TFLOP/s): 198.750
MLP duration (in seconds): 0.2733
MLP throughput (in TFLOP/s): 243.358
Transformer duration (in seconds): 0.4534
Transformer throughput (in TFLOP/s): 223.388
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1492
Attention throughput (in TFLOP/s): 233.098
MLP duration (in seconds): 0.2778
MLP throughput (in TFLOP/s): 239.457
Transformer duration (in seconds): 0.4367
Transformer throughput (in TFLOP/s): 231.959
Transformer - MLP - Attention (in seconds): 0.0097
========================================================================================================================
num_attention_heads: 64, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22592x67776, b=2048): 0.0997
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22592x67776, b=2048): 251.699
Elapsed time for attention_key_query_prob (256x2048x353x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x353x2048): 70.325
Elapsed time for attention_prob_times_values (256x2048x2048x353): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x353): 66.037
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6451429376, 42481549312)
Elapsed time for attention_linear_projection (4x22592x22592, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_linear_projection (4x22592x22592, b=2048): 258.196
(6451429376, 42481549312)
Elapsed time for mlp_h_to_4h (4x22592x90368, b=2048): 0.1333
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22592x90368, b=2048): 251.005
Elapsed time for mlp_fused_gelu (2048x4x90368): 0.0026
Elapsed time for mlp_4h_to_h (4x90368x22592, b=2048): 0.1389
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90368x22592, b=2048): 240.891
Elapsed time for transformer_add_bias_dropout (2048x4x22592): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22592): 0.0009

Attention duration (in seconds): 0.1897
Attention throughput (in TFLOP/s): 184.293
MLP duration (in seconds): 0.2747
MLP throughput (in TFLOP/s): 243.552
Transformer duration (in seconds): 0.4696
Transformer throughput (in TFLOP/s): 216.921
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1624
Attention throughput (in TFLOP/s): 215.343
MLP duration (in seconds): 0.2790
MLP throughput (in TFLOP/s): 239.792
Transformer duration (in seconds): 0.4472
Transformer throughput (in TFLOP/s): 227.767
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.0997
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 253.156
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 105.077
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 113.081
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6392709120, 42481549312)
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 257.317
(6392709120, 42481549312)
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.1342
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 250.627
Elapsed time for mlp_fused_gelu (2048x4x90624): 0.0026
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1396
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 241.005
Elapsed time for transformer_add_bias_dropout (2048x4x22656): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22656): 0.0009

Attention duration (in seconds): 0.1817
Attention throughput (in TFLOP/s): 193.483
MLP duration (in seconds): 0.2764
MLP throughput (in TFLOP/s): 243.438
Transformer duration (in seconds): 0.4632
Transformer throughput (in TFLOP/s): 221.131
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1534
Attention throughput (in TFLOP/s): 229.160
MLP duration (in seconds): 0.2800
MLP throughput (in TFLOP/s): 240.281
Transformer duration (in seconds): 0.4419
Transformer throughput (in TFLOP/s): 231.829
Transformer - MLP - Attention (in seconds): 0.0084
========================================================================================================================
num_attention_heads: 64, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22720x68160, b=2048): 0.1003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22720x68160, b=2048): 252.873
Elapsed time for attention_key_query_prob (256x2048x355x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x355x2048): 69.189
Elapsed time for attention_prob_times_values (256x2048x2048x355): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x355): 66.742
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6344474624, 42481549312)
Elapsed time for attention_linear_projection (4x22720x22720, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_linear_projection (4x22720x22720, b=2048): 259.055
(6344474624, 42481549312)
Elapsed time for mlp_h_to_4h (4x22720x90880, b=2048): 0.1346
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22720x90880, b=2048): 251.261
Elapsed time for mlp_fused_gelu (2048x4x90880): 0.0026
Elapsed time for mlp_4h_to_h (4x90880x22720, b=2048): 0.1406
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90880x22720, b=2048): 240.612
Elapsed time for transformer_add_bias_dropout (2048x4x22720): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22720): 0.0009

Attention duration (in seconds): 0.1908
Attention throughput (in TFLOP/s): 185.253
MLP duration (in seconds): 0.2778
MLP throughput (in TFLOP/s): 243.541
Transformer duration (in seconds): 0.4739
Transformer throughput (in TFLOP/s): 217.389
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1637
Attention throughput (in TFLOP/s): 216.021
MLP duration (in seconds): 0.2810
MLP throughput (in TFLOP/s): 240.820
Transformer duration (in seconds): 0.4515
Transformer throughput (in TFLOP/s): 228.179
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.1012
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 252.056
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 105.240
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 113.098
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6289948672, 42481549312)
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 258.405
(6289948672, 42481549312)
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1358
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 250.584
Elapsed time for mlp_fused_gelu (2048x4x91136): 0.0026
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1417
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 240.034
Elapsed time for transformer_add_bias_dropout (2048x4x22784): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22784): 0.0009

Attention duration (in seconds): 0.1836
Attention throughput (in TFLOP/s): 193.637
MLP duration (in seconds): 0.2801
MLP throughput (in TFLOP/s): 242.934
Transformer duration (in seconds): 0.4688
Transformer throughput (in TFLOP/s): 220.951
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1551
Attention throughput (in TFLOP/s): 229.255
MLP duration (in seconds): 0.2844
MLP throughput (in TFLOP/s): 239.261
Transformer duration (in seconds): 0.4495
Transformer throughput (in TFLOP/s): 230.436
Transformer - MLP - Attention (in seconds): 0.0101
========================================================================================================================
num_attention_heads: 64, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22848x68544, b=2048): 0.1007
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22848x68544, b=2048): 254.899
Elapsed time for attention_key_query_prob (256x2048x357x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x357x2048): 69.104
Elapsed time for attention_prob_times_values (256x2048x2048x357): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x357): 66.580
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6241714176, 42481549312)
Elapsed time for attention_linear_projection (4x22848x22848, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_linear_projection (4x22848x22848, b=2048): 256.605
(6241714176, 42481549312)
Elapsed time for mlp_h_to_4h (4x22848x91392, b=2048): 0.1358
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22848x91392, b=2048): 252.015
Elapsed time for mlp_fused_gelu (2048x4x91392): 0.0026
Elapsed time for mlp_4h_to_h (4x91392x22848, b=2048): 0.1427
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91392x22848, b=2048): 239.807
Elapsed time for transformer_add_bias_dropout (2048x4x22848): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22848): 0.0009

Attention duration (in seconds): 0.1920
Attention throughput (in TFLOP/s): 186.167
MLP duration (in seconds): 0.2810
MLP throughput (in TFLOP/s): 243.490
Transformer duration (in seconds): 0.4782
Transformer throughput (in TFLOP/s): 217.817
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1665
Attention throughput (in TFLOP/s): 214.707
MLP duration (in seconds): 0.2864
MLP throughput (in TFLOP/s): 238.876
Transformer duration (in seconds): 0.4624
Transformer throughput (in TFLOP/s): 225.290
Transformer - MLP - Attention (in seconds): 0.0095
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.1024
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 252.060
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 105.710
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 113.943
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6187188224, 42481549312)
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 257.322
(6187188224, 42481549312)
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.1373
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 250.491
Elapsed time for mlp_fused_gelu (2048x4x91648): 0.0026
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.1425
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 241.353
Elapsed time for transformer_add_bias_dropout (2048x4x22912): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22912): 0.0009

Attention duration (in seconds): 0.1852
Attention throughput (in TFLOP/s): 194.039
MLP duration (in seconds): 0.2825
MLP throughput (in TFLOP/s): 243.577
Transformer duration (in seconds): 0.4729
Transformer throughput (in TFLOP/s): 221.494
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1571
Attention throughput (in TFLOP/s): 228.844
MLP duration (in seconds): 0.2869
MLP throughput (in TFLOP/s): 239.807
Transformer duration (in seconds): 0.4543
Transformer throughput (in TFLOP/s): 230.595
Transformer - MLP - Attention (in seconds): 0.0103
========================================================================================================================
num_attention_heads: 64, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22976x68928, b=2048): 0.1022
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22976x68928, b=2048): 253.900
Elapsed time for attention_key_query_prob (256x2048x359x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x359x2048): 68.788
Elapsed time for attention_prob_times_values (256x2048x2048x359): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x359): 65.561
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6138953728, 42481549312)
Elapsed time for attention_linear_projection (4x22976x22976, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_linear_projection (4x22976x22976, b=2048): 242.855
(6138953728, 42481549312)
Elapsed time for mlp_h_to_4h (4x22976x91904, b=2048): 0.1387
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22976x91904, b=2048): 249.419
Elapsed time for mlp_fused_gelu (2048x4x91904): 0.0026
Elapsed time for mlp_4h_to_h (4x91904x22976, b=2048): 0.1435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91904x22976, b=2048): 241.067
Elapsed time for transformer_add_bias_dropout (2048x4x22976): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22976): 0.0010

Attention duration (in seconds): 0.1962
Attention throughput (in TFLOP/s): 184.197
MLP duration (in seconds): 0.2848
MLP throughput (in TFLOP/s): 242.929
Transformer duration (in seconds): 0.4863
Transformer throughput (in TFLOP/s): 216.598
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1701
Attention throughput (in TFLOP/s): 212.507
MLP duration (in seconds): 0.2879
MLP throughput (in TFLOP/s): 240.319
Transformer duration (in seconds): 0.4632
Transformer throughput (in TFLOP/s): 227.394
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.1036
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 251.891
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 180.857
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 173.577
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6084427776, 42481549312)
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 244.214
(6084427776, 42481549312)
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.1389
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 250.391
Elapsed time for mlp_fused_gelu (2048x4x92160): 0.0026
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1445
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 240.759
Elapsed time for transformer_add_bias_dropout (2048x4x23040): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23040): 0.0009

Attention duration (in seconds): 0.1833
Attention throughput (in TFLOP/s): 198.192
MLP duration (in seconds): 0.2861
MLP throughput (in TFLOP/s): 243.238
Transformer duration (in seconds): 0.4746
Transformer throughput (in TFLOP/s): 223.148
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1590
Attention throughput (in TFLOP/s): 228.458
MLP duration (in seconds): 0.2905
MLP throughput (in TFLOP/s): 239.498
Transformer duration (in seconds): 0.4570
Transformer throughput (in TFLOP/s): 231.755
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23104x69312, b=2048): 0.1040
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23104x69312, b=2048): 252.371
Elapsed time for attention_key_query_prob (256x2048x361x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x361x2048): 68.731
Elapsed time for attention_prob_times_values (256x2048x2048x361): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x361): 66.227
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6036193280, 42481549312)
Elapsed time for attention_linear_projection (4x23104x23104, b=2048): 0.0338
Throughput (in TFLOP/s) for attention_linear_projection (4x23104x23104, b=2048): 259.045
(6036193280, 42481549312)
Elapsed time for mlp_h_to_4h (4x23104x92416, b=2048): 0.1397
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23104x92416, b=2048): 250.430
Elapsed time for mlp_fused_gelu (2048x4x92416): 0.0026
Elapsed time for mlp_4h_to_h (4x92416x23104, b=2048): 0.1459
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92416x23104, b=2048): 239.788
Elapsed time for transformer_add_bias_dropout (2048x4x23104): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23104): 0.0010

Attention duration (in seconds): 0.1961
Attention throughput (in TFLOP/s): 186.274
MLP duration (in seconds): 0.2882
MLP throughput (in TFLOP/s): 242.766
Transformer duration (in seconds): 0.4897
Transformer throughput (in TFLOP/s): 217.498
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1696
Attention throughput (in TFLOP/s): 215.394
MLP duration (in seconds): 0.2905
MLP throughput (in TFLOP/s): 240.831
Transformer duration (in seconds): 0.4667
Transformer throughput (in TFLOP/s): 228.189
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.1045
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 252.501
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 106.933
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 115.098
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5977473024, 42481549312)
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0360
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 244.386
(5977473024, 42481549312)
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.1402
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 250.921
Elapsed time for mlp_fused_gelu (2048x4x92672): 0.0026
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1455
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 241.687
Elapsed time for transformer_add_bias_dropout (2048x4x23168): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23168): 0.0009

Attention duration (in seconds): 0.1899
Attention throughput (in TFLOP/s): 193.416
MLP duration (in seconds): 0.2884
MLP throughput (in TFLOP/s): 243.973
Transformer duration (in seconds): 0.4835
Transformer throughput (in TFLOP/s): 221.458
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1622
Attention throughput (in TFLOP/s): 226.513
MLP duration (in seconds): 0.2933
MLP throughput (in TFLOP/s): 239.831
Transformer duration (in seconds): 0.4628
Transformer throughput (in TFLOP/s): 231.376
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23232x69696, b=2048): 0.1051
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23232x69696, b=2048): 252.332
Elapsed time for attention_key_query_prob (256x2048x363x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x363x2048): 68.675
Elapsed time for attention_prob_times_values (256x2048x2048x363): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x363): 67.853
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5929238528, 42481549312)
Elapsed time for attention_linear_projection (4x23232x23232, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x23232x23232, b=2048): 257.096
(5929238528, 42481549312)
Elapsed time for mlp_h_to_4h (4x23232x92928, b=2048): 0.1413
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23232x92928, b=2048): 250.410
Elapsed time for mlp_fused_gelu (2048x4x92928): 0.0026
Elapsed time for mlp_4h_to_h (4x92928x23232, b=2048): 0.1478
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92928x23232, b=2048): 239.307
Elapsed time for transformer_add_bias_dropout (2048x4x23232): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23232): 0.0010

Attention duration (in seconds): 0.1977
Attention throughput (in TFLOP/s): 186.794
MLP duration (in seconds): 0.2917
MLP throughput (in TFLOP/s): 242.518
Transformer duration (in seconds): 0.4948
Transformer throughput (in TFLOP/s): 217.631
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1712
Attention throughput (in TFLOP/s): 215.660
MLP duration (in seconds): 0.2960
MLP throughput (in TFLOP/s): 239.025
Transformer duration (in seconds): 0.4783
Transformer throughput (in TFLOP/s): 225.100
Transformer - MLP - Attention (in seconds): 0.0111
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1060
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 251.573
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 106.924
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 115.489
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5998444544, 42481549312)
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0343
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 259.080
(5998444544, 42481549312)
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.1422
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 250.171
Elapsed time for mlp_fused_gelu (2048x4x93184): 0.0026
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.1476
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 240.906
Elapsed time for transformer_add_bias_dropout (2048x4x23296): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23296): 0.0010

Attention duration (in seconds): 0.1898
Attention throughput (in TFLOP/s): 195.582
MLP duration (in seconds): 0.2925
MLP throughput (in TFLOP/s): 243.231
Transformer duration (in seconds): 0.4876
Transformer throughput (in TFLOP/s): 222.022
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1615
Attention throughput (in TFLOP/s): 229.886
MLP duration (in seconds): 0.2971
MLP throughput (in TFLOP/s): 239.434
Transformer duration (in seconds): 0.4693
Transformer throughput (in TFLOP/s): 230.708
Transformer - MLP - Attention (in seconds): 0.0107
========================================================================================================================
num_attention_heads: 64, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23360x70080, b=2048): 0.1068
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23360x70080, b=2048): 251.223
Elapsed time for attention_key_query_prob (256x2048x365x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x365x2048): 69.106
Elapsed time for attention_prob_times_values (256x2048x2048x365): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x365): 67.783
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5872615424, 42481549312)
Elapsed time for attention_linear_projection (4x23360x23360, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_linear_projection (4x23360x23360, b=2048): 244.252
(5872615424, 42481549312)
Elapsed time for mlp_h_to_4h (4x23360x93440, b=2048): 0.1432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23360x93440, b=2048): 249.680
Elapsed time for mlp_fused_gelu (2048x4x93440): 0.0026
Elapsed time for mlp_4h_to_h (4x93440x23360, b=2048): 0.1488
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93440x23360, b=2048): 240.361
Elapsed time for transformer_add_bias_dropout (2048x4x23360): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23360): 0.0010

Attention duration (in seconds): 0.2017
Attention throughput (in TFLOP/s): 185.100
MLP duration (in seconds): 0.2947
MLP throughput (in TFLOP/s): 242.730
Transformer duration (in seconds): 0.5017
Transformer throughput (in TFLOP/s): 216.975
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1758
Attention throughput (in TFLOP/s): 212.326
MLP duration (in seconds): 0.3000
MLP throughput (in TFLOP/s): 238.412
Transformer duration (in seconds): 0.4846
Transformer throughput (in TFLOP/s): 224.614
Transformer - MLP - Attention (in seconds): 0.0088
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1068
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 252.546
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 107.079
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 116.547
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5744689152, 42481549312)
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0367
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 244.722
(5744689152, 42481549312)
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.1427
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 251.939
Elapsed time for mlp_fused_gelu (2048x4x93696): 0.0027
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1495
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 240.584
Elapsed time for transformer_add_bias_dropout (2048x4x23424): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23424): 0.0010

Attention duration (in seconds): 0.1930
Attention throughput (in TFLOP/s): 194.454
MLP duration (in seconds): 0.2949
MLP throughput (in TFLOP/s): 243.911
Transformer duration (in seconds): 0.4932
Transformer throughput (in TFLOP/s): 221.914
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1659
Attention throughput (in TFLOP/s): 226.287
MLP duration (in seconds): 0.2987
MLP throughput (in TFLOP/s): 240.756
Transformer duration (in seconds): 0.4717
Transformer throughput (in TFLOP/s): 232.015
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23488x70464, b=2048): 0.1075
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23488x70464, b=2048): 252.361
Elapsed time for attention_key_query_prob (256x2048x367x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x367x2048): 69.408
Elapsed time for attention_prob_times_values (256x2048x2048x367): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x367): 67.508
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5618860032, 42481549312)
Elapsed time for attention_linear_projection (4x23488x23488, b=2048): 0.0352
Throughput (in TFLOP/s) for attention_linear_projection (4x23488x23488, b=2048): 256.857
(5618860032, 42481549312)
Elapsed time for mlp_h_to_4h (4x23488x93952, b=2048): 0.1442
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23488x93952, b=2048): 250.651
Elapsed time for mlp_fused_gelu (2048x4x93952): 0.0027
Elapsed time for mlp_4h_to_h (4x93952x23488, b=2048): 0.1508
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93952x23488, b=2048): 239.826
Elapsed time for transformer_add_bias_dropout (2048x4x23488): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23488): 0.0010

Attention duration (in seconds): 0.2011
Attention throughput (in TFLOP/s): 187.656
MLP duration (in seconds): 0.2977
MLP throughput (in TFLOP/s): 242.924
Transformer duration (in seconds): 0.5041
Transformer throughput (in TFLOP/s): 218.282
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1748
Attention throughput (in TFLOP/s): 215.894
MLP duration (in seconds): 0.3035
MLP throughput (in TFLOP/s): 238.225
Transformer duration (in seconds): 0.4888
Transformer throughput (in TFLOP/s): 225.111
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1084
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 251.417
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 187.427
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 178.533
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5438504960, 42481549312)
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 256.298
(5438504960, 42481549312)
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.1450
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 250.703
Elapsed time for mlp_fused_gelu (2048x4x94208): 0.0027
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.1513
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 240.254
Elapsed time for transformer_add_bias_dropout (2048x4x23552): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23552): 0.0010

Attention duration (in seconds): 0.1880
Attention throughput (in TFLOP/s): 201.812
MLP duration (in seconds): 0.2990
MLP throughput (in TFLOP/s): 243.173
Transformer duration (in seconds): 0.4923
Transformer throughput (in TFLOP/s): 224.728
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1629
Attention throughput (in TFLOP/s): 232.904
MLP duration (in seconds): 0.3032
MLP throughput (in TFLOP/s): 239.772
Transformer duration (in seconds): 0.4750
Transformer throughput (in TFLOP/s): 232.910
Transformer - MLP - Attention (in seconds): 0.0089
========================================================================================================================
num_attention_heads: 64, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23616x70848, b=2048): 0.1092
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23616x70848, b=2048): 251.073
Elapsed time for attention_key_query_prob (256x2048x369x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x369x2048): 69.817
Elapsed time for attention_prob_times_values (256x2048x2048x369): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x369): 68.415
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5323161600, 42481549312)
Elapsed time for attention_linear_projection (4x23616x23616, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_linear_projection (4x23616x23616, b=2048): 256.812
(5323161600, 42481549312)
Elapsed time for mlp_h_to_4h (4x23616x94464, b=2048): 0.1466
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23616x94464, b=2048): 249.273
Elapsed time for mlp_fused_gelu (2048x4x94464): 0.0027
Elapsed time for mlp_4h_to_h (4x94464x23616, b=2048): 0.1527
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94464x23616, b=2048): 239.302
Elapsed time for transformer_add_bias_dropout (2048x4x23616): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23616): 0.0010

Attention duration (in seconds): 0.2031
Attention throughput (in TFLOP/s): 187.759
MLP duration (in seconds): 0.3020
MLP throughput (in TFLOP/s): 242.018
Transformer duration (in seconds): 0.5106
Transformer throughput (in TFLOP/s): 217.870
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1766
Attention throughput (in TFLOP/s): 215.963
MLP duration (in seconds): 0.3068
MLP throughput (in TFLOP/s): 238.297
Transformer duration (in seconds): 0.4942
Transformer throughput (in TFLOP/s): 225.074
Transformer - MLP - Attention (in seconds): 0.0109
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1099
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 250.864
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 106.420
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 118.714
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5186846720, 42481549312)
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 257.236
(5186846720, 42481549312)
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.1463
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 251.202
Elapsed time for mlp_fused_gelu (2048x4x94720): 0.0027
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.1531
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 240.076
Elapsed time for transformer_add_bias_dropout (2048x4x23680): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23680): 0.0010

Attention duration (in seconds): 0.1951
Attention throughput (in TFLOP/s): 196.455
MLP duration (in seconds): 0.3021
MLP throughput (in TFLOP/s): 243.328
Transformer duration (in seconds): 0.5026
Transformer throughput (in TFLOP/s): 222.525
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1655
Attention throughput (in TFLOP/s): 231.705
MLP duration (in seconds): 0.3073
MLP throughput (in TFLOP/s): 239.152
Transformer duration (in seconds): 0.4822
Transformer throughput (in TFLOP/s): 231.914
Transformer - MLP - Attention (in seconds): 0.0094
========================================================================================================================
num_attention_heads: 64, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23744x71232, b=2048): 0.1105
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23744x71232, b=2048): 250.730
Elapsed time for attention_key_query_prob (256x2048x371x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x371x2048): 69.718
Elapsed time for attention_prob_times_values (256x2048x2048x371): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x371): 69.717
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5042143232, 42481549312)
Elapsed time for attention_linear_projection (4x23744x23744, b=2048): 0.0379
Throughput (in TFLOP/s) for attention_linear_projection (4x23744x23744, b=2048): 243.402
(5042143232, 42481549312)
Elapsed time for mlp_h_to_4h (4x23744x94976, b=2048): 0.1481
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23744x94976, b=2048): 249.534
Elapsed time for mlp_fused_gelu (2048x4x94976): 0.0027
Elapsed time for mlp_4h_to_h (4x94976x23744, b=2048): 0.1546
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94976x23744, b=2048): 238.959
Elapsed time for transformer_add_bias_dropout (2048x4x23744): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23744): 0.0010

Attention duration (in seconds): 0.2067
Attention throughput (in TFLOP/s): 186.438
MLP duration (in seconds): 0.3054
MLP throughput (in TFLOP/s): 241.979
Transformer duration (in seconds): 0.5175
Transformer throughput (in TFLOP/s): 217.255
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1814
Attention throughput (in TFLOP/s): 212.411
MLP duration (in seconds): 0.3088
MLP throughput (in TFLOP/s): 239.299
Transformer duration (in seconds): 0.4945
Transformer throughput (in TFLOP/s): 227.374
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1100
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 253.299
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 107.304
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 119.407
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4543021056, 42481549312)
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 244.556
(4543021056, 42481549312)
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.1484
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 250.245
Elapsed time for mlp_fused_gelu (2048x4x95232): 0.0027
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1549
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 239.786
Elapsed time for transformer_add_bias_dropout (2048x4x23808): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23808): 0.0010

Attention duration (in seconds): 0.1975
Attention throughput (in TFLOP/s): 196.184
MLP duration (in seconds): 0.3061
MLP throughput (in TFLOP/s): 242.746
Transformer duration (in seconds): 0.5090
Transformer throughput (in TFLOP/s): 222.100
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1686
Attention throughput (in TFLOP/s): 229.811
MLP duration (in seconds): 0.3094
MLP throughput (in TFLOP/s): 240.089
Transformer duration (in seconds): 0.4847
Transformer throughput (in TFLOP/s): 233.195
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23872x71616, b=2048): 0.1109
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23872x71616, b=2048): 252.614
Elapsed time for attention_key_query_prob (256x2048x373x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x373x2048): 70.367
Elapsed time for attention_prob_times_values (256x2048x2048x373): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x373): 69.822
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4415094784, 42481549312)
Elapsed time for attention_linear_projection (4x23872x23872, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_linear_projection (4x23872x23872, b=2048): 256.939
(4415094784, 42481549312)
Elapsed time for mlp_h_to_4h (4x23872x95488, b=2048): 0.1496
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23872x95488, b=2048): 249.578
Elapsed time for mlp_fused_gelu (2048x4x95488): 0.0027
Elapsed time for mlp_4h_to_h (4x95488x23872, b=2048): 0.1558
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95488x23872, b=2048): 239.662
Elapsed time for transformer_add_bias_dropout (2048x4x23872): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23872): 0.0010

Attention duration (in seconds): 0.2055
Attention throughput (in TFLOP/s): 189.565
MLP duration (in seconds): 0.3082
MLP throughput (in TFLOP/s): 242.371
Transformer duration (in seconds): 0.5191
Transformer throughput (in TFLOP/s): 218.919
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1792
Attention throughput (in TFLOP/s): 217.308
MLP duration (in seconds): 0.3139
MLP throughput (in TFLOP/s): 237.971
Transformer duration (in seconds): 0.5053
Transformer throughput (in TFLOP/s): 224.884
Transformer - MLP - Attention (in seconds): 0.0122
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1115
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 252.652
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 107.674
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 120.418
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4610129920, 42481549312)
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0365
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 257.132
(4610129920, 42481549312)
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.1507
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 249.224
Elapsed time for mlp_fused_gelu (2048x4x95744): 0.0027
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.1568
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 239.468
Elapsed time for transformer_add_bias_dropout (2048x4x23936): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23936): 0.0010

Attention duration (in seconds): 0.1975
Attention throughput (in TFLOP/s): 198.262
MLP duration (in seconds): 0.3102
MLP throughput (in TFLOP/s): 242.113
Transformer duration (in seconds): 0.5131
Transformer throughput (in TFLOP/s): 222.672
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1691
Attention throughput (in TFLOP/s): 231.481
MLP duration (in seconds): 0.3144
MLP throughput (in TFLOP/s): 238.855
Transformer duration (in seconds): 0.4881
Transformer throughput (in TFLOP/s): 234.090
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24000x72000, b=2048): 0.1127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24000x72000, b=2048): 251.161
Elapsed time for attention_key_query_prob (256x2048x375x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x375x2048): 70.065
Elapsed time for attention_prob_times_values (256x2048x2048x375): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x375): 69.004
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4123590656, 42481549312)
Elapsed time for attention_linear_projection (4x24000x24000, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_linear_projection (4x24000x24000, b=2048): 257.613
(4123590656, 42481549312)
Elapsed time for mlp_h_to_4h (4x24000x96000, b=2048): 0.1512
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24000x96000, b=2048): 249.720
Elapsed time for mlp_fused_gelu (2048x4x96000): 0.0027
Elapsed time for mlp_4h_to_h (4x96000x24000, b=2048): 0.1579
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96000x24000, b=2048): 239.100
Elapsed time for transformer_add_bias_dropout (2048x4x24000): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24000): 0.0010

Attention duration (in seconds): 0.2079
Attention throughput (in TFLOP/s): 189.355
MLP duration (in seconds): 0.3118
MLP throughput (in TFLOP/s): 242.161
Transformer duration (in seconds): 0.5251
Transformer throughput (in TFLOP/s): 218.727
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1808
Attention throughput (in TFLOP/s): 217.653
MLP duration (in seconds): 0.3158
MLP throughput (in TFLOP/s): 239.031
Transformer duration (in seconds): 0.5066
Transformer throughput (in TFLOP/s): 226.713
Transformer - MLP - Attention (in seconds): 0.0099
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1126
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 252.704
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 183.761
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 179.276
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4301848576, 42481549312)
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0369
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 257.222
(4301848576, 42481549312)
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.1520
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 249.668
Elapsed time for mlp_fused_gelu (2048x4x96256): 0.0027
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.1584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 239.520
Elapsed time for transformer_add_bias_dropout (2048x4x24064): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24064): 0.0010

Attention duration (in seconds): 0.1938
Attention throughput (in TFLOP/s): 204.160
MLP duration (in seconds): 0.3132
MLP throughput (in TFLOP/s): 242.360
Transformer duration (in seconds): 0.5124
Transformer throughput (in TFLOP/s): 225.327
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1694
Attention throughput (in TFLOP/s): 233.568
MLP duration (in seconds): 0.3182
MLP throughput (in TFLOP/s): 238.495
Transformer duration (in seconds): 0.4987
Transformer throughput (in TFLOP/s): 231.523
Transformer - MLP - Attention (in seconds): 0.0111
========================================================================================================================
num_attention_heads: 64, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24128x72384, b=2048): 0.1139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24128x72384, b=2048): 251.277
Elapsed time for attention_key_query_prob (256x2048x377x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x377x2048): 70.516
Elapsed time for attention_prob_times_values (256x2048x2048x377): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x377): 70.057
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4161339392, 42481549312)
Elapsed time for attention_linear_projection (4x24128x24128, b=2048): 0.0370
Throughput (in TFLOP/s) for attention_linear_projection (4x24128x24128, b=2048): 257.515
(4161339392, 42481549312)
Elapsed time for mlp_h_to_4h (4x24128x96512, b=2048): 0.1531
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24128x96512, b=2048): 249.158
Elapsed time for mlp_fused_gelu (2048x4x96512): 0.0027
Elapsed time for mlp_4h_to_h (4x96512x24128, b=2048): 0.1596
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96512x24128, b=2048): 239.040
Elapsed time for transformer_add_bias_dropout (2048x4x24128): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24128): 0.0010

Attention duration (in seconds): 0.2093
Attention throughput (in TFLOP/s): 189.982
MLP duration (in seconds): 0.3155
MLP throughput (in TFLOP/s): 241.879
Transformer duration (in seconds): 0.5303
Transformer throughput (in TFLOP/s): 218.873
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1825
Attention throughput (in TFLOP/s): 217.985
MLP duration (in seconds): 0.3194
MLP throughput (in TFLOP/s): 238.875
Transformer duration (in seconds): 0.5065
Transformer throughput (in TFLOP/s): 229.179
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1141
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 252.164
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 108.255
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 121.950
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4001955840, 42481549312)
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0369
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 259.974
(4001955840, 42481549312)
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.1534
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 249.974
Elapsed time for mlp_fused_gelu (2048x4x96768): 0.0027
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1596
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 240.339
Elapsed time for transformer_add_bias_dropout (2048x4x24192): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24192): 0.0010

Attention duration (in seconds): 0.2005
Attention throughput (in TFLOP/s): 199.392
MLP duration (in seconds): 0.3158
MLP throughput (in TFLOP/s): 242.935
Transformer duration (in seconds): 0.5218
Transformer throughput (in TFLOP/s): 223.644
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1709
Attention throughput (in TFLOP/s): 233.869
MLP duration (in seconds): 0.3204
MLP throughput (in TFLOP/s): 239.383
Transformer duration (in seconds): 0.5001
Transformer throughput (in TFLOP/s): 233.314
Transformer - MLP - Attention (in seconds): 0.0087
========================================================================================================================
num_attention_heads: 64, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24256x72768, b=2048): 0.1151
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24256x72768, b=2048): 251.140
Elapsed time for attention_key_query_prob (256x2048x379x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x379x2048): 71.013
Elapsed time for attention_prob_times_values (256x2048x2048x379): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x379): 71.863
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3850960896, 42481549312)
Elapsed time for attention_linear_projection (4x24256x24256, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_linear_projection (4x24256x24256, b=2048): 243.383
(3850960896, 42481549312)
Elapsed time for mlp_h_to_4h (4x24256x97024, b=2048): 0.1549
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24256x97024, b=2048): 248.869
Elapsed time for mlp_fused_gelu (2048x4x97024): 0.0028
Elapsed time for mlp_4h_to_h (4x97024x24256, b=2048): 0.1611
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97024x24256, b=2048): 239.390
Elapsed time for transformer_add_bias_dropout (2048x4x24256): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24256): 0.0010

Attention duration (in seconds): 0.2129
Attention throughput (in TFLOP/s): 188.733
MLP duration (in seconds): 0.3188
MLP throughput (in TFLOP/s): 241.930
Transformer duration (in seconds): 0.5372
Transformer throughput (in TFLOP/s): 218.341
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1875
Attention throughput (in TFLOP/s): 214.319
MLP duration (in seconds): 0.3229
MLP throughput (in TFLOP/s): 238.857
Transformer duration (in seconds): 0.5155
Transformer throughput (in TFLOP/s): 227.532
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 251.831
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 109.025
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 122.692
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3691577344, 42481549312)
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0397
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 244.394
(3691577344, 42481549312)
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.1562
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 248.125
Elapsed time for mlp_fused_gelu (2048x4x97280): 0.0028
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1622
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 239.014
Elapsed time for transformer_add_bias_dropout (2048x4x24320): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24320): 0.0010

Attention duration (in seconds): 0.2046
Attention throughput (in TFLOP/s): 197.427
MLP duration (in seconds): 0.3212
MLP throughput (in TFLOP/s): 241.393
Transformer duration (in seconds): 0.5313
Transformer throughput (in TFLOP/s): 221.952
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1760
Attention throughput (in TFLOP/s): 229.487
MLP duration (in seconds): 0.3239
MLP throughput (in TFLOP/s): 239.357
Transformer duration (in seconds): 0.5051
Transformer throughput (in TFLOP/s): 233.438
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 64, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24384x73152, b=2048): 0.1163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24384x73152, b=2048): 251.391
Elapsed time for attention_key_query_prob (256x2048x381x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x381x2048): 71.589
Elapsed time for attention_prob_times_values (256x2048x2048x381): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x381): 71.988
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3540582400, 42481549312)
Elapsed time for attention_linear_projection (4x24384x24384, b=2048): 0.0376
Throughput (in TFLOP/s) for attention_linear_projection (4x24384x24384, b=2048): 259.080
(3540582400, 42481549312)
Elapsed time for mlp_h_to_4h (4x24384x97536, b=2048): 0.1561
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24384x97536, b=2048): 249.688
Elapsed time for mlp_fused_gelu (2048x4x97536): 0.0028
Elapsed time for mlp_4h_to_h (4x97536x24384, b=2048): 0.1629
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97536x24384, b=2048): 239.223
Elapsed time for transformer_add_bias_dropout (2048x4x24384): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24384): 0.0010

Attention duration (in seconds): 0.2120
Attention throughput (in TFLOP/s): 191.493
MLP duration (in seconds): 0.3217
MLP throughput (in TFLOP/s): 242.244
Transformer duration (in seconds): 0.5393
Transformer throughput (in TFLOP/s): 219.788
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1862
Attention throughput (in TFLOP/s): 218.057
MLP duration (in seconds): 0.3277
MLP throughput (in TFLOP/s): 237.811
Transformer duration (in seconds): 0.5239
Transformer throughput (in TFLOP/s): 226.252
Transformer - MLP - Attention (in seconds): 0.0100
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1161
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 252.966
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 109.421
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 123.679
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3381198848, 42481549312)
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0381
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 256.744
(3381198848, 42481549312)
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.1572
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 249.135
Elapsed time for mlp_fused_gelu (2048x4x97792): 0.0028
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.1636
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 239.471
Elapsed time for transformer_add_bias_dropout (2048x4x24448): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24448): 0.0010

Attention duration (in seconds): 0.2038
Attention throughput (in TFLOP/s): 200.276
MLP duration (in seconds): 0.3236
MLP throughput (in TFLOP/s): 242.116
Transformer duration (in seconds): 0.5329
Transformer throughput (in TFLOP/s): 223.595
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1751
Attention throughput (in TFLOP/s): 233.042
MLP duration (in seconds): 0.3280
MLP throughput (in TFLOP/s): 238.883
Transformer duration (in seconds): 0.5145
Transformer throughput (in TFLOP/s): 231.585
Transformer - MLP - Attention (in seconds): 0.0114
========================================================================================================================
num_attention_heads: 64, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24512x73536, b=2048): 0.1180
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24512x73536, b=2048): 250.359
Elapsed time for attention_key_query_prob (256x2048x383x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x383x2048): 72.492
Elapsed time for attention_prob_times_values (256x2048x2048x383): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x383): 71.910
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3230203904, 42481549312)
Elapsed time for attention_linear_projection (4x24512x24512, b=2048): 0.0383
Throughput (in TFLOP/s) for attention_linear_projection (4x24512x24512, b=2048): 257.169
(3230203904, 42481549312)
Elapsed time for mlp_h_to_4h (4x24512x98048, b=2048): 0.1571
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24512x98048, b=2048): 250.673
Elapsed time for mlp_fused_gelu (2048x4x98048): 0.0028
Elapsed time for mlp_4h_to_h (4x98048x24512, b=2048): 0.1651
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98048x24512, b=2048): 238.500
Elapsed time for transformer_add_bias_dropout (2048x4x24512): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24512): 0.0010

Attention duration (in seconds): 0.2144
Attention throughput (in TFLOP/s): 191.311
MLP duration (in seconds): 0.3250
MLP throughput (in TFLOP/s): 242.346
Transformer duration (in seconds): 0.5450
Transformer throughput (in TFLOP/s): 219.770
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1882
Attention throughput (in TFLOP/s): 217.966
MLP duration (in seconds): 0.3331
MLP throughput (in TFLOP/s): 236.426
Transformer duration (in seconds): 0.5267
Transformer throughput (in TFLOP/s): 227.409
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.1181
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 251.378
Elapsed time for attention_key_query_prob (256x2048x384x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x384x2048): 197.892
Elapsed time for attention_prob_times_values (256x2048x2048x384): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x384): 186.617
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3070820352, 42481549312)
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0383
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 258.392
(3070820352, 42481549312)
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.1586
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 249.514
Elapsed time for mlp_fused_gelu (2048x4x98304): 0.0028
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.1647
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 240.273
Elapsed time for transformer_add_bias_dropout (2048x4x24576): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24576): 0.0010

Attention duration (in seconds): 0.2004
Attention throughput (in TFLOP/s): 205.748
MLP duration (in seconds): 0.3262
MLP throughput (in TFLOP/s): 242.716
Transformer duration (in seconds): 0.5322
Transformer throughput (in TFLOP/s): 226.242
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1756
Attention throughput (in TFLOP/s): 234.767
MLP duration (in seconds): 0.3316
MLP throughput (in TFLOP/s): 238.704
Transformer duration (in seconds): 0.5138
Transformer throughput (in TFLOP/s): 234.325
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24640x73920, b=2048): 0.1189
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24640x73920, b=2048): 250.934
Elapsed time for attention_key_query_prob (256x2048x385x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x385x2048): 70.977
Elapsed time for attention_prob_times_values (256x2048x2048x385): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x385): 71.523
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2932408320, 42481549312)
Elapsed time for attention_linear_projection (4x24640x24640, b=2048): 0.0409
Throughput (in TFLOP/s) for attention_linear_projection (4x24640x24640, b=2048): 243.464
(2932408320, 42481549312)
Elapsed time for mlp_h_to_4h (4x24640x98560, b=2048): 0.1598
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24640x98560, b=2048): 249.036
Elapsed time for mlp_fused_gelu (2048x4x98560): 0.0028
Elapsed time for mlp_4h_to_h (4x98560x24640, b=2048): 0.1665
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98560x24640, b=2048): 238.958
Elapsed time for transformer_add_bias_dropout (2048x4x24640): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24640): 0.0010

Attention duration (in seconds): 0.2184
Attention throughput (in TFLOP/s): 189.750
MLP duration (in seconds): 0.3291
MLP throughput (in TFLOP/s): 241.823
Transformer duration (in seconds): 0.5531
Transformer throughput (in TFLOP/s): 218.792
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1929
Attention throughput (in TFLOP/s): 214.852
MLP duration (in seconds): 0.3329
MLP throughput (in TFLOP/s): 239.039
Transformer duration (in seconds): 0.5320
Transformer throughput (in TFLOP/s): 227.486
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1192
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 251.750
Elapsed time for attention_key_query_prob (256x2048x386x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x386x2048): 102.333
Elapsed time for attention_prob_times_values (256x2048x2048x386): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x386): 123.971
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2355691520, 42481549312)
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 243.611
(2355691520, 42481549312)
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.1607
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 248.877
Elapsed time for mlp_fused_gelu (2048x4x98816): 0.0028
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.1669
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 239.583
Elapsed time for transformer_add_bias_dropout (2048x4x24704): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24704): 0.0010

Attention duration (in seconds): 0.2104
Attention throughput (in TFLOP/s): 197.989
MLP duration (in seconds): 0.3304
MLP throughput (in TFLOP/s): 242.070
Transformer duration (in seconds): 0.5465
Transformer throughput (in TFLOP/s): 222.603
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1820
Attention throughput (in TFLOP/s): 228.910
MLP duration (in seconds): 0.3348
MLP throughput (in TFLOP/s): 238.890
Transformer duration (in seconds): 0.5227
Transformer throughput (in TFLOP/s): 232.746
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24768x74304, b=2048): 0.1195
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24768x74304, b=2048): 252.362
Elapsed time for attention_key_query_prob (256x2048x387x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x387x2048): 70.305
Elapsed time for attention_prob_times_values (256x2048x2048x387): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x387): 71.888
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2204696576, 42481549312)
Elapsed time for attention_linear_projection (4x24768x24768, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_linear_projection (4x24768x24768, b=2048): 255.936
(2204696576, 42481549312)
Elapsed time for mlp_h_to_4h (4x24768x99072, b=2048): 0.1608
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24768x99072, b=2048): 250.059
Elapsed time for mlp_fused_gelu (2048x4x99072): 0.0028
Elapsed time for mlp_4h_to_h (4x99072x24768, b=2048): 0.1686
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99072x24768, b=2048): 238.410
Elapsed time for transformer_add_bias_dropout (2048x4x24768): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24768): 0.0010

Attention duration (in seconds): 0.2175
Attention throughput (in TFLOP/s): 192.469
MLP duration (in seconds): 0.3322
MLP throughput (in TFLOP/s): 242.034
Transformer duration (in seconds): 0.5554
Transformer throughput (in TFLOP/s): 220.148
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1920
Attention throughput (in TFLOP/s): 218.060
MLP duration (in seconds): 0.3379
MLP throughput (in TFLOP/s): 237.946
Transformer duration (in seconds): 0.5352
Transformer throughput (in TFLOP/s): 228.445
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 252.079
Elapsed time for attention_key_query_prob (256x2048x388x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x388x2048): 102.891
Elapsed time for attention_prob_times_values (256x2048x2048x388): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x388): 122.870
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2043215872, 42481549312)
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 259.432
(2043215872, 42481549312)
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.1621
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 249.362
Elapsed time for mlp_fused_gelu (2048x4x99328): 0.0028
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.1686
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 239.692
Elapsed time for transformer_add_bias_dropout (2048x4x24832): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24832): 0.0010

Attention duration (in seconds): 0.2094
Attention throughput (in TFLOP/s): 200.905
MLP duration (in seconds): 0.3335
MLP throughput (in TFLOP/s): 242.370
Transformer duration (in seconds): 0.5485
Transformer throughput (in TFLOP/s): 224.046
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1800
Attention throughput (in TFLOP/s): 233.741
MLP duration (in seconds): 0.3392
MLP throughput (in TFLOP/s): 238.288
Transformer duration (in seconds): 0.5297
Transformer throughput (in TFLOP/s): 232.006
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================
num_attention_heads: 64, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24896x74688, b=2048): 0.1207
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24896x74688, b=2048): 252.325
Elapsed time for attention_key_query_prob (256x2048x389x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x389x2048): 70.751
Elapsed time for attention_prob_times_values (256x2048x2048x389): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x389): 71.597
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1892220928, 42481549312)
Elapsed time for attention_linear_projection (4x24896x24896, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_linear_projection (4x24896x24896, b=2048): 256.719
(1892220928, 42481549312)
Elapsed time for mlp_h_to_4h (4x24896x99584, b=2048): 0.1626
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24896x99584, b=2048): 249.753
Elapsed time for mlp_fused_gelu (2048x4x99584): 0.0028
Elapsed time for mlp_4h_to_h (4x99584x24896, b=2048): 0.1701
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99584x24896, b=2048): 238.815
Elapsed time for transformer_add_bias_dropout (2048x4x24896): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24896): 0.0010

Attention duration (in seconds): 0.2192
Attention throughput (in TFLOP/s): 192.953
MLP duration (in seconds): 0.3356
MLP throughput (in TFLOP/s): 242.108
Transformer duration (in seconds): 0.5604
Transformer throughput (in TFLOP/s): 220.428
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1934
Attention throughput (in TFLOP/s): 218.655
MLP duration (in seconds): 0.3395
MLP throughput (in TFLOP/s): 239.261
Transformer duration (in seconds): 0.5408
Transformer throughput (in TFLOP/s): 228.405
Transformer - MLP - Attention (in seconds): 0.0079
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1217
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 251.619
Elapsed time for attention_key_query_prob (256x2048x390x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x390x2048): 103.446
Elapsed time for attention_prob_times_values (256x2048x2048x390): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x390): 122.023
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1730740224, 42481549312)
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0397
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 257.199
(1730740224, 42481549312)
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.1642
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 248.601
Elapsed time for mlp_fused_gelu (2048x4x99840): 0.0028
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.1708
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 239.043
Elapsed time for transformer_add_bias_dropout (2048x4x24960): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24960): 0.0010

Attention duration (in seconds): 0.2117
Attention throughput (in TFLOP/s): 200.732
MLP duration (in seconds): 0.3379
MLP throughput (in TFLOP/s): 241.688
Transformer duration (in seconds): 0.5553
Transformer throughput (in TFLOP/s): 223.606
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1823
Attention throughput (in TFLOP/s): 233.214
MLP duration (in seconds): 0.3434
MLP throughput (in TFLOP/s): 237.809
Transformer duration (in seconds): 0.5379
Transformer throughput (in TFLOP/s): 230.807
Transformer - MLP - Attention (in seconds): 0.0123
========================================================================================================================
num_attention_heads: 64, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25024x75072, b=2048): 0.1234
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25024x75072, b=2048): 249.425
Elapsed time for attention_key_query_prob (256x2048x391x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x391x2048): 70.624
Elapsed time for attention_prob_times_values (256x2048x2048x391): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x391): 70.315
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1579745280, 42481549312)
Elapsed time for attention_linear_projection (4x25024x25024, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x25024x25024, b=2048): 243.912
(1579745280, 42481549312)
Elapsed time for mlp_h_to_4h (4x25024x100096, b=2048): 0.1649
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25024x100096, b=2048): 248.834
Elapsed time for mlp_fused_gelu (2048x4x100096): 0.0028
Elapsed time for mlp_4h_to_h (4x100096x25024, b=2048): 0.1722
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100096x25024, b=2048): 238.284
Elapsed time for transformer_add_bias_dropout (2048x4x25024): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25024): 0.0010

Attention duration (in seconds): 0.2247
Attention throughput (in TFLOP/s): 190.125
MLP duration (in seconds): 0.3400
MLP throughput (in TFLOP/s): 241.414
Transformer duration (in seconds): 0.5704
Transformer throughput (in TFLOP/s): 218.793
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1995
Attention throughput (in TFLOP/s): 214.158
MLP duration (in seconds): 0.3475
MLP throughput (in TFLOP/s): 236.169
Transformer duration (in seconds): 0.5527
Transformer throughput (in TFLOP/s): 225.799
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1242
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 248.992
Elapsed time for attention_key_query_prob (256x2048x392x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x392x2048): 180.208
Elapsed time for attention_prob_times_values (256x2048x2048x392): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x392): 184.599
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1418264576, 42481549312)
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0418
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 246.991
(1418264576, 42481549312)
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.1655
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 249.232
Elapsed time for mlp_fused_gelu (2048x4x100352): 0.0028
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.1729
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 238.600
Elapsed time for transformer_add_bias_dropout (2048x4x25088): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25088): 0.0010

Attention duration (in seconds): 0.2106
Attention throughput (in TFLOP/s): 203.823
MLP duration (in seconds): 0.3412
MLP throughput (in TFLOP/s): 241.770
Transformer duration (in seconds): 0.5576
Transformer throughput (in TFLOP/s): 224.964
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1868
Attention throughput (in TFLOP/s): 229.818
MLP duration (in seconds): 0.3486
MLP throughput (in TFLOP/s): 236.627
Transformer duration (in seconds): 0.5383
Transformer throughput (in TFLOP/s): 233.027
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 64, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25152x75456, b=2048): 0.1238
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25152x75456, b=2048): 251.263
Elapsed time for attention_key_query_prob (256x2048x393x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x393x2048): 70.920
Elapsed time for attention_prob_times_values (256x2048x2048x393): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x393): 69.891
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1267269632, 42481549312)
Elapsed time for attention_linear_projection (4x25152x25152, b=2048): 0.0406
Throughput (in TFLOP/s) for attention_linear_projection (4x25152x25152, b=2048): 255.395
(1267269632, 42481549312)
Elapsed time for mlp_h_to_4h (4x25152x100608, b=2048): 0.1659
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25152x100608, b=2048): 249.970
Elapsed time for mlp_fused_gelu (2048x4x100608): 0.0029
Elapsed time for mlp_4h_to_h (4x100608x25152, b=2048): 0.1744
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100608x25152, b=2048): 237.739
Elapsed time for transformer_add_bias_dropout (2048x4x25152): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25152): 0.0010

Attention duration (in seconds): 0.2237
Attention throughput (in TFLOP/s): 192.888
MLP duration (in seconds): 0.3431
MLP throughput (in TFLOP/s): 241.676
Transformer duration (in seconds): 0.5726
Transformer throughput (in TFLOP/s): 220.182
Transformer - MLP - Attention (in seconds): 0.0058


Actual
------
Attention duration (in seconds): 0.1979
Attention throughput (in TFLOP/s): 218.031
MLP duration (in seconds): 0.3524
MLP throughput (in TFLOP/s): 235.294
Transformer duration (in seconds): 0.5589
Transformer throughput (in TFLOP/s): 225.558
Transformer - MLP - Attention (in seconds): 0.0086
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.1249
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 250.259
Elapsed time for attention_key_query_prob (256x2048x394x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x394x2048): 103.834
Elapsed time for attention_prob_times_values (256x2048x2048x394): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x394): 121.046
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1095303168, 42481549312)
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0407
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 256.213
(1095303168, 42481549312)
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.1669
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 249.641
Elapsed time for mlp_fused_gelu (2048x4x100864): 0.0029
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.1742
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 239.227
Elapsed time for transformer_add_bias_dropout (2048x4x25216): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25216): 0.0010

Attention duration (in seconds): 0.2161
Attention throughput (in TFLOP/s): 200.696
MLP duration (in seconds): 0.3440
MLP throughput (in TFLOP/s): 242.294
Transformer duration (in seconds): 0.5658
Transformer throughput (in TFLOP/s): 223.950
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1859
Attention throughput (in TFLOP/s): 233.250
MLP duration (in seconds): 0.3490
MLP throughput (in TFLOP/s): 238.816
Transformer duration (in seconds): 0.5442
Transformer throughput (in TFLOP/s): 232.840
Transformer - MLP - Attention (in seconds): 0.0093
========================================================================================================================
num_attention_heads: 64, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25280x75840, b=2048): 0.1250
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25280x75840, b=2048): 251.265
Elapsed time for attention_key_query_prob (256x2048x395x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x395x2048): 71.908
Elapsed time for attention_prob_times_values (256x2048x2048x395): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x395): 71.807
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(942211072, 42481549312)
Elapsed time for attention_linear_projection (4x25280x25280, b=2048): 0.0408
Throughput (in TFLOP/s) for attention_linear_projection (4x25280x25280, b=2048): 256.609
(942211072, 42481549312)
Elapsed time for mlp_h_to_4h (4x25280x101120, b=2048): 0.1673
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25280x101120, b=2048): 250.383
Elapsed time for mlp_fused_gelu (2048x4x101120): 0.0029
Elapsed time for mlp_4h_to_h (4x101120x25280, b=2048): 0.1759
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101120x25280, b=2048): 238.135
Elapsed time for transformer_add_bias_dropout (2048x4x25280): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25280): 0.0010

Attention duration (in seconds): 0.2248
Attention throughput (in TFLOP/s): 193.859
MLP duration (in seconds): 0.3460
MLP throughput (in TFLOP/s): 242.085
Transformer duration (in seconds): 0.5766
Transformer throughput (in TFLOP/s): 220.853
Transformer - MLP - Attention (in seconds): 0.0058


Actual
------
Attention duration (in seconds): 0.1991
Attention throughput (in TFLOP/s): 218.915
MLP duration (in seconds): 0.3510
MLP throughput (in TFLOP/s): 238.664
Transformer duration (in seconds): 0.5568
Transformer throughput (in TFLOP/s): 228.690
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.1250
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 252.483
Elapsed time for attention_key_query_prob (256x2048x396x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x396x2048): 104.291
Elapsed time for attention_prob_times_values (256x2048x2048x396): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x396): 120.914
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(780730368, 42481549312)
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0409
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 257.252
(780730368, 42481549312)
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.1692
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 248.799
Elapsed time for mlp_fused_gelu (2048x4x101376): 0.0029
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.1758
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 239.487
Elapsed time for transformer_add_bias_dropout (2048x4x25344): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25344): 0.0010

Attention duration (in seconds): 0.2165
Attention throughput (in TFLOP/s): 202.267
MLP duration (in seconds): 0.3478
MLP throughput (in TFLOP/s): 242.041
Transformer duration (in seconds): 0.5701
Transformer throughput (in TFLOP/s): 224.493
Transformer - MLP - Attention (in seconds): 0.0058


Actual
------
Attention duration (in seconds): 0.1869
Attention throughput (in TFLOP/s): 234.389
MLP duration (in seconds): 0.3532
MLP throughput (in TFLOP/s): 238.385
Transformer duration (in seconds): 0.5516
Transformer throughput (in TFLOP/s): 232.006
Transformer - MLP - Attention (in seconds): 0.0116
========================================================================================================================
num_attention_heads: 64, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25408x76224, b=2048): 0.1279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25408x76224, b=2048): 248.098
Elapsed time for attention_key_query_prob (256x2048x397x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x397x2048): 72.119
Elapsed time for attention_prob_times_values (256x2048x2048x397): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x397): 72.244
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(619249664, 42481549312)
Elapsed time for attention_linear_projection (4x25408x25408, b=2048): 0.0411
Throughput (in TFLOP/s) for attention_linear_projection (4x25408x25408, b=2048): 257.531
(619249664, 42481549312)
Elapsed time for mlp_h_to_4h (4x25408x101632, b=2048): 0.1695
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25408x101632, b=2048): 249.589
Elapsed time for mlp_fused_gelu (2048x4x101632): 0.0029
Elapsed time for mlp_4h_to_h (4x101632x25408, b=2048): 0.1772
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101632x25408, b=2048): 238.743
Elapsed time for transformer_add_bias_dropout (2048x4x25408): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25408): 0.0011

Attention duration (in seconds): 0.2280
Attention throughput (in TFLOP/s): 193.042
MLP duration (in seconds): 0.3496
MLP throughput (in TFLOP/s): 242.036
Transformer duration (in seconds): 0.5834
Transformer throughput (in TFLOP/s): 220.478
Transformer - MLP - Attention (in seconds): 0.0058


Actual
------
Attention duration (in seconds): 0.2018
Attention throughput (in TFLOP/s): 218.060
MLP duration (in seconds): 0.3565
MLP throughput (in TFLOP/s): 237.330
Transformer duration (in seconds): 0.5670
Transformer throughput (in TFLOP/s): 226.865
Transformer - MLP - Attention (in seconds): 0.0086
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.1266
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 251.843
Elapsed time for attention_key_query_prob (256x2048x398x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x398x2048): 104.793
Elapsed time for attention_prob_times_values (256x2048x2048x398): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x398): 121.513
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(457768960, 42481549312)
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0412
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 258.306
(457768960, 42481549312)
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.1700
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 250.131
Elapsed time for mlp_fused_gelu (2048x4x101888): 0.0029
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.1778
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 239.118
Elapsed time for transformer_add_bias_dropout (2048x4x25472): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25472): 0.0010

Attention duration (in seconds): 0.2184
Attention throughput (in TFLOP/s): 202.537
MLP duration (in seconds): 0.3507
MLP throughput (in TFLOP/s): 242.487
Transformer duration (in seconds): 0.5749
Transformer throughput (in TFLOP/s): 224.868
Transformer - MLP - Attention (in seconds): 0.0058


Actual
------
Attention duration (in seconds): 0.1890
Attention throughput (in TFLOP/s): 234.071
MLP duration (in seconds): 0.3575
MLP throughput (in TFLOP/s): 237.860
Transformer duration (in seconds): 0.5570
Transformer throughput (in TFLOP/s): 232.082
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================
num_attention_heads: 64, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25536x76608, b=2048): 0.1288
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25536x76608, b=2048): 248.764
Elapsed time for attention_key_query_prob (256x2048x399x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x399x2048): 72.866
Elapsed time for attention_prob_times_values (256x2048x2048x399): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x399): 71.945
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(294191104, 42481549312)
Elapsed time for attention_linear_projection (4x25536x25536, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_linear_projection (4x25536x25536, b=2048): 255.271
(294191104, 42481549312)
Elapsed time for mlp_h_to_4h (4x25536x102144, b=2048): 0.1708
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25536x102144, b=2048): 250.207
Elapsed time for mlp_fused_gelu (2048x4x102144): 0.0029
Elapsed time for mlp_4h_to_h (4x102144x25536, b=2048): 0.1793
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102144x25536, b=2048): 238.305
Elapsed time for transformer_add_bias_dropout (2048x4x25536): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25536): 0.0011

Attention duration (in seconds): 0.2297
Attention throughput (in TFLOP/s): 193.468
MLP duration (in seconds): 0.3530
MLP throughput (in TFLOP/s): 242.110
Transformer duration (in seconds): 0.5886
Transformer throughput (in TFLOP/s): 220.725
Transformer - MLP - Attention (in seconds): 0.0058


Actual
------
Attention duration (in seconds): 0.2029
Attention throughput (in TFLOP/s): 219.019
MLP duration (in seconds): 0.3601
MLP throughput (in TFLOP/s): 237.332
Transformer duration (in seconds): 0.5705
Transformer throughput (in TFLOP/s): 227.729
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.1271
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 253.342
Elapsed time for attention_key_query_prob (256x2048x400x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x400x2048): 189.985
Elapsed time for attention_prob_times_values (256x2048x2048x400): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x400): 191.755
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(132710400, 42481549312)
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0420
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 255.908
(132710400, 42481549312)
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.1718
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 249.966
Elapsed time for mlp_fused_gelu (2048x4x102400): 0.0029
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.1802
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 238.303
Elapsed time for transformer_add_bias_dropout (2048x4x25600): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25600): 0.0010

Attention duration (in seconds): 0.2135
Attention throughput (in TFLOP/s): 209.210
MLP duration (in seconds): 0.3550
MLP throughput (in TFLOP/s): 242.002
Transformer duration (in seconds): 0.5743
Transformer throughput (in TFLOP/s): 227.363
Transformer - MLP - Attention (in seconds): 0.0058


Actual
------
Attention duration (in seconds): 0.1900
Attention throughput (in TFLOP/s): 235.042
MLP duration (in seconds): 0.3644
MLP throughput (in TFLOP/s): 235.731
Transformer duration (in seconds): 0.5559
Transformer throughput (in TFLOP/s): 234.881
Transformer - MLP - Attention (in seconds): 0.0014
========================================================================================================================
num_attention_heads: 64, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25664x76992, b=2048): 0.1282
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25664x76992, b=2048): 252.600
Elapsed time for attention_key_query_prob (256x2048x401x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x401x2048): 73.477
Elapsed time for attention_prob_times_values (256x2048x2048x401): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x401): 71.931
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(401145856, 42481549312)
Elapsed time for attention_linear_projection (4x25664x25664, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x25664x25664, b=2048): 256.542
(401145856, 42481549312)
Elapsed time for mlp_h_to_4h (4x25664x102656, b=2048): 0.1727
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25664x102656, b=2048): 249.946
Elapsed time for mlp_fused_gelu (2048x4x102656): 0.0029
Elapsed time for mlp_4h_to_h (4x102656x25664, b=2048): 0.1815
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102656x25664, b=2048): 237.835
Elapsed time for transformer_add_bias_dropout (2048x4x25664): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25664): 0.0011

Attention duration (in seconds): 0.2293
Attention throughput (in TFLOP/s): 195.758
MLP duration (in seconds): 0.3571
MLP throughput (in TFLOP/s): 241.756
Transformer duration (in seconds): 0.5923
Transformer throughput (in TFLOP/s): 221.547
Transformer - MLP - Attention (in seconds): 0.0059


Actual
------
Attention duration (in seconds): 0.2048
Attention throughput (in TFLOP/s): 219.216
MLP duration (in seconds): 0.3656
MLP throughput (in TFLOP/s): 236.163
Transformer duration (in seconds): 0.5808
Transformer throughput (in TFLOP/s): 225.908
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.1324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 245.772
Elapsed time for attention_key_query_prob (256x2048x402x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x402x2048): 105.529
Elapsed time for attention_prob_times_values (256x2048x2048x402): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x402): 121.961
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(229179392, 42481549312)
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 256.814
(229179392, 42481549312)
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.1743
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 248.812
Elapsed time for mlp_fused_gelu (2048x4x102912): 0.0029
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.1821
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 238.235
Elapsed time for transformer_add_bias_dropout (2048x4x25728): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25728): 0.0011

Attention duration (in seconds): 0.2252
Attention throughput (in TFLOP/s): 200.260
MLP duration (in seconds): 0.3594
MLP throughput (in TFLOP/s): 241.432
Transformer duration (in seconds): 0.5905
Transformer throughput (in TFLOP/s): 223.325
Transformer - MLP - Attention (in seconds): 0.0059


Actual
------
Attention duration (in seconds): 0.1933
Attention throughput (in TFLOP/s): 233.337
MLP duration (in seconds): 0.3682
MLP throughput (in TFLOP/s): 235.618
Transformer duration (in seconds): 0.5710
Transformer throughput (in TFLOP/s): 230.945
Transformer - MLP - Attention (in seconds): 0.0095
========================================================================================================================
num_attention_heads: 64, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25792x77376, b=2048): 0.1314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25792x77376, b=2048): 248.877
Elapsed time for attention_key_query_prob (256x2048x403x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x403x2048): 73.749
Elapsed time for attention_prob_times_values (256x2048x2048x403): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x403): 73.455
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(67698688, 42481549312)
Elapsed time for attention_linear_projection (4x25792x25792, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_linear_projection (4x25792x25792, b=2048): 257.107
(67698688, 42481549312)
Elapsed time for mlp_h_to_4h (4x25792x103168, b=2048): 0.1739
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25792x103168, b=2048): 250.658
Elapsed time for mlp_fused_gelu (2048x4x103168): 0.0029
Elapsed time for mlp_4h_to_h (4x103168x25792, b=2048): 0.1838
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103168x25792, b=2048): 237.226
Elapsed time for transformer_add_bias_dropout (2048x4x25792): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25792): 0.0011

Attention duration (in seconds): 0.2327
Attention throughput (in TFLOP/s): 194.812
MLP duration (in seconds): 0.3606
MLP throughput (in TFLOP/s): 241.781
Transformer duration (in seconds): 0.5992
Transformer throughput (in TFLOP/s): 221.159
Transformer - MLP - Attention (in seconds): 0.0059


Actual
------
Attention duration (in seconds): 0.2071
Attention throughput (in TFLOP/s): 218.895
MLP duration (in seconds): 0.3701
MLP throughput (in TFLOP/s): 235.573
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 259.050
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 63.122
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 81.565
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12331843584, 42481549312)
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 261.108
(12331843584, 42481549312)
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0170
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 259.415
Elapsed time for mlp_fused_gelu (2048x4x32768): 0.0009
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0178
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 246.542
Elapsed time for transformer_add_bias_dropout (2048x4x8192): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8192): 0.0003

Attention duration (in seconds): 0.0945
Attention throughput (in TFLOP/s): 52.349
MLP duration (in seconds): 0.0357
MLP throughput (in TFLOP/s): 246.323
Transformer duration (in seconds): 0.1321
Transformer throughput (in TFLOP/s): 104.056
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0323
Attention throughput (in TFLOP/s): 153.090
MLP duration (in seconds): 0.0359
MLP throughput (in TFLOP/s): 244.977
Transformer duration (in seconds): 0.0706
Transformer throughput (in TFLOP/s): 194.554
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8256x24768, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8256x24768, b=2048): 259.493
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 62.155
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 81.479
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x8256x8256, b=2048): 0.0044
Throughput (in TFLOP/s) for attention_linear_projection (4x8256x8256, b=2048): 253.767
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x8256x33024, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8256x33024, b=2048): 258.611
Elapsed time for mlp_fused_gelu (2048x4x33024): 0.0009
Elapsed time for mlp_4h_to_h (4x33024x8256, b=2048): 0.0182
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33024x8256, b=2048): 245.186
Elapsed time for transformer_add_bias_dropout (2048x4x8256): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8256): 0.0003

Attention duration (in seconds): 0.0950
Attention throughput (in TFLOP/s): 52.880
MLP duration (in seconds): 0.0364
MLP throughput (in TFLOP/s): 245.327
Transformer duration (in seconds): 0.1333
Transformer throughput (in TFLOP/s): 104.718
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
[2023-06-27 00:11:51,672] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-27 00:11:52,631] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.152.191, master_port=6000
[2023-06-27 00:11:52,631] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-27 00:11:55,366] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 256.276
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 63.008
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 81.571
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12331843584, 42481549312)
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 256.733
(12331843584, 42481549312)
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 253.853
Elapsed time for mlp_fused_gelu (2048x4x32768): 0.0009
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0180
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 243.766
Elapsed time for transformer_add_bias_dropout (2048x4x8192): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8192): 0.0003

Attention duration (in seconds): 0.0947
Attention throughput (in TFLOP/s): 52.250
MLP duration (in seconds): 0.0363
MLP throughput (in TFLOP/s): 242.422
Transformer duration (in seconds): 0.1328
Transformer throughput (in TFLOP/s): 103.466
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0326
Attention throughput (in TFLOP/s): 151.863
MLP duration (in seconds): 0.0367
MLP throughput (in TFLOP/s): 239.632
Transformer duration (in seconds): 0.0717
Transformer throughput (in TFLOP/s): 191.688
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 255.049
Elapsed time for attention_key_query_prob (512x2048x65x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x65x2048): 27.156
Elapsed time for attention_prob_times_values (512x2048x2048x65): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x65): 41.736
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 251.983
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0178
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 254.609
Elapsed time for mlp_fused_gelu (2048x4x33280): 0.0009
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0187
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 243.197
Elapsed time for transformer_add_bias_dropout (2048x4x8320): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8320): 0.0003

Attention duration (in seconds): 0.1046
Attention throughput (in TFLOP/s): 48.709
MLP duration (in seconds): 0.0374
MLP throughput (in TFLOP/s): 242.584
Transformer duration (in seconds): 0.1439
Transformer throughput (in TFLOP/s): 98.464
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0469
Attention throughput (in TFLOP/s): 108.546
MLP duration (in seconds): 0.0381
MLP throughput (in TFLOP/s): 238.336
Transformer duration (in seconds): 0.0884
Transformer throughput (in TFLOP/s): 160.309
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 255.749
Elapsed time for attention_key_query_prob (512x2048x66x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x66x2048): 19.901
Elapsed time for attention_prob_times_values (512x2048x2048x66): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x66): 71.259
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 256.052
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0183
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 255.498
Elapsed time for mlp_fused_gelu (2048x4x33792): 0.0009
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0193
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 242.748
Elapsed time for transformer_add_bias_dropout (2048x4x8448): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8448): 0.0003

Attention duration (in seconds): 0.1063
Attention throughput (in TFLOP/s): 49.339
MLP duration (in seconds): 0.0385
MLP throughput (in TFLOP/s): 242.861
Transformer duration (in seconds): 0.1467
Transformer throughput (in TFLOP/s): 99.496
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0480
Attention throughput (in TFLOP/s): 109.293
MLP duration (in seconds): 0.0391
MLP throughput (in TFLOP/s): 239.030
Transformer duration (in seconds): 0.0906
Transformer throughput (in TFLOP/s): 161.213
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 255.025
Elapsed time for attention_key_query_prob (512x2048x67x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x67x2048): 28.049
Elapsed time for attention_prob_times_values (512x2048x2048x67): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x67): 43.137
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 256.622
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0189
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 255.316
Elapsed time for mlp_fused_gelu (2048x4x34304): 0.0010
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 243.120
Elapsed time for transformer_add_bias_dropout (2048x4x8576): 0.0006
Elapsed time for transformer_layer_norm (2048x4x8576): 0.0003

Attention duration (in seconds): 0.1056
Attention throughput (in TFLOP/s): 51.096
MLP duration (in seconds): 0.0397
MLP throughput (in TFLOP/s): 243.049
Transformer duration (in seconds): 0.1472
Transformer throughput (in TFLOP/s): 102.137
Transformer - MLP - Attention (in seconds): 0.0019


Actual
------
Attention duration (in seconds): 0.0485
Attention throughput (in TFLOP/s): 111.197
MLP duration (in seconds): 0.0403
MLP throughput (in TFLOP/s): 239.375
Transformer duration (in seconds): 0.0925
Transformer throughput (in TFLOP/s): 162.577
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 258.229
Elapsed time for attention_key_query_prob (512x2048x68x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x68x2048): 19.322
Elapsed time for attention_prob_times_values (512x2048x2048x68): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x68): 72.656
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 251.992
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0191
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 259.381
Elapsed time for mlp_fused_gelu (2048x4x34816): 0.0010
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 243.883
Elapsed time for transformer_add_bias_dropout (2048x4x8704): 0.0007
Elapsed time for transformer_layer_norm (2048x4x8704): 0.0003

Attention duration (in seconds): 0.1083
Attention throughput (in TFLOP/s): 51.252
MLP duration (in seconds): 0.0405
MLP throughput (in TFLOP/s): 245.358
Transformer duration (in seconds): 0.1507
Transformer throughput (in TFLOP/s): 102.693
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 110.727
MLP duration (in seconds): 0.0413
MLP throughput (in TFLOP/s): 240.574
Transformer duration (in seconds): 0.0952
Transformer throughput (in TFLOP/s): 162.637
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 257.832
Elapsed time for attention_key_query_prob (512x2048x69x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x69x2048): 28.367
Elapsed time for attention_prob_times_values (512x2048x2048x69): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x69): 44.169
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 255.391
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0199
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 256.961
Elapsed time for mlp_fused_gelu (2048x4x35328): 0.0010
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0211
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 242.671
Elapsed time for transformer_add_bias_dropout (2048x4x8832): 0.0007
Elapsed time for transformer_layer_norm (2048x4x8832): 0.0003

Attention duration (in seconds): 0.1068
Attention throughput (in TFLOP/s): 53.403
MLP duration (in seconds): 0.0419
MLP throughput (in TFLOP/s): 243.739
Transformer duration (in seconds): 0.1508
Transformer throughput (in TFLOP/s): 105.634
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0505
Attention throughput (in TFLOP/s): 112.927
MLP duration (in seconds): 0.0425
MLP throughput (in TFLOP/s): 240.717
Transformer duration (in seconds): 0.0968
Transformer throughput (in TFLOP/s): 164.572
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 257.715
Elapsed time for attention_key_query_prob (512x2048x70x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x70x2048): 20.290
Elapsed time for attention_prob_times_values (512x2048x2048x70): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x70): 74.046
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 256.100
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0205
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 256.914
Elapsed time for mlp_fused_gelu (2048x4x35840): 0.0010
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0207
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 253.908
Elapsed time for transformer_add_bias_dropout (2048x4x8960): 0.0007
Elapsed time for transformer_layer_norm (2048x4x8960): 0.0004

Attention duration (in seconds): 0.1091
Attention throughput (in TFLOP/s): 53.731
MLP duration (in seconds): 0.0422
MLP throughput (in TFLOP/s): 249.353
Transformer duration (in seconds): 0.1534
Transformer throughput (in TFLOP/s): 106.845
Transformer - MLP - Attention (in seconds): 0.0020


Actual
------
Attention duration (in seconds): 0.0523
Attention throughput (in TFLOP/s): 112.172
MLP duration (in seconds): 0.0427
MLP throughput (in TFLOP/s): 246.578
Transformer duration (in seconds): 0.0980
Transformer throughput (in TFLOP/s): 167.227
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 257.400
Elapsed time for attention_key_query_prob (512x2048x71x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x71x2048): 28.080
Elapsed time for attention_prob_times_values (512x2048x2048x71): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x71): 44.843
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 251.436
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 255.499
Elapsed time for mlp_fused_gelu (2048x4x36352): 0.0010
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0215
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 251.938
Elapsed time for transformer_add_bias_dropout (2048x4x9088): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9088): 0.0004

Attention duration (in seconds): 0.1086
Attention throughput (in TFLOP/s): 55.447
MLP duration (in seconds): 0.0437
MLP throughput (in TFLOP/s): 247.816
Transformer duration (in seconds): 0.1544
Transformer throughput (in TFLOP/s): 109.135
Transformer - MLP - Attention (in seconds): 0.0021


Actual
------
Attention duration (in seconds): 0.0528
Attention throughput (in TFLOP/s): 114.171
MLP duration (in seconds): 0.0441
MLP throughput (in TFLOP/s): 245.734
Transformer duration (in seconds): 0.0999
Transformer throughput (in TFLOP/s): 168.639
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 257.122
Elapsed time for attention_key_query_prob (512x2048x72x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x72x2048): 68.111
Elapsed time for attention_prob_times_values (512x2048x2048x72): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x72): 85.557
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 254.698
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 257.860
Elapsed time for mlp_fused_gelu (2048x4x36864): 0.0010
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0229
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 243.235
Elapsed time for transformer_add_bias_dropout (2048x4x9216): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9216): 0.0004

Attention duration (in seconds): 0.0996
Attention throughput (in TFLOP/s): 62.066
MLP duration (in seconds): 0.0455
MLP throughput (in TFLOP/s): 244.676
Transformer duration (in seconds): 0.1473
Transformer throughput (in TFLOP/s): 117.602
Transformer - MLP - Attention (in seconds): 0.0021


Actual
------
Attention duration (in seconds): 0.0384
Attention throughput (in TFLOP/s): 161.010
MLP duration (in seconds): 0.0462
MLP throughput (in TFLOP/s): 240.707
Transformer duration (in seconds): 0.0876
Transformer throughput (in TFLOP/s): 197.770
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 253.814
Elapsed time for attention_key_query_prob (512x2048x73x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x73x2048): 28.492
Elapsed time for attention_prob_times_values (512x2048x2048x73): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x73): 45.910
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 256.243
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 259.906
Elapsed time for mlp_fused_gelu (2048x4x37376): 0.0010
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0235
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 243.508
Elapsed time for transformer_add_bias_dropout (2048x4x9344): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9344): 0.0004

Attention duration (in seconds): 0.1101
Attention throughput (in TFLOP/s): 57.657
MLP duration (in seconds): 0.0466
MLP throughput (in TFLOP/s): 245.810
Transformer duration (in seconds): 0.1588
Transformer throughput (in TFLOP/s): 112.031
Transformer - MLP - Attention (in seconds): 0.0021


Actual
------
Attention duration (in seconds): 0.0541
Attention throughput (in TFLOP/s): 117.440
MLP duration (in seconds): 0.0475
MLP throughput (in TFLOP/s): 240.877
Transformer duration (in seconds): 0.1056
Transformer throughput (in TFLOP/s): 168.567
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 249.528
Elapsed time for attention_key_query_prob (512x2048x74x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x74x2048): 51.693
Elapsed time for attention_prob_times_values (512x2048x2048x74): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x74): 77.337
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0060
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 245.380
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0235
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 249.744
Elapsed time for mlp_fused_gelu (2048x4x37888): 0.0011
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 242.612
Elapsed time for transformer_add_bias_dropout (2048x4x9472): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9472): 0.0004

Attention duration (in seconds): 0.1037
Attention throughput (in TFLOP/s): 62.816
MLP duration (in seconds): 0.0488
MLP throughput (in TFLOP/s): 240.809
Transformer duration (in seconds): 0.1547
Transformer throughput (in TFLOP/s): 118.117
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0423
Attention throughput (in TFLOP/s): 154.165
MLP duration (in seconds): 0.0495
MLP throughput (in TFLOP/s): 237.744
Transformer duration (in seconds): 0.0949
Transformer throughput (in TFLOP/s): 192.497
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 249.689
Elapsed time for attention_key_query_prob (512x2048x75x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x75x2048): 29.940
Elapsed time for attention_prob_times_values (512x2048x2048x75): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x75): 47.423
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 244.421
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0242
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 250.089
Elapsed time for mlp_fused_gelu (2048x4x38400): 0.0011
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 242.363
Elapsed time for transformer_add_bias_dropout (2048x4x9600): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9600): 0.0004

Attention duration (in seconds): 0.1117
Attention throughput (in TFLOP/s): 59.857
MLP duration (in seconds): 0.0501
MLP throughput (in TFLOP/s): 240.919
Transformer duration (in seconds): 0.1640
Transformer throughput (in TFLOP/s): 114.406
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0560
Attention throughput (in TFLOP/s): 119.367
MLP duration (in seconds): 0.0508
MLP throughput (in TFLOP/s): 237.775
Transformer duration (in seconds): 0.1106
Transformer throughput (in TFLOP/s): 169.592
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 249.876
Elapsed time for attention_key_query_prob (512x2048x76x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x76x2048): 52.785
Elapsed time for attention_prob_times_values (512x2048x2048x76): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x76): 79.345
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 245.265
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 250.354
Elapsed time for mlp_fused_gelu (2048x4x38912): 0.0011
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0243
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 255.398
Elapsed time for transformer_add_bias_dropout (2048x4x9728): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9728): 0.0004

Attention duration (in seconds): 0.1050
Attention throughput (in TFLOP/s): 65.269
MLP duration (in seconds): 0.0501
MLP throughput (in TFLOP/s): 247.391
Transformer duration (in seconds): 0.1574
Transformer throughput (in TFLOP/s): 122.366
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0434
Attention throughput (in TFLOP/s): 157.764
MLP duration (in seconds): 0.0505
MLP throughput (in TFLOP/s): 245.525
Transformer duration (in seconds): 0.0969
Transformer throughput (in TFLOP/s): 198.735
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0191
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 250.283
Elapsed time for attention_key_query_prob (512x2048x77x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x77x2048): 30.088
Elapsed time for attention_prob_times_values (512x2048x2048x77): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x77): 48.619
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 245.611
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 250.460
Elapsed time for mlp_fused_gelu (2048x4x39424): 0.0011
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 243.759
Elapsed time for transformer_add_bias_dropout (2048x4x9856): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9856): 0.0004

Attention duration (in seconds): 0.1131
Attention throughput (in TFLOP/s): 62.111
MLP duration (in seconds): 0.0526
MLP throughput (in TFLOP/s): 241.915
Transformer duration (in seconds): 0.1681
Transformer throughput (in TFLOP/s): 117.567
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0574
Attention throughput (in TFLOP/s): 122.501
MLP duration (in seconds): 0.0535
MLP throughput (in TFLOP/s): 237.848
Transformer duration (in seconds): 0.1146
Transformer throughput (in TFLOP/s): 172.480
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 250.459
Elapsed time for attention_key_query_prob (512x2048x78x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x78x2048): 54.183
Elapsed time for attention_prob_times_values (512x2048x2048x78): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x78): 81.207
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 244.476
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 250.571
Elapsed time for mlp_fused_gelu (2048x4x39936): 0.0011
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0256
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 255.137
Elapsed time for transformer_add_bias_dropout (2048x4x9984): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9984): 0.0004

Attention duration (in seconds): 0.1063
Attention throughput (in TFLOP/s): 67.732
MLP duration (in seconds): 0.0528
MLP throughput (in TFLOP/s): 247.513
Transformer duration (in seconds): 0.1614
Transformer throughput (in TFLOP/s): 125.566
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0449
Attention throughput (in TFLOP/s): 160.551
MLP duration (in seconds): 0.0535
MLP throughput (in TFLOP/s): 244.021
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 199.985
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 250.392
Elapsed time for attention_key_query_prob (512x2048x79x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x79x2048): 30.197
Elapsed time for attention_prob_times_values (512x2048x2048x79): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x79): 49.514
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 246.777
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0267
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 250.812
Elapsed time for mlp_fused_gelu (2048x4x40448): 0.0011
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0277
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 241.570
Elapsed time for transformer_add_bias_dropout (2048x4x10112): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10112): 0.0004

Attention duration (in seconds): 0.1147
Attention throughput (in TFLOP/s): 64.318
MLP duration (in seconds): 0.0556
MLP throughput (in TFLOP/s): 241.124
Transformer duration (in seconds): 0.1726
Transformer throughput (in TFLOP/s): 120.384
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0592
Attention throughput (in TFLOP/s): 124.752
MLP duration (in seconds): 0.0566
MLP throughput (in TFLOP/s): 236.798
Transformer duration (in seconds): 0.1197
Transformer throughput (in TFLOP/s): 173.553
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 250.914
Elapsed time for attention_key_query_prob (512x2048x80x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x80x2048): 77.862
Elapsed time for attention_prob_times_values (512x2048x2048x80): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x80): 94.144
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 245.888
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0274
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 251.070
Elapsed time for mlp_fused_gelu (2048x4x40960): 0.0011
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0271
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 253.470
Elapsed time for transformer_add_bias_dropout (2048x4x10240): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10240): 0.0004

Attention duration (in seconds): 0.1054
Attention throughput (in TFLOP/s): 71.735
MLP duration (in seconds): 0.0556
MLP throughput (in TFLOP/s): 247.095
Transformer duration (in seconds): 0.1634
Transformer throughput (in TFLOP/s): 130.411
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 172.072
MLP duration (in seconds): 0.0562
MLP throughput (in TFLOP/s): 244.767
Transformer duration (in seconds): 0.1028
Transformer throughput (in TFLOP/s): 207.152
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 251.097
Elapsed time for attention_key_query_prob (512x2048x81x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x81x2048): 30.675
Elapsed time for attention_prob_times_values (512x2048x2048x81): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x81): 50.590
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 247.215
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0283
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 249.189
Elapsed time for mlp_fused_gelu (2048x4x41472): 0.0012
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0290
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 242.964
Elapsed time for transformer_add_bias_dropout (2048x4x10368): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10368): 0.0004

Attention duration (in seconds): 0.1162
Attention throughput (in TFLOP/s): 66.632
MLP duration (in seconds): 0.0584
MLP throughput (in TFLOP/s): 241.176
Transformer duration (in seconds): 0.1770
Transformer throughput (in TFLOP/s): 123.349
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0605
Attention throughput (in TFLOP/s): 127.853
MLP duration (in seconds): 0.0592
MLP throughput (in TFLOP/s): 238.177
Transformer duration (in seconds): 0.1232
Transformer throughput (in TFLOP/s): 177.161
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 251.227
Elapsed time for attention_key_query_prob (512x2048x82x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x82x2048): 56.712
Elapsed time for attention_prob_times_values (512x2048x2048x82): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x82): 85.027
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 246.863
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0287
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 251.363
Elapsed time for mlp_fused_gelu (2048x4x41984): 0.0012
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 243.293
Elapsed time for transformer_add_bias_dropout (2048x4x10496): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10496): 0.0004

Attention duration (in seconds): 0.1090
Attention throughput (in TFLOP/s): 72.699
MLP duration (in seconds): 0.0596
MLP throughput (in TFLOP/s): 242.422
Transformer duration (in seconds): 0.1710
Transformer throughput (in TFLOP/s): 130.813
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 165.794
MLP duration (in seconds): 0.0607
MLP throughput (in TFLOP/s): 238.051
Transformer duration (in seconds): 0.1119
Transformer throughput (in TFLOP/s): 199.785
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 251.203
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 31.385
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 52.019
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 245.842
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0297
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 248.969
Elapsed time for mlp_fused_gelu (2048x4x42496): 0.0012
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 242.920
Elapsed time for transformer_add_bias_dropout (2048x4x10624): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10624): 0.0004

Attention duration (in seconds): 0.1176
Attention throughput (in TFLOP/s): 68.953
MLP duration (in seconds): 0.0613
MLP throughput (in TFLOP/s): 241.177
Transformer duration (in seconds): 0.1814
Transformer throughput (in TFLOP/s): 126.244
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0611
Attention throughput (in TFLOP/s): 132.628
MLP duration (in seconds): 0.0621
MLP throughput (in TFLOP/s): 238.255
Transformer duration (in seconds): 0.1283
Transformer throughput (in TFLOP/s): 178.546
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 251.462
Elapsed time for attention_key_query_prob (512x2048x84x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x84x2048): 57.855
Elapsed time for attention_prob_times_values (512x2048x2048x84): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x84): 87.041
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 247.943
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 249.147
Elapsed time for mlp_fused_gelu (2048x4x43008): 0.0012
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0299
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 253.710
Elapsed time for transformer_add_bias_dropout (2048x4x10752): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10752): 0.0004

Attention duration (in seconds): 0.1104
Attention throughput (in TFLOP/s): 75.161
MLP duration (in seconds): 0.0615
MLP throughput (in TFLOP/s): 246.522
Transformer duration (in seconds): 0.1743
Transformer throughput (in TFLOP/s): 134.515
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 169.079
MLP duration (in seconds): 0.0619
MLP throughput (in TFLOP/s): 244.677
Transformer duration (in seconds): 0.1141
Transformer throughput (in TFLOP/s): 205.558
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 249.843
Elapsed time for attention_key_query_prob (512x2048x85x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x85x2048): 31.800
Elapsed time for attention_prob_times_values (512x2048x2048x85): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x85): 53.214
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 246.751
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0311
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 249.565
Elapsed time for mlp_fused_gelu (2048x4x43520): 0.0012
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0319
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 242.973
Elapsed time for transformer_add_bias_dropout (2048x4x10880): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10880): 0.0004

Attention duration (in seconds): 0.1193
Attention throughput (in TFLOP/s): 71.162
MLP duration (in seconds): 0.0642
MLP throughput (in TFLOP/s): 241.593
Transformer duration (in seconds): 0.1860
Transformer throughput (in TFLOP/s): 129.057
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0634
Attention throughput (in TFLOP/s): 133.916
MLP duration (in seconds): 0.0652
MLP throughput (in TFLOP/s): 238.034
Transformer duration (in seconds): 0.1331
Transformer throughput (in TFLOP/s): 180.340
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0239
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 249.652
Elapsed time for attention_key_query_prob (512x2048x86x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x86x2048): 59.353
Elapsed time for attention_prob_times_values (512x2048x2048x86): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x86): 88.965
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 248.431
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0318
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 249.383
Elapsed time for mlp_fused_gelu (2048x4x44032): 0.0012
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0313
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 253.562
Elapsed time for transformer_add_bias_dropout (2048x4x11008): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11008): 0.0004

Attention duration (in seconds): 0.1120
Attention throughput (in TFLOP/s): 77.488
MLP duration (in seconds): 0.0644
MLP throughput (in TFLOP/s): 246.679
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 137.279
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0505
Attention throughput (in TFLOP/s): 172.051
MLP duration (in seconds): 0.0648
MLP throughput (in TFLOP/s): 245.023
Transformer duration (in seconds): 0.1185
Transformer throughput (in TFLOP/s): 207.346
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 251.994
Elapsed time for attention_key_query_prob (512x2048x87x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x87x2048): 32.029
Elapsed time for attention_prob_times_values (512x2048x2048x87): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x87): 54.112
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 248.012
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0326
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 249.338
Elapsed time for mlp_fused_gelu (2048x4x44544): 0.0012
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0335
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 242.462
Elapsed time for transformer_add_bias_dropout (2048x4x11136): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11136): 0.0005

Attention duration (in seconds): 0.1207
Attention throughput (in TFLOP/s): 73.496
MLP duration (in seconds): 0.0674
MLP throughput (in TFLOP/s): 241.335
Transformer duration (in seconds): 0.1907
Transformer throughput (in TFLOP/s): 131.795
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0647
Attention throughput (in TFLOP/s): 137.178
MLP duration (in seconds): 0.0683
MLP throughput (in TFLOP/s): 237.834
Transformer duration (in seconds): 0.1373
Transformer throughput (in TFLOP/s): 183.022
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 249.686
Elapsed time for attention_key_query_prob (512x2048x88x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x88x2048): 76.598
Elapsed time for attention_prob_times_values (512x2048x2048x88): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x88): 102.242
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 244.386
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0333
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 249.479
Elapsed time for mlp_fused_gelu (2048x4x45056): 0.0013
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0329
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 252.933
Elapsed time for transformer_add_bias_dropout (2048x4x11264): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11264): 0.0005

Attention duration (in seconds): 0.1119
Attention throughput (in TFLOP/s): 81.062
MLP duration (in seconds): 0.0675
MLP throughput (in TFLOP/s): 246.534
Transformer duration (in seconds): 0.1820
Transformer throughput (in TFLOP/s): 141.252
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0502
Attention throughput (in TFLOP/s): 180.546
MLP duration (in seconds): 0.0679
MLP throughput (in TFLOP/s): 244.886
Transformer duration (in seconds): 0.1215
Transformer throughput (in TFLOP/s): 211.534
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 249.545
Elapsed time for attention_key_query_prob (512x2048x89x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x89x2048): 32.599
Elapsed time for attention_prob_times_values (512x2048x2048x89): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x89): 55.236
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 245.824
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0341
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 249.641
Elapsed time for mlp_fused_gelu (2048x4x45568): 0.0013
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0338
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 251.733
Elapsed time for transformer_add_bias_dropout (2048x4x11392): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11392): 0.0005

Attention duration (in seconds): 0.1226
Attention throughput (in TFLOP/s): 75.582
MLP duration (in seconds): 0.0691
MLP throughput (in TFLOP/s): 246.100
Transformer duration (in seconds): 0.1944
Transformer throughput (in TFLOP/s): 135.199
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0664
Attention throughput (in TFLOP/s): 139.545
MLP duration (in seconds): 0.0696
MLP throughput (in TFLOP/s): 244.464
Transformer duration (in seconds): 0.1392
Transformer throughput (in TFLOP/s): 188.818
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0259
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 251.972
Elapsed time for attention_key_query_prob (512x2048x90x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x90x2048): 61.929
Elapsed time for attention_prob_times_values (512x2048x2048x90): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x90): 92.773
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 247.948
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0348
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 249.945
Elapsed time for mlp_fused_gelu (2048x4x46080): 0.0013
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 243.199
Elapsed time for transformer_add_bias_dropout (2048x4x11520): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11520): 0.0005

Attention duration (in seconds): 0.1149
Attention throughput (in TFLOP/s): 82.456
MLP duration (in seconds): 0.0718
MLP throughput (in TFLOP/s): 242.139
Transformer duration (in seconds): 0.1893
Transformer throughput (in TFLOP/s): 141.894
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0538
Attention throughput (in TFLOP/s): 176.036
MLP duration (in seconds): 0.0729
MLP throughput (in TFLOP/s): 238.760
Transformer duration (in seconds): 0.1307
Transformer throughput (in TFLOP/s): 205.475
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 250.065
Elapsed time for attention_key_query_prob (512x2048x91x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x91x2048): 33.285
Elapsed time for attention_prob_times_values (512x2048x2048x91): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x91): 56.804
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 246.721
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0356
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 250.019
Elapsed time for mlp_fused_gelu (2048x4x46592): 0.0013
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0365
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 243.792
Elapsed time for transformer_add_bias_dropout (2048x4x11648): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11648): 0.0005

Attention duration (in seconds): 0.1241
Attention throughput (in TFLOP/s): 77.957
MLP duration (in seconds): 0.0733
MLP throughput (in TFLOP/s): 242.511
Transformer duration (in seconds): 0.2001
Transformer throughput (in TFLOP/s): 137.223
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0678
Attention throughput (in TFLOP/s): 142.741
MLP duration (in seconds): 0.0747
MLP throughput (in TFLOP/s): 238.052
Transformer duration (in seconds): 0.1467
Transformer throughput (in TFLOP/s): 187.101
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0273
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 250.073
Elapsed time for attention_key_query_prob (512x2048x92x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x92x2048): 63.035
Elapsed time for attention_prob_times_values (512x2048x2048x92): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x92): 94.612
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 245.881
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0363
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 250.116
Elapsed time for mlp_fused_gelu (2048x4x47104): 0.0013
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0357
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 254.534
Elapsed time for transformer_add_bias_dropout (2048x4x11776): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11776): 0.0005

Attention duration (in seconds): 0.1167
Attention throughput (in TFLOP/s): 84.628
MLP duration (in seconds): 0.0733
MLP throughput (in TFLOP/s): 247.808
Transformer duration (in seconds): 0.1928
Transformer throughput (in TFLOP/s): 145.525
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0552
Attention throughput (in TFLOP/s): 178.893
MLP duration (in seconds): 0.0741
MLP throughput (in TFLOP/s): 245.249
Transformer duration (in seconds): 0.1325
Transformer throughput (in TFLOP/s): 211.684
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0278
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 250.231
Elapsed time for attention_key_query_prob (512x2048x93x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x93x2048): 33.923
Elapsed time for attention_prob_times_values (512x2048x2048x93): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x93): 58.049
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 245.182
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0371
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 250.313
Elapsed time for mlp_fused_gelu (2048x4x47616): 0.0013
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0384
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 241.547
Elapsed time for transformer_add_bias_dropout (2048x4x11904): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11904): 0.0005

Attention duration (in seconds): 0.1257
Attention throughput (in TFLOP/s): 80.207
MLP duration (in seconds): 0.0769
MLP throughput (in TFLOP/s): 241.632
Transformer duration (in seconds): 0.2053
Transformer throughput (in TFLOP/s): 139.570
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0696
Attention throughput (in TFLOP/s): 144.837
MLP duration (in seconds): 0.0780
MLP throughput (in TFLOP/s): 238.182
Transformer duration (in seconds): 0.1523
Transformer throughput (in TFLOP/s): 188.184
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 255.119
Elapsed time for attention_key_query_prob (512x2048x94x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x94x2048): 64.474
Elapsed time for attention_prob_times_values (512x2048x2048x94): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x94): 96.438
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 260.165
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0372
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 255.377
Elapsed time for mlp_fused_gelu (2048x4x48128): 0.0013
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0373
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 254.259
Elapsed time for transformer_add_bias_dropout (2048x4x12032): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12032): 0.0005

Attention duration (in seconds): 0.1172
Attention throughput (in TFLOP/s): 87.810
MLP duration (in seconds): 0.0758
MLP throughput (in TFLOP/s): 250.329
Transformer duration (in seconds): 0.1958
Transformer throughput (in TFLOP/s): 149.485
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0564
Attention throughput (in TFLOP/s): 182.479
MLP duration (in seconds): 0.0766
MLP throughput (in TFLOP/s): 247.747
Transformer duration (in seconds): 0.1364
Transformer throughput (in TFLOP/s): 214.624
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 254.904
Elapsed time for attention_key_query_prob (512x2048x95x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x95x2048): 35.382
Elapsed time for attention_prob_times_values (512x2048x2048x95): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x95): 58.824
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 250.808
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0379
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 255.840
Elapsed time for mlp_fused_gelu (2048x4x48640): 0.0013
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0403
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 240.608
Elapsed time for transformer_add_bias_dropout (2048x4x12160): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12160): 0.0005

Attention duration (in seconds): 0.1264
Attention throughput (in TFLOP/s): 83.104
MLP duration (in seconds): 0.0795
MLP throughput (in TFLOP/s): 243.780
Transformer duration (in seconds): 0.2087
Transformer throughput (in TFLOP/s): 143.193
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0714
Attention throughput (in TFLOP/s): 147.203
MLP duration (in seconds): 0.0805
MLP throughput (in TFLOP/s): 240.883
Transformer duration (in seconds): 0.1560
Transformer throughput (in TFLOP/s): 191.532
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 254.666
Elapsed time for attention_key_query_prob (512x2048x96x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x96x2048): 93.871
Elapsed time for attention_prob_times_values (512x2048x2048x96): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x96): 111.315
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 257.062
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0386
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 256.127
Elapsed time for mlp_fused_gelu (2048x4x49152): 0.0014
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0391
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 253.205
Elapsed time for transformer_add_bias_dropout (2048x4x12288): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12288): 0.0005

Attention duration (in seconds): 0.1167
Attention throughput (in TFLOP/s): 91.901
MLP duration (in seconds): 0.0791
MLP throughput (in TFLOP/s): 250.269
Transformer duration (in seconds): 0.1986
Transformer throughput (in TFLOP/s): 153.669
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0559
Attention throughput (in TFLOP/s): 191.804
MLP duration (in seconds): 0.0801
MLP throughput (in TFLOP/s): 247.230
Transformer duration (in seconds): 0.1393
Transformer throughput (in TFLOP/s): 219.079
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 254.368
Elapsed time for attention_key_query_prob (512x2048x97x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x97x2048): 32.098
Elapsed time for attention_prob_times_values (512x2048x2048x97): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x97): 59.818
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 259.558
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0391
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 258.541
Elapsed time for mlp_fused_gelu (2048x4x49664): 0.0014
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0416
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 242.743
Elapsed time for transformer_add_bias_dropout (2048x4x12416): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12416): 0.0005

Attention duration (in seconds): 0.1292
Attention throughput (in TFLOP/s): 84.612
MLP duration (in seconds): 0.0821
MLP throughput (in TFLOP/s): 246.196
Transformer duration (in seconds): 0.2142
Transformer throughput (in TFLOP/s): 145.403
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0748
Attention throughput (in TFLOP/s): 146.254
MLP duration (in seconds): 0.0836
MLP throughput (in TFLOP/s): 241.766
Transformer duration (in seconds): 0.1622
Transformer throughput (in TFLOP/s): 192.015
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 256.132
Elapsed time for attention_key_query_prob (512x2048x98x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x98x2048): 60.470
Elapsed time for attention_prob_times_values (512x2048x2048x98): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x98): 100.086
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 252.166
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0402
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 256.524
Elapsed time for mlp_fused_gelu (2048x4x50176): 0.0014
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0404
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 255.269
Elapsed time for transformer_add_bias_dropout (2048x4x12544): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12544): 0.0005

Attention duration (in seconds): 0.1214
Attention throughput (in TFLOP/s): 91.902
MLP duration (in seconds): 0.0820
MLP throughput (in TFLOP/s): 251.553
Transformer duration (in seconds): 0.2062
Transformer throughput (in TFLOP/s): 154.092
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0609
Attention throughput (in TFLOP/s): 183.043
MLP duration (in seconds): 0.0830
MLP throughput (in TFLOP/s): 248.523
Transformer duration (in seconds): 0.1472
Transformer throughput (in TFLOP/s): 215.904
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0307
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 256.844
Elapsed time for attention_key_query_prob (512x2048x99x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x99x2048): 31.475
Elapsed time for attention_prob_times_values (512x2048x2048x99): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x99): 61.531
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 256.495
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 257.273
Elapsed time for mlp_fused_gelu (2048x4x50688): 0.0014
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 242.877
Elapsed time for transformer_add_bias_dropout (2048x4x12672): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12672): 0.0005

Attention duration (in seconds): 0.1312
Attention throughput (in TFLOP/s): 86.696
MLP duration (in seconds): 0.0856
MLP throughput (in TFLOP/s): 245.771
Transformer duration (in seconds): 0.2197
Transformer throughput (in TFLOP/s): 147.547
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 147.141
MLP duration (in seconds): 0.0872
MLP throughput (in TFLOP/s): 241.349
Transformer duration (in seconds): 0.1688
Transformer throughput (in TFLOP/s): 192.026
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 255.977
Elapsed time for attention_key_query_prob (512x2048x100x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x100x2048): 61.451
Elapsed time for attention_prob_times_values (512x2048x2048x100): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x100): 102.030
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 258.941
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0418
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 256.949
Elapsed time for mlp_fused_gelu (2048x4x51200): 0.0014
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0425
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 252.788
Elapsed time for transformer_add_bias_dropout (2048x4x12800): 0.0010
Elapsed time for transformer_layer_norm (2048x4x12800): 0.0005

Attention duration (in seconds): 0.1228
Attention throughput (in TFLOP/s): 94.425
MLP duration (in seconds): 0.0857
MLP throughput (in TFLOP/s): 250.631
Transformer duration (in seconds): 0.2114
Transformer throughput (in TFLOP/s): 156.420
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0624
Attention throughput (in TFLOP/s): 185.864
MLP duration (in seconds): 0.0864
MLP throughput (in TFLOP/s): 248.528
Transformer duration (in seconds): 0.1519
Transformer throughput (in TFLOP/s): 217.737
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0320
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 256.317
Elapsed time for attention_key_query_prob (512x2048x101x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x101x2048): 31.447
Elapsed time for attention_prob_times_values (512x2048x2048x101): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x101): 62.720
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 261.042
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0427
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 256.443
Elapsed time for mlp_fused_gelu (2048x4x51712): 0.0014
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 251.398
Elapsed time for transformer_add_bias_dropout (2048x4x12928): 0.0010
Elapsed time for transformer_layer_norm (2048x4x12928): 0.0005

Attention duration (in seconds): 0.1330
Attention throughput (in TFLOP/s): 88.850
MLP duration (in seconds): 0.0877
MLP throughput (in TFLOP/s): 249.752
Transformer duration (in seconds): 0.2237
Transformer throughput (in TFLOP/s): 150.757
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0795
Attention throughput (in TFLOP/s): 148.623
MLP duration (in seconds): 0.0885
MLP throughput (in TFLOP/s): 247.462
Transformer duration (in seconds): 0.1713
Transformer throughput (in TFLOP/s): 196.930
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 255.903
Elapsed time for attention_key_query_prob (512x2048x102x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x102x2048): 62.482
Elapsed time for attention_prob_times_values (512x2048x2048x102): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x102): 104.012
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.018
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 256.517
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0014
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 254.735
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.1247
Attention throughput (in TFLOP/s): 96.638
MLP duration (in seconds): 0.0889
MLP throughput (in TFLOP/s): 251.457
Transformer duration (in seconds): 0.2165
Transformer throughput (in TFLOP/s): 158.834
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0644
Attention throughput (in TFLOP/s): 187.145
MLP duration (in seconds): 0.0899
MLP throughput (in TFLOP/s): 248.411
Transformer duration (in seconds): 0.1580
Transformer throughput (in TFLOP/s): 217.660
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 255.702
Elapsed time for attention_key_query_prob (512x2048x103x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x103x2048): 31.092
Elapsed time for attention_prob_times_values (512x2048x2048x103): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x103): 62.998
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 258.325
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0444
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 256.578
Elapsed time for mlp_fused_gelu (2048x4x52736): 0.0015
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0452
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 251.971
Elapsed time for transformer_add_bias_dropout (2048x4x13184): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13184): 0.0005

Attention duration (in seconds): 0.1355
Attention throughput (in TFLOP/s): 90.615
MLP duration (in seconds): 0.0911
MLP throughput (in TFLOP/s): 250.172
Transformer duration (in seconds): 0.2296
Transformer throughput (in TFLOP/s): 152.718
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0820
Attention throughput (in TFLOP/s): 149.723
MLP duration (in seconds): 0.0922
MLP throughput (in TFLOP/s): 247.007
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 196.532
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 255.493
Elapsed time for attention_key_query_prob (512x2048x104x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x104x2048): 84.632
Elapsed time for attention_prob_times_values (512x2048x2048x104): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x104): 118.719
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 260.771
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0452
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 257.017
Elapsed time for mlp_fused_gelu (2048x4x53248): 0.0015
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0456
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 254.714
Elapsed time for transformer_add_bias_dropout (2048x4x13312): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13312): 0.0005

Attention duration (in seconds): 0.1241
Attention throughput (in TFLOP/s): 100.819
MLP duration (in seconds): 0.0923
MLP throughput (in TFLOP/s): 251.771
Transformer duration (in seconds): 0.2194
Transformer throughput (in TFLOP/s): 162.901
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0645
Attention throughput (in TFLOP/s): 194.003
MLP duration (in seconds): 0.0936
MLP throughput (in TFLOP/s): 248.141
Transformer duration (in seconds): 0.1619
Transformer throughput (in TFLOP/s): 220.739
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0348
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 255.230
Elapsed time for attention_key_query_prob (512x2048x105x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x105x2048): 31.476
Elapsed time for attention_prob_times_values (512x2048x2048x105): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x105): 64.102
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 255.576
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 255.976
Elapsed time for mlp_fused_gelu (2048x4x53760): 0.0015
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0469
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 252.507
Elapsed time for transformer_add_bias_dropout (2048x4x13440): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13440): 0.0005

Attention duration (in seconds): 0.1375
Attention throughput (in TFLOP/s): 92.645
MLP duration (in seconds): 0.0946
MLP throughput (in TFLOP/s): 250.230
Transformer duration (in seconds): 0.2352
Transformer throughput (in TFLOP/s): 154.827
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0846
Attention throughput (in TFLOP/s): 150.625
MLP duration (in seconds): 0.0958
MLP throughput (in TFLOP/s): 247.139
Transformer duration (in seconds): 0.1841
Transformer throughput (in TFLOP/s): 197.766
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 254.994
Elapsed time for attention_key_query_prob (512x2048x106x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x106x2048): 64.594
Elapsed time for attention_prob_times_values (512x2048x2048x106): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x106): 107.962
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 257.797
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 256.328
Elapsed time for mlp_fused_gelu (2048x4x54272): 0.0015
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0476
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 253.215
Elapsed time for transformer_add_bias_dropout (2048x4x13568): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13568): 0.0005

Attention duration (in seconds): 0.1282
Attention throughput (in TFLOP/s): 101.180
MLP duration (in seconds): 0.0962
MLP throughput (in TFLOP/s): 250.782
Transformer duration (in seconds): 0.2276
Transformer throughput (in TFLOP/s): 163.047
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 188.916
MLP duration (in seconds): 0.0972
MLP throughput (in TFLOP/s): 248.246
Transformer duration (in seconds): 0.1694
Transformer throughput (in TFLOP/s): 218.986
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0359
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 256.792
Elapsed time for attention_key_query_prob (512x2048x107x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x107x2048): 32.060
Elapsed time for attention_prob_times_values (512x2048x2048x107): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x107): 66.218
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 260.092
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0479
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 256.743
Elapsed time for mlp_fused_gelu (2048x4x54784): 0.0015
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 253.442
Elapsed time for transformer_add_bias_dropout (2048x4x13696): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13696): 0.0006

Attention duration (in seconds): 0.1388
Attention throughput (in TFLOP/s): 95.199
MLP duration (in seconds): 0.0979
MLP throughput (in TFLOP/s): 251.129
Transformer duration (in seconds): 0.2398
Transformer throughput (in TFLOP/s): 157.610
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0861
Attention throughput (in TFLOP/s): 153.406
MLP duration (in seconds): 0.0992
MLP throughput (in TFLOP/s): 247.861
Transformer duration (in seconds): 0.1893
Transformer throughput (in TFLOP/s): 199.646
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 256.644
Elapsed time for attention_key_query_prob (512x2048x108x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x108x2048): 65.569
Elapsed time for attention_prob_times_values (512x2048x2048x108): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x108): 109.770
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 261.927
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0487
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 257.236
Elapsed time for mlp_fused_gelu (2048x4x55296): 0.0015
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0490
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 255.823
Elapsed time for transformer_add_bias_dropout (2048x4x13824): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13824): 0.0006

Attention duration (in seconds): 0.1297
Attention throughput (in TFLOP/s): 103.744
MLP duration (in seconds): 0.0992
MLP throughput (in TFLOP/s): 252.552
Transformer duration (in seconds): 0.2320
Transformer throughput (in TFLOP/s): 165.943
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0696
Attention throughput (in TFLOP/s): 193.266
MLP duration (in seconds): 0.1003
MLP throughput (in TFLOP/s): 249.638
Transformer duration (in seconds): 0.1742
Transformer throughput (in TFLOP/s): 221.002
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 256.754
Elapsed time for attention_key_query_prob (512x2048x109x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x109x2048): 32.608
Elapsed time for attention_prob_times_values (512x2048x2048x109): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x109): 67.319
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 257.143
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0498
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 256.117
Elapsed time for mlp_fused_gelu (2048x4x55808): 0.0015
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0505
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 252.367
Elapsed time for transformer_add_bias_dropout (2048x4x13952): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13952): 0.0006

Attention duration (in seconds): 0.1408
Attention throughput (in TFLOP/s): 97.266
MLP duration (in seconds): 0.1019
MLP throughput (in TFLOP/s): 250.362
Transformer duration (in seconds): 0.2459
Transformer throughput (in TFLOP/s): 159.439
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0884
Attention throughput (in TFLOP/s): 154.873
MLP duration (in seconds): 0.1032
MLP throughput (in TFLOP/s): 247.259
Transformer duration (in seconds): 0.1951
Transformer throughput (in TFLOP/s): 200.957
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 256.622
Elapsed time for attention_key_query_prob (512x2048x110x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x110x2048): 66.577
Elapsed time for attention_prob_times_values (512x2048x2048x110): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x110): 111.519
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 259.454
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0507
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 256.073
Elapsed time for mlp_fused_gelu (2048x4x56320): 0.0016
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0514
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 252.568
Elapsed time for transformer_add_bias_dropout (2048x4x14080): 0.0010
Elapsed time for transformer_layer_norm (2048x4x14080): 0.0006

Attention duration (in seconds): 0.1316
Attention throughput (in TFLOP/s): 105.885
MLP duration (in seconds): 0.1037
MLP throughput (in TFLOP/s): 250.476
Transformer duration (in seconds): 0.2386
Transformer throughput (in TFLOP/s): 167.305
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0719
Attention throughput (in TFLOP/s): 193.762
MLP duration (in seconds): 0.1048
MLP throughput (in TFLOP/s): 247.877
Transformer duration (in seconds): 0.1812
Transformer throughput (in TFLOP/s): 220.287
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0387
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 256.409
Elapsed time for attention_key_query_prob (512x2048x111x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x111x2048): 32.914
Elapsed time for attention_prob_times_values (512x2048x2048x111): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x111): 67.515
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 261.508
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0516
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 256.488
Elapsed time for mlp_fused_gelu (2048x4x56832): 0.0016
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0527
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 250.973
Elapsed time for transformer_add_bias_dropout (2048x4x14208): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14208): 0.0006

Attention duration (in seconds): 0.1427
Attention throughput (in TFLOP/s): 99.410
MLP duration (in seconds): 0.1059
MLP throughput (in TFLOP/s): 249.932
Transformer duration (in seconds): 0.2518
Transformer throughput (in TFLOP/s): 161.415
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0903
Attention throughput (in TFLOP/s): 156.983
MLP duration (in seconds): 0.1071
MLP throughput (in TFLOP/s): 247.101
Transformer duration (in seconds): 0.2026
Transformer throughput (in TFLOP/s): 200.572
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 256.203
Elapsed time for attention_key_query_prob (512x2048x112x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x112x2048): 96.036
Elapsed time for attention_prob_times_values (512x2048x2048x112): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x112): 127.270
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 256.899
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0524
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 256.863
Elapsed time for mlp_fused_gelu (2048x4x57344): 0.0016
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0530
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 253.945
Elapsed time for transformer_add_bias_dropout (2048x4x14336): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14336): 0.0006

Attention duration (in seconds): 0.1311
Attention throughput (in TFLOP/s): 110.072
MLP duration (in seconds): 0.1071
MLP throughput (in TFLOP/s): 251.609
Transformer duration (in seconds): 0.2414
Transformer throughput (in TFLOP/s): 171.337
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0724
Attention throughput (in TFLOP/s): 199.443
MLP duration (in seconds): 0.1083
MLP throughput (in TFLOP/s): 248.782
Transformer duration (in seconds): 0.1852
Transformer throughput (in TFLOP/s): 223.433
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0402
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 255.968
Elapsed time for attention_key_query_prob (512x2048x113x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x113x2048): 33.582
Elapsed time for attention_prob_times_values (512x2048x2048x113): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x113): 68.637
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 259.047
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 257.203
Elapsed time for mlp_fused_gelu (2048x4x57856): 0.0016
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0547
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 250.774
Elapsed time for transformer_add_bias_dropout (2048x4x14464): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14464): 0.0006

Attention duration (in seconds): 0.1447
Attention throughput (in TFLOP/s): 101.446
MLP duration (in seconds): 0.1096
MLP throughput (in TFLOP/s): 250.240
Transformer duration (in seconds): 0.2576
Transformer throughput (in TFLOP/s): 163.438
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0922
Attention throughput (in TFLOP/s): 159.248
MLP duration (in seconds): 0.1111
MLP throughput (in TFLOP/s): 246.878
Transformer duration (in seconds): 0.2071
Transformer throughput (in TFLOP/s): 203.260
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0409
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 255.691
Elapsed time for attention_key_query_prob (512x2048x114x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x114x2048): 68.819
Elapsed time for attention_prob_times_values (512x2048x2048x114): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x114): 115.069
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 261.187
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0543
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 257.090
Elapsed time for mlp_fused_gelu (2048x4x58368): 0.0016
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0554
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 251.960
Elapsed time for transformer_add_bias_dropout (2048x4x14592): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14592): 0.0006

Attention duration (in seconds): 0.1354
Attention throughput (in TFLOP/s): 110.255
MLP duration (in seconds): 0.1113
MLP throughput (in TFLOP/s): 250.809
Transformer duration (in seconds): 0.2501
Transformer throughput (in TFLOP/s): 171.307
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0763
Attention throughput (in TFLOP/s): 195.607
MLP duration (in seconds): 0.1126
MLP throughput (in TFLOP/s): 247.875
Transformer duration (in seconds): 0.1932
Transformer throughput (in TFLOP/s): 221.731
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 255.553
Elapsed time for attention_key_query_prob (512x2048x115x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x115x2048): 34.419
Elapsed time for attention_prob_times_values (512x2048x2048x115): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x115): 70.814
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 256.530
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0554
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 256.217
Elapsed time for mlp_fused_gelu (2048x4x58880): 0.0016
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0566
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 250.974
Elapsed time for transformer_add_bias_dropout (2048x4x14720): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14720): 0.0006

Attention duration (in seconds): 0.1466
Attention throughput (in TFLOP/s): 103.581
MLP duration (in seconds): 0.1136
MLP throughput (in TFLOP/s): 249.936
Transformer duration (in seconds): 0.2636
Transformer throughput (in TFLOP/s): 165.342
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0946
Attention throughput (in TFLOP/s): 160.488
MLP duration (in seconds): 0.1150
MLP throughput (in TFLOP/s): 247.055
Transformer duration (in seconds): 0.2127
Transformer throughput (in TFLOP/s): 204.932
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 257.261
Elapsed time for attention_key_query_prob (512x2048x116x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x116x2048): 70.046
Elapsed time for attention_prob_times_values (512x2048x2048x116): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x116): 116.892
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 258.517
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 256.618
Elapsed time for mlp_fused_gelu (2048x4x59392): 0.0016
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0574
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 251.786
Elapsed time for transformer_add_bias_dropout (2048x4x14848): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14848): 0.0006

Attention duration (in seconds): 0.1373
Attention throughput (in TFLOP/s): 112.522
MLP duration (in seconds): 0.1153
MLP throughput (in TFLOP/s): 250.561
Transformer duration (in seconds): 0.2560
Transformer throughput (in TFLOP/s): 173.219
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0779
Attention throughput (in TFLOP/s): 198.276
MLP duration (in seconds): 0.1170
MLP throughput (in TFLOP/s): 247.012
Transformer duration (in seconds): 0.1986
Transformer throughput (in TFLOP/s): 223.249
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0429
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 257.010
Elapsed time for attention_key_query_prob (512x2048x117x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x117x2048): 35.270
Elapsed time for attention_prob_times_values (512x2048x2048x117): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x117): 71.930
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 260.614
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0572
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 256.906
Elapsed time for mlp_fused_gelu (2048x4x59904): 0.0017
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0585
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 251.460
Elapsed time for transformer_add_bias_dropout (2048x4x14976): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14976): 0.0006

Attention duration (in seconds): 0.1480
Attention throughput (in TFLOP/s): 106.098
MLP duration (in seconds): 0.1173
MLP throughput (in TFLOP/s): 250.568
Transformer duration (in seconds): 0.2688
Transformer throughput (in TFLOP/s): 167.812
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0967
Attention throughput (in TFLOP/s): 162.457
MLP duration (in seconds): 0.1190
MLP throughput (in TFLOP/s): 247.009
Transformer duration (in seconds): 0.2195
Transformer throughput (in TFLOP/s): 205.482
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0437
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 256.867
Elapsed time for attention_key_query_prob (512x2048x118x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x118x2048): 71.294
Elapsed time for attention_prob_times_values (512x2048x2048x118): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x118): 118.555
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 259.949
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0581
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 257.193
Elapsed time for mlp_fused_gelu (2048x4x60416): 0.0017
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0592
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 252.452
Elapsed time for transformer_add_bias_dropout (2048x4x15104): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15104): 0.0006

Attention duration (in seconds): 0.1392
Attention throughput (in TFLOP/s): 114.682
MLP duration (in seconds): 0.1190
MLP throughput (in TFLOP/s): 251.226
Transformer duration (in seconds): 0.2617
Transformer throughput (in TFLOP/s): 175.270
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0794
Attention throughput (in TFLOP/s): 200.946
MLP duration (in seconds): 0.1207
MLP throughput (in TFLOP/s): 247.791
Transformer duration (in seconds): 0.2039
Transformer throughput (in TFLOP/s): 224.995
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 256.934
Elapsed time for attention_key_query_prob (512x2048x119x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x119x2048): 34.146
Elapsed time for attention_prob_times_values (512x2048x2048x119): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x119): 71.971
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 258.050
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0592
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 257.056
Elapsed time for mlp_fused_gelu (2048x4x60928): 0.0017
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0610
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 249.441
Elapsed time for transformer_add_bias_dropout (2048x4x15232): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15232): 0.0006

Attention duration (in seconds): 0.1510
Attention throughput (in TFLOP/s): 107.486
MLP duration (in seconds): 0.1218
MLP throughput (in TFLOP/s): 249.690
Transformer duration (in seconds): 0.2763
Transformer throughput (in TFLOP/s): 168.824
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0986
Attention throughput (in TFLOP/s): 164.620
MLP duration (in seconds): 0.1228
MLP throughput (in TFLOP/s): 247.585
Transformer duration (in seconds): 0.2248
Transformer throughput (in TFLOP/s): 207.482
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0452
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 256.805
Elapsed time for attention_key_query_prob (512x2048x120x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x120x2048): 92.063
Elapsed time for attention_prob_times_values (512x2048x2048x120): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x120): 133.978
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 260.044
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0603
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 256.378
Elapsed time for mlp_fused_gelu (2048x4x61440): 0.0017
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0613
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 252.328
Elapsed time for transformer_add_bias_dropout (2048x4x15360): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15360): 0.0006

Attention duration (in seconds): 0.1393
Attention throughput (in TFLOP/s): 118.439
MLP duration (in seconds): 0.1233
MLP throughput (in TFLOP/s): 250.836
Transformer duration (in seconds): 0.2660
Transformer throughput (in TFLOP/s): 178.232
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0802
Attention throughput (in TFLOP/s): 205.620
MLP duration (in seconds): 0.1245
MLP throughput (in TFLOP/s): 248.311
Transformer duration (in seconds): 0.2091
Transformer throughput (in TFLOP/s): 226.816
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0460
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 256.593
Elapsed time for attention_key_query_prob (512x2048x121x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x121x2048): 34.891
Elapsed time for attention_prob_times_values (512x2048x2048x121): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x121): 73.133
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 262.001
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0612
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 256.681
Elapsed time for mlp_fused_gelu (2048x4x61952): 0.0017
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0626
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 251.158
Elapsed time for transformer_add_bias_dropout (2048x4x15488): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15488): 0.0006

Attention duration (in seconds): 0.1527
Attention throughput (in TFLOP/s): 109.725
MLP duration (in seconds): 0.1256
MLP throughput (in TFLOP/s): 250.426
Transformer duration (in seconds): 0.2818
Transformer throughput (in TFLOP/s): 171.028
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.1006
Attention throughput (in TFLOP/s): 166.617
MLP duration (in seconds): 0.1277
MLP throughput (in TFLOP/s): 246.256
Transformer duration (in seconds): 0.2326
Transformer throughput (in TFLOP/s): 207.226
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 256.144
Elapsed time for attention_key_query_prob (512x2048x122x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x122x2048): 73.696
Elapsed time for attention_prob_times_values (512x2048x2048x122): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x122): 122.561
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 257.544
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.0622
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 256.926
Elapsed time for mlp_fused_gelu (2048x4x62464): 0.0017
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0637
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 250.938
Elapsed time for transformer_add_bias_dropout (2048x4x15616): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15616): 0.0006

Attention duration (in seconds): 0.1435
Attention throughput (in TFLOP/s): 118.672
MLP duration (in seconds): 0.1276
MLP throughput (in TFLOP/s): 250.450
Transformer duration (in seconds): 0.2747
Transformer throughput (in TFLOP/s): 178.349
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0847
Attention throughput (in TFLOP/s): 200.989
MLP duration (in seconds): 0.1295
MLP throughput (in TFLOP/s): 246.799
Transformer duration (in seconds): 0.2182
Transformer throughput (in TFLOP/s): 224.499
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 255.982
Elapsed time for attention_key_query_prob (512x2048x123x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x123x2048): 36.650
Elapsed time for attention_prob_times_values (512x2048x2048x123): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x123): 75.402
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 259.400
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0633
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 256.605
Elapsed time for mlp_fused_gelu (2048x4x62976): 0.0017
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0650
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 249.899
Elapsed time for transformer_add_bias_dropout (2048x4x15744): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15744): 0.0006

Attention duration (in seconds): 0.1545
Attention throughput (in TFLOP/s): 112.003
MLP duration (in seconds): 0.1301
MLP throughput (in TFLOP/s): 249.810
Transformer duration (in seconds): 0.2881
Transformer throughput (in TFLOP/s): 172.802
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.1027
Attention throughput (in TFLOP/s): 168.467
MLP duration (in seconds): 0.1320
MLP throughput (in TFLOP/s): 246.164
Transformer duration (in seconds): 0.2389
Transformer throughput (in TFLOP/s): 208.454
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0484
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 255.850
Elapsed time for attention_key_query_prob (512x2048x124x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x124x2048): 75.191
Elapsed time for attention_prob_times_values (512x2048x2048x124): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x124): 124.876
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 261.473
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0641
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 257.511
Elapsed time for mlp_fused_gelu (2048x4x63488): 0.0018
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0660
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 250.175
Elapsed time for transformer_add_bias_dropout (2048x4x15872): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15872): 0.0006

Attention duration (in seconds): 0.1453
Attention throughput (in TFLOP/s): 120.934
MLP duration (in seconds): 0.1319
MLP throughput (in TFLOP/s): 250.415
Transformer duration (in seconds): 0.2808
Transformer throughput (in TFLOP/s): 180.175
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0861
Attention throughput (in TFLOP/s): 204.071
MLP duration (in seconds): 0.1334
MLP throughput (in TFLOP/s): 247.505
Transformer duration (in seconds): 0.2231
Transformer throughput (in TFLOP/s): 226.757
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0489
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 257.483
Elapsed time for attention_key_query_prob (512x2048x125x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x125x2048): 37.463
Elapsed time for attention_prob_times_values (512x2048x2048x125): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x125): 76.587
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 257.245
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0653
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 256.827
Elapsed time for mlp_fused_gelu (2048x4x64000): 0.0018
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0667
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 251.409
Elapsed time for transformer_add_bias_dropout (2048x4x16000): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16000): 0.0007

Attention duration (in seconds): 0.1563
Attention throughput (in TFLOP/s): 114.203
MLP duration (in seconds): 0.1338
MLP throughput (in TFLOP/s): 250.729
Transformer duration (in seconds): 0.2938
Transformer throughput (in TFLOP/s): 174.965
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.1052
Attention throughput (in TFLOP/s): 169.669
MLP duration (in seconds): 0.1363
MLP throughput (in TFLOP/s): 246.207
Transformer duration (in seconds): 0.2456
Transformer throughput (in TFLOP/s): 209.330
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0498
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 256.982
Elapsed time for attention_key_query_prob (512x2048x126x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x126x2048): 76.487
Elapsed time for attention_prob_times_values (512x2048x2048x126): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x126): 126.410
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 259.062
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 256.650
Elapsed time for mlp_fused_gelu (2048x4x64512): 0.0018
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0681
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 250.355
Elapsed time for transformer_add_bias_dropout (2048x4x16128): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16128): 0.0006

Attention duration (in seconds): 0.1473
Attention throughput (in TFLOP/s): 123.037
MLP duration (in seconds): 0.1363
MLP throughput (in TFLOP/s): 250.149
Transformer duration (in seconds): 0.2873
Transformer throughput (in TFLOP/s): 181.762
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0884
Attention throughput (in TFLOP/s): 204.992
MLP duration (in seconds): 0.1383
MLP throughput (in TFLOP/s): 246.477
Transformer duration (in seconds): 0.2305
Transformer throughput (in TFLOP/s): 226.540
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 256.341
Elapsed time for attention_key_query_prob (512x2048x127x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x127x2048): 37.158
Elapsed time for attention_prob_times_values (512x2048x2048x127): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x127): 76.665
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 261.016
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0674
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 257.086
Elapsed time for mlp_fused_gelu (2048x4x65024): 0.0018
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0691
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 250.575
Elapsed time for transformer_add_bias_dropout (2048x4x16256): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16256): 0.0007

Attention duration (in seconds): 0.1588
Attention throughput (in TFLOP/s): 115.897
MLP duration (in seconds): 0.1383
MLP throughput (in TFLOP/s): 250.487
Transformer duration (in seconds): 0.3008
Transformer throughput (in TFLOP/s): 176.334
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.1070
Attention throughput (in TFLOP/s): 172.073
MLP duration (in seconds): 0.1403
MLP throughput (in TFLOP/s): 246.884
Transformer duration (in seconds): 0.2508
Transformer throughput (in TFLOP/s): 211.468
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 257.185
Elapsed time for attention_key_query_prob (512x2048x128x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x128x2048): 111.902
Elapsed time for attention_prob_times_values (512x2048x2048x128): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x128): 144.196
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 262.285
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.0685
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 256.953
Elapsed time for mlp_fused_gelu (2048x4x65536): 0.0018
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0705
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 249.362
Elapsed time for transformer_add_bias_dropout (2048x4x16384): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16384): 0.0007

Attention duration (in seconds): 0.1466
Attention throughput (in TFLOP/s): 127.509
MLP duration (in seconds): 0.1408
MLP throughput (in TFLOP/s): 249.846
Transformer duration (in seconds): 0.2912
Transformer throughput (in TFLOP/s): 185.046
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0885
Attention throughput (in TFLOP/s): 211.161
MLP duration (in seconds): 0.1432
MLP throughput (in TFLOP/s): 245.631
Transformer duration (in seconds): 0.2359
Transformer throughput (in TFLOP/s): 228.393
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0523
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 256.298
Elapsed time for attention_key_query_prob (512x2048x129x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x129x2048): 34.299
Elapsed time for attention_prob_times_values (512x2048x2048x129): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x129): 55.970
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 258.579
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.0693
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 257.663
Elapsed time for mlp_fused_gelu (2048x4x66048): 0.0018
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0716
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 249.513
Elapsed time for transformer_add_bias_dropout (2048x4x16512): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16512): 0.0007

Attention duration (in seconds): 0.1654
Attention throughput (in TFLOP/s): 114.726
MLP duration (in seconds): 0.1428
MLP throughput (in TFLOP/s): 250.286
Transformer duration (in seconds): 0.3120
Transformer throughput (in TFLOP/s): 175.386
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.1149
Attention throughput (in TFLOP/s): 165.126
MLP duration (in seconds): 0.1452
MLP throughput (in TFLOP/s): 246.165
Transformer duration (in seconds): 0.2644
Transformer throughput (in TFLOP/s): 206.909
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 256.130
Elapsed time for attention_key_query_prob (512x2048x130x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x130x2048): 72.242
Elapsed time for attention_prob_times_values (512x2048x2048x130): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x130): 95.276
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 260.415
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.0706
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 257.086
Elapsed time for mlp_fused_gelu (2048x4x66560): 0.0018
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0726
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 249.858
Elapsed time for transformer_add_bias_dropout (2048x4x16640): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16640): 0.0007

Attention duration (in seconds): 0.1539
Attention throughput (in TFLOP/s): 125.141
MLP duration (in seconds): 0.1450
MLP throughput (in TFLOP/s): 250.209
Transformer duration (in seconds): 0.3028
Transformer throughput (in TFLOP/s): 183.490
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0956
Attention throughput (in TFLOP/s): 201.580
MLP duration (in seconds): 0.1472
MLP throughput (in TFLOP/s): 246.576
Transformer duration (in seconds): 0.2468
Transformer throughput (in TFLOP/s): 225.076
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0540
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 255.831
Elapsed time for attention_key_query_prob (512x2048x131x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x131x2048): 34.978
Elapsed time for attention_prob_times_values (512x2048x2048x131): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x131): 59.064
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0176
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 262.297
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.0717
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 256.905
Elapsed time for mlp_fused_gelu (2048x4x67072): 0.0019
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0736
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 250.192
Elapsed time for transformer_add_bias_dropout (2048x4x16768): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16768): 0.0007

Attention duration (in seconds): 0.1670
Attention throughput (in TFLOP/s): 117.089
MLP duration (in seconds): 0.1472
MLP throughput (in TFLOP/s): 250.317
Transformer duration (in seconds): 0.3181
Transformer throughput (in TFLOP/s): 177.342
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.1177
Attention throughput (in TFLOP/s): 166.084
MLP duration (in seconds): 0.1497
MLP throughput (in TFLOP/s): 246.238
Transformer duration (in seconds): 0.2715
Transformer throughput (in TFLOP/s): 207.728
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 255.532
Elapsed time for attention_key_query_prob (512x2048x132x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x132x2048): 73.087
Elapsed time for attention_prob_times_values (512x2048x2048x132): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x132): 95.246
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 258.131
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.0730
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 256.288
Elapsed time for mlp_fused_gelu (2048x4x67584): 0.0019
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0750
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 249.498
Elapsed time for transformer_add_bias_dropout (2048x4x16896): 0.0013
Elapsed time for transformer_layer_norm (2048x4x16896): 0.0007

Attention duration (in seconds): 0.1565
Attention throughput (in TFLOP/s): 126.759
MLP duration (in seconds): 0.1499
MLP throughput (in TFLOP/s): 249.691
Transformer duration (in seconds): 0.3103
Transformer throughput (in TFLOP/s): 184.554
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.0977
Attention throughput (in TFLOP/s): 203.097
MLP duration (in seconds): 0.1522
MLP throughput (in TFLOP/s): 245.890
Transformer duration (in seconds): 0.2531
Transformer throughput (in TFLOP/s): 226.248
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0558
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 255.421
Elapsed time for attention_key_query_prob (512x2048x133x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x133x2048): 34.670
Elapsed time for attention_prob_times_values (512x2048x2048x133): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x133): 59.609
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 256.859
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.0738
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 257.365
Elapsed time for mlp_fused_gelu (2048x4x68096): 0.0019
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0763
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 249.060
Elapsed time for transformer_add_bias_dropout (2048x4x17024): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17024): 0.0007

Attention duration (in seconds): 0.1701
Attention throughput (in TFLOP/s): 118.365
MLP duration (in seconds): 0.1519
MLP throughput (in TFLOP/s): 250.003
Transformer duration (in seconds): 0.3259
Transformer throughput (in TFLOP/s): 178.323
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1216
Attention throughput (in TFLOP/s): 165.611
MLP duration (in seconds): 0.1546
MLP throughput (in TFLOP/s): 245.642
Transformer duration (in seconds): 0.2797
Transformer throughput (in TFLOP/s): 207.834
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0568
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 254.785
Elapsed time for attention_key_query_prob (512x2048x134x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x134x2048): 73.710
Elapsed time for attention_prob_times_values (512x2048x2048x134): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x134): 94.048
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 258.681
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.0748
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 257.681
Elapsed time for mlp_fused_gelu (2048x4x68608): 0.0019
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0768
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 251.042
Elapsed time for transformer_add_bias_dropout (2048x4x17152): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17152): 0.0007

Attention duration (in seconds): 0.1591
Attention throughput (in TFLOP/s): 128.418
MLP duration (in seconds): 0.1535
MLP throughput (in TFLOP/s): 251.183
Transformer duration (in seconds): 0.3165
Transformer throughput (in TFLOP/s): 186.381
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.0996
Attention throughput (in TFLOP/s): 205.053
MLP duration (in seconds): 0.1562
MLP throughput (in TFLOP/s): 246.785
Transformer duration (in seconds): 0.2593
Transformer throughput (in TFLOP/s): 227.537
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0571
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 257.125
Elapsed time for attention_key_query_prob (512x2048x135x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x135x2048): 33.565
Elapsed time for attention_prob_times_values (512x2048x2048x135): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x135): 58.588
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 263.101
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 257.034
Elapsed time for mlp_fused_gelu (2048x4x69120): 0.0019
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0780
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 250.975
Elapsed time for transformer_add_bias_dropout (2048x4x17280): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17280): 0.0007

Attention duration (in seconds): 0.1726
Attention throughput (in TFLOP/s): 120.073
MLP duration (in seconds): 0.1560
MLP throughput (in TFLOP/s): 250.863
Transformer duration (in seconds): 0.3326
Transformer throughput (in TFLOP/s): 180.009
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1242
Attention throughput (in TFLOP/s): 166.963
MLP duration (in seconds): 0.1582
MLP throughput (in TFLOP/s): 247.338
Transformer duration (in seconds): 0.2861
Transformer throughput (in TFLOP/s): 209.239
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 257.135
Elapsed time for attention_key_query_prob (512x2048x136x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x136x2048): 99.475
Elapsed time for attention_prob_times_values (512x2048x2048x136): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x136): 127.662
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0191
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 259.478
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.0774
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 256.684
Elapsed time for mlp_fused_gelu (2048x4x69632): 0.0019
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0792
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 250.643
Elapsed time for transformer_add_bias_dropout (2048x4x17408): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17408): 0.0007

Attention duration (in seconds): 0.1573
Attention throughput (in TFLOP/s): 133.687
MLP duration (in seconds): 0.1585
MLP throughput (in TFLOP/s): 250.555
Transformer duration (in seconds): 0.3198
Transformer throughput (in TFLOP/s): 189.961
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1006
Attention throughput (in TFLOP/s): 208.993
MLP duration (in seconds): 0.1612
MLP throughput (in TFLOP/s): 246.409
Transformer duration (in seconds): 0.2661
Transformer throughput (in TFLOP/s): 228.282
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0594
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 254.452
Elapsed time for attention_key_query_prob (512x2048x137x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x137x2048): 33.821
Elapsed time for attention_prob_times_values (512x2048x2048x137): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x137): 59.140
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 261.263
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.0784
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 257.152
Elapsed time for mlp_fused_gelu (2048x4x70144): 0.0019
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0808
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 249.342
Elapsed time for transformer_add_bias_dropout (2048x4x17536): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17536): 0.0007

Attention duration (in seconds): 0.1758
Attention throughput (in TFLOP/s): 121.318
MLP duration (in seconds): 0.1611
MLP throughput (in TFLOP/s): 250.146
Transformer duration (in seconds): 0.3409
Transformer throughput (in TFLOP/s): 180.780
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1271
Attention throughput (in TFLOP/s): 167.766
MLP duration (in seconds): 0.1640
MLP throughput (in TFLOP/s): 245.795
Transformer duration (in seconds): 0.2956
Transformer throughput (in TFLOP/s): 208.508
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0603
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 254.285
Elapsed time for attention_key_query_prob (512x2048x138x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x138x2048): 75.142
Elapsed time for attention_prob_times_values (512x2048x2048x138): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x138): 94.795
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 259.568
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 256.675
Elapsed time for mlp_fused_gelu (2048x4x70656): 0.0019
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0818
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 249.924
Elapsed time for transformer_add_bias_dropout (2048x4x17664): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17664): 0.0007

Attention duration (in seconds): 0.1639
Attention throughput (in TFLOP/s): 131.968
MLP duration (in seconds): 0.1634
MLP throughput (in TFLOP/s): 250.235
Transformer duration (in seconds): 0.3314
Transformer throughput (in TFLOP/s): 188.688
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1056
Attention throughput (in TFLOP/s): 204.843
MLP duration (in seconds): 0.1662
MLP throughput (in TFLOP/s): 246.116
Transformer duration (in seconds): 0.2762
Transformer throughput (in TFLOP/s): 226.390
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 254.344
Elapsed time for attention_key_query_prob (512x2048x139x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x139x2048): 35.514
Elapsed time for attention_prob_times_values (512x2048x2048x139): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x139): 61.093
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 259.100
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.0810
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 256.012
Elapsed time for mlp_fused_gelu (2048x4x71168): 0.0020
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0834
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 248.767
Elapsed time for transformer_add_bias_dropout (2048x4x17792): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17792): 0.0007

Attention duration (in seconds): 0.1776
Attention throughput (in TFLOP/s): 123.557
MLP duration (in seconds): 0.1664
MLP throughput (in TFLOP/s): 249.351
Transformer duration (in seconds): 0.3480
Transformer throughput (in TFLOP/s): 182.259
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1304
Attention throughput (in TFLOP/s): 168.277
MLP duration (in seconds): 0.1687
MLP throughput (in TFLOP/s): 245.951
Transformer duration (in seconds): 0.3032
Transformer throughput (in TFLOP/s): 209.239
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0621
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 254.043
Elapsed time for attention_key_query_prob (512x2048x140x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x140x2048): 76.261
Elapsed time for attention_prob_times_values (512x2048x2048x140): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x140): 96.132
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 260.757
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.0820
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 256.691
Elapsed time for mlp_fused_gelu (2048x4x71680): 0.0020
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0843
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 249.594
Elapsed time for transformer_add_bias_dropout (2048x4x17920): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17920): 0.0007

Attention duration (in seconds): 0.1663
Attention throughput (in TFLOP/s): 133.815
MLP duration (in seconds): 0.1683
MLP throughput (in TFLOP/s): 250.121
Transformer duration (in seconds): 0.3386
Transformer throughput (in TFLOP/s): 189.998
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1076
Attention throughput (in TFLOP/s): 206.694
MLP duration (in seconds): 0.1710
MLP throughput (in TFLOP/s): 246.120
Transformer duration (in seconds): 0.2828
Transformer throughput (in TFLOP/s): 227.536
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 256.402
Elapsed time for attention_key_query_prob (512x2048x141x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x141x2048): 35.187
Elapsed time for attention_prob_times_values (512x2048x2048x141): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x141): 60.109
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 259.359
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.0832
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 256.692
Elapsed time for mlp_fused_gelu (2048x4x72192): 0.0020
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0852
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 250.674
Elapsed time for transformer_add_bias_dropout (2048x4x18048): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18048): 0.0007

Attention duration (in seconds): 0.1801
Attention throughput (in TFLOP/s): 125.259
MLP duration (in seconds): 0.1703
MLP throughput (in TFLOP/s): 250.684
Transformer duration (in seconds): 0.3545
Transformer throughput (in TFLOP/s): 184.060
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1317
Attention throughput (in TFLOP/s): 171.245
MLP duration (in seconds): 0.1732
MLP throughput (in TFLOP/s): 246.497
Transformer duration (in seconds): 0.3095
Transformer throughput (in TFLOP/s): 210.852
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0639
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 254.066
Elapsed time for attention_key_query_prob (512x2048x142x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x142x2048): 77.045
Elapsed time for attention_prob_times_values (512x2048x2048x142): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x142): 96.658
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0209
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 258.672
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.0847
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 255.615
Elapsed time for mlp_fused_gelu (2048x4x72704): 0.0020
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0868
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 249.332
Elapsed time for transformer_add_bias_dropout (2048x4x18176): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18176): 0.0007

Attention duration (in seconds): 0.1689
Attention throughput (in TFLOP/s): 135.449
MLP duration (in seconds): 0.1735
MLP throughput (in TFLOP/s): 249.518
Transformer duration (in seconds): 0.3466
Transformer throughput (in TFLOP/s): 190.947
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1097
Attention throughput (in TFLOP/s): 208.574
MLP duration (in seconds): 0.1764
MLP throughput (in TFLOP/s): 245.470
Transformer duration (in seconds): 0.2904
Transformer throughput (in TFLOP/s): 227.901
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0646
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 254.828
Elapsed time for attention_key_query_prob (512x2048x143x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x143x2048): 35.243
Elapsed time for attention_prob_times_values (512x2048x2048x143): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x143): 59.853
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 260.268
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.0858
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 255.909
Elapsed time for mlp_fused_gelu (2048x4x73216): 0.0020
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0880
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 249.430
Elapsed time for transformer_add_bias_dropout (2048x4x18304): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18304): 0.0007

Attention duration (in seconds): 0.1832
Attention throughput (in TFLOP/s): 126.554
MLP duration (in seconds): 0.1759
MLP throughput (in TFLOP/s): 249.722
Transformer duration (in seconds): 0.3633
Transformer throughput (in TFLOP/s): 184.718
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1352
Attention throughput (in TFLOP/s): 171.474
MLP duration (in seconds): 0.1782
MLP throughput (in TFLOP/s): 246.362
Transformer duration (in seconds): 0.3160
Transformer throughput (in TFLOP/s): 212.308
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0648
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 257.709
Elapsed time for attention_key_query_prob (512x2048x144x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x144x2048): 111.186
Elapsed time for attention_prob_times_values (512x2048x2048x144): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x144): 137.895
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 262.063
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.0866
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 257.230
Elapsed time for mlp_fused_gelu (2048x4x73728): 0.0020
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 248.915
Elapsed time for transformer_add_bias_dropout (2048x4x18432): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18432): 0.0007

Attention duration (in seconds): 0.1659
Attention throughput (in TFLOP/s): 141.684
MLP duration (in seconds): 0.1780
MLP throughput (in TFLOP/s): 250.115
Transformer duration (in seconds): 0.3481
Transformer throughput (in TFLOP/s): 195.416
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1094
Attention throughput (in TFLOP/s): 214.779
MLP duration (in seconds): 0.1806
MLP throughput (in TFLOP/s): 246.603
Transformer duration (in seconds): 0.2943
Transformer throughput (in TFLOP/s): 231.189
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.0659
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 256.780
Elapsed time for attention_key_query_prob (512x2048x145x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x145x2048): 57.237
Elapsed time for attention_prob_times_values (512x2048x2048x145): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x145): 59.665
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0217
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 259.921
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.0884
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 255.252
Elapsed time for mlp_fused_gelu (2048x4x74240): 0.0021
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.0903
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 249.903
Elapsed time for transformer_add_bias_dropout (2048x4x18560): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18560): 0.0007

Attention duration (in seconds): 0.1788
Attention throughput (in TFLOP/s): 133.260
MLP duration (in seconds): 0.1808
MLP throughput (in TFLOP/s): 249.684
Transformer duration (in seconds): 0.3638
Transformer throughput (in TFLOP/s): 189.574
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1240
Attention throughput (in TFLOP/s): 192.161
MLP duration (in seconds): 0.1826
MLP throughput (in TFLOP/s): 247.297
Transformer duration (in seconds): 0.3120
Transformer throughput (in TFLOP/s): 221.064
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.0674
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 254.750
Elapsed time for attention_key_query_prob (512x2048x146x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x146x2048): 78.611
Elapsed time for attention_prob_times_values (512x2048x2048x146): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x146): 98.292
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(12329746432, 42481549312)
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 259.900
(12329746432, 42481549312)
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.0898
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 254.782
Elapsed time for mlp_fused_gelu (2048x4x74752): 0.0021
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0921
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 248.475
Elapsed time for transformer_add_bias_dropout (2048x4x18688): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18688): 0.0008

Attention duration (in seconds): 0.1735
Attention throughput (in TFLOP/s): 139.113
MLP duration (in seconds): 0.1840
MLP throughput (in TFLOP/s): 248.764
Transformer duration (in seconds): 0.3618
Transformer throughput (in TFLOP/s): 193.237
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1149
Attention throughput (in TFLOP/s): 210.083
MLP duration (in seconds): 0.1862
MLP throughput (in TFLOP/s): 245.798
Transformer duration (in seconds): 0.3062
Transformer throughput (in TFLOP/s): 228.367
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.0684
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 254.439
Elapsed time for attention_key_query_prob (512x2048x147x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x147x2048): 57.894
Elapsed time for attention_prob_times_values (512x2048x2048x147): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x147): 60.377
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(8034779136, 42481549312)
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 258.311
(8034779136, 42481549312)
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.0910
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 254.878
Elapsed time for mlp_fused_gelu (2048x4x75264): 0.0021
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0932
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 249.085
Elapsed time for transformer_add_bias_dropout (2048x4x18816): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18816): 0.0008

Attention duration (in seconds): 0.1820
Attention throughput (in TFLOP/s): 134.420
MLP duration (in seconds): 0.1863
MLP throughput (in TFLOP/s): 249.137
Transformer duration (in seconds): 0.3726
Transformer throughput (in TFLOP/s): 190.220
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1268
Attention throughput (in TFLOP/s): 192.911
MLP duration (in seconds): 0.1888
MLP throughput (in TFLOP/s): 245.769
Transformer duration (in seconds): 0.3203
Transformer throughput (in TFLOP/s): 221.291
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.0694
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 254.306
Elapsed time for attention_key_query_prob (512x2048x148x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x148x2048): 79.667
Elapsed time for attention_prob_times_values (512x2048x2048x148): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x148): 99.564
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(8034779136, 42481549312)
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 260.013
(8034779136, 42481549312)
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.0920
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 255.517
Elapsed time for mlp_fused_gelu (2048x4x75776): 0.0021
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.0931
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 252.621
Elapsed time for transformer_add_bias_dropout (2048x4x18944): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18944): 0.0008

Attention duration (in seconds): 0.1761
Attention throughput (in TFLOP/s): 140.748
MLP duration (in seconds): 0.1872
MLP throughput (in TFLOP/s): 251.227
Transformer duration (in seconds): 0.3677
Transformer throughput (in TFLOP/s): 195.357
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1168
Attention throughput (in TFLOP/s): 212.193
MLP duration (in seconds): 0.1901
MLP throughput (in TFLOP/s): 247.500
Transformer duration (in seconds): 0.3124
Transformer throughput (in TFLOP/s): 229.909
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.0703
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 254.164
Elapsed time for attention_key_query_prob (512x2048x149x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x149x2048): 58.251
Elapsed time for attention_prob_times_values (512x2048x2048x149): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x149): 60.615
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(8034779136, 42481549312)
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 259.430
(8034779136, 42481549312)
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.0933
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 255.548
Elapsed time for mlp_fused_gelu (2048x4x76288): 0.0021
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0947
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 251.856
Elapsed time for transformer_add_bias_dropout (2048x4x19072): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19072): 0.0008

Attention duration (in seconds): 0.1846
Attention throughput (in TFLOP/s): 136.034
MLP duration (in seconds): 0.1900
MLP throughput (in TFLOP/s): 250.881
Transformer duration (in seconds): 0.3790
Transformer throughput (in TFLOP/s): 192.056
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1293
Attention throughput (in TFLOP/s): 194.303
MLP duration (in seconds): 0.1931
MLP throughput (in TFLOP/s): 246.874
Transformer duration (in seconds): 0.3278
Transformer throughput (in TFLOP/s): 222.101
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0713
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 253.977
Elapsed time for attention_key_query_prob (512x2048x150x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x150x2048): 81.007
Elapsed time for attention_prob_times_values (512x2048x2048x150): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x150): 99.777
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(8034779136, 42481549312)
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 257.871
(8034779136, 42481549312)
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.0948
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 254.852
Elapsed time for mlp_fused_gelu (2048x4x76800): 0.0021
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0956
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 252.752
Elapsed time for transformer_add_bias_dropout (2048x4x19200): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19200): 0.0008

Attention duration (in seconds): 0.1790
Attention throughput (in TFLOP/s): 142.185
MLP duration (in seconds): 0.1925
MLP throughput (in TFLOP/s): 251.007
Transformer duration (in seconds): 0.3759
Transformer throughput (in TFLOP/s): 196.263
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1198
Attention throughput (in TFLOP/s): 212.351
MLP duration (in seconds): 0.1950
MLP throughput (in TFLOP/s): 247.844
Transformer duration (in seconds): 0.3205
Transformer throughput (in TFLOP/s): 230.172
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.0722
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 254.455
Elapsed time for attention_key_query_prob (512x2048x151x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x151x2048): 58.459
Elapsed time for attention_prob_times_values (512x2048x2048x151): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x151): 60.112
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(8034779136, 42481549312)
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 259.425
(8034779136, 42481549312)
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.0957
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 255.752
Elapsed time for mlp_fused_gelu (2048x4x77312): 0.0021
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.0971
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 252.037
Elapsed time for transformer_add_bias_dropout (2048x4x19328): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19328): 0.0008

Attention duration (in seconds): 0.1874
Attention throughput (in TFLOP/s): 137.537
MLP duration (in seconds): 0.1950
MLP throughput (in TFLOP/s): 251.105
Transformer duration (in seconds): 0.3868
Transformer throughput (in TFLOP/s): 193.213
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1317
Attention throughput (in TFLOP/s): 195.773
MLP duration (in seconds): 0.1983
MLP throughput (in TFLOP/s): 246.950
Transformer duration (in seconds): 0.3351
Transformer throughput (in TFLOP/s): 223.076
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.0730
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 254.928
Elapsed time for attention_key_query_prob (512x2048x152x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x152x2048): 105.113
Elapsed time for attention_prob_times_values (512x2048x2048x152): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x152): 144.153
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(8034779136, 42481549312)
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0241
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 257.642
(8034779136, 42481549312)
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.0974
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 254.668
Elapsed time for mlp_fused_gelu (2048x4x77824): 0.0022
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.0984
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 252.067
Elapsed time for transformer_add_bias_dropout (2048x4x19456): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19456): 0.0008

Attention duration (in seconds): 0.1776
Attention throughput (in TFLOP/s): 147.025
MLP duration (in seconds): 0.1980
MLP throughput (in TFLOP/s): 250.609
Transformer duration (in seconds): 0.3800
Transformer throughput (in TFLOP/s): 199.267
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1203
Attention throughput (in TFLOP/s): 217.026
MLP duration (in seconds): 0.2003
MLP throughput (in TFLOP/s): 247.656
Transformer duration (in seconds): 0.3258
Transformer throughput (in TFLOP/s): 232.405
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.0740
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 254.808
Elapsed time for attention_key_query_prob (512x2048x153x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x153x2048): 58.956
Elapsed time for attention_prob_times_values (512x2048x2048x153): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x153): 60.394
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(8034779136, 42481549312)
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 257.430
(8034779136, 42481549312)
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.0988
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 254.358
Elapsed time for mlp_fused_gelu (2048x4x78336): 0.0022
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.1000
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 251.427
Elapsed time for transformer_add_bias_dropout (2048x4x19584): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19584): 0.0008

Attention duration (in seconds): 0.1902
Attention throughput (in TFLOP/s): 139.048
MLP duration (in seconds): 0.2010
MLP throughput (in TFLOP/s): 250.163
Transformer duration (in seconds): 0.3956
Transformer throughput (in TFLOP/s): 193.923
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1347
Attention throughput (in TFLOP/s): 196.362
MLP duration (in seconds): 0.2035
MLP throughput (in TFLOP/s): 247.057
Transformer duration (in seconds): 0.3437
Transformer throughput (in TFLOP/s): 223.205
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.0749
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 255.030
Elapsed time for attention_key_query_prob (512x2048x154x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x154x2048): 82.788
Elapsed time for attention_prob_times_values (512x2048x2048x154): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x154): 102.389
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(8034779136, 42481549312)
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 258.877
(8034779136, 42481549312)
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.1000
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 254.558
Elapsed time for mlp_fused_gelu (2048x4x78848): 0.0022
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.1014
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 251.216
Elapsed time for transformer_add_bias_dropout (2048x4x19712): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19712): 0.0008

Attention duration (in seconds): 0.1837
Attention throughput (in TFLOP/s): 145.793
MLP duration (in seconds): 0.2036
MLP throughput (in TFLOP/s): 250.170
Transformer duration (in seconds): 0.3919
Transformer throughput (in TFLOP/s): 198.328
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1249
Attention throughput (in TFLOP/s): 214.507
MLP duration (in seconds): 0.2060
MLP throughput (in TFLOP/s): 247.176
Transformer duration (in seconds): 0.3370
Transformer throughput (in TFLOP/s): 230.592
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.0759
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 254.764
Elapsed time for attention_key_query_prob (512x2048x155x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x155x2048): 59.483
Elapsed time for attention_prob_times_values (512x2048x2048x155): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x155): 63.150
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(8034779136, 42481549312)
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0248
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 260.146
(8034779136, 42481549312)
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.1012
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 254.860
Elapsed time for mlp_fused_gelu (2048x4x79360): 0.0022
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.1023
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 252.187
Elapsed time for transformer_add_bias_dropout (2048x4x19840): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19840): 0.0008

Attention duration (in seconds): 0.1923
Attention throughput (in TFLOP/s): 141.104
MLP duration (in seconds): 0.2057
MLP throughput (in TFLOP/s): 250.823
Transformer duration (in seconds): 0.4025
Transformer throughput (in TFLOP/s): 195.593
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1370
Attention throughput (in TFLOP/s): 197.973
MLP duration (in seconds): 0.2080
MLP throughput (in TFLOP/s): 248.011
Transformer duration (in seconds): 0.3507
Transformer throughput (in TFLOP/s): 224.470
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.0769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 254.760
Elapsed time for attention_key_query_prob (512x2048x156x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x156x2048): 83.313
Elapsed time for attention_prob_times_values (512x2048x2048x156): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x156): 103.516
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 257.003
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.1025
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 254.943
Elapsed time for mlp_fused_gelu (2048x4x79872): 0.0022
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.1039
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 251.444
Elapsed time for transformer_add_bias_dropout (2048x4x19968): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19968): 0.0008

Attention duration (in seconds): 0.1866
Attention throughput (in TFLOP/s): 147.179
MLP duration (in seconds): 0.2086
MLP throughput (in TFLOP/s): 250.504
Transformer duration (in seconds): 0.3998
Transformer throughput (in TFLOP/s): 199.412
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1271
Attention throughput (in TFLOP/s): 216.099
MLP duration (in seconds): 0.2104
MLP throughput (in TFLOP/s): 248.422
Transformer duration (in seconds): 0.3443
Transformer throughput (in TFLOP/s): 231.593
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.0780
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 254.556
Elapsed time for attention_key_query_prob (512x2048x157x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x157x2048): 60.037
Elapsed time for attention_prob_times_values (512x2048x2048x157): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x157): 63.865
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 258.486
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1038
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 255.078
Elapsed time for mlp_fused_gelu (2048x4x80384): 0.0022
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.1055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 250.893
Elapsed time for transformer_add_bias_dropout (2048x4x20096): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20096): 0.0008

Attention duration (in seconds): 0.1952
Attention throughput (in TFLOP/s): 142.524
MLP duration (in seconds): 0.2115
MLP throughput (in TFLOP/s): 250.311
Transformer duration (in seconds): 0.4112
Transformer throughput (in TFLOP/s): 196.364
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1399
Attention throughput (in TFLOP/s): 198.757
MLP duration (in seconds): 0.2141
MLP throughput (in TFLOP/s): 247.284
Transformer duration (in seconds): 0.3588
Transformer throughput (in TFLOP/s): 225.052
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.0792
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 253.815
Elapsed time for attention_key_query_prob (512x2048x158x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x158x2048): 84.547
Elapsed time for attention_prob_times_values (512x2048x2048x158): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x158): 104.466
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0258
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 259.862
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1052
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 254.691
Elapsed time for mlp_fused_gelu (2048x4x80896): 0.0022
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.1065
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 251.750
Elapsed time for transformer_add_bias_dropout (2048x4x20224): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20224): 0.0008

Attention duration (in seconds): 0.1893
Attention throughput (in TFLOP/s): 148.757
MLP duration (in seconds): 0.2140
MLP throughput (in TFLOP/s): 250.567
Transformer duration (in seconds): 0.4079
Transformer throughput (in TFLOP/s): 200.485
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1299
Attention throughput (in TFLOP/s): 216.803
MLP duration (in seconds): 0.2164
MLP throughput (in TFLOP/s): 247.778
Transformer duration (in seconds): 0.3510
Transformer throughput (in TFLOP/s): 232.965
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.0796
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 255.913
Elapsed time for attention_key_query_prob (512x2048x159x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x159x2048): 60.932
Elapsed time for attention_prob_times_values (512x2048x2048x159): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x159): 63.582
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 258.520
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1066
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 254.720
Elapsed time for mlp_fused_gelu (2048x4x81408): 0.0022
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1088
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 249.551
Elapsed time for transformer_add_bias_dropout (2048x4x20352): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20352): 0.0008

Attention duration (in seconds): 0.1975
Attention throughput (in TFLOP/s): 144.330
MLP duration (in seconds): 0.2176
MLP throughput (in TFLOP/s): 249.512
Transformer duration (in seconds): 0.4198
Transformer throughput (in TFLOP/s): 197.258
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1418
Attention throughput (in TFLOP/s): 201.076
MLP duration (in seconds): 0.2199
MLP throughput (in TFLOP/s): 246.941
Transformer duration (in seconds): 0.3680
Transformer throughput (in TFLOP/s): 224.991
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.0809
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 254.751
Elapsed time for attention_key_query_prob (512x2048x160x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x160x2048): 125.035
Elapsed time for attention_prob_times_values (512x2048x2048x160): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x160): 152.157
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0269
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 255.085
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1078
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 254.928
Elapsed time for mlp_fused_gelu (2048x4x81920): 0.0023
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1088
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 252.638
Elapsed time for transformer_add_bias_dropout (2048x4x20480): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20480): 0.0008

Attention duration (in seconds): 0.1877
Attention throughput (in TFLOP/s): 153.800
MLP duration (in seconds): 0.2189
MLP throughput (in TFLOP/s): 251.161
Transformer duration (in seconds): 0.4112
Transformer throughput (in TFLOP/s): 203.875
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1304
Attention throughput (in TFLOP/s): 221.293
MLP duration (in seconds): 0.2214
MLP throughput (in TFLOP/s): 248.363
Transformer duration (in seconds): 0.3586
Transformer throughput (in TFLOP/s): 233.797
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.0818
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 255.333
Elapsed time for attention_key_query_prob (512x2048x161x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x161x2048): 58.217
Elapsed time for attention_prob_times_values (512x2048x2048x161): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x161): 63.849
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 256.709
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1092
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 254.860
Elapsed time for mlp_fused_gelu (2048x4x82432): 0.0023
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1709
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 162.882
Elapsed time for transformer_add_bias_dropout (2048x4x20608): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20608): 0.0008

Attention duration (in seconds): 0.2014
Attention throughput (in TFLOP/s): 145.095
MLP duration (in seconds): 0.2824
MLP throughput (in TFLOP/s): 197.146
Transformer duration (in seconds): 0.4884
Transformer throughput (in TFLOP/s): 173.795
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1461
Attention throughput (in TFLOP/s): 199.982
MLP duration (in seconds): 0.2856
MLP throughput (in TFLOP/s): 194.875
Transformer duration (in seconds): 0.4404
Transformer throughput (in TFLOP/s): 192.756
Transformer - MLP - Attention (in seconds): 0.0086
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.0828
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 255.264
Elapsed time for attention_key_query_prob (512x2048x162x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x162x2048): 80.484
Elapsed time for attention_prob_times_values (512x2048x2048x162): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x162): 107.012
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 258.636
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1104
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 255.136
Elapsed time for mlp_fused_gelu (2048x4x82944): 0.0023
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1771
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 159.079
Elapsed time for transformer_add_bias_dropout (2048x4x20736): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20736): 0.0008

Attention duration (in seconds): 0.1950
Attention throughput (in TFLOP/s): 151.674
MLP duration (in seconds): 0.2899
MLP throughput (in TFLOP/s): 194.424
Transformer duration (in seconds): 0.4896
Transformer throughput (in TFLOP/s): 175.522
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1365
Attention throughput (in TFLOP/s): 216.664
MLP duration (in seconds): 0.2896
MLP throughput (in TFLOP/s): 194.637
Transformer duration (in seconds): 0.4278
Transformer throughput (in TFLOP/s): 200.851
Transformer - MLP - Attention (in seconds): 0.0018
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.0838
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 255.199
Elapsed time for attention_key_query_prob (512x2048x163x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x163x2048): 58.254
Elapsed time for attention_prob_times_values (512x2048x2048x163): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x163): 65.836
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 257.626
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1114
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 256.105
Elapsed time for mlp_fused_gelu (2048x4x83456): 0.0023
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1789
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 159.499
Elapsed time for transformer_add_bias_dropout (2048x4x20864): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20864): 0.0008

Attention duration (in seconds): 0.2040
Attention throughput (in TFLOP/s): 146.734
MLP duration (in seconds): 0.2926
MLP throughput (in TFLOP/s): 195.029
Transformer duration (in seconds): 0.5013
Transformer throughput (in TFLOP/s): 173.534
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1487
Attention throughput (in TFLOP/s): 201.326
MLP duration (in seconds): 0.2903
MLP throughput (in TFLOP/s): 196.556
Transformer duration (in seconds): 0.4510
Transformer throughput (in TFLOP/s): 192.862
Transformer - MLP - Attention (in seconds): 0.0121
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.0849
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 255.011
Elapsed time for attention_key_query_prob (512x2048x164x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x164x2048): 81.446
Elapsed time for attention_prob_times_values (512x2048x2048x164): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x164): 107.934
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 259.084
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1134
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 254.611
Elapsed time for mlp_fused_gelu (2048x4x83968): 0.0023
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1874
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 154.093
Elapsed time for transformer_add_bias_dropout (2048x4x20992): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20992): 0.0008

Attention duration (in seconds): 0.1978
Attention throughput (in TFLOP/s): 153.142
MLP duration (in seconds): 0.3032
MLP throughput (in TFLOP/s): 190.526
Transformer duration (in seconds): 0.5057
Transformer throughput (in TFLOP/s): 174.112
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1387
Attention throughput (in TFLOP/s): 218.354
MLP duration (in seconds): 0.3022
MLP throughput (in TFLOP/s): 191.125
Transformer duration (in seconds): 0.4479
Transformer throughput (in TFLOP/s): 196.598
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.0860
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 254.875
Elapsed time for attention_key_query_prob (512x2048x165x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x165x2048): 58.846
Elapsed time for attention_prob_times_values (512x2048x2048x165): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x165): 66.374
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0284
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 257.752
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1151
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 253.941
Elapsed time for mlp_fused_gelu (2048x4x84480): 0.0023
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1929
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 151.510
Elapsed time for transformer_add_bias_dropout (2048x4x21120): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21120): 0.0009

Attention duration (in seconds): 0.2069
Attention throughput (in TFLOP/s): 148.148
MLP duration (in seconds): 0.3104
MLP throughput (in TFLOP/s): 188.364
Transformer duration (in seconds): 0.5221
Transformer throughput (in TFLOP/s): 170.689
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1520
Attention throughput (in TFLOP/s): 201.678
MLP duration (in seconds): 0.3102
MLP throughput (in TFLOP/s): 188.465
Transformer duration (in seconds): 0.4704
Transformer throughput (in TFLOP/s): 189.447
Transformer - MLP - Attention (in seconds): 0.0082
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.0871
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 254.772
Elapsed time for attention_key_query_prob (512x2048x166x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x166x2048): 82.371
Elapsed time for attention_prob_times_values (512x2048x2048x166): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x166): 108.363
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0288
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 257.237
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1160
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 255.114
Elapsed time for mlp_fused_gelu (2048x4x84992): 0.0023
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1869
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 158.270
Elapsed time for transformer_add_bias_dropout (2048x4x21248): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21248): 0.0009

Attention duration (in seconds): 0.2009
Attention throughput (in TFLOP/s): 154.393
MLP duration (in seconds): 0.3053
MLP throughput (in TFLOP/s): 193.851
Transformer duration (in seconds): 0.5110
Transformer throughput (in TFLOP/s): 176.483
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1417
Attention throughput (in TFLOP/s): 218.831
MLP duration (in seconds): 0.3046
MLP throughput (in TFLOP/s): 194.265
Transformer duration (in seconds): 0.4541
Transformer throughput (in TFLOP/s): 198.611
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.0877
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 256.059
Elapsed time for attention_key_query_prob (512x2048x167x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x167x2048): 59.337
Elapsed time for attention_prob_times_values (512x2048x2048x167): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x167): 65.131
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 257.347
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1172
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 255.415
Elapsed time for mlp_fused_gelu (2048x4x85504): 0.0024
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1890
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 158.435
Elapsed time for transformer_add_bias_dropout (2048x4x21376): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21376): 0.0009

Attention duration (in seconds): 0.2097
Attention throughput (in TFLOP/s): 149.659
MLP duration (in seconds): 0.3086
MLP throughput (in TFLOP/s): 194.071
Transformer duration (in seconds): 0.5231
Transformer throughput (in TFLOP/s): 174.467
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1550
Attention throughput (in TFLOP/s): 202.407
MLP duration (in seconds): 0.3121
MLP throughput (in TFLOP/s): 191.886
Transformer duration (in seconds): 0.4730
Transformer throughput (in TFLOP/s): 192.968
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.0893
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 254.462
Elapsed time for attention_key_query_prob (512x2048x168x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x168x2048): 111.643
Elapsed time for attention_prob_times_values (512x2048x2048x168): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x168): 155.090
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0294
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 257.614
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1186
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 255.580
Elapsed time for mlp_fused_gelu (2048x4x86016): 0.0024
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 160.023
Elapsed time for transformer_add_bias_dropout (2048x4x21504): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21504): 0.0009

Attention duration (in seconds): 0.1996
Attention throughput (in TFLOP/s): 159.038
MLP duration (in seconds): 0.3103
MLP throughput (in TFLOP/s): 195.315
Transformer duration (in seconds): 0.5148
Transformer throughput (in TFLOP/s): 179.393
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1432
Attention throughput (in TFLOP/s): 221.651
MLP duration (in seconds): 0.3113
MLP throughput (in TFLOP/s): 194.730
Transformer duration (in seconds): 0.4619
Transformer throughput (in TFLOP/s): 199.962
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.0902
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 255.100
Elapsed time for attention_key_query_prob (512x2048x169x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x169x2048): 59.900
Elapsed time for attention_prob_times_values (512x2048x2048x169): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x169): 65.719
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 256.876
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1202
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 255.146
Elapsed time for mlp_fused_gelu (2048x4x86528): 0.0024
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1945
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 157.674
Elapsed time for transformer_add_bias_dropout (2048x4x21632): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21632): 0.0009

Attention duration (in seconds): 0.2130
Attention throughput (in TFLOP/s): 150.820
MLP duration (in seconds): 0.3171
MLP throughput (in TFLOP/s): 193.437
Transformer duration (in seconds): 0.5349
Transformer throughput (in TFLOP/s): 174.698
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1580
Attention throughput (in TFLOP/s): 203.324
MLP duration (in seconds): 0.3130
MLP throughput (in TFLOP/s): 195.952
Transformer duration (in seconds): 0.4788
Transformer throughput (in TFLOP/s): 195.200
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.0911
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 255.405
Elapsed time for attention_key_query_prob (512x2048x170x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x170x2048): 84.335
Elapsed time for attention_prob_times_values (512x2048x2048x170): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x170): 111.944
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 258.066
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1219
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 254.494
Elapsed time for mlp_fused_gelu (2048x4x87040): 0.0024
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1936
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 160.277
Elapsed time for transformer_add_bias_dropout (2048x4x21760): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21760): 0.0009

Attention duration (in seconds): 0.2062
Attention throughput (in TFLOP/s): 157.610
MLP duration (in seconds): 0.3179
MLP throughput (in TFLOP/s): 195.202
Transformer duration (in seconds): 0.5290
Transformer throughput (in TFLOP/s): 178.729
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1477
Attention throughput (in TFLOP/s): 219.927
MLP duration (in seconds): 0.3134
MLP throughput (in TFLOP/s): 198.052
Transformer duration (in seconds): 0.4699
Transformer throughput (in TFLOP/s): 201.211
Transformer - MLP - Attention (in seconds): 0.0088
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.0923
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 255.177
Elapsed time for attention_key_query_prob (512x2048x171x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x171x2048): 60.476
Elapsed time for attention_prob_times_values (512x2048x2048x171): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x171): 68.974
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 257.082
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.1233
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 254.567
Elapsed time for mlp_fused_gelu (2048x4x87552): 0.0024
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.2026
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 154.951
Elapsed time for transformer_add_bias_dropout (2048x4x21888): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21888): 0.0009

Attention duration (in seconds): 0.2154
Attention throughput (in TFLOP/s): 152.589
MLP duration (in seconds): 0.3284
MLP throughput (in TFLOP/s): 191.229
Transformer duration (in seconds): 0.5488
Transformer throughput (in TFLOP/s): 174.321
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1607
Attention throughput (in TFLOP/s): 204.483
MLP duration (in seconds): 0.3292
MLP throughput (in TFLOP/s): 190.758
Transformer duration (in seconds): 0.4967
Transformer throughput (in TFLOP/s): 192.593
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.0934
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 255.186
Elapsed time for attention_key_query_prob (512x2048x172x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x172x2048): 84.939
Elapsed time for attention_prob_times_values (512x2048x2048x172): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x172): 113.091
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 260.458
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.1247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 254.732
Elapsed time for mlp_fused_gelu (2048x4x88064): 0.0024
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.2066
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 153.772
Elapsed time for transformer_add_bias_dropout (2048x4x22016): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22016): 0.0009

Attention duration (in seconds): 0.2089
Attention throughput (in TFLOP/s): 159.153
MLP duration (in seconds): 0.3337
MLP throughput (in TFLOP/s): 190.383
Transformer duration (in seconds): 0.5476
Transformer throughput (in TFLOP/s): 176.731
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1499
Attention throughput (in TFLOP/s): 221.819
MLP duration (in seconds): 0.3327
MLP throughput (in TFLOP/s): 190.937
Transformer duration (in seconds): 0.4878
Transformer throughput (in TFLOP/s): 198.379
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.0939
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 256.551
Elapsed time for attention_key_query_prob (512x2048x173x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x173x2048): 60.865
Elapsed time for attention_prob_times_values (512x2048x2048x173): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x173): 69.302
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 257.905
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.1262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 254.580
Elapsed time for mlp_fused_gelu (2048x4x88576): 0.0024
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.2097
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 153.233
Elapsed time for transformer_add_bias_dropout (2048x4x22144): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22144): 0.0009

Attention duration (in seconds): 0.2178
Attention throughput (in TFLOP/s): 154.362
MLP duration (in seconds): 0.3384
MLP throughput (in TFLOP/s): 189.935
Transformer duration (in seconds): 0.5613
Transformer throughput (in TFLOP/s): 174.418
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1631
Attention throughput (in TFLOP/s): 206.130
MLP duration (in seconds): 0.3352
MLP throughput (in TFLOP/s): 191.767
Transformer duration (in seconds): 0.5061
Transformer throughput (in TFLOP/s): 193.428
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.0956
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 255.131
Elapsed time for attention_key_query_prob (512x2048x174x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x174x2048): 85.444
Elapsed time for attention_prob_times_values (512x2048x2048x174): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x174): 113.746
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 258.607
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.1275
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 254.944
Elapsed time for mlp_fused_gelu (2048x4x89088): 0.0025
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.2070
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 157.078
Elapsed time for transformer_add_bias_dropout (2048x4x22272): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22272): 0.0009

Attention duration (in seconds): 0.2121
Attention throughput (in TFLOP/s): 160.310
MLP duration (in seconds): 0.3369
MLP throughput (in TFLOP/s): 192.974
Transformer duration (in seconds): 0.5541
Transformer throughput (in TFLOP/s): 178.704
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1531
Attention throughput (in TFLOP/s): 222.072
MLP duration (in seconds): 0.3324
MLP throughput (in TFLOP/s): 195.600
Transformer duration (in seconds): 0.4944
Transformer throughput (in TFLOP/s): 200.264
Transformer - MLP - Attention (in seconds): 0.0089
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.0967
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 254.950
Elapsed time for attention_key_query_prob (512x2048x175x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x175x2048): 61.144
Elapsed time for attention_prob_times_values (512x2048x2048x175): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x175): 67.951
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0316
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 260.383
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.1289
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 255.122
Elapsed time for mlp_fused_gelu (2048x4x89600): 0.0025
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1366
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 240.643
Elapsed time for transformer_add_bias_dropout (2048x4x22400): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22400): 0.0009

Attention duration (in seconds): 0.2215
Attention throughput (in TFLOP/s): 155.274
MLP duration (in seconds): 0.2680
MLP throughput (in TFLOP/s): 245.391
Transformer duration (in seconds): 0.4946
Transformer throughput (in TFLOP/s): 202.505
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1667
Attention throughput (in TFLOP/s): 206.279
MLP duration (in seconds): 0.2719
MLP throughput (in TFLOP/s): 241.853
Transformer duration (in seconds): 0.4450
Transformer throughput (in TFLOP/s): 225.081
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.0980
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 254.422
Elapsed time for attention_key_query_prob (512x2048x176x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x176x2048): 123.641
Elapsed time for attention_prob_times_values (512x2048x2048x176): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x176): 165.141
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 257.493
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.1301
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 255.733
Elapsed time for mlp_fused_gelu (2048x4x90112): 0.0025
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1382
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 240.743
Elapsed time for transformer_add_bias_dropout (2048x4x22528): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22528): 0.0009

Attention duration (in seconds): 0.2108
Attention throughput (in TFLOP/s): 164.936
MLP duration (in seconds): 0.2707
MLP throughput (in TFLOP/s): 245.741
Transformer duration (in seconds): 0.4867
Transformer throughput (in TFLOP/s): 208.138
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1551
Attention throughput (in TFLOP/s): 224.193
MLP duration (in seconds): 0.2740
MLP throughput (in TFLOP/s): 242.770
Transformer duration (in seconds): 0.4378
Transformer throughput (in TFLOP/s): 231.359
Transformer - MLP - Attention (in seconds): 0.0087
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.0991
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 254.641
Elapsed time for attention_key_query_prob (512x2048x177x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x177x2048): 61.513
Elapsed time for attention_prob_times_values (512x2048x2048x177): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x177): 68.361
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 257.644
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.1314
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 256.043
Elapsed time for mlp_fused_gelu (2048x4x90624): 0.0025
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1394
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 241.377
Elapsed time for transformer_add_bias_dropout (2048x4x22656): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22656): 0.0009

Attention duration (in seconds): 0.2250
Attention throughput (in TFLOP/s): 156.276
MLP duration (in seconds): 0.2732
MLP throughput (in TFLOP/s): 246.226
Transformer duration (in seconds): 0.5034
Transformer throughput (in TFLOP/s): 203.500
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1695
Attention throughput (in TFLOP/s): 207.391
MLP duration (in seconds): 0.2772
MLP throughput (in TFLOP/s): 242.672
Transformer duration (in seconds): 0.4549
Transformer throughput (in TFLOP/s): 225.202
Transformer - MLP - Attention (in seconds): 0.0081
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.0996
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 256.064
Elapsed time for attention_key_query_prob (512x2048x178x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x178x2048): 86.765
Elapsed time for attention_prob_times_values (512x2048x2048x178): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x178): 115.623
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0607
(3739811840, 42481549312)
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0328
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 258.950
(3739811840, 42481549312)
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 255.437
Elapsed time for mlp_fused_gelu (2048x4x91136): 0.0025
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1411
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 241.106
Elapsed time for transformer_add_bias_dropout (2048x4x22784): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22784): 0.0009

Attention duration (in seconds): 0.2177
Attention throughput (in TFLOP/s): 163.293
MLP duration (in seconds): 0.2768
MLP throughput (in TFLOP/s): 245.818
Transformer duration (in seconds): 0.4997
Transformer throughput (in TFLOP/s): 207.320
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1593
Attention throughput (in TFLOP/s): 223.117
MLP duration (in seconds): 0.2806
MLP throughput (in TFLOP/s): 242.509
[2023-06-27 14:08:51,322] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-27 14:08:52,188] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.146.165, master_port=6000
[2023-06-27 14:08:52,188] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-27 14:08:54,963] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[2023-06-27 15:32:43,596] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-27 15:32:44,336] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.156.211, master_port=6000
[2023-06-27 15:32:44,336] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-27 15:32:46,653] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25792x77376, b=2048): 0.1302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25792x77376, b=2048): 251.157
Elapsed time for attention_key_query_prob (256x2048x403x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x403x2048): 73.653
Elapsed time for attention_prob_times_values (256x2048x2048x403): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x403): 73.343
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24765857792, 42481549312)
Elapsed time for attention_linear_projection (4x25792x25792, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_linear_projection (4x25792x25792, b=2048): 260.390
(24765857792, 42481549312)
Elapsed time for mlp_h_to_4h (4x25792x103168, b=2048): 0.1733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25792x103168, b=2048): 251.545
Elapsed time for mlp_fused_gelu (2048x4x103168): 0.0029
Elapsed time for mlp_4h_to_h (4x103168x25792, b=2048): 0.1812
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103168x25792, b=2048): 240.599
Elapsed time for transformer_add_bias_dropout (2048x4x25792): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25792): 0.0011

Attention duration (in seconds): 0.2311
Attention throughput (in TFLOP/s): 196.175
MLP duration (in seconds): 0.3574
MLP throughput (in TFLOP/s): 243.933
Transformer duration (in seconds): 0.5944
Transformer throughput (in TFLOP/s): 222.946
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.1302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 252.381
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 106.007
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 121.678
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(19441188864, 42481549312)
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0420
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 260.993
(19441188864, 42481549312)
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.1740
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 251.727
Elapsed time for mlp_fused_gelu (2048x4x103424): 0.0029
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.1809
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 242.210
Elapsed time for transformer_add_bias_dropout (2048x4x25856): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25856): 0.0010

Attention duration (in seconds): 0.2229
Attention throughput (in TFLOP/s): 204.299
MLP duration (in seconds): 0.3579
MLP throughput (in TFLOP/s): 244.848
Transformer duration (in seconds): 0.5867
Transformer throughput (in TFLOP/s): 226.995
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25920x77760, b=2048): 0.1305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25920x77760, b=2048): 253.106
Elapsed time for attention_key_query_prob (256x2048x405x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x405x2048): 74.035
Elapsed time for attention_prob_times_values (256x2048x2048x405): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x405): 73.562
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(14091354112, 42481549312)
Elapsed time for attention_linear_projection (4x25920x25920, b=2048): 0.0427
Throughput (in TFLOP/s) for attention_linear_projection (4x25920x25920, b=2048): 258.047
(14091354112, 42481549312)
Elapsed time for mlp_h_to_4h (4x25920x103680, b=2048): 0.1749
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25920x103680, b=2048): 251.806
Elapsed time for mlp_fused_gelu (2048x4x103680): 0.0029
Elapsed time for mlp_4h_to_h (4x103680x25920, b=2048): 0.1824
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103680x25920, b=2048): 241.397
Elapsed time for transformer_add_bias_dropout (2048x4x25920): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25920): 0.0011

Attention duration (in seconds): 0.2321
Attention throughput (in TFLOP/s): 197.164
MLP duration (in seconds): 0.3602
MLP throughput (in TFLOP/s): 244.475
Transformer duration (in seconds): 0.5983
Transformer throughput (in TFLOP/s): 223.699
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.1313
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 252.718
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 106.207
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 122.129
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(8716353536, 42481549312)
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0428
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 258.476
(8716353536, 42481549312)
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.1760
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 251.461
Elapsed time for mlp_fused_gelu (2048x4x103936): 0.0029
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.1832
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 241.543
Elapsed time for transformer_add_bias_dropout (2048x4x25984): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25984): 0.0011

Attention duration (in seconds): 0.2249
Attention throughput (in TFLOP/s): 204.499
MLP duration (in seconds): 0.3621
MLP throughput (in TFLOP/s): 244.398
Transformer duration (in seconds): 0.5929
Transformer throughput (in TFLOP/s): 226.830
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26048x78144, b=2048): 0.1320
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26048x78144, b=2048): 252.603
Elapsed time for attention_key_query_prob (256x2048x407x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x407x2048): 74.470
Elapsed time for attention_prob_times_values (256x2048x2048x407): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x407): 72.600
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(3314089984, 42481549312)
Elapsed time for attention_linear_projection (4x26048x26048, b=2048): 0.0429
Throughput (in TFLOP/s) for attention_linear_projection (4x26048x26048, b=2048): 258.969
(3314089984, 42481549312)
Elapsed time for mlp_h_to_4h (4x26048x104192, b=2048): 0.1755
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26048x104192, b=2048): 253.424
Elapsed time for mlp_fused_gelu (2048x4x104192): 0.0030
Elapsed time for mlp_4h_to_h (4x104192x26048, b=2048): 0.1841
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104192x26048, b=2048): 241.474
Elapsed time for transformer_add_bias_dropout (2048x4x26048): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26048): 0.0011

Attention duration (in seconds): 0.2342
Attention throughput (in TFLOP/s): 197.358
MLP duration (in seconds): 0.3626
MLP throughput (in TFLOP/s): 245.291
Transformer duration (in seconds): 0.6027
Transformer throughput (in TFLOP/s): 224.237
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1321
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 253.615
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 189.283
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 194.123
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25032196096, 42481549312)
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0430
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 259.501
(25032196096, 42481549312)
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.1784
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 250.530
Elapsed time for mlp_fused_gelu (2048x4x104448): 0.0030
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1845
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 242.224
Elapsed time for transformer_add_bias_dropout (2048x4x26112): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26112): 0.0011

Attention duration (in seconds): 0.2198
Attention throughput (in TFLOP/s): 211.294
MLP duration (in seconds): 0.3658
MLP throughput (in TFLOP/s): 244.316
Transformer duration (in seconds): 0.5915
Transformer throughput (in TFLOP/s): 229.598
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26176x78528, b=2048): 0.1328
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26176x78528, b=2048): 253.637
Elapsed time for attention_key_query_prob (256x2048x409x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x409x2048): 74.695
Elapsed time for attention_prob_times_values (256x2048x2048x409): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x409): 72.506
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19577503744, 42481549312)
Elapsed time for attention_linear_projection (4x26176x26176, b=2048): 0.0432
Throughput (in TFLOP/s) for attention_linear_projection (4x26176x26176, b=2048): 259.978
(19577503744, 42481549312)
Elapsed time for mlp_h_to_4h (4x26176x104704, b=2048): 0.1779
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26176x104704, b=2048): 252.383
Elapsed time for mlp_fused_gelu (2048x4x104704): 0.0030
Elapsed time for mlp_4h_to_h (4x104704x26176, b=2048): 0.1858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104704x26176, b=2048): 241.670
Elapsed time for transformer_add_bias_dropout (2048x4x26176): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26176): 0.0011

Attention duration (in seconds): 0.2352
Attention throughput (in TFLOP/s): 198.353
MLP duration (in seconds): 0.3667
MLP throughput (in TFLOP/s): 244.913
Transformer duration (in seconds): 0.6079
Transformer throughput (in TFLOP/s): 224.478
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1343
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 251.956
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 107.123
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 122.636
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(14095548416, 42481549312)
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0433
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 260.674
(14095548416, 42481549312)
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.1799
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 250.816
Elapsed time for mlp_fused_gelu (2048x4x104960): 0.0030
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1873
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 240.954
Elapsed time for transformer_add_bias_dropout (2048x4x26240): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26240): 0.0011

Attention duration (in seconds): 0.2284
Attention throughput (in TFLOP/s): 205.273
MLP duration (in seconds): 0.3702
MLP throughput (in TFLOP/s): 243.812
Transformer duration (in seconds): 0.6045
Transformer throughput (in TFLOP/s): 226.850
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26304x78912, b=2048): 0.1351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26304x78912, b=2048): 251.646
Elapsed time for attention_key_query_prob (256x2048x411x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x411x2048): 74.621
Elapsed time for attention_prob_times_values (256x2048x2048x411): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x411): 74.917
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8586330112, 42481549312)
Elapsed time for attention_linear_projection (4x26304x26304, b=2048): 0.0440
Throughput (in TFLOP/s) for attention_linear_projection (4x26304x26304, b=2048): 257.518
(8586330112, 42481549312)
Elapsed time for mlp_h_to_4h (4x26304x105216, b=2048): 0.1810
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26304x105216, b=2048): 250.484
Elapsed time for mlp_fused_gelu (2048x4x105216): 0.0030
Elapsed time for mlp_4h_to_h (4x105216x26304, b=2048): 0.1881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105216x26304, b=2048): 241.072
Elapsed time for transformer_add_bias_dropout (2048x4x26304): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26304): 0.0011

Attention duration (in seconds): 0.2382
Attention throughput (in TFLOP/s): 197.790
MLP duration (in seconds): 0.3721
MLP throughput (in TFLOP/s): 243.720
Transformer duration (in seconds): 0.6163
Transformer throughput (in TFLOP/s): 223.591
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.1357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 251.753
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 107.627
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 123.034
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3049848832, 42481549312)
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0441
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 258.117
(3049848832, 42481549312)
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.1817
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 250.760
Elapsed time for mlp_fused_gelu (2048x4x105472): 0.0030
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.1880
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 242.424
Elapsed time for transformer_add_bias_dropout (2048x4x26368): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26368): 0.0011

Attention duration (in seconds): 0.2307
Attention throughput (in TFLOP/s): 205.185
MLP duration (in seconds): 0.3727
MLP throughput (in TFLOP/s): 244.545
Transformer duration (in seconds): 0.6093
Transformer throughput (in TFLOP/s): 227.244
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26432x79296, b=2048): 0.1357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26432x79296, b=2048): 253.100
Elapsed time for attention_key_query_prob (256x2048x413x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x413x2048): 74.951
Elapsed time for attention_prob_times_values (256x2048x2048x413): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x413): 75.422
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24872812544, 42481549312)
Elapsed time for attention_linear_projection (4x26432x26432, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_linear_projection (4x26432x26432, b=2048): 258.713
(24872812544, 42481549312)
Elapsed time for mlp_h_to_4h (4x26432x105728, b=2048): 0.1823
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26432x105728, b=2048): 251.158
Elapsed time for mlp_fused_gelu (2048x4x105728): 0.0030
Elapsed time for mlp_4h_to_h (4x105728x26432, b=2048): 0.1894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105728x26432, b=2048): 241.742
Elapsed time for transformer_add_bias_dropout (2048x4x26432): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26432): 0.0011

Attention duration (in seconds): 0.2390
Attention throughput (in TFLOP/s): 199.035
MLP duration (in seconds): 0.3747
MLP throughput (in TFLOP/s): 244.390
Transformer duration (in seconds): 0.6197
Transformer throughput (in TFLOP/s): 224.512
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1372
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 251.591
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 107.942
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 124.287
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(19281805312, 42481549312)
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 259.344
(19281805312, 42481549312)
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.1837
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 250.468
Elapsed time for mlp_fused_gelu (2048x4x105984): 0.0030
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1907
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 241.239
Elapsed time for transformer_add_bias_dropout (2048x4x26496): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26496): 0.0011

Attention duration (in seconds): 0.2323
Attention throughput (in TFLOP/s): 205.689
MLP duration (in seconds): 0.3774
MLP throughput (in TFLOP/s): 243.812
Transformer duration (in seconds): 0.6157
Transformer throughput (in TFLOP/s): 227.051
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26560x79680, b=2048): 0.1379
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26560x79680, b=2048): 251.517
Elapsed time for attention_key_query_prob (256x2048x415x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x415x2048): 75.761
Elapsed time for attention_prob_times_values (256x2048x2048x415): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x415): 74.919
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(13663535104, 42481549312)
Elapsed time for attention_linear_projection (4x26560x26560, b=2048): 0.0445
Throughput (in TFLOP/s) for attention_linear_projection (4x26560x26560, b=2048): 259.544
(13663535104, 42481549312)
Elapsed time for mlp_h_to_4h (4x26560x106240, b=2048): 0.1848
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26560x106240, b=2048): 250.215
Elapsed time for mlp_fused_gelu (2048x4x106240): 0.0030
Elapsed time for mlp_4h_to_h (4x106240x26560, b=2048): 0.1913
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106240x26560, b=2048): 241.695
Elapsed time for transformer_add_bias_dropout (2048x4x26560): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26560): 0.0011

Attention duration (in seconds): 0.2415
Attention throughput (in TFLOP/s): 198.833
MLP duration (in seconds): 0.3791
MLP throughput (in TFLOP/s): 243.929
Transformer duration (in seconds): 0.6266
Transformer throughput (in TFLOP/s): 224.192
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.1376
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 253.231
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 203.807
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 201.777
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(8018001920, 42481549312)
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 260.280
(8018001920, 42481549312)
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.1842
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 252.229
Elapsed time for mlp_fused_gelu (2048x4x106496): 0.0030
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1916
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 242.460
Elapsed time for transformer_add_bias_dropout (2048x4x26624): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26624): 0.0011

Attention duration (in seconds): 0.2264
Attention throughput (in TFLOP/s): 213.035
MLP duration (in seconds): 0.3788
MLP throughput (in TFLOP/s): 245.280
Transformer duration (in seconds): 0.6113
Transformer throughput (in TFLOP/s): 230.913
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26688x80064, b=2048): 0.1385
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26688x80064, b=2048): 252.805
Elapsed time for attention_key_query_prob (256x2048x417x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x417x2048): 73.982
Elapsed time for attention_prob_times_values (256x2048x2048x417): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x417): 74.628
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(2347302912, 42481549312)
Elapsed time for attention_linear_projection (4x26688x26688, b=2048): 0.0449
Throughput (in TFLOP/s) for attention_linear_projection (4x26688x26688, b=2048): 260.009
(2347302912, 42481549312)
Elapsed time for mlp_h_to_4h (4x26688x106752, b=2048): 0.1864
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26688x106752, b=2048): 250.371
Elapsed time for mlp_fused_gelu (2048x4x106752): 0.0030
Elapsed time for mlp_4h_to_h (4x106752x26688, b=2048): 0.1937
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106752x26688, b=2048): 240.998
Elapsed time for transformer_add_bias_dropout (2048x4x26688): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26688): 0.0011

Attention duration (in seconds): 0.2429
Attention throughput (in TFLOP/s): 199.547
MLP duration (in seconds): 0.3831
MLP throughput (in TFLOP/s): 243.658
Transformer duration (in seconds): 0.6322
Transformer throughput (in TFLOP/s): 224.350
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.1390
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 253.062
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 109.074
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 125.333
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24709234688, 42481549312)
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0450
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 260.847
(24709234688, 42481549312)
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.1870
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 250.834
Elapsed time for mlp_fused_gelu (2048x4x107008): 0.0030
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.1937
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 242.151
Elapsed time for transformer_add_bias_dropout (2048x4x26752): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26752): 0.0011

Attention duration (in seconds): 0.2348
Attention throughput (in TFLOP/s): 207.410
MLP duration (in seconds): 0.3837
MLP throughput (in TFLOP/s): 244.467
Transformer duration (in seconds): 0.6246
Transformer throughput (in TFLOP/s): 228.157
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26816x80448, b=2048): 0.1400
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26816x80448, b=2048): 252.389
Elapsed time for attention_key_query_prob (256x2048x419x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x419x2048): 72.966
Elapsed time for attention_prob_times_values (256x2048x2048x419): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x419): 76.244
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18981912576, 42481549312)
Elapsed time for attention_linear_projection (4x26816x26816, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_linear_projection (4x26816x26816, b=2048): 258.332
(18981912576, 42481549312)
Elapsed time for mlp_h_to_4h (4x26816x107264, b=2048): 0.1879
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26816x107264, b=2048): 250.830
Elapsed time for mlp_fused_gelu (2048x4x107264): 0.0030
Elapsed time for mlp_4h_to_h (4x107264x26816, b=2048): 0.1957
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107264x26816, b=2048): 240.807
Elapsed time for transformer_add_bias_dropout (2048x4x26816): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26816): 0.0011

Attention duration (in seconds): 0.2452
Attention throughput (in TFLOP/s): 199.550
MLP duration (in seconds): 0.3866
MLP throughput (in TFLOP/s): 243.785
Transformer duration (in seconds): 0.6379
Transformer throughput (in TFLOP/s): 224.441
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.1406
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 252.593
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 109.523
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 124.832
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(13227327488, 42481549312)
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 259.040
(13227327488, 42481549312)
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.1886
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 251.021
Elapsed time for mlp_fused_gelu (2048x4x107520): 0.0030
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.1967
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 240.746
Elapsed time for transformer_add_bias_dropout (2048x4x26880): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26880): 0.0011

Attention duration (in seconds): 0.2372
Attention throughput (in TFLOP/s): 207.278
MLP duration (in seconds): 0.3884
MLP throughput (in TFLOP/s): 243.850
Transformer duration (in seconds): 0.6316
Transformer throughput (in TFLOP/s): 227.765
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26944x80832, b=2048): 0.1417
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26944x80832, b=2048): 251.835
Elapsed time for attention_key_query_prob (256x2048x421x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x421x2048): 72.960
Elapsed time for attention_prob_times_values (256x2048x2048x421): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x421): 76.122
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7445479424, 42481549312)
Elapsed time for attention_linear_projection (4x26944x26944, b=2048): 0.0459
Throughput (in TFLOP/s) for attention_linear_projection (4x26944x26944, b=2048): 259.323
(7445479424, 42481549312)
Elapsed time for mlp_h_to_4h (4x26944x107776, b=2048): 0.1906
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26944x107776, b=2048): 249.627
Elapsed time for mlp_fused_gelu (2048x4x107776): 0.0031
Elapsed time for mlp_4h_to_h (4x107776x26944, b=2048): 0.1975
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107776x26944, b=2048): 240.919
Elapsed time for transformer_add_bias_dropout (2048x4x26944): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26944): 0.0011

Attention duration (in seconds): 0.2472
Attention throughput (in TFLOP/s): 199.769
MLP duration (in seconds): 0.3911
MLP throughput (in TFLOP/s): 243.283
Transformer duration (in seconds): 0.6445
Transformer throughput (in TFLOP/s): 224.269
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.1419
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 252.741
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 109.780
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 125.608
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1636368384, 42481549312)
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0460
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 259.849
(1636368384, 42481549312)
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.1909
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 250.467
Elapsed time for mlp_fused_gelu (2048x4x108032): 0.0031
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.1976
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 241.932
Elapsed time for transformer_add_bias_dropout (2048x4x27008): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27008): 0.0011

Attention duration (in seconds): 0.2387
Attention throughput (in TFLOP/s): 207.833
MLP duration (in seconds): 0.3915
MLP throughput (in TFLOP/s): 244.202
Transformer duration (in seconds): 0.6364
Transformer throughput (in TFLOP/s): 228.208
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27072x81216, b=2048): 0.1434
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27072x81216, b=2048): 251.195
Elapsed time for attention_key_query_prob (256x2048x423x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x423x2048): 72.575
Elapsed time for attention_prob_times_values (256x2048x2048x423): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x423): 74.827
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24547753984, 42481549312)
Elapsed time for attention_linear_projection (4x27072x27072, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_linear_projection (4x27072x27072, b=2048): 260.209
(24547753984, 42481549312)
Elapsed time for mlp_h_to_4h (4x27072x108288, b=2048): 0.1916
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27072x108288, b=2048): 250.738
Elapsed time for mlp_fused_gelu (2048x4x108288): 0.0031
Elapsed time for mlp_4h_to_h (4x108288x27072, b=2048): 0.1988
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108288x27072, b=2048): 241.647
Elapsed time for transformer_add_bias_dropout (2048x4x27072): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27072): 0.0011

Attention duration (in seconds): 0.2496
Attention throughput (in TFLOP/s): 199.673
MLP duration (in seconds): 0.3934
MLP throughput (in TFLOP/s): 244.191
Transformer duration (in seconds): 0.6492
Transformer throughput (in TFLOP/s): 224.747
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.1431
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 252.879
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 185.148
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 199.013
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18684116992, 42481549312)
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0463
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 260.792
(18684116992, 42481549312)
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.1928
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 250.360
Elapsed time for mlp_fused_gelu (2048x4x108544): 0.0031
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.2001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 241.211
Elapsed time for transformer_add_bias_dropout (2048x4x27136): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27136): 0.0011

Attention duration (in seconds): 0.2343
Attention throughput (in TFLOP/s): 213.753
MLP duration (in seconds): 0.3959
MLP throughput (in TFLOP/s): 243.793
Transformer duration (in seconds): 0.6363
Transformer throughput (in TFLOP/s): 230.374
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27200x81600, b=2048): 0.1442
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27200x81600, b=2048): 252.235
Elapsed time for attention_key_query_prob (256x2048x425x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x425x2048): 72.578
Elapsed time for attention_prob_times_values (256x2048x2048x425): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x425): 74.915
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12793217024, 42481549312)
Elapsed time for attention_linear_projection (4x27200x27200, b=2048): 0.0470
Throughput (in TFLOP/s) for attention_linear_projection (4x27200x27200, b=2048): 258.017
(12793217024, 42481549312)
Elapsed time for mlp_h_to_4h (4x27200x108800, b=2048): 0.1940
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27200x108800, b=2048): 249.945
Elapsed time for mlp_fused_gelu (2048x4x108800): 0.0031
Elapsed time for mlp_4h_to_h (4x108800x27200, b=2048): 0.2018
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108800x27200, b=2048): 240.266
Elapsed time for transformer_add_bias_dropout (2048x4x27200): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27200): 0.0011

Attention duration (in seconds): 0.2513
Attention throughput (in TFLOP/s): 200.195
MLP duration (in seconds): 0.3989
MLP throughput (in TFLOP/s): 243.118
Transformer duration (in seconds): 0.6564
Transformer throughput (in TFLOP/s): 224.379
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.1452
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 251.628
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 110.448
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 126.925
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6872956928, 42481549312)
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 258.680
(6872956928, 42481549312)
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.1944
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 250.557
Elapsed time for mlp_fused_gelu (2048x4x109056): 0.0031
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.2015
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 241.728
Elapsed time for transformer_add_bias_dropout (2048x4x27264): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27264): 0.0011

Attention duration (in seconds): 0.2432
Attention throughput (in TFLOP/s): 207.851
MLP duration (in seconds): 0.3990
MLP throughput (in TFLOP/s): 244.159
Transformer duration (in seconds): 0.6484
Transformer throughput (in TFLOP/s): 228.214
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27328x81984, b=2048): 0.1461
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27328x81984, b=2048): 251.200
Elapsed time for attention_key_query_prob (256x2048x427x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x427x2048): 72.822
Elapsed time for attention_prob_times_values (256x2048x2048x427): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x427): 77.447
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(925433856, 42481549312)
Elapsed time for attention_linear_projection (4x27328x27328, b=2048): 0.0477
Throughput (in TFLOP/s) for attention_linear_projection (4x27328x27328, b=2048): 256.337
(925433856, 42481549312)
Elapsed time for mlp_h_to_4h (4x27328x109312, b=2048): 0.1957
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27328x109312, b=2048): 250.156
Elapsed time for mlp_fused_gelu (2048x4x109312): 0.0031
Elapsed time for mlp_4h_to_h (4x109312x27328, b=2048): 0.2038
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109312x27328, b=2048): 240.199
Elapsed time for transformer_add_bias_dropout (2048x4x27328): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27328): 0.0011

Attention duration (in seconds): 0.2537
Attention throughput (in TFLOP/s): 200.153
MLP duration (in seconds): 0.4025
MLP throughput (in TFLOP/s): 243.187
Transformer duration (in seconds): 0.6625
Transformer throughput (in TFLOP/s): 224.407
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.1466
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 251.564
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 111.522
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 127.441
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24382078976, 42481549312)
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0473
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 259.723
(24382078976, 42481549312)
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.1966
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 250.095
Elapsed time for mlp_fused_gelu (2048x4x109568): 0.0031
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.2030
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 242.190
Elapsed time for transformer_add_bias_dropout (2048x4x27392): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27392): 0.0011

Attention duration (in seconds): 0.2448
Attention throughput (in TFLOP/s): 208.350
MLP duration (in seconds): 0.4028
MLP throughput (in TFLOP/s): 244.179
Transformer duration (in seconds): 0.6539
Transformer throughput (in TFLOP/s): 228.421
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27456x82368, b=2048): 0.1462
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27456x82368, b=2048): 253.362
Elapsed time for attention_key_query_prob (256x2048x429x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x429x2048): 73.293
Elapsed time for attention_prob_times_values (256x2048x2048x429): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x429): 77.623
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18377932800, 42481549312)
Elapsed time for attention_linear_projection (4x27456x27456, b=2048): 0.0475
Throughput (in TFLOP/s) for attention_linear_projection (4x27456x27456, b=2048): 260.027
(18377932800, 42481549312)
Elapsed time for mlp_h_to_4h (4x27456x109824, b=2048): 0.1980
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27456x109824, b=2048): 249.556
Elapsed time for mlp_fused_gelu (2048x4x109824): 0.0031
Elapsed time for mlp_4h_to_h (4x109824x27456, b=2048): 0.2055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109824x27456, b=2048): 240.439
Elapsed time for transformer_add_bias_dropout (2048x4x27456): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27456): 0.0011

Attention duration (in seconds): 0.2536
Attention throughput (in TFLOP/s): 202.087
MLP duration (in seconds): 0.4065
MLP throughput (in TFLOP/s): 243.037
Transformer duration (in seconds): 0.6664
Transformer throughput (in TFLOP/s): 225.159
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.1476
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 252.166
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 111.991
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 128.053
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12346523648, 42481549312)
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 260.706
(12346523648, 42481549312)
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.1979
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 250.828
Elapsed time for mlp_fused_gelu (2048x4x110080): 0.0031
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.2064
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 240.520
Elapsed time for transformer_add_bias_dropout (2048x4x27520): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27520): 0.0011

Attention duration (in seconds): 0.2461
Attention throughput (in TFLOP/s): 209.204
MLP duration (in seconds): 0.4074
MLP throughput (in TFLOP/s): 243.686
Transformer duration (in seconds): 0.6597
Transformer throughput (in TFLOP/s): 228.515
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27584x82752, b=2048): 0.1486
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27584x82752, b=2048): 251.656
Elapsed time for attention_key_query_prob (256x2048x431x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x431x2048): 73.509
Elapsed time for attention_prob_times_values (256x2048x2048x431): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x431): 76.561
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6285754368, 42481549312)
Elapsed time for attention_linear_projection (4x27584x27584, b=2048): 0.0485
Throughput (in TFLOP/s) for attention_linear_projection (4x27584x27584, b=2048): 257.241
(6285754368, 42481549312)
Elapsed time for mlp_h_to_4h (4x27584x110336, b=2048): 0.1995
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27584x110336, b=2048): 249.907
Elapsed time for mlp_fused_gelu (2048x4x110336): 0.0031
Elapsed time for mlp_4h_to_h (4x110336x27584, b=2048): 0.2075
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110336x27584, b=2048): 240.299
Elapsed time for transformer_add_bias_dropout (2048x4x27584): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27584): 0.0011

Attention duration (in seconds): 0.2571
Attention throughput (in TFLOP/s): 201.116
MLP duration (in seconds): 0.4102
MLP throughput (in TFLOP/s): 243.142
Transformer duration (in seconds): 0.6736
Transformer throughput (in TFLOP/s): 224.822
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.1491
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 252.037
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 195.151
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 206.820
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(197722112, 42481549312)
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0480
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 261.052
(197722112, 42481549312)
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.1996
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 250.947
Elapsed time for mlp_fused_gelu (2048x4x110592): 0.0031
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.2069
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 242.074
Elapsed time for transformer_add_bias_dropout (2048x4x27648): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27648): 0.0011

Attention duration (in seconds): 0.2417
Attention throughput (in TFLOP/s): 214.951
MLP duration (in seconds): 0.4097
MLP throughput (in TFLOP/s): 244.543
Transformer duration (in seconds): 0.6577
Transformer throughput (in TFLOP/s): 231.328
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27712x83136, b=2048): 0.1503
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27712x83136, b=2048): 251.171
Elapsed time for attention_key_query_prob (256x2048x433x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x433x2048): 73.939
Elapsed time for attention_prob_times_values (256x2048x2048x433): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x433): 76.738
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24216403968, 42481549312)
Elapsed time for attention_linear_projection (4x27712x27712, b=2048): 0.0486
Throughput (in TFLOP/s) for attention_linear_projection (4x27712x27712, b=2048): 258.898
(24216403968, 42481549312)
Elapsed time for mlp_h_to_4h (4x27712x110848, b=2048): 0.2015
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27712x110848, b=2048): 249.806
Elapsed time for mlp_fused_gelu (2048x4x110848): 0.0031
Elapsed time for mlp_4h_to_h (4x110848x27712, b=2048): 0.2098
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110848x27712, b=2048): 239.861
Elapsed time for transformer_add_bias_dropout (2048x4x27712): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27712): 0.0012

Attention duration (in seconds): 0.2590
Attention throughput (in TFLOP/s): 201.484
MLP duration (in seconds): 0.4144
MLP throughput (in TFLOP/s): 242.876
Transformer duration (in seconds): 0.6798
Transformer throughput (in TFLOP/s): 224.836
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.1508
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 251.540
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 112.997
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 129.176
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18071748608, 42481549312)
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0487
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 259.440
(18071748608, 42481549312)
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.2021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 250.196
Elapsed time for mlp_fused_gelu (2048x4x111104): 0.0032
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.2098
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 241.001
Elapsed time for transformer_add_bias_dropout (2048x4x27776): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27776): 0.0011

Attention duration (in seconds): 0.2503
Attention throughput (in TFLOP/s): 209.412
MLP duration (in seconds): 0.4150
MLP throughput (in TFLOP/s): 243.648
Transformer duration (in seconds): 0.6717
Transformer throughput (in TFLOP/s): 228.592
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27840x83520, b=2048): 0.1510
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27840x83520, b=2048): 252.248
Elapsed time for attention_key_query_prob (256x2048x435x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x435x2048): 74.287
Elapsed time for attention_prob_times_values (256x2048x2048x435): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x435): 78.869
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11897733120, 42481549312)
Elapsed time for attention_linear_projection (4x27840x27840, b=2048): 0.0489
Throughput (in TFLOP/s) for attention_linear_projection (4x27840x27840, b=2048): 259.581
(11897733120, 42481549312)
Elapsed time for mlp_h_to_4h (4x27840x111360, b=2048): 0.2025
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27840x111360, b=2048): 250.825
Elapsed time for mlp_fused_gelu (2048x4x111360): 0.0032
Elapsed time for mlp_4h_to_h (4x111360x27840, b=2048): 0.2112
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111360x27840, b=2048): 240.541
Elapsed time for transformer_add_bias_dropout (2048x4x27840): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27840): 0.0012

Attention duration (in seconds): 0.2598
Attention throughput (in TFLOP/s): 202.730
MLP duration (in seconds): 0.4168
MLP throughput (in TFLOP/s): 243.715
Transformer duration (in seconds): 0.6830
Transformer throughput (in TFLOP/s): 225.841
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.1520
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 251.799
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 113.314
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 129.489
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5696454656, 42481549312)
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 257.097
(5696454656, 42481549312)
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.2037
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 250.565
Elapsed time for mlp_fused_gelu (2048x4x111616): 0.0032
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.2116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 241.203
Elapsed time for transformer_add_bias_dropout (2048x4x27904): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27904): 0.0011

Attention duration (in seconds): 0.2525
Attention throughput (in TFLOP/s): 209.497
MLP duration (in seconds): 0.4184
MLP throughput (in TFLOP/s): 243.938
Transformer duration (in seconds): 0.6773
Transformer throughput (in TFLOP/s): 228.804
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27968x83904, b=2048): 0.1528
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27968x83904, b=2048): 251.688
Elapsed time for attention_key_query_prob (256x2048x437x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x437x2048): 75.142
Elapsed time for attention_prob_times_values (256x2048x2048x437): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x437): 79.085
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24080089088, 42481549312)
Elapsed time for attention_linear_projection (4x27968x27968, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_linear_projection (4x27968x27968, b=2048): 256.178
(24080089088, 42481549312)
Elapsed time for mlp_h_to_4h (4x27968x111872, b=2048): 0.2052
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27968x111872, b=2048): 249.846
Elapsed time for mlp_fused_gelu (2048x4x111872): 0.0032
Elapsed time for mlp_4h_to_h (4x111872x27968, b=2048): 0.2142
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111872x27968, b=2048): 239.346
Elapsed time for transformer_add_bias_dropout (2048x4x27968): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27968): 0.0012

Attention duration (in seconds): 0.2626
Attention throughput (in TFLOP/s): 202.395
MLP duration (in seconds): 0.4225
MLP throughput (in TFLOP/s): 242.650
Transformer duration (in seconds): 0.6915
Transformer throughput (in TFLOP/s): 225.119
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.1535
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 251.645
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 113.677
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 130.186
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17822187520, 42481549312)
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 257.499
(17822187520, 42481549312)
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.2060
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 249.994
Elapsed time for mlp_fused_gelu (2048x4x112128): 0.0032
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.2140
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 240.635
Elapsed time for transformer_add_bias_dropout (2048x4x28032): 0.0020
Elapsed time for transformer_layer_norm (2048x4x28032): 0.0011

Attention duration (in seconds): 0.2544
Attention throughput (in TFLOP/s): 209.850
MLP duration (in seconds): 0.4232
MLP throughput (in TFLOP/s): 243.385
Transformer duration (in seconds): 0.6839
Transformer throughput (in TFLOP/s): 228.644
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28096x84288, b=2048): 0.1543
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28096x84288, b=2048): 251.451
Elapsed time for attention_key_query_prob (256x2048x439x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x439x2048): 74.659
Elapsed time for attention_prob_times_values (256x2048x2048x439): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x439): 77.571
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11534925824, 42481549312)
Elapsed time for attention_linear_projection (4x28096x28096, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x28096x28096, b=2048): 255.336
(11534925824, 42481549312)
Elapsed time for mlp_h_to_4h (4x28096x112384, b=2048): 0.2072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28096x112384, b=2048): 249.624
Elapsed time for mlp_fused_gelu (2048x4x112384): 0.0032
Elapsed time for mlp_4h_to_h (4x112384x28096, b=2048): 0.2148
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112384x28096, b=2048): 240.810
Elapsed time for transformer_add_bias_dropout (2048x4x28096): 0.0020
Elapsed time for transformer_layer_norm (2048x4x28096): 0.0012

Attention duration (in seconds): 0.2651
Attention throughput (in TFLOP/s): 202.240
MLP duration (in seconds): 0.4253
MLP throughput (in TFLOP/s): 243.303
Transformer duration (in seconds): 0.6968
Transformer throughput (in TFLOP/s): 225.425
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.1547
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 251.914
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 187.207
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 206.544
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5218304000, 42481549312)
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 256.210
(5218304000, 42481549312)
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.2074
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 250.519
Elapsed time for mlp_fused_gelu (2048x4x112640): 0.0032
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.2155
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 241.201
Elapsed time for transformer_add_bias_dropout (2048x4x28160): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28160): 0.0012

Attention duration (in seconds): 0.2504
Attention throughput (in TFLOP/s): 215.050
MLP duration (in seconds): 0.4261
MLP throughput (in TFLOP/s): 243.934
Transformer duration (in seconds): 0.6830
Transformer throughput (in TFLOP/s): 231.052
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28224x84672, b=2048): 0.1554
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28224x84672, b=2048): 251.923
Elapsed time for attention_key_query_prob (256x2048x441x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x441x2048): 75.151
Elapsed time for attention_prob_times_values (256x2048x2048x441): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x441): 77.683
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23945871360, 42481549312)
Elapsed time for attention_linear_projection (4x28224x28224, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_linear_projection (4x28224x28224, b=2048): 259.484
(23945871360, 42481549312)
Elapsed time for mlp_h_to_4h (4x28224x112896, b=2048): 0.2087
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28224x112896, b=2048): 250.173
Elapsed time for mlp_fused_gelu (2048x4x112896): 0.0032
Elapsed time for mlp_4h_to_h (4x112896x28224, b=2048): 0.2175
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112896x28224, b=2048): 240.065
Elapsed time for transformer_add_bias_dropout (2048x4x28224): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28224): 0.0012

Attention duration (in seconds): 0.2659
Attention throughput (in TFLOP/s): 203.445
MLP duration (in seconds): 0.4293
MLP throughput (in TFLOP/s): 243.191
Transformer duration (in seconds): 0.7017
Transformer throughput (in TFLOP/s): 225.894
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.1564
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 251.457
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 115.166
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 131.579
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17572626432, 42481549312)
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0511
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 256.775
(17572626432, 42481549312)
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.2104
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 249.244
Elapsed time for mlp_fused_gelu (2048x4x113152): 0.0032
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.2182
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 240.319
Elapsed time for transformer_add_bias_dropout (2048x4x28288): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28288): 0.0011

Attention duration (in seconds): 0.2583
Attention throughput (in TFLOP/s): 210.361
MLP duration (in seconds): 0.4318
MLP throughput (in TFLOP/s): 242.885
Transformer duration (in seconds): 0.6966
Transformer throughput (in TFLOP/s): 228.588
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28352x85056, b=2048): 0.1576
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28352x85056, b=2048): 250.688
Elapsed time for attention_key_query_prob (256x2048x443x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x443x2048): 76.227
Elapsed time for attention_prob_times_values (256x2048x2048x443): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x443): 80.709
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11170021376, 42481549312)
Elapsed time for attention_linear_projection (4x28352x28352, b=2048): 0.0514
Throughput (in TFLOP/s) for attention_linear_projection (4x28352x28352, b=2048): 256.378
(11170021376, 42481549312)
Elapsed time for mlp_h_to_4h (4x28352x113408, b=2048): 0.2107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28352x113408, b=2048): 250.036
Elapsed time for mlp_fused_gelu (2048x4x113408): 0.0032
Elapsed time for mlp_4h_to_h (4x113408x28352, b=2048): 0.2194
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113408x28352, b=2048): 240.126
Elapsed time for transformer_add_bias_dropout (2048x4x28352): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28352): 0.0012

Attention duration (in seconds): 0.2686
Attention throughput (in TFLOP/s): 203.186
MLP duration (in seconds): 0.4333
MLP throughput (in TFLOP/s): 243.166
Transformer duration (in seconds): 0.7084
Transformer throughput (in TFLOP/s): 225.781
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.1578
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 251.538
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 115.711
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 132.402
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4738056192, 42481549312)
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 257.679
(4738056192, 42481549312)
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.2115
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 250.157
Elapsed time for mlp_fused_gelu (2048x4x113664): 0.0032
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.2205
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 239.998
Elapsed time for transformer_add_bias_dropout (2048x4x28416): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28416): 0.0011

Attention duration (in seconds): 0.2600
Attention throughput (in TFLOP/s): 210.892
MLP duration (in seconds): 0.4353
MLP throughput (in TFLOP/s): 243.159
Transformer duration (in seconds): 0.7017
Transformer throughput (in TFLOP/s): 228.970
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28480x85440, b=2048): 0.1585
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28480x85440, b=2048): 251.532
Elapsed time for attention_key_query_prob (256x2048x445x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x445x2048): 76.928
Elapsed time for attention_prob_times_values (256x2048x2048x445): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x445): 81.136
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25954942976, 42481549312)
Elapsed time for attention_linear_projection (4x28480x28480, b=2048): 0.0521
Throughput (in TFLOP/s) for attention_linear_projection (4x28480x28480, b=2048): 255.127
(25954942976, 42481549312)
Elapsed time for mlp_h_to_4h (4x28480x113920, b=2048): 0.2126
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28480x113920, b=2048): 250.086
Elapsed time for mlp_fused_gelu (2048x4x113920): 0.0032
Elapsed time for mlp_4h_to_h (4x113920x28480, b=2048): 0.2218
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113920x28480, b=2048): 239.621
Elapsed time for transformer_add_bias_dropout (2048x4x28480): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28480): 0.0012

Attention duration (in seconds): 0.2702
Attention throughput (in TFLOP/s): 203.813
MLP duration (in seconds): 0.4376
MLP throughput (in TFLOP/s): 242.938
Transformer duration (in seconds): 0.7143
Transformer throughput (in TFLOP/s): 225.927
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.1592
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 251.473
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 116.081
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 133.418
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19464257536, 42481549312)
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 258.634
(19464257536, 42481549312)
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.2133
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 250.331
Elapsed time for mlp_fused_gelu (2048x4x114176): 0.0032
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.2218
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 240.737
Elapsed time for transformer_add_bias_dropout (2048x4x28544): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28544): 0.0012

Attention duration (in seconds): 0.2617
Attention throughput (in TFLOP/s): 211.355
MLP duration (in seconds): 0.4383
MLP throughput (in TFLOP/s): 243.631
Transformer duration (in seconds): 0.7065
Transformer throughput (in TFLOP/s): 229.444
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28608x85824, b=2048): 0.1598
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28608x85824, b=2048): 251.743
Elapsed time for attention_key_query_prob (256x2048x447x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x447x2048): 77.108
Elapsed time for attention_prob_times_values (256x2048x2048x447): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x447): 80.186
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12944211968, 42481549312)
Elapsed time for attention_linear_projection (4x28608x28608, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_linear_projection (4x28608x28608, b=2048): 255.820
(12944211968, 42481549312)
Elapsed time for mlp_h_to_4h (4x28608x114432, b=2048): 0.2144
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28608x114432, b=2048): 250.169
Elapsed time for mlp_fused_gelu (2048x4x114432): 0.0032
Elapsed time for mlp_4h_to_h (4x114432x28608, b=2048): 0.2239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114432x28608, b=2048): 239.522
Elapsed time for transformer_add_bias_dropout (2048x4x28608): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28608): 0.0012

Attention duration (in seconds): 0.2720
Attention throughput (in TFLOP/s): 204.229
MLP duration (in seconds): 0.4416
MLP throughput (in TFLOP/s): 242.934
Transformer duration (in seconds): 0.7202
Transformer throughput (in TFLOP/s): 226.099
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.1607
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 251.441
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 202.908
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 214.537
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6394806272, 42481549312)
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0526
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 256.098
(6394806272, 42481549312)
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.2153
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 250.270
Elapsed time for mlp_fused_gelu (2048x4x114688): 0.0032
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2244
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 240.122
Elapsed time for transformer_add_bias_dropout (2048x4x28672): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28672): 0.0012

Attention duration (in seconds): 0.2579
Attention throughput (in TFLOP/s): 216.335
MLP duration (in seconds): 0.4429
MLP throughput (in TFLOP/s): 243.292
Transformer duration (in seconds): 0.7073
Transformer throughput (in TFLOP/s): 231.225
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28736x86208, b=2048): 0.1615
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28736x86208, b=2048): 251.270
Elapsed time for attention_key_query_prob (256x2048x449x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x449x2048): 75.706
Elapsed time for attention_prob_times_values (256x2048x2048x449): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x449): 79.620
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25818628096, 42481549312)
Elapsed time for attention_linear_projection (4x28736x28736, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_linear_projection (4x28736x28736, b=2048): 256.099
(25818628096, 42481549312)
Elapsed time for mlp_h_to_4h (4x28736x114944, b=2048): 0.2179
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28736x114944, b=2048): 248.340
Elapsed time for mlp_fused_gelu (2048x4x114944): 0.0033
Elapsed time for mlp_4h_to_h (4x114944x28736, b=2048): 0.2267
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114944x28736, b=2048): 238.690
Elapsed time for transformer_add_bias_dropout (2048x4x28736): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28736): 0.0012

Attention duration (in seconds): 0.2746
Attention throughput (in TFLOP/s): 204.100
MLP duration (in seconds): 0.4479
MLP throughput (in TFLOP/s): 241.653
Transformer duration (in seconds): 0.7291
Transformer throughput (in TFLOP/s): 225.331
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.1621
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 251.500
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 108.823
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 133.713
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(19210502144, 42481549312)
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0527
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 257.628
(19210502144, 42481549312)
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.2168
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 250.705
Elapsed time for mlp_fused_gelu (2048x4x115200): 0.0033
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.2263
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 240.195
Elapsed time for transformer_add_bias_dropout (2048x4x28800): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28800): 0.0012

Attention duration (in seconds): 0.2664
Attention throughput (in TFLOP/s): 211.317
MLP duration (in seconds): 0.4464
MLP throughput (in TFLOP/s): 243.547
Transformer duration (in seconds): 0.7193
Transformer throughput (in TFLOP/s): 229.397
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28864x86592, b=2048): 0.1632
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28864x86592, b=2048): 250.929
Elapsed time for attention_key_query_prob (256x2048x451x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x451x2048): 75.839
Elapsed time for attention_prob_times_values (256x2048x2048x451): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x451): 81.486
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(12573016064, 42481549312)
Elapsed time for attention_linear_projection (4x28864x28864, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_linear_projection (4x28864x28864, b=2048): 256.838
(12573016064, 42481549312)
Elapsed time for mlp_h_to_4h (4x28864x115456, b=2048): 0.2185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28864x115456, b=2048): 249.841
Elapsed time for mlp_fused_gelu (2048x4x115456): 0.0033
Elapsed time for mlp_4h_to_h (4x115456x28864, b=2048): 0.2281
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115456x28864, b=2048): 239.406
Elapsed time for transformer_add_bias_dropout (2048x4x28864): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28864): 0.0012

Attention duration (in seconds): 0.2764
Attention throughput (in TFLOP/s): 204.530
MLP duration (in seconds): 0.4499
MLP throughput (in TFLOP/s): 242.736
Transformer duration (in seconds): 0.7329
Transformer throughput (in TFLOP/s): 226.141
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.1634
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 251.783
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 109.284
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 132.740
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(5906169856, 42481549312)
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 257.076
(5906169856, 42481549312)
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.2192
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 250.240
Elapsed time for mlp_fused_gelu (2048x4x115712): 0.0033
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.2286
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 239.858
Elapsed time for transformer_add_bias_dropout (2048x4x28928): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28928): 0.0012

Attention duration (in seconds): 0.2683
Attention throughput (in TFLOP/s): 211.622
MLP duration (in seconds): 0.4511
MLP throughput (in TFLOP/s): 243.156
Transformer duration (in seconds): 0.7260
Transformer throughput (in TFLOP/s): 229.305
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28992x86976, b=2048): 0.1644
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28992x86976, b=2048): 251.362
Elapsed time for attention_key_query_prob (256x2048x453x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x453x2048): 76.001
Elapsed time for attention_prob_times_values (256x2048x2048x453): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x453): 81.538
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25678118912, 42481549312)
Elapsed time for attention_linear_projection (4x28992x28992, b=2048): 0.0539
Throughput (in TFLOP/s) for attention_linear_projection (4x28992x28992, b=2048): 255.652
(25678118912, 42481549312)
Elapsed time for mlp_h_to_4h (4x28992x115968, b=2048): 0.2203
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28992x115968, b=2048): 250.054
Elapsed time for mlp_fused_gelu (2048x4x115968): 0.0033
Elapsed time for mlp_4h_to_h (4x115968x28992, b=2048): 0.2306
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115968x28992, b=2048): 238.888
Elapsed time for transformer_add_bias_dropout (2048x4x28992): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28992): 0.0012

Attention duration (in seconds): 0.2784
Attention throughput (in TFLOP/s): 204.878
MLP duration (in seconds): 0.4542
MLP throughput (in TFLOP/s): 242.579
Transformer duration (in seconds): 0.7392
Transformer throughput (in TFLOP/s): 226.208
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.1652
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 251.173
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 109.714
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 131.976
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18952552448, 42481549312)
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 259.592
(18952552448, 42481549312)
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.2212
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 250.143
Elapsed time for mlp_fused_gelu (2048x4x116224): 0.0033
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.2309
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 239.611
Elapsed time for transformer_add_bias_dropout (2048x4x29056): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29056): 0.0012

Attention duration (in seconds): 0.2702
Attention throughput (in TFLOP/s): 212.010
MLP duration (in seconds): 0.4554
MLP throughput (in TFLOP/s): 242.996
Transformer duration (in seconds): 0.7321
Transformer throughput (in TFLOP/s): 229.379
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29120x87360, b=2048): 0.1657
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29120x87360, b=2048): 251.510
Elapsed time for attention_key_query_prob (256x2048x455x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x455x2048): 75.300
Elapsed time for attention_prob_times_values (256x2048x2048x455): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x455): 79.143
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12197625856, 42481549312)
Elapsed time for attention_linear_projection (4x29120x29120, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_linear_projection (4x29120x29120, b=2048): 256.093
(12197625856, 42481549312)
Elapsed time for mlp_h_to_4h (4x29120x116480, b=2048): 0.2223
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29120x116480, b=2048): 250.026
Elapsed time for mlp_fused_gelu (2048x4x116480): 0.0033
Elapsed time for mlp_4h_to_h (4x116480x29120, b=2048): 0.2324
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116480x29120, b=2048): 239.141
Elapsed time for transformer_add_bias_dropout (2048x4x29120): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29120): 0.0012

Attention duration (in seconds): 0.2807
Attention throughput (in TFLOP/s): 204.952
MLP duration (in seconds): 0.4579
MLP throughput (in TFLOP/s): 242.703
Transformer duration (in seconds): 0.7453
Transformer throughput (in TFLOP/s): 226.318
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.1665
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 251.457
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 187.830
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 211.160
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5413339136, 42481549312)
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0544
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 256.384
(5413339136, 42481549312)
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.2235
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 249.788
Elapsed time for mlp_fused_gelu (2048x4x116736): 0.0033
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.2323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 240.302
Elapsed time for transformer_add_bias_dropout (2048x4x29184): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29184): 0.0012

Attention duration (in seconds): 0.2662
Attention throughput (in TFLOP/s): 217.072
MLP duration (in seconds): 0.4590
MLP throughput (in TFLOP/s): 243.188
Transformer duration (in seconds): 0.7318
Transformer throughput (in TFLOP/s): 231.485
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29248x87744, b=2048): 0.1679
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29248x87744, b=2048): 250.373
Elapsed time for attention_key_query_prob (256x2048x457x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x457x2048): 75.333
Elapsed time for attention_prob_times_values (256x2048x2048x457): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x457): 78.705
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25539706880, 42481549312)
Elapsed time for attention_linear_projection (4x29248x29248, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_linear_projection (4x29248x29248, b=2048): 256.465
(25539706880, 42481549312)
Elapsed time for mlp_h_to_4h (4x29248x116992, b=2048): 0.2250
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29248x116992, b=2048): 249.134
Elapsed time for mlp_fused_gelu (2048x4x116992): 0.0033
Elapsed time for mlp_4h_to_h (4x116992x29248, b=2048): 0.2341
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116992x29248, b=2048): 239.455
Elapsed time for transformer_add_bias_dropout (2048x4x29248): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29248): 0.0012

Attention duration (in seconds): 0.2835
Attention throughput (in TFLOP/s): 204.659
MLP duration (in seconds): 0.4625
MLP throughput (in TFLOP/s): 242.447
Transformer duration (in seconds): 0.7527
Transformer throughput (in TFLOP/s): 226.062
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.1680
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 251.338
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 110.349
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 131.660
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(18694602752, 42481549312)
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 257.683
(18694602752, 42481549312)
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.2265
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 248.616
Elapsed time for mlp_fused_gelu (2048x4x117248): 0.0033
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2349
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 239.733
Elapsed time for transformer_add_bias_dropout (2048x4x29312): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29312): 0.0012

Attention duration (in seconds): 0.2745
Attention throughput (in TFLOP/s): 212.317
MLP duration (in seconds): 0.4647
MLP throughput (in TFLOP/s): 242.350
Transformer duration (in seconds): 0.7458
Transformer throughput (in TFLOP/s): 229.138
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29376x88128, b=2048): 0.1683
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29376x88128, b=2048): 252.035
Elapsed time for attention_key_query_prob (256x2048x459x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x459x2048): 76.327
Elapsed time for attention_prob_times_values (256x2048x2048x459): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x459): 82.133
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(11820138496, 42481549312)
Elapsed time for attention_linear_projection (4x29376x29376, b=2048): 0.0552
Throughput (in TFLOP/s) for attention_linear_projection (4x29376x29376, b=2048): 256.272
(11820138496, 42481549312)
Elapsed time for mlp_h_to_4h (4x29376x117504, b=2048): 0.2258
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29376x117504, b=2048): 250.494
Elapsed time for mlp_fused_gelu (2048x4x117504): 0.0033
Elapsed time for mlp_4h_to_h (4x117504x29376, b=2048): 0.2367
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117504x29376, b=2048): 238.894
Elapsed time for transformer_add_bias_dropout (2048x4x29376): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29376): 0.0012

Attention duration (in seconds): 0.2838
Attention throughput (in TFLOP/s): 206.205
MLP duration (in seconds): 0.4658
MLP throughput (in TFLOP/s): 242.811
Transformer duration (in seconds): 0.7564
Transformer throughput (in TFLOP/s): 226.922
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.1697
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 251.062
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 110.997
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 132.603
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(4916314112, 42481549312)
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 256.150
(4916314112, 42481549312)
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.2269
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 250.379
Elapsed time for mlp_fused_gelu (2048x4x117760): 0.0033
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2370
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 239.707
Elapsed time for transformer_add_bias_dropout (2048x4x29440): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29440): 0.0012

Attention duration (in seconds): 0.2769
Attention throughput (in TFLOP/s): 212.260
MLP duration (in seconds): 0.4672
MLP throughput (in TFLOP/s): 243.175
Transformer duration (in seconds): 0.7507
Transformer throughput (in TFLOP/s): 229.610
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29504x88512, b=2048): 0.1702
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29504x88512, b=2048): 251.389
Elapsed time for attention_key_query_prob (256x2048x461x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x461x2048): 76.016
Elapsed time for attention_prob_times_values (256x2048x2048x461): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x461): 82.340
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25397100544, 42481549312)
Elapsed time for attention_linear_projection (4x29504x29504, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_linear_projection (4x29504x29504, b=2048): 255.349
(25397100544, 42481549312)
Elapsed time for mlp_h_to_4h (4x29504x118016, b=2048): 0.2281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29504x118016, b=2048): 250.066
Elapsed time for mlp_fused_gelu (2048x4x118016): 0.0033
Elapsed time for mlp_4h_to_h (4x118016x29504, b=2048): 0.2386
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118016x29504, b=2048): 239.089
Elapsed time for transformer_add_bias_dropout (2048x4x29504): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29504): 0.0012

Attention duration (in seconds): 0.2865
Attention throughput (in TFLOP/s): 206.044
MLP duration (in seconds): 0.4701
MLP throughput (in TFLOP/s): 242.713
Transformer duration (in seconds): 0.7633
Transformer throughput (in TFLOP/s): 226.805
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.1710
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 251.284
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 111.221
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 132.873
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18432458752, 42481549312)
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 256.099
(18432458752, 42481549312)
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.2293
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 249.873
Elapsed time for mlp_fused_gelu (2048x4x118272): 0.0033
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2394
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 239.283
Elapsed time for transformer_add_bias_dropout (2048x4x29568): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29568): 0.0012

Attention duration (in seconds): 0.2787
Attention throughput (in TFLOP/s): 212.679
MLP duration (in seconds): 0.4721
MLP throughput (in TFLOP/s): 242.730
Transformer duration (in seconds): 0.7576
Transformer throughput (in TFLOP/s): 229.514
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29632x88896, b=2048): 0.1724
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29632x88896, b=2048): 250.276
Elapsed time for attention_key_query_prob (256x2048x463x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x463x2048): 75.830
Elapsed time for attention_prob_times_values (256x2048x2048x463): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x463): 80.650
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11436359680, 42481549312)
Elapsed time for attention_linear_projection (4x29632x29632, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_linear_projection (4x29632x29632, b=2048): 256.017
(11436359680, 42481549312)
Elapsed time for mlp_h_to_4h (4x29632x118528, b=2048): 0.2302
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29632x118528, b=2048): 250.008
Elapsed time for mlp_fused_gelu (2048x4x118528): 0.0034
Elapsed time for mlp_4h_to_h (4x118528x29632, b=2048): 0.2414
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118528x29632, b=2048): 238.409
Elapsed time for transformer_add_bias_dropout (2048x4x29632): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29632): 0.0012

Attention duration (in seconds): 0.2895
Attention throughput (in TFLOP/s): 205.661
MLP duration (in seconds): 0.4749
MLP throughput (in TFLOP/s): 242.344
Transformer duration (in seconds): 0.7711
Transformer throughput (in TFLOP/s): 226.446
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.1714
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 252.874
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 197.316
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 216.613
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4410900480, 42481549312)
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 256.888
(4410900480, 42481549312)
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.2323
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 248.803
Elapsed time for mlp_fused_gelu (2048x4x118784): 0.0034
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2412
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 239.569
Elapsed time for transformer_add_bias_dropout (2048x4x29696): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29696): 0.0012

Attention duration (in seconds): 0.2727
Attention throughput (in TFLOP/s): 219.241
MLP duration (in seconds): 0.4769
MLP throughput (in TFLOP/s): 242.379
Transformer duration (in seconds): 0.7563
Transformer throughput (in TFLOP/s): 231.872
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29760x89280, b=2048): 0.1733
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29760x89280, b=2048): 251.234
Elapsed time for attention_key_query_prob (256x2048x465x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x465x2048): 76.177
Elapsed time for attention_prob_times_values (256x2048x2048x465): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x465): 80.702
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25256591360, 42481549312)
Elapsed time for attention_linear_projection (4x29760x29760, b=2048): 0.0569
Throughput (in TFLOP/s) for attention_linear_projection (4x29760x29760, b=2048): 255.099
(25256591360, 42481549312)
Elapsed time for mlp_h_to_4h (4x29760x119040, b=2048): 0.2330
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29760x119040, b=2048): 249.138
Elapsed time for mlp_fused_gelu (2048x4x119040): 0.0034
Elapsed time for mlp_4h_to_h (4x119040x29760, b=2048): 0.2431
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119040x29760, b=2048): 238.723
Elapsed time for transformer_add_bias_dropout (2048x4x29760): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29760): 0.0012

Attention duration (in seconds): 0.2911
Attention throughput (in TFLOP/s): 206.276
MLP duration (in seconds): 0.4795
MLP throughput (in TFLOP/s): 242.101
Transformer duration (in seconds): 0.7774
Transformer throughput (in TFLOP/s): 226.561
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.1735
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 251.918
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 111.941
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 134.088
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18170314752, 42481549312)
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0565
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 257.901
(18170314752, 42481549312)
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.2331
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 250.102
Elapsed time for mlp_fused_gelu (2048x4x119296): 0.0034
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.2439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 238.997
Elapsed time for transformer_add_bias_dropout (2048x4x29824): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29824): 0.0012

Attention duration (in seconds): 0.2819
Attention throughput (in TFLOP/s): 213.915
MLP duration (in seconds): 0.4804
MLP throughput (in TFLOP/s): 242.700
Transformer duration (in seconds): 0.7690
Transformer throughput (in TFLOP/s): 230.004
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29888x89664, b=2048): 0.1743
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29888x89664, b=2048): 251.951
Elapsed time for attention_key_query_prob (256x2048x467x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x467x2048): 76.314
Elapsed time for attention_prob_times_values (256x2048x2048x467): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x467): 83.125
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(11052580864, 42481549312)
Elapsed time for attention_linear_projection (4x29888x29888, b=2048): 0.0573
Throughput (in TFLOP/s) for attention_linear_projection (4x29888x29888, b=2048): 255.229
(11052580864, 42481549312)
Elapsed time for mlp_h_to_4h (4x29888x119552, b=2048): 0.2354
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29888x119552, b=2048): 248.726
Elapsed time for mlp_fused_gelu (2048x4x119552): 0.0034
Elapsed time for mlp_4h_to_h (4x119552x29888, b=2048): 0.2451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119552x29888, b=2048): 238.813
Elapsed time for transformer_add_bias_dropout (2048x4x29888): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29888): 0.0012

Attention duration (in seconds): 0.2922
Attention throughput (in TFLOP/s): 207.188
MLP duration (in seconds): 0.4839
MLP throughput (in TFLOP/s): 241.964
Transformer duration (in seconds): 0.7830
Transformer throughput (in TFLOP/s): 226.870
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.1758
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 250.788
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 112.768
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 134.672
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3905486848, 42481549312)
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0565
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 259.960
(3905486848, 42481549312)
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.2368
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 248.254
Elapsed time for mlp_fused_gelu (2048x4x119808): 0.0034
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.2450
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 239.977
Elapsed time for transformer_add_bias_dropout (2048x4x29952): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29952): 0.0012

Attention duration (in seconds): 0.2841
Attention throughput (in TFLOP/s): 213.995
MLP duration (in seconds): 0.4852
MLP throughput (in TFLOP/s): 242.337
Transformer duration (in seconds): 0.7761
Transformer throughput (in TFLOP/s): 229.844
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30016x90048, b=2048): 0.1763
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30016x90048, b=2048): 251.171
Elapsed time for attention_key_query_prob (256x2048x469x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x469x2048): 76.563
Elapsed time for attention_prob_times_values (256x2048x2048x469): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x469): 83.264
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25111887872, 42481549312)
Elapsed time for attention_linear_projection (4x30016x30016, b=2048): 0.0577
Throughput (in TFLOP/s) for attention_linear_projection (4x30016x30016, b=2048): 255.856
(25111887872, 42481549312)
Elapsed time for mlp_h_to_4h (4x30016x120064, b=2048): 0.2366
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30016x120064, b=2048): 249.540
Elapsed time for mlp_fused_gelu (2048x4x120064): 0.0034
Elapsed time for mlp_4h_to_h (4x120064x30016, b=2048): 0.2471
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120064x30016, b=2048): 238.928
Elapsed time for transformer_add_bias_dropout (2048x4x30016): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30016): 0.0012

Attention duration (in seconds): 0.2947
Attention throughput (in TFLOP/s): 207.215
MLP duration (in seconds): 0.4871
MLP throughput (in TFLOP/s): 242.416
Transformer duration (in seconds): 0.7887
Transformer throughput (in TFLOP/s): 227.160
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.1769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 251.379
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 112.938
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 135.468
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17903976448, 42481549312)
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 256.083
(17903976448, 42481549312)
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.2386
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 248.499
Elapsed time for mlp_fused_gelu (2048x4x120320): 0.0034
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2480
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 239.137
Elapsed time for transformer_add_bias_dropout (2048x4x30080): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30080): 0.0012

Attention duration (in seconds): 0.2866
Attention throughput (in TFLOP/s): 213.939
MLP duration (in seconds): 0.4900
MLP throughput (in TFLOP/s): 242.031
Transformer duration (in seconds): 0.7834
Transformer throughput (in TFLOP/s): 229.652
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30144x90432, b=2048): 0.1780
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30144x90432, b=2048): 250.884
Elapsed time for attention_key_query_prob (256x2048x471x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x471x2048): 76.423
Elapsed time for attention_prob_times_values (256x2048x2048x471): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x471): 81.605
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(10664607744, 42481549312)
Elapsed time for attention_linear_projection (4x30144x30144, b=2048): 0.0580
Throughput (in TFLOP/s) for attention_linear_projection (4x30144x30144, b=2048): 256.547
(10664607744, 42481549312)
Elapsed time for mlp_h_to_4h (4x30144x120576, b=2048): 0.2384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30144x120576, b=2048): 249.840
Elapsed time for mlp_fused_gelu (2048x4x120576): 0.0034
Elapsed time for mlp_4h_to_h (4x120576x30144, b=2048): 0.2484
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120576x30144, b=2048): 239.712
Elapsed time for transformer_add_bias_dropout (2048x4x30144): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30144): 0.0012

Attention duration (in seconds): 0.2971
Attention throughput (in TFLOP/s): 207.254
MLP duration (in seconds): 0.4902
MLP throughput (in TFLOP/s): 242.968
Transformer duration (in seconds): 0.7942
Transformer throughput (in TFLOP/s): 227.500
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.1777
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 252.390
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 191.160
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 218.255
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3393781760, 42481549312)
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0583
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 256.652
(3393781760, 42481549312)
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.2394
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 249.851
Elapsed time for mlp_fused_gelu (2048x4x120832): 0.0034
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2492
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 239.937
Elapsed time for transformer_add_bias_dropout (2048x4x30208): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30208): 0.0012

Attention duration (in seconds): 0.2813
Attention throughput (in TFLOP/s): 219.789
MLP duration (in seconds): 0.4920
MLP throughput (in TFLOP/s): 243.093
Transformer duration (in seconds): 0.7802
Transformer throughput (in TFLOP/s): 232.552
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30272x90816, b=2048): 0.1798
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30272x90816, b=2048): 250.559
Elapsed time for attention_key_query_prob (256x2048x473x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x473x2048): 76.443
Elapsed time for attention_prob_times_values (256x2048x2048x473): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x473): 81.534
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24969281536, 42481549312)
Elapsed time for attention_linear_projection (4x30272x30272, b=2048): 0.0590
Throughput (in TFLOP/s) for attention_linear_projection (4x30272x30272, b=2048): 254.679
(24969281536, 42481549312)
Elapsed time for mlp_h_to_4h (4x30272x121088, b=2048): 0.2420
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30272x121088, b=2048): 248.162
Elapsed time for mlp_fused_gelu (2048x4x121088): 0.0034
Elapsed time for mlp_4h_to_h (4x121088x30272, b=2048): 0.2508
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121088x30272, b=2048): 239.474
Elapsed time for transformer_add_bias_dropout (2048x4x30272): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30272): 0.0013

Attention duration (in seconds): 0.2999
Attention throughput (in TFLOP/s): 207.048
MLP duration (in seconds): 0.4962
MLP throughput (in TFLOP/s): 242.058
Transformer duration (in seconds): 0.8030
Transformer throughput (in TFLOP/s): 226.889
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.1801
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 251.189
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 113.699
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 136.351
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17637638144, 42481549312)
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0591
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 255.262
(17637638144, 42481549312)
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.2419
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 249.338
Elapsed time for mlp_fused_gelu (2048x4x121344): 0.0034
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2515
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 239.801
Elapsed time for transformer_add_bias_dropout (2048x4x30336): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30336): 0.0012

Attention duration (in seconds): 0.2910
Attention throughput (in TFLOP/s): 214.273
MLP duration (in seconds): 0.4968
MLP throughput (in TFLOP/s): 242.788
Transformer duration (in seconds): 0.7947
Transformer throughput (in TFLOP/s): 230.242
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30400x91200, b=2048): 0.1813
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30400x91200, b=2048): 250.616
Elapsed time for attention_key_query_prob (256x2048x475x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x475x2048): 76.736
Elapsed time for attention_prob_times_values (256x2048x2048x475): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x475): 84.602
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(10274537472, 42481549312)
Elapsed time for attention_linear_projection (4x30400x30400, b=2048): 0.0594
Throughput (in TFLOP/s) for attention_linear_projection (4x30400x30400, b=2048): 254.974
(10274537472, 42481549312)
Elapsed time for mlp_h_to_4h (4x30400x121600, b=2048): 0.2436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30400x121600, b=2048): 248.585
Elapsed time for mlp_fused_gelu (2048x4x121600): 0.0034
Elapsed time for mlp_4h_to_h (4x121600x30400, b=2048): 0.2525
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121600x30400, b=2048): 239.870
Elapsed time for transformer_add_bias_dropout (2048x4x30400): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30400): 0.0013

Attention duration (in seconds): 0.3014
Attention throughput (in TFLOP/s): 207.722
MLP duration (in seconds): 0.4996
MLP throughput (in TFLOP/s): 242.469
Transformer duration (in seconds): 0.8079
Transformer throughput (in TFLOP/s): 227.425
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.1819
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 250.717
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 114.380
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 137.334
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2879979520, 42481549312)
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0595
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 255.520
(2879979520, 42481549312)
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.2432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 250.039
Elapsed time for mlp_fused_gelu (2048x4x121856): 0.0034
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 242.190
Elapsed time for transformer_add_bias_dropout (2048x4x30464): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30464): 0.0012

Attention duration (in seconds): 0.2932
Attention throughput (in TFLOP/s): 214.387
MLP duration (in seconds): 0.4978
MLP throughput (in TFLOP/s): 244.347
Transformer duration (in seconds): 0.7980
Transformer throughput (in TFLOP/s): 231.224
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30528x91584, b=2048): 0.1825
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30528x91584, b=2048): 250.986
Elapsed time for attention_key_query_prob (256x2048x477x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x477x2048): 77.008
Elapsed time for attention_prob_times_values (256x2048x2048x477): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x477): 85.054
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24822480896, 42481549312)
Elapsed time for attention_linear_projection (4x30528x30528, b=2048): 0.0597
Throughput (in TFLOP/s) for attention_linear_projection (4x30528x30528, b=2048): 255.615
(24822480896, 42481549312)
Elapsed time for mlp_h_to_4h (4x30528x122112, b=2048): 0.2446
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30528x122112, b=2048): 249.708
Elapsed time for mlp_fused_gelu (2048x4x122112): 0.0035
Elapsed time for mlp_4h_to_h (4x122112x30528, b=2048): 0.2548
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122112x30528, b=2048): 239.698
Elapsed time for transformer_add_bias_dropout (2048x4x30528): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30528): 0.0013

Attention duration (in seconds): 0.3030
Attention throughput (in TFLOP/s): 208.337
MLP duration (in seconds): 0.5029
MLP throughput (in TFLOP/s): 242.915
Transformer duration (in seconds): 0.8128
Transformer throughput (in TFLOP/s): 227.938
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.1832
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 251.034
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 114.842
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 138.479
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17365008384, 42481549312)
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0597
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 256.721
(17365008384, 42481549312)
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.2457
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 249.599
Elapsed time for mlp_fused_gelu (2048x4x122368): 0.0035
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2546
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 240.943
Elapsed time for transformer_add_bias_dropout (2048x4x30592): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30592): 0.0013

Attention duration (in seconds): 0.2947
Attention throughput (in TFLOP/s): 215.052
MLP duration (in seconds): 0.5038
MLP throughput (in TFLOP/s): 243.506
Transformer duration (in seconds): 0.8055
Transformer throughput (in TFLOP/s): 230.983
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30656x91968, b=2048): 0.1835
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30656x91968, b=2048): 251.758
Elapsed time for attention_key_query_prob (256x2048x479x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x479x2048): 77.822
Elapsed time for attention_prob_times_values (256x2048x2048x479): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x479): 83.983
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(9876078592, 42481549312)
Elapsed time for attention_linear_projection (4x30656x30656, b=2048): 0.0603
Throughput (in TFLOP/s) for attention_linear_projection (4x30656x30656, b=2048): 255.430
(9876078592, 42481549312)
Elapsed time for mlp_h_to_4h (4x30656x122624, b=2048): 0.2479
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30656x122624, b=2048): 248.488
Elapsed time for mlp_fused_gelu (2048x4x122624): 0.0035
Elapsed time for mlp_4h_to_h (4x122624x30656, b=2048): 0.2565
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122624x30656, b=2048): 240.149
Elapsed time for transformer_add_bias_dropout (2048x4x30656): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30656): 0.0013

Attention duration (in seconds): 0.3047
Attention throughput (in TFLOP/s): 208.912
MLP duration (in seconds): 0.5078
MLP throughput (in TFLOP/s): 242.579
Transformer duration (in seconds): 0.8195
Transformer throughput (in TFLOP/s): 227.985
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.1844
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 251.602
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 206.735
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 224.025
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2355691520, 42481549312)
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0603
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 256.506
(2355691520, 42481549312)
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.2471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 250.281
Elapsed time for mlp_fused_gelu (2048x4x122880): 0.0035
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2570
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 240.679
Elapsed time for transformer_add_bias_dropout (2048x4x30720): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30720): 0.0013

Attention duration (in seconds): 0.2897
Attention throughput (in TFLOP/s): 220.639
MLP duration (in seconds): 0.5076
MLP throughput (in TFLOP/s): 243.706
Transformer duration (in seconds): 0.8042
Transformer throughput (in TFLOP/s): 233.282
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30784x92352, b=2048): 0.1857
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30784x92352, b=2048): 250.764
Elapsed time for attention_key_query_prob (256x2048x481x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x481x2048): 76.421
Elapsed time for attention_prob_times_values (256x2048x2048x481): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x481): 83.466
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24677777408, 42481549312)
Elapsed time for attention_linear_projection (4x30784x30784, b=2048): 0.0607
Throughput (in TFLOP/s) for attention_linear_projection (4x30784x30784, b=2048): 255.889
(24677777408, 42481549312)
Elapsed time for mlp_h_to_4h (4x30784x123136, b=2048): 0.2489
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30784x123136, b=2048): 249.559
Elapsed time for mlp_fused_gelu (2048x4x123136): 0.0035
Elapsed time for mlp_4h_to_h (4x123136x30784, b=2048): 0.2584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123136x30784, b=2048): 240.324
Elapsed time for transformer_add_bias_dropout (2048x4x30784): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30784): 0.0013

Attention duration (in seconds): 0.3077
Attention throughput (in TFLOP/s): 208.543
MLP duration (in seconds): 0.5108
MLP throughput (in TFLOP/s): 243.183
Transformer duration (in seconds): 0.8255
Transformer throughput (in TFLOP/s): 228.202
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.1867
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 250.479
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 116.014
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 139.966
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17094475776, 42481549312)
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0610
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 255.705
(17094475776, 42481549312)
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.2498
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 249.701
Elapsed time for mlp_fused_gelu (2048x4x123392): 0.0035
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.2584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 241.305
Elapsed time for transformer_add_bias_dropout (2048x4x30848): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30848): 0.0012

Attention duration (in seconds): 0.2994
Attention throughput (in TFLOP/s): 215.189
MLP duration (in seconds): 0.5117
MLP throughput (in TFLOP/s): 243.755
Transformer duration (in seconds): 0.8181
Transformer throughput (in TFLOP/s): 231.222
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30912x92736, b=2048): 0.1878
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30912x92736, b=2048): 250.135
Elapsed time for attention_key_query_prob (256x2048x483x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x483x2048): 75.755
Elapsed time for attention_prob_times_values (256x2048x2048x483): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x483): 85.820
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(9479716864, 42481549312)
Elapsed time for attention_linear_projection (4x30912x30912, b=2048): 0.0610
Throughput (in TFLOP/s) for attention_linear_projection (4x30912x30912, b=2048): 256.718
(9479716864, 42481549312)
Elapsed time for mlp_h_to_4h (4x30912x123648, b=2048): 0.2515
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30912x123648, b=2048): 249.010
Elapsed time for mlp_fused_gelu (2048x4x123648): 0.0035
Elapsed time for mlp_4h_to_h (4x123648x30912, b=2048): 0.2608
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123648x30912, b=2048): 240.157
Elapsed time for transformer_add_bias_dropout (2048x4x30912): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30912): 0.0013

Attention duration (in seconds): 0.3099
Attention throughput (in TFLOP/s): 208.745
MLP duration (in seconds): 0.5157
MLP throughput (in TFLOP/s): 242.844
Transformer duration (in seconds): 0.8327
Transformer throughput (in TFLOP/s): 228.098
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.1880
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 250.826
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 116.473
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 140.028
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1833500672, 42481549312)
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 256.923
(1833500672, 42481549312)
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.2518
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 249.743
Elapsed time for mlp_fused_gelu (2048x4x123904): 0.0035
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.2588
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 243.015
Elapsed time for transformer_add_bias_dropout (2048x4x30976): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30976): 0.0012

Attention duration (in seconds): 0.3010
Attention throughput (in TFLOP/s): 215.842
MLP duration (in seconds): 0.5141
MLP throughput (in TFLOP/s): 244.653
Transformer duration (in seconds): 0.8220
Transformer throughput (in TFLOP/s): 232.019
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31040x93120, b=2048): 0.1882
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31040x93120, b=2048): 251.648
Elapsed time for attention_key_query_prob (256x2048x485x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x485x2048): 76.114
Elapsed time for attention_prob_times_values (256x2048x2048x485): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x485): 86.006
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24528879616, 42481549312)
Elapsed time for attention_linear_projection (4x31040x31040, b=2048): 0.0615
Throughput (in TFLOP/s) for attention_linear_projection (4x31040x31040, b=2048): 256.849
(24528879616, 42481549312)
Elapsed time for mlp_h_to_4h (4x31040x124160, b=2048): 0.2530
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31040x124160, b=2048): 249.569
Elapsed time for mlp_fused_gelu (2048x4x124160): 0.0035
Elapsed time for mlp_4h_to_h (4x124160x31040, b=2048): 0.2627
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124160x31040, b=2048): 240.374
Elapsed time for transformer_add_bias_dropout (2048x4x31040): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31040): 0.0013

Attention duration (in seconds): 0.3108
Attention throughput (in TFLOP/s): 209.831
MLP duration (in seconds): 0.5192
MLP throughput (in TFLOP/s): 243.229
Transformer duration (in seconds): 0.8372
Transformer throughput (in TFLOP/s): 228.765
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x93312, b=2048): 0.1888
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x93312, b=2048): 251.851
Elapsed time for attention_key_query_prob (256x2048x486x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x486x2048): 116.839
Elapsed time for attention_prob_times_values (256x2048x2048x486): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x486): 140.448
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(16819748864, 42481549312)
Elapsed time for attention_linear_projection (4x31104x31104, b=2048): 0.0616
Throughput (in TFLOP/s) for attention_linear_projection (4x31104x31104, b=2048): 257.349
(16819748864, 42481549312)
Elapsed time for mlp_h_to_4h (4x31104x124416, b=2048): 0.2546
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x124416, b=2048): 249.039
Elapsed time for mlp_fused_gelu (2048x4x124416): 0.0035
Elapsed time for mlp_4h_to_h (4x124416x31104, b=2048): 0.2627
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124416x31104, b=2048): 241.364
Elapsed time for transformer_add_bias_dropout (2048x4x31104): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31104): 0.0013

Attention duration (in seconds): 0.3022
Attention throughput (in TFLOP/s): 216.723
MLP duration (in seconds): 0.5208
MLP throughput (in TFLOP/s): 243.478
Transformer duration (in seconds): 0.8301
Transformer throughput (in TFLOP/s): 231.659
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31168x93504, b=2048): 0.1905
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31168x93504, b=2048): 250.696
Elapsed time for attention_key_query_prob (256x2048x487x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x487x2048): 75.819
Elapsed time for attention_prob_times_values (256x2048x2048x487): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x487): 84.049
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(9079160832, 42481549312)
Elapsed time for attention_linear_projection (4x31168x31168, b=2048): 0.0623
Throughput (in TFLOP/s) for attention_linear_projection (4x31168x31168, b=2048): 255.434
(9079160832, 42481549312)
Elapsed time for mlp_h_to_4h (4x31168x124672, b=2048): 0.2549
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31168x124672, b=2048): 249.786
Elapsed time for mlp_fused_gelu (2048x4x124672): 0.0035
Elapsed time for mlp_4h_to_h (4x124672x31168, b=2048): 0.2655
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124672x31168, b=2048): 239.788
Elapsed time for transformer_add_bias_dropout (2048x4x31168): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31168): 0.0013

Attention duration (in seconds): 0.3144
Attention throughput (in TFLOP/s): 209.124
MLP duration (in seconds): 0.5239
MLP throughput (in TFLOP/s): 243.035
Transformer duration (in seconds): 0.8455
Transformer throughput (in TFLOP/s): 228.370
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x93696, b=2048): 0.1914
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x93696, b=2048): 250.523
Elapsed time for attention_key_query_prob (256x2048x488x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x488x2048): 191.517
Elapsed time for attention_prob_times_values (256x2048x2048x488): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x488): 222.344
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1307115520, 42481549312)
Elapsed time for attention_linear_projection (4x31232x31232, b=2048): 0.0625
Throughput (in TFLOP/s) for attention_linear_projection (4x31232x31232, b=2048): 255.865
(1307115520, 42481549312)
Elapsed time for mlp_h_to_4h (4x31232x124928, b=2048): 0.2576
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x124928, b=2048): 248.136
Elapsed time for mlp_fused_gelu (2048x4x124928): 0.0035
Elapsed time for mlp_4h_to_h (4x124928x31232, b=2048): 0.2654
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124928x31232, b=2048): 240.872
Elapsed time for transformer_add_bias_dropout (2048x4x31232): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31232): 0.0013

Attention duration (in seconds): 0.2994
Attention throughput (in TFLOP/s): 220.479
MLP duration (in seconds): 0.5266
MLP throughput (in TFLOP/s): 242.811
Transformer duration (in seconds): 0.8331
Transformer throughput (in TFLOP/s): 232.720
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31296x93888, b=2048): 0.1923
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31296x93888, b=2048): 250.374
Elapsed time for attention_key_query_prob (256x2048x489x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x489x2048): 76.198
Elapsed time for attention_prob_times_values (256x2048x2048x489): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x489): 84.091
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24382078976, 42481549312)
Elapsed time for attention_linear_projection (4x31296x31296, b=2048): 0.0627
Throughput (in TFLOP/s) for attention_linear_projection (4x31296x31296, b=2048): 256.027
(24382078976, 42481549312)
Elapsed time for mlp_h_to_4h (4x31296x125184, b=2048): 0.2568
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31296x125184, b=2048): 249.968
Elapsed time for mlp_fused_gelu (2048x4x125184): 0.0035
Elapsed time for mlp_4h_to_h (4x125184x31296, b=2048): 0.2663
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125184x31296, b=2048): 241.016
Elapsed time for transformer_add_bias_dropout (2048x4x31296): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31296): 0.0013

Attention duration (in seconds): 0.3166
Attention throughput (in TFLOP/s): 209.351
MLP duration (in seconds): 0.5267
MLP throughput (in TFLOP/s): 243.761
Transformer duration (in seconds): 0.8504
Transformer throughput (in TFLOP/s): 228.899
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x94080, b=2048): 0.1933
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x94080, b=2048): 250.110
Elapsed time for attention_key_query_prob (256x2048x490x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x490x2048): 117.616
Elapsed time for attention_prob_times_values (256x2048x2048x490): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x490): 141.773
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(16545021952, 42481549312)
Elapsed time for attention_linear_projection (4x31360x31360, b=2048): 0.0630
Throughput (in TFLOP/s) for attention_linear_projection (4x31360x31360, b=2048): 255.630
(16545021952, 42481549312)
Elapsed time for mlp_h_to_4h (4x31360x125440, b=2048): 0.2589
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x125440, b=2048): 248.938
Elapsed time for mlp_fused_gelu (2048x4x125440): 0.0036
Elapsed time for mlp_4h_to_h (4x125440x31360, b=2048): 0.2679
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125440x31360, b=2048): 240.577
Elapsed time for transformer_add_bias_dropout (2048x4x31360): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31360): 0.0013

Attention duration (in seconds): 0.3081
Attention throughput (in TFLOP/s): 216.020
MLP duration (in seconds): 0.5304
MLP throughput (in TFLOP/s): 243.047
Transformer duration (in seconds): 0.8456
Transformer throughput (in TFLOP/s): 231.156
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31424x94272, b=2048): 0.1944
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31424x94272, b=2048): 249.619
Elapsed time for attention_key_query_prob (256x2048x491x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x491x2048): 77.173
Elapsed time for attention_prob_times_values (256x2048x2048x491): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x491): 87.242
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8676507648, 42481549312)
Elapsed time for attention_linear_projection (4x31424x31424, b=2048): 0.0635
Throughput (in TFLOP/s) for attention_linear_projection (4x31424x31424, b=2048): 254.708
(8676507648, 42481549312)
Elapsed time for mlp_h_to_4h (4x31424x125696, b=2048): 0.2610
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31424x125696, b=2048): 247.920
Elapsed time for mlp_fused_gelu (2048x4x125696): 0.0036
Elapsed time for mlp_4h_to_h (4x125696x31424, b=2048): 0.2691
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125696x31424, b=2048): 240.463
Elapsed time for transformer_add_bias_dropout (2048x4x31424): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31424): 0.0013

Attention duration (in seconds): 0.3191
Attention throughput (in TFLOP/s): 209.392
MLP duration (in seconds): 0.5337
MLP throughput (in TFLOP/s): 242.509
Transformer duration (in seconds): 0.8600
Transformer throughput (in TFLOP/s): 228.201
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x94464, b=2048): 0.1946
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x94464, b=2048): 250.426
Elapsed time for attention_key_query_prob (256x2048x492x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x492x2048): 118.744
Elapsed time for attention_prob_times_values (256x2048x2048x492): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x492): 142.911
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(776536064, 42481549312)
Elapsed time for attention_linear_projection (4x31488x31488, b=2048): 0.0634
Throughput (in TFLOP/s) for attention_linear_projection (4x31488x31488, b=2048): 256.407
(776536064, 42481549312)
Elapsed time for mlp_h_to_4h (4x31488x125952, b=2048): 0.2610
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x125952, b=2048): 248.955
Elapsed time for mlp_fused_gelu (2048x4x125952): 0.0036
Elapsed time for mlp_4h_to_h (4x125952x31488, b=2048): 0.2691
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125952x31488, b=2048): 241.500
Elapsed time for transformer_add_bias_dropout (2048x4x31488): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31488): 0.0013

Attention duration (in seconds): 0.3097
Attention throughput (in TFLOP/s): 216.656
MLP duration (in seconds): 0.5336
MLP throughput (in TFLOP/s): 243.535
Transformer duration (in seconds): 0.8504
Transformer throughput (in TFLOP/s): 231.710
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31552x94656, b=2048): 0.1956
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31552x94656, b=2048): 250.117
Elapsed time for attention_key_query_prob (256x2048x493x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x493x2048): 77.934
Elapsed time for attention_prob_times_values (256x2048x2048x493): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x493): 87.313
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24231084032, 42481549312)
Elapsed time for attention_linear_projection (4x31552x31552, b=2048): 0.0640
Throughput (in TFLOP/s) for attention_linear_projection (4x31552x31552, b=2048): 254.661
(24231084032, 42481549312)
Elapsed time for mlp_h_to_4h (4x31552x126208, b=2048): 0.2622
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31552x126208, b=2048): 248.852
Elapsed time for mlp_fused_gelu (2048x4x126208): 0.0036
Elapsed time for mlp_4h_to_h (4x126208x31552, b=2048): 0.2714
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126208x31552, b=2048): 240.411
Elapsed time for transformer_add_bias_dropout (2048x4x31552): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31552): 0.0013

Attention duration (in seconds): 0.3208
Attention throughput (in TFLOP/s): 209.976
MLP duration (in seconds): 0.5371
MLP throughput (in TFLOP/s): 242.933
Transformer duration (in seconds): 0.8651
Transformer throughput (in TFLOP/s): 228.691
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x94848, b=2048): 0.1965
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x94848, b=2048): 249.967
Elapsed time for attention_key_query_prob (256x2048x494x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x494x2048): 119.186
Elapsed time for attention_prob_times_values (256x2048x2048x494): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x494): 143.432
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(16266100736, 42481549312)
Elapsed time for attention_linear_projection (4x31616x31616, b=2048): 0.0641
Throughput (in TFLOP/s) for attention_linear_projection (4x31616x31616, b=2048): 255.511
(16266100736, 42481549312)
Elapsed time for mlp_h_to_4h (4x31616x126464, b=2048): 0.2629
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x126464, b=2048): 249.221
Elapsed time for mlp_fused_gelu (2048x4x126464): 0.0036
Elapsed time for mlp_4h_to_h (4x126464x31616, b=2048): 0.2724
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126464x31616, b=2048): 240.497
Elapsed time for transformer_add_bias_dropout (2048x4x31616): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31616): 0.0013

Attention duration (in seconds): 0.3124
Attention throughput (in TFLOP/s): 216.515
MLP duration (in seconds): 0.5388
MLP throughput (in TFLOP/s): 243.151
Transformer duration (in seconds): 0.8583
Transformer throughput (in TFLOP/s): 231.434
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31680x95040, b=2048): 0.1971
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31680x95040, b=2048): 250.244
Elapsed time for attention_key_query_prob (256x2048x495x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x495x2048): 77.765
Elapsed time for attention_prob_times_values (256x2048x2048x495): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x495): 85.965
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8267563008, 42481549312)
Elapsed time for attention_linear_projection (4x31680x31680, b=2048): 0.0644
Throughput (in TFLOP/s) for attention_linear_projection (4x31680x31680, b=2048): 255.142
(8267563008, 42481549312)
Elapsed time for mlp_h_to_4h (4x31680x126720, b=2048): 0.2635
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31680x126720, b=2048): 249.638
Elapsed time for mlp_fused_gelu (2048x4x126720): 0.0036
Elapsed time for mlp_4h_to_h (4x126720x31680, b=2048): 0.2733
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126720x31680, b=2048): 240.680
Elapsed time for transformer_add_bias_dropout (2048x4x31680): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31680): 0.0013

Attention duration (in seconds): 0.3230
Attention throughput (in TFLOP/s): 210.203
MLP duration (in seconds): 0.5403
MLP throughput (in TFLOP/s): 243.452
Transformer duration (in seconds): 0.8706
Transformer throughput (in TFLOP/s): 229.097
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x95232, b=2048): 0.1977
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x95232, b=2048): 250.476
Elapsed time for attention_key_query_prob (256x2048x496x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x496x2048): 202.620
Elapsed time for attention_prob_times_values (256x2048x2048x496): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x496): 231.217
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(237568000, 42481549312)
Elapsed time for attention_linear_projection (4x31744x31744, b=2048): 0.0646
Throughput (in TFLOP/s) for attention_linear_projection (4x31744x31744, b=2048): 255.713
(237568000, 42481549312)
Elapsed time for mlp_h_to_4h (4x31744x126976, b=2048): 0.2663
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x126976, b=2048): 247.958
Elapsed time for mlp_fused_gelu (2048x4x126976): 0.0036
Elapsed time for mlp_4h_to_h (4x126976x31744, b=2048): 0.2745
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126976x31744, b=2048): 240.562
Elapsed time for transformer_add_bias_dropout (2048x4x31744): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31744): 0.0013

Attention duration (in seconds): 0.3076
Attention throughput (in TFLOP/s): 221.634
MLP duration (in seconds): 0.5444
MLP throughput (in TFLOP/s): 242.594
Transformer duration (in seconds): 0.8592
Transformer throughput (in TFLOP/s): 233.062
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31808x95424, b=2048): 0.1990
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31808x95424, b=2048): 249.847
Elapsed time for attention_key_query_prob (256x2048x497x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x497x2048): 78.262
Elapsed time for attention_prob_times_values (256x2048x2048x497): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x497): 86.380
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24082186240, 42481549312)
Elapsed time for attention_linear_projection (4x31808x31808, b=2048): 0.0650
Throughput (in TFLOP/s) for attention_linear_projection (4x31808x31808, b=2048): 255.131
(24082186240, 42481549312)
Elapsed time for mlp_h_to_4h (4x31808x127232, b=2048): 0.2663
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31808x127232, b=2048): 249.008
Elapsed time for mlp_fused_gelu (2048x4x127232): 0.0036
Elapsed time for mlp_4h_to_h (4x127232x31808, b=2048): 0.2760
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127232x31808, b=2048): 240.278
Elapsed time for transformer_add_bias_dropout (2048x4x31808): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31808): 0.0013

Attention duration (in seconds): 0.3254
Attention throughput (in TFLOP/s): 210.324
MLP duration (in seconds): 0.5458
MLP throughput (in TFLOP/s): 242.953
Transformer duration (in seconds): 0.8785
Transformer throughput (in TFLOP/s): 228.862
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x95616, b=2048): 0.2001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x95616, b=2048): 249.550
Elapsed time for attention_key_query_prob (256x2048x498x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x498x2048): 120.587
Elapsed time for attention_prob_times_values (256x2048x2048x498): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x498): 145.721
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15987179520, 42481549312)
Elapsed time for attention_linear_projection (4x31872x31872, b=2048): 0.0647
Throughput (in TFLOP/s) for attention_linear_projection (4x31872x31872, b=2048): 257.094
(15987179520, 42481549312)
Elapsed time for mlp_h_to_4h (4x31872x127488, b=2048): 0.2678
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x127488, b=2048): 248.597
Elapsed time for mlp_fused_gelu (2048x4x127488): 0.0036
Elapsed time for mlp_4h_to_h (4x127488x31872, b=2048): 0.2764
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127488x31872, b=2048): 240.820
Elapsed time for transformer_add_bias_dropout (2048x4x31872): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31872): 0.0013

Attention duration (in seconds): 0.3164
Attention throughput (in TFLOP/s): 217.142
MLP duration (in seconds): 0.5478
MLP throughput (in TFLOP/s): 243.037
Transformer duration (in seconds): 0.8715
Transformer throughput (in TFLOP/s): 231.609
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31936x95808, b=2048): 0.2009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31936x95808, b=2048): 249.539
Elapsed time for attention_key_query_prob (256x2048x499x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x499x2048): 79.477
Elapsed time for attention_prob_times_values (256x2048x2048x499): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x499): 88.900
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7858618368, 42481549312)
Elapsed time for attention_linear_projection (4x31936x31936, b=2048): 0.0657
Throughput (in TFLOP/s) for attention_linear_projection (4x31936x31936, b=2048): 254.411
(7858618368, 42481549312)
Elapsed time for mlp_h_to_4h (4x31936x127744, b=2048): 0.2682
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31936x127744, b=2048): 249.250
Elapsed time for mlp_fused_gelu (2048x4x127744): 0.0036
Elapsed time for mlp_4h_to_h (4x127744x31936, b=2048): 0.2782
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127744x31936, b=2048): 240.223
Elapsed time for transformer_add_bias_dropout (2048x4x31936): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31936): 0.0013

Attention duration (in seconds): 0.3275
Attention throughput (in TFLOP/s): 210.626
MLP duration (in seconds): 0.5500
MLP throughput (in TFLOP/s): 243.042
Transformer duration (in seconds): 0.8848
Transformer throughput (in TFLOP/s): 229.041
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x96000, b=2048): 0.2012
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x96000, b=2048): 250.144
Elapsed time for attention_key_query_prob (256x2048x500x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x500x2048): 121.420
Elapsed time for attention_prob_times_values (256x2048x2048x500): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x500): 146.740
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(23966842880, 42481549312)
Elapsed time for attention_linear_projection (4x32000x32000, b=2048): 0.0655
Throughput (in TFLOP/s) for attention_linear_projection (4x32000x32000, b=2048): 256.008
(23966842880, 42481549312)
Elapsed time for mlp_h_to_4h (4x32000x128000, b=2048): 0.2698
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x128000, b=2048): 248.706
Elapsed time for mlp_fused_gelu (2048x4x128000): 0.0036
Elapsed time for mlp_4h_to_h (4x128000x32000, b=2048): 0.2779
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128000x32000, b=2048): 241.447
Elapsed time for transformer_add_bias_dropout (2048x4x32000): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32000): 0.0013

Attention duration (in seconds): 0.3183
Attention throughput (in TFLOP/s): 217.552
MLP duration (in seconds): 0.5514
MLP throughput (in TFLOP/s): 243.414
Transformer duration (in seconds): 0.8770
Transformer throughput (in TFLOP/s): 232.014
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32064x96192, b=2048): 0.2022
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32064x96192, b=2048): 249.864
Elapsed time for attention_key_query_prob (256x2048x501x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x501x2048): 80.293
Elapsed time for attention_prob_times_values (256x2048x2048x501): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x501): 89.117
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(15773270016, 42481549312)
Elapsed time for attention_linear_projection (4x32064x32064, b=2048): 0.0659
Throughput (in TFLOP/s) for attention_linear_projection (4x32064x32064, b=2048): 255.617
(15773270016, 42481549312)
Elapsed time for mlp_h_to_4h (4x32064x128256, b=2048): 0.2700
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32064x128256, b=2048): 249.538
Elapsed time for mlp_fused_gelu (2048x4x128256): 0.0036
Elapsed time for mlp_4h_to_h (4x128256x32064, b=2048): 0.2797
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128256x32064, b=2048): 240.863
Elapsed time for transformer_add_bias_dropout (2048x4x32064): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32064): 0.0013

Attention duration (in seconds): 0.3291
Attention throughput (in TFLOP/s): 211.302
MLP duration (in seconds): 0.5534
MLP throughput (in TFLOP/s): 243.517
Transformer duration (in seconds): 0.8898
Transformer throughput (in TFLOP/s): 229.592
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x96384, b=2048): 0.2028
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x96384, b=2048): 250.116
Elapsed time for attention_key_query_prob (256x2048x502x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x502x2048): 121.814
Elapsed time for attention_prob_times_values (256x2048x2048x502): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x502): 148.085
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(7548239872, 42481549312)
Elapsed time for attention_linear_projection (4x32128x32128, b=2048): 0.0660
Throughput (in TFLOP/s) for attention_linear_projection (4x32128x32128, b=2048): 256.296
(7548239872, 42481549312)
Elapsed time for mlp_h_to_4h (4x32128x128512, b=2048): 0.2724
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x128512, b=2048): 248.294
Elapsed time for mlp_fused_gelu (2048x4x128512): 0.0036
Elapsed time for mlp_4h_to_h (4x128512x32128, b=2048): 0.2807
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128512x32128, b=2048): 241.018
Elapsed time for transformer_add_bias_dropout (2048x4x32128): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32128): 0.0013

Attention duration (in seconds): 0.3204
Attention throughput (in TFLOP/s): 217.861
MLP duration (in seconds): 0.5568
MLP throughput (in TFLOP/s): 243.005
Transformer duration (in seconds): 0.8844
Transformer throughput (in TFLOP/s): 231.899
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32192x96576, b=2048): 0.2035
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32192x96576, b=2048): 250.318
Elapsed time for attention_key_query_prob (256x2048x503x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x503x2048): 78.993
Elapsed time for attention_prob_times_values (256x2048x2048x503): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x503): 87.977
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23853596672, 42481549312)
Elapsed time for attention_linear_projection (4x32192x32192, b=2048): 0.0663
Throughput (in TFLOP/s) for attention_linear_projection (4x32192x32192, b=2048): 256.263
(23853596672, 42481549312)
Elapsed time for mlp_h_to_4h (4x32192x128768, b=2048): 0.2730
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32192x128768, b=2048): 248.743
Elapsed time for mlp_fused_gelu (2048x4x128768): 0.0036
Elapsed time for mlp_4h_to_h (4x128768x32192, b=2048): 0.2822
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128768x32192, b=2048): 240.659
Elapsed time for transformer_add_bias_dropout (2048x4x32192): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32192): 0.0013

Attention duration (in seconds): 0.3311
Attention throughput (in TFLOP/s): 211.647
MLP duration (in seconds): 0.5589
MLP throughput (in TFLOP/s): 243.040
Transformer duration (in seconds): 0.8973
Transformer throughput (in TFLOP/s): 229.467
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x96768, b=2048): 0.2041
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x96768, b=2048): 250.588
Elapsed time for attention_key_query_prob (256x2048x504x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x504x2048): 193.621
Elapsed time for attention_prob_times_values (256x2048x2048x504): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x504): 229.506
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15561457664, 42481549312)
Elapsed time for attention_linear_projection (4x32256x32256, b=2048): 0.0664
Throughput (in TFLOP/s) for attention_linear_projection (4x32256x32256, b=2048): 256.829
(15561457664, 42481549312)
Elapsed time for mlp_h_to_4h (4x32256x129024, b=2048): 0.2730
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x129024, b=2048): 249.751
Elapsed time for mlp_fused_gelu (2048x4x129024): 0.0037
Elapsed time for mlp_4h_to_h (4x129024x32256, b=2048): 0.2835
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129024x32256, b=2048): 240.494
Elapsed time for transformer_add_bias_dropout (2048x4x32256): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32256): 0.0013

Attention duration (in seconds): 0.3162
Attention throughput (in TFLOP/s): 222.519
MLP duration (in seconds): 0.5602
MLP throughput (in TFLOP/s): 243.435
Transformer duration (in seconds): 0.8837
Transformer throughput (in TFLOP/s): 233.942
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32320x96960, b=2048): 0.2059
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32320x96960, b=2048): 249.391
Elapsed time for attention_key_query_prob (256x2048x505x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x505x2048): 79.354
Elapsed time for attention_prob_times_values (256x2048x2048x505): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x505): 88.350
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7237861376, 42481549312)
Elapsed time for attention_linear_projection (4x32320x32320, b=2048): 0.0670
Throughput (in TFLOP/s) for attention_linear_projection (4x32320x32320, b=2048): 255.392
(7237861376, 42481549312)
Elapsed time for mlp_h_to_4h (4x32320x129280, b=2048): 0.2748
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32320x129280, b=2048): 249.140
Elapsed time for mlp_fused_gelu (2048x4x129280): 0.0037
Elapsed time for mlp_4h_to_h (4x129280x32320, b=2048): 0.2848
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129280x32320, b=2048): 240.359
Elapsed time for transformer_add_bias_dropout (2048x4x32320): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32320): 0.0013

Attention duration (in seconds): 0.3342
Attention throughput (in TFLOP/s): 211.315
MLP duration (in seconds): 0.5632
MLP throughput (in TFLOP/s): 243.081
Transformer duration (in seconds): 0.9048
Transformer throughput (in TFLOP/s): 229.368
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x97152, b=2048): 0.2064
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x97152, b=2048): 249.735
Elapsed time for attention_key_query_prob (256x2048x506x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x506x2048): 123.150
Elapsed time for attention_prob_times_values (256x2048x2048x506): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x506): 150.606
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23738253312, 42481549312)
Elapsed time for attention_linear_projection (4x32384x32384, b=2048): 0.0675
Throughput (in TFLOP/s) for attention_linear_projection (4x32384x32384, b=2048): 254.704
(23738253312, 42481549312)
Elapsed time for mlp_h_to_4h (4x32384x129536, b=2048): 0.2762
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x129536, b=2048): 248.809
Elapsed time for mlp_fused_gelu (2048x4x129536): 0.0037
Elapsed time for mlp_4h_to_h (4x129536x32384, b=2048): 0.2859
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129536x32384, b=2048): 240.425
Elapsed time for transformer_add_bias_dropout (2048x4x32384): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32384): 0.0013

Attention duration (in seconds): 0.3253
Attention throughput (in TFLOP/s): 217.944
MLP duration (in seconds): 0.5658
MLP throughput (in TFLOP/s): 242.961
Transformer duration (in seconds): 0.8984
Transformer throughput (in TFLOP/s): 231.919
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32448x97344, b=2048): 0.2075
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32448x97344, b=2048): 249.370
Elapsed time for attention_key_query_prob (256x2048x507x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x507x2048): 80.960
Elapsed time for attention_prob_times_values (256x2048x2048x507): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x507): 91.202
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(15347548160, 42481549312)
Elapsed time for attention_linear_projection (4x32448x32448, b=2048): 0.0676
Throughput (in TFLOP/s) for attention_linear_projection (4x32448x32448, b=2048): 255.127
(15347548160, 42481549312)
Elapsed time for mlp_h_to_4h (4x32448x129792, b=2048): 0.2772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32448x129792, b=2048): 248.959
Elapsed time for mlp_fused_gelu (2048x4x129792): 0.0037
Elapsed time for mlp_4h_to_h (4x129792x32448, b=2048): 0.2874
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129792x32448, b=2048): 240.091
Elapsed time for transformer_add_bias_dropout (2048x4x32448): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32448): 0.0013

Attention duration (in seconds): 0.3360
Attention throughput (in TFLOP/s): 211.865
MLP duration (in seconds): 0.5682
MLP throughput (in TFLOP/s): 242.865
Transformer duration (in seconds): 0.9116
Transformer throughput (in TFLOP/s): 229.471
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x97536, b=2048): 0.2074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x97536, b=2048): 250.502
Elapsed time for attention_key_query_prob (256x2048x508x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x508x2048): 124.059
Elapsed time for attention_prob_times_values (256x2048x2048x508): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x508): 152.136
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(6923288576, 42481549312)
Elapsed time for attention_linear_projection (4x32512x32512, b=2048): 0.0678
Throughput (in TFLOP/s) for attention_linear_projection (4x32512x32512, b=2048): 255.385
(6923288576, 42481549312)
Elapsed time for mlp_h_to_4h (4x32512x130048, b=2048): 0.2774
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x130048, b=2048): 249.749
Elapsed time for mlp_fused_gelu (2048x4x130048): 0.0037
Elapsed time for mlp_4h_to_h (4x130048x32512, b=2048): 0.2870
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130048x32512, b=2048): 241.394
Elapsed time for transformer_add_bias_dropout (2048x4x32512): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32512): 0.0013

Attention duration (in seconds): 0.3266
Attention throughput (in TFLOP/s): 218.768
MLP duration (in seconds): 0.5680
MLP throughput (in TFLOP/s): 243.906
Transformer duration (in seconds): 0.9020
Transformer throughput (in TFLOP/s): 232.816
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32576x97728, b=2048): 0.2091
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32576x97728, b=2048): 249.439
Elapsed time for attention_key_query_prob (256x2048x509x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x509x2048): 81.793
Elapsed time for attention_prob_times_values (256x2048x2048x509): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x509): 91.701
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23622909952, 42481549312)
Elapsed time for attention_linear_projection (4x32576x32576, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_linear_projection (4x32576x32576, b=2048): 254.979
(23622909952, 42481549312)
Elapsed time for mlp_h_to_4h (4x32576x130304, b=2048): 0.2788
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32576x130304, b=2048): 249.482
Elapsed time for mlp_fused_gelu (2048x4x130304): 0.0037
Elapsed time for mlp_4h_to_h (4x130304x32576, b=2048): 0.2904
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130304x32576, b=2048): 239.479
Elapsed time for transformer_add_bias_dropout (2048x4x32576): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32576): 0.0013

Attention duration (in seconds): 0.3380
Attention throughput (in TFLOP/s): 212.236
MLP duration (in seconds): 0.5729
MLP throughput (in TFLOP/s): 242.805
Transformer duration (in seconds): 0.9183
Transformer throughput (in TFLOP/s): 229.589
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x97920, b=2048): 0.2085
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x97920, b=2048): 251.101
Elapsed time for attention_key_query_prob (256x2048x510x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x510x2048): 124.416
Elapsed time for attention_prob_times_values (256x2048x2048x510): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x510): 153.306
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15131541504, 42481549312)
Elapsed time for attention_linear_projection (4x32640x32640, b=2048): 0.0683
Throughput (in TFLOP/s) for attention_linear_projection (4x32640x32640, b=2048): 255.399
(15131541504, 42481549312)
Elapsed time for mlp_h_to_4h (4x32640x130560, b=2048): 0.2796
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x130560, b=2048): 249.694
Elapsed time for mlp_fused_gelu (2048x4x130560): 0.0037
Elapsed time for mlp_4h_to_h (4x130560x32640, b=2048): 0.2919
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130560x32640, b=2048): 239.182
Elapsed time for transformer_add_bias_dropout (2048x4x32640): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32640): 0.0013

Attention duration (in seconds): 0.3282
Attention throughput (in TFLOP/s): 219.382
MLP duration (in seconds): 0.5752
MLP throughput (in TFLOP/s): 242.757
Transformer duration (in seconds): 0.9108
Transformer throughput (in TFLOP/s): 232.368
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32704x98112, b=2048): 0.2097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32704x98112, b=2048): 250.680
Elapsed time for attention_key_query_prob (256x2048x511x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x511x2048): 81.223
Elapsed time for attention_prob_times_values (256x2048x2048x511): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x511): 91.319
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6606618624, 42481549312)
Elapsed time for attention_linear_projection (4x32704x32704, b=2048): 0.0689
Throughput (in TFLOP/s) for attention_linear_projection (4x32704x32704, b=2048): 254.376
(6606618624, 42481549312)
Elapsed time for mlp_h_to_4h (4x32704x130816, b=2048): 0.2809
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32704x130816, b=2048): 249.492
Elapsed time for mlp_fused_gelu (2048x4x130816): 0.0037
Elapsed time for mlp_4h_to_h (4x130816x32704, b=2048): 0.2928
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130816x32704, b=2048): 239.394
Elapsed time for transformer_add_bias_dropout (2048x4x32704): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32704): 0.0013

Attention duration (in seconds): 0.3395
Attention throughput (in TFLOP/s): 212.906
MLP duration (in seconds): 0.5774
MLP throughput (in TFLOP/s): 242.773
Transformer duration (in seconds): 0.9244
Transformer throughput (in TFLOP/s): 229.847
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32768x98304, b=2048): 0.2106
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32768x98304, b=2048): 250.589
Elapsed time for attention_key_query_prob (256x2048x512x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x512x2048): 211.129
Elapsed time for attention_prob_times_values (256x2048x2048x512): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x512): 238.010
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23505469440, 42481549312)
Elapsed time for attention_linear_projection (4x32768x32768, b=2048): 0.0691
Throughput (in TFLOP/s) for attention_linear_projection (4x32768x32768, b=2048): 254.462
(23505469440, 42481549312)
Elapsed time for mlp_h_to_4h (4x32768x131072, b=2048): 0.2839
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32768x131072, b=2048): 247.863
Elapsed time for mlp_fused_gelu (2048x4x131072): 0.0037
Elapsed time for mlp_4h_to_h (4x131072x32768, b=2048): 0.2922
Throughput (in TFLOP/s) for mlp_4h_to_h (4x131072x32768, b=2048): 240.858
Elapsed time for transformer_add_bias_dropout (2048x4x32768): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32768): 0.0013

Attention duration (in seconds): 0.3250
Attention throughput (in TFLOP/s): 223.306
MLP duration (in seconds): 0.5798
MLP throughput (in TFLOP/s): 242.748
Transformer duration (in seconds): 0.9121
Transformer throughput (in TFLOP/s): 233.850
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.1008
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 253.151
Elapsed time for attention_key_query_prob (512x2048x178x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x178x2048): 86.643
Elapsed time for attention_prob_times_values (512x2048x2048x178): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x178): 115.369
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10809311232, 42481549312)
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0331
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 256.703
(10809311232, 42481549312)
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1351
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 251.813
Elapsed time for mlp_fused_gelu (2048x4x91136): 0.0025
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1404
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 242.227
Elapsed time for transformer_add_bias_dropout (2048x4x22784): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22784): 0.0009

Attention duration (in seconds): 0.2192
Attention throughput (in TFLOP/s): 162.175
MLP duration (in seconds): 0.2781
MLP throughput (in TFLOP/s): 244.694
Transformer duration (in seconds): 0.5025
Transformer throughput (in TFLOP/s): 206.167
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.1019
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 253.225
Elapsed time for attention_key_query_prob (512x2048x179x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x179x2048): 61.459
Elapsed time for attention_prob_times_values (512x2048x2048x179): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x179): 70.935
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 255.853
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.1368
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 251.528
Elapsed time for mlp_fused_gelu (2048x4x91648): 0.0025
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.1416
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 242.907
Elapsed time for transformer_add_bias_dropout (2048x4x22912): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22912): 0.0009

Attention duration (in seconds): 0.2287
Attention throughput (in TFLOP/s): 157.164
MLP duration (in seconds): 0.2809
MLP throughput (in TFLOP/s): 244.919
Transformer duration (in seconds): 0.5148
Transformer throughput (in TFLOP/s): 203.463
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.1033
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 252.645
Elapsed time for attention_key_query_prob (512x2048x180x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x180x2048): 87.808
Elapsed time for attention_prob_times_values (512x2048x2048x180): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x180): 116.318
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0350
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 248.724
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.1388
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 250.688
Elapsed time for mlp_fused_gelu (2048x4x92160): 0.0025
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 242.996
Elapsed time for transformer_add_bias_dropout (2048x4x23040): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23040): 0.0009

Attention duration (in seconds): 0.2235
Attention throughput (in TFLOP/s): 162.559
MLP duration (in seconds): 0.2845
MLP throughput (in TFLOP/s): 244.579
Transformer duration (in seconds): 0.5133
Transformer throughput (in TFLOP/s): 206.359
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.1046
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 252.236
Elapsed time for attention_key_query_prob (512x2048x181x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x181x2048): 62.084
Elapsed time for attention_prob_times_values (512x2048x2048x181): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x181): 71.654
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 246.844
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.1404
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 250.615
Elapsed time for mlp_fused_gelu (2048x4x92672): 0.0026
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 242.409
Elapsed time for transformer_add_bias_dropout (2048x4x23168): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23168): 0.0009

Attention duration (in seconds): 0.2334
Attention throughput (in TFLOP/s): 157.359
MLP duration (in seconds): 0.2880
MLP throughput (in TFLOP/s): 244.258
Transformer duration (in seconds): 0.5267
Transformer throughput (in TFLOP/s): 203.304
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1057
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 252.362
Elapsed time for attention_key_query_prob (512x2048x182x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x182x2048): 88.736
Elapsed time for attention_prob_times_values (512x2048x2048x182): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x182): 117.261
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0346
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 257.206
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.1413
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 251.670
Elapsed time for mlp_fused_gelu (2048x4x93184): 0.0026
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.1469
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 242.143
Elapsed time for transformer_add_bias_dropout (2048x4x23296): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23296): 0.0009

Attention duration (in seconds): 0.2256
Attention throughput (in TFLOP/s): 164.600
MLP duration (in seconds): 0.2908
MLP throughput (in TFLOP/s): 244.636
Transformer duration (in seconds): 0.5217
Transformer throughput (in TFLOP/s): 207.537
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1067
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 252.851
Elapsed time for attention_key_query_prob (512x2048x183x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x183x2048): 61.994
Elapsed time for attention_prob_times_values (512x2048x2048x183): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x183): 69.795
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0365
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 246.283
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.1427
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 252.032
Elapsed time for mlp_fused_gelu (2048x4x93696): 0.0026
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1484
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 242.294
Elapsed time for transformer_add_bias_dropout (2048x4x23424): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23424): 0.0009

Attention duration (in seconds): 0.2369
Attention throughput (in TFLOP/s): 158.406
MLP duration (in seconds): 0.2937
MLP throughput (in TFLOP/s): 244.895
Transformer duration (in seconds): 0.5359
Transformer throughput (in TFLOP/s): 204.224
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1081
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 252.235
Elapsed time for attention_key_query_prob (512x2048x184x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x184x2048): 115.064
Elapsed time for attention_prob_times_values (512x2048x2048x184): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x184): 169.730
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 255.739
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.1448
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 251.060
Elapsed time for mlp_fused_gelu (2048x4x94208): 0.0026
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.1496
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 243.022
Elapsed time for transformer_add_bias_dropout (2048x4x23552): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23552): 0.0009

Attention duration (in seconds): 0.2250
Attention throughput (in TFLOP/s): 168.605
MLP duration (in seconds): 0.2970
MLP throughput (in TFLOP/s): 244.816
Transformer duration (in seconds): 0.5273
Transformer throughput (in TFLOP/s): 209.814
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1091
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 252.547
Elapsed time for attention_key_query_prob (512x2048x185x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x185x2048): 62.549
Elapsed time for attention_prob_times_values (512x2048x2048x185): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x185): 70.219
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0358
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 256.977
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.1469
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 250.126
Elapsed time for mlp_fused_gelu (2048x4x94720): 0.0026
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.1513
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 242.954
Elapsed time for transformer_add_bias_dropout (2048x4x23680): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23680): 0.0009

Attention duration (in seconds): 0.2387
Attention throughput (in TFLOP/s): 160.592
MLP duration (in seconds): 0.3008
MLP throughput (in TFLOP/s): 244.349
Transformer duration (in seconds): 0.5449
Transformer throughput (in TFLOP/s): 205.244
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1105
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 252.172
Elapsed time for attention_key_query_prob (512x2048x186x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x186x2048): 90.520
Elapsed time for attention_prob_times_values (512x2048x2048x186): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x186): 119.616
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0375
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 247.381
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.1482
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 250.695
Elapsed time for mlp_fused_gelu (2048x4x95232): 0.0026
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1535
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 242.055
Elapsed time for transformer_add_bias_dropout (2048x4x23808): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23808): 0.0010

Attention duration (in seconds): 0.2333
Attention throughput (in TFLOP/s): 166.040
MLP duration (in seconds): 0.3043
MLP throughput (in TFLOP/s): 244.178
Transformer duration (in seconds): 0.5430
Transformer throughput (in TFLOP/s): 208.169
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1116
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 252.356
Elapsed time for attention_key_query_prob (512x2048x187x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x187x2048): 63.213
Elapsed time for attention_prob_times_values (512x2048x2048x187): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x187): 74.290
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 256.214
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.1491
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 251.861
Elapsed time for mlp_fused_gelu (2048x4x95744): 0.0026
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.1550
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 242.296
Elapsed time for transformer_add_bias_dropout (2048x4x23936): 0.0018
Elapsed time for transformer_layer_norm (2048x4x23936): 0.0010

Attention duration (in seconds): 0.2416
Attention throughput (in TFLOP/s): 162.081
MLP duration (in seconds): 0.3067
MLP throughput (in TFLOP/s): 244.863
Transformer duration (in seconds): 0.5537
Transformer throughput (in TFLOP/s): 206.341
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1128
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 252.231
Elapsed time for attention_key_query_prob (512x2048x188x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x188x2048): 91.483
Elapsed time for attention_prob_times_values (512x2048x2048x188): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x188): 121.006
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0369
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 257.146
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.1515
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 250.558
Elapsed time for mlp_fused_gelu (2048x4x96256): 0.0027
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.1566
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 242.363
Elapsed time for transformer_add_bias_dropout (2048x4x24064): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24064): 0.0010

Attention duration (in seconds): 0.2351
Attention throughput (in TFLOP/s): 168.317
MLP duration (in seconds): 0.3107
MLP throughput (in TFLOP/s): 244.291
Transformer duration (in seconds): 0.5512
Transformer throughput (in TFLOP/s): 209.468
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1146
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 250.925
Elapsed time for attention_key_query_prob (512x2048x189x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x189x2048): 63.803
Elapsed time for attention_prob_times_values (512x2048x2048x189): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x189): 75.126
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 258.353
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.1526
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 251.282
Elapsed time for mlp_fused_gelu (2048x4x96768): 0.0027
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1580
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 242.682
Elapsed time for transformer_add_bias_dropout (2048x4x24192): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24192): 0.0010

Attention duration (in seconds): 0.2451
Attention throughput (in TFLOP/s): 163.105
MLP duration (in seconds): 0.3133
MLP throughput (in TFLOP/s): 244.807
Transformer duration (in seconds): 0.5640
Transformer throughput (in TFLOP/s): 206.911
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1149
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 252.921
Elapsed time for attention_key_query_prob (512x2048x190x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x190x2048): 92.704
Elapsed time for attention_prob_times_values (512x2048x2048x190): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x190): 121.858
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 244.731
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.1549
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 250.305
Elapsed time for mlp_fused_gelu (2048x4x97280): 0.0027
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1598
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 242.556
Elapsed time for transformer_add_bias_dropout (2048x4x24320): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24320): 0.0010

Attention duration (in seconds): 0.2399
Attention throughput (in TFLOP/s): 168.400
MLP duration (in seconds): 0.3173
MLP throughput (in TFLOP/s): 244.290
Transformer duration (in seconds): 0.5627
Transformer throughput (in TFLOP/s): 209.543
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1169
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 251.417
Elapsed time for attention_key_query_prob (512x2048x191x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x191x2048): 64.442
Elapsed time for attention_prob_times_values (512x2048x2048x191): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x191): 73.871
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0382
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 256.333
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.1569
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 249.723
Elapsed time for mlp_fused_gelu (2048x4x97792): 0.0027
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.1620
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 241.849
Elapsed time for transformer_add_bias_dropout (2048x4x24448): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24448): 0.0010

Attention duration (in seconds): 0.2487
Attention throughput (in TFLOP/s): 164.089
MLP duration (in seconds): 0.3215
MLP throughput (in TFLOP/s): 243.665
Transformer duration (in seconds): 0.5758
Transformer throughput (in TFLOP/s): 206.942
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.1175
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 252.605
Elapsed time for attention_key_query_prob (512x2048x192x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x192x2048): 138.973
Elapsed time for attention_prob_times_values (512x2048x2048x192): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x192): 178.609
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0385
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 256.701
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.1571
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 251.884
Elapsed time for mlp_fused_gelu (2048x4x98304): 0.0027
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.1635
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 242.087
Elapsed time for transformer_add_bias_dropout (2048x4x24576): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24576): 0.0010

Attention duration (in seconds): 0.2364
Attention throughput (in TFLOP/s): 174.380
MLP duration (in seconds): 0.3234
MLP throughput (in TFLOP/s): 244.822
Transformer duration (in seconds): 0.5654
Transformer throughput (in TFLOP/s): 212.942
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1192
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 251.690
Elapsed time for attention_key_query_prob (512x2048x193x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x193x2048): 62.023
Elapsed time for attention_prob_times_values (512x2048x2048x193): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x193): 73.946
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0404
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 247.695
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.1593
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 251.003
Elapsed time for mlp_fused_gelu (2048x4x98816): 0.0027
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.1649
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 242.589
Elapsed time for transformer_add_bias_dropout (2048x4x24704): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24704): 0.0010

Attention duration (in seconds): 0.2540
Attention throughput (in TFLOP/s): 164.020
MLP duration (in seconds): 0.3269
MLP throughput (in TFLOP/s): 244.672
Transformer duration (in seconds): 0.5865
Transformer throughput (in TFLOP/s): 207.397
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 252.234
Elapsed time for attention_key_query_prob (512x2048x194x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x194x2048): 88.297
Elapsed time for attention_prob_times_values (512x2048x2048x194): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x194): 122.200
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0395
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 255.501
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.1616
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 250.124
Elapsed time for mlp_fused_gelu (2048x4x99328): 0.0027
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.1669
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 242.064
Elapsed time for transformer_add_bias_dropout (2048x4x24832): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24832): 0.0010

Attention duration (in seconds): 0.2458
Attention throughput (in TFLOP/s): 171.189
MLP duration (in seconds): 0.3312
MLP throughput (in TFLOP/s): 243.997
Transformer duration (in seconds): 0.5827
Transformer throughput (in TFLOP/s): 210.921
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1218
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 251.513
Elapsed time for attention_key_query_prob (512x2048x195x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x195x2048): 62.019
Elapsed time for attention_prob_times_values (512x2048x2048x195): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x195): 76.375
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0398
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 256.603
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.1632
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 250.166
Elapsed time for mlp_fused_gelu (2048x4x99840): 0.0027
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.1685
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 242.319
Elapsed time for transformer_add_bias_dropout (2048x4x24960): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24960): 0.0010

Attention duration (in seconds): 0.2558
Attention throughput (in TFLOP/s): 166.148
MLP duration (in seconds): 0.3344
MLP throughput (in TFLOP/s): 244.158
Transformer duration (in seconds): 0.5959
Transformer throughput (in TFLOP/s): 208.349
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1225
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 252.443
Elapsed time for attention_key_query_prob (512x2048x196x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x196x2048): 88.980
Elapsed time for attention_prob_times_values (512x2048x2048x196): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x196): 121.280
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 245.970
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.1651
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 249.914
Elapsed time for mlp_fused_gelu (2048x4x100352): 0.0028
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.1705
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 241.983
Elapsed time for transformer_add_bias_dropout (2048x4x25088): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25088): 0.0010

Attention duration (in seconds): 0.2507
Attention throughput (in TFLOP/s): 171.244
MLP duration (in seconds): 0.3383
MLP throughput (in TFLOP/s): 243.877
Transformer duration (in seconds): 0.5947
Transformer throughput (in TFLOP/s): 210.918
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.1242
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 251.541
Elapsed time for attention_key_query_prob (512x2048x197x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x197x2048): 62.460
Elapsed time for attention_prob_times_values (512x2048x2048x197): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x197): 76.345
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0409
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 254.771
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.1667
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 249.913
Elapsed time for mlp_fused_gelu (2048x4x100864): 0.0028
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.1723
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 241.808
Elapsed time for transformer_add_bias_dropout (2048x4x25216): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25216): 0.0010

Attention duration (in seconds): 0.2596
Attention throughput (in TFLOP/s): 167.037
MLP duration (in seconds): 0.3418
MLP throughput (in TFLOP/s): 243.798
Transformer duration (in seconds): 0.6072
Transformer throughput (in TFLOP/s): 208.676
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.1257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 251.246
Elapsed time for attention_key_query_prob (512x2048x198x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x198x2048): 89.637
Elapsed time for attention_prob_times_values (512x2048x2048x198): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x198): 120.209
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0412
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 255.659
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.1683
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 250.087
Elapsed time for mlp_fused_gelu (2048x4x101376): 0.0028
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.1737
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 242.391
Elapsed time for transformer_add_bias_dropout (2048x4x25344): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25344): 0.0010

Attention duration (in seconds): 0.2532
Attention throughput (in TFLOP/s): 172.954
MLP duration (in seconds): 0.3448
MLP throughput (in TFLOP/s): 244.188
Transformer duration (in seconds): 0.6037
Transformer throughput (in TFLOP/s): 211.986
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.1272
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 250.746
Elapsed time for attention_key_query_prob (512x2048x199x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x199x2048): 62.446
Elapsed time for attention_prob_times_values (512x2048x2048x199): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x199): 74.126
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0414
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 256.697
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.1699
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 250.274
Elapsed time for mlp_fused_gelu (2048x4x101888): 0.0028
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.1757
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 241.991
Elapsed time for transformer_add_bias_dropout (2048x4x25472): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25472): 0.0010

Attention duration (in seconds): 0.2637
Attention throughput (in TFLOP/s): 167.755
MLP duration (in seconds): 0.3484
MLP throughput (in TFLOP/s): 244.082
Transformer duration (in seconds): 0.6179
Transformer throughput (in TFLOP/s): 209.222
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.1288
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 250.148
Elapsed time for attention_key_query_prob (512x2048x200x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x200x2048): 121.958
Elapsed time for attention_prob_times_values (512x2048x2048x200): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x200): 181.798
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 254.893
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.1724
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 249.140
Elapsed time for mlp_fused_gelu (2048x4x102400): 0.0028
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.1777
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 241.680
Elapsed time for transformer_add_bias_dropout (2048x4x25600): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25600): 0.0010

Attention duration (in seconds): 0.2525
Attention throughput (in TFLOP/s): 176.902
MLP duration (in seconds): 0.3529
MLP throughput (in TFLOP/s): 243.394
Transformer duration (in seconds): 0.6112
Transformer throughput (in TFLOP/s): 213.611
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.1295
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 251.149
Elapsed time for attention_key_query_prob (512x2048x201x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x201x2048): 62.908
Elapsed time for attention_prob_times_values (512x2048x2048x201): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x201): 73.944
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 255.814
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.1740
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 249.273
Elapsed time for mlp_fused_gelu (2048x4x102912): 0.0028
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.1794
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 241.754
Elapsed time for transformer_add_bias_dropout (2048x4x25728): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25728): 0.0010

Attention duration (in seconds): 0.2672
Attention throughput (in TFLOP/s): 168.838
MLP duration (in seconds): 0.3563
MLP throughput (in TFLOP/s): 243.505
Transformer duration (in seconds): 0.6293
Transformer throughput (in TFLOP/s): 209.543
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.1314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 250.145
Elapsed time for attention_key_query_prob (512x2048x202x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x202x2048): 90.807
Elapsed time for attention_prob_times_values (512x2048x2048x202): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x202): 120.774
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0428
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 255.836
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.1758
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 249.191
Elapsed time for mlp_fused_gelu (2048x4x103424): 0.0028
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.1808
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 242.372
Elapsed time for transformer_add_bias_dropout (2048x4x25856): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25856): 0.0010

Attention duration (in seconds): 0.2608
Attention throughput (in TFLOP/s): 174.680
MLP duration (in seconds): 0.3594
MLP throughput (in TFLOP/s): 243.788
Transformer duration (in seconds): 0.6260
Transformer throughput (in TFLOP/s): 212.724
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.1321
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 251.309
Elapsed time for attention_key_query_prob (512x2048x203x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x203x2048): 63.889
Elapsed time for attention_prob_times_values (512x2048x2048x203): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x203): 76.665
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0432
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 256.317
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.1763
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 250.978
Elapsed time for mlp_fused_gelu (2048x4x103936): 0.0029
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.1828
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 242.106
Elapsed time for transformer_add_bias_dropout (2048x4x25984): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25984): 0.0010

Attention duration (in seconds): 0.2701
Attention throughput (in TFLOP/s): 170.299
MLP duration (in seconds): 0.3619
MLP throughput (in TFLOP/s): 244.514
Transformer duration (in seconds): 0.6379
Transformer throughput (in TFLOP/s): 210.830
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1334
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 251.242
Elapsed time for attention_key_query_prob (512x2048x204x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x204x2048): 91.738
Elapsed time for attention_prob_times_values (512x2048x2048x204): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x204): 122.046
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0437
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 255.344
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.1794
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 249.124
Elapsed time for mlp_fused_gelu (2048x4x104448): 0.0029
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1846
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 242.075
Elapsed time for transformer_add_bias_dropout (2048x4x26112): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26112): 0.0010

Attention duration (in seconds): 0.2637
Attention throughput (in TFLOP/s): 176.091
MLP duration (in seconds): 0.3668
MLP throughput (in TFLOP/s): 243.626
Transformer duration (in seconds): 0.6365
Transformer throughput (in TFLOP/s): 213.375
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 250.941
Elapsed time for attention_key_query_prob (512x2048x205x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x205x2048): 64.254
Elapsed time for attention_prob_times_values (512x2048x2048x205): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x205): 76.912
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 255.163
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.1808
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 249.521
Elapsed time for mlp_fused_gelu (2048x4x104960): 0.0029
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1865
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 242.016
Elapsed time for transformer_add_bias_dropout (2048x4x26240): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26240): 0.0011

Attention duration (in seconds): 0.2741
Attention throughput (in TFLOP/s): 171.075
MLP duration (in seconds): 0.3702
MLP throughput (in TFLOP/s): 243.796
Transformer duration (in seconds): 0.6502
Transformer throughput (in TFLOP/s): 210.911
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.1358
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 251.582
Elapsed time for attention_key_query_prob (512x2048x206x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x206x2048): 92.127
Elapsed time for attention_prob_times_values (512x2048x2048x206): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x206): 122.620
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0447
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 255.056
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.1824
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 249.776
Elapsed time for mlp_fused_gelu (2048x4x105472): 0.0029
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.1883
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 241.927
Elapsed time for transformer_add_bias_dropout (2048x4x26368): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26368): 0.0011

Attention duration (in seconds): 0.2671
Attention throughput (in TFLOP/s): 177.191
MLP duration (in seconds): 0.3737
MLP throughput (in TFLOP/s): 243.881
Transformer duration (in seconds): 0.6468
Transformer throughput (in TFLOP/s): 214.082
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1373
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 251.352
Elapsed time for attention_key_query_prob (512x2048x207x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x207x2048): 64.617
Elapsed time for attention_prob_times_values (512x2048x2048x207): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x207): 76.287
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0451
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 255.043
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.1845
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 249.396
Elapsed time for mlp_fused_gelu (2048x4x105984): 0.0029
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1909
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 240.961
Elapsed time for transformer_add_bias_dropout (2048x4x26496): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26496): 0.0011

Attention duration (in seconds): 0.2776
Attention throughput (in TFLOP/s): 172.119
MLP duration (in seconds): 0.3783
MLP throughput (in TFLOP/s): 243.218
Transformer duration (in seconds): 0.6620
Transformer throughput (in TFLOP/s): 211.191
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.1395
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 249.669
Elapsed time for attention_key_query_prob (512x2048x208x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x208x2048): 135.251
Elapsed time for attention_prob_times_values (512x2048x2048x208): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x208): 190.406
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0454
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 255.987
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.1869
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 248.613
Elapsed time for mlp_fused_gelu (2048x4x106496): 0.0029
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1926
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 241.244
Elapsed time for transformer_add_bias_dropout (2048x4x26624): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26624): 0.0011

Attention duration (in seconds): 0.2661
Attention throughput (in TFLOP/s): 181.322
MLP duration (in seconds): 0.3823
MLP throughput (in TFLOP/s): 242.998
Transformer duration (in seconds): 0.6544
Transformer throughput (in TFLOP/s): 215.682
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.1403
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 250.786
Elapsed time for attention_key_query_prob (512x2048x209x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x209x2048): 64.939
Elapsed time for attention_prob_times_values (512x2048x2048x209): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x209): 76.169
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 256.032
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.1875
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 250.108
Elapsed time for mlp_fused_gelu (2048x4x107008): 0.0029
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.1938
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 241.976
Elapsed time for transformer_add_bias_dropout (2048x4x26752): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26752): 0.0011

Attention duration (in seconds): 0.2815
Attention throughput (in TFLOP/s): 172.994
MLP duration (in seconds): 0.3843
MLP throughput (in TFLOP/s): 244.091
Transformer duration (in seconds): 0.6719
Transformer throughput (in TFLOP/s): 212.094
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.1413
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 251.412
Elapsed time for attention_key_query_prob (512x2048x210x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x210x2048): 93.429
Elapsed time for attention_prob_times_values (512x2048x2048x210): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x210): 124.323
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0463
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 255.851
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.1893
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 250.086
Elapsed time for mlp_fused_gelu (2048x4x107520): 0.0030
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.1967
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 240.769
Elapsed time for transformer_add_bias_dropout (2048x4x26880): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26880): 0.0011

Attention duration (in seconds): 0.2743
Attention throughput (in TFLOP/s): 179.227
MLP duration (in seconds): 0.3890
MLP throughput (in TFLOP/s): 243.475
Transformer duration (in seconds): 0.6693
Transformer throughput (in TFLOP/s): 214.931
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.1437
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 249.538
Elapsed time for attention_key_query_prob (512x2048x211x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x211x2048): 65.106
Elapsed time for attention_prob_times_values (512x2048x2048x211): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x211): 78.044
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0469
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 254.666
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.1916
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 249.487
Elapsed time for mlp_fused_gelu (2048x4x108032): 0.0030
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.1978
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 241.722
Elapsed time for transformer_add_bias_dropout (2048x4x27008): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27008): 0.0011

Attention duration (in seconds): 0.2860
Attention throughput (in TFLOP/s): 173.501
MLP duration (in seconds): 0.3923
MLP throughput (in TFLOP/s): 243.684
Transformer duration (in seconds): 0.6844
Transformer throughput (in TFLOP/s): 212.181
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.1437
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 251.943
Elapsed time for attention_key_query_prob (512x2048x212x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x212x2048): 94.466
Elapsed time for attention_prob_times_values (512x2048x2048x212): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x212): 125.244
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 256.214
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.1928
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 250.357
Elapsed time for mlp_fused_gelu (2048x4x108544): 0.0030
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.2000
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 241.285
Elapsed time for transformer_add_bias_dropout (2048x4x27136): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27136): 0.0011

Attention duration (in seconds): 0.2775
Attention throughput (in TFLOP/s): 180.472
MLP duration (in seconds): 0.3957
MLP throughput (in TFLOP/s): 243.884
Transformer duration (in seconds): 0.6794
Transformer throughput (in TFLOP/s): 215.774
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.1457
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 250.772
Elapsed time for attention_key_query_prob (512x2048x213x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x213x2048): 65.415
Elapsed time for attention_prob_times_values (512x2048x2048x213): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x213): 78.653
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0479
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 254.090
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.1953
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 249.476
Elapsed time for mlp_fused_gelu (2048x4x109056): 0.0030
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.2022
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 240.924
Elapsed time for transformer_add_bias_dropout (2048x4x27264): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27264): 0.0011

Attention duration (in seconds): 0.2891
Attention throughput (in TFLOP/s): 174.847
MLP duration (in seconds): 0.4005
MLP throughput (in TFLOP/s): 243.287
Transformer duration (in seconds): 0.6958
Transformer throughput (in TFLOP/s): 212.679
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.1477
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 249.765
Elapsed time for attention_key_query_prob (512x2048x214x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x214x2048): 95.847
Elapsed time for attention_prob_times_values (512x2048x2048x214): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x214): 126.198
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0483
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 254.690
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.1975
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 249.009
Elapsed time for mlp_fused_gelu (2048x4x109568): 0.0030
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.2029
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 242.330
Elapsed time for transformer_add_bias_dropout (2048x4x27392): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27392): 0.0011

Attention duration (in seconds): 0.2826
Attention throughput (in TFLOP/s): 180.482
MLP duration (in seconds): 0.4034
MLP throughput (in TFLOP/s): 243.790
Transformer duration (in seconds): 0.6923
Transformer throughput (in TFLOP/s): 215.747
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.1481
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 251.346
Elapsed time for attention_key_query_prob (512x2048x215x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x215x2048): 65.803
Elapsed time for attention_prob_times_values (512x2048x2048x215): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x215): 77.722
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0485
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 256.076
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.1994
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 248.855
Elapsed time for mlp_fused_gelu (2048x4x110080): 0.0030
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.2060
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 240.893
Elapsed time for transformer_add_bias_dropout (2048x4x27520): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27520): 0.0011

Attention duration (in seconds): 0.2923
Attention throughput (in TFLOP/s): 176.115
MLP duration (in seconds): 0.4085
MLP throughput (in TFLOP/s): 242.995
Transformer duration (in seconds): 0.7071
Transformer throughput (in TFLOP/s): 213.200
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.1499
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 250.640
Elapsed time for attention_key_query_prob (512x2048x216x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x216x2048): 125.084
Elapsed time for attention_prob_times_values (512x2048x2048x216): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x216): 192.677
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0491
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 255.079
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.2011
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 249.082
Elapsed time for mlp_fused_gelu (2048x4x110592): 0.0030
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.2078
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 241.071
Elapsed time for transformer_add_bias_dropout (2048x4x27648): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27648): 0.0011

Attention duration (in seconds): 0.2811
Attention throughput (in TFLOP/s): 184.829
MLP duration (in seconds): 0.4120
MLP throughput (in TFLOP/s): 243.202
Transformer duration (in seconds): 0.6993
Transformer throughput (in TFLOP/s): 217.561
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.1509
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 251.381
Elapsed time for attention_key_query_prob (512x2048x217x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x217x2048): 66.302
Elapsed time for attention_prob_times_values (512x2048x2048x217): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x217): 78.005
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0495
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 255.309
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.2030
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 249.038
Elapsed time for mlp_fused_gelu (2048x4x111104): 0.0031
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.2090
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 241.882
Elapsed time for transformer_add_bias_dropout (2048x4x27776): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27776): 0.0011

Attention duration (in seconds): 0.2962
Attention throughput (in TFLOP/s): 176.993
MLP duration (in seconds): 0.4151
MLP throughput (in TFLOP/s): 243.602
Transformer duration (in seconds): 0.7176
Transformer throughput (in TFLOP/s): 213.966
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.1528
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 250.481
Elapsed time for attention_key_query_prob (512x2048x218x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x218x2048): 97.149
Elapsed time for attention_prob_times_values (512x2048x2048x218): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x218): 128.162
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0498
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 255.995
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.2049
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 249.082
Elapsed time for mlp_fused_gelu (2048x4x111616): 0.0031
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.2109
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 241.956
Elapsed time for transformer_add_bias_dropout (2048x4x27904): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27904): 0.0011

Attention duration (in seconds): 0.2894
Attention throughput (in TFLOP/s): 182.791
MLP duration (in seconds): 0.4188
MLP throughput (in TFLOP/s): 243.669
Transformer duration (in seconds): 0.7146
Transformer throughput (in TFLOP/s): 216.848
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.1536
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 251.512
Elapsed time for attention_key_query_prob (512x2048x219x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x219x2048): 67.033
Elapsed time for attention_prob_times_values (512x2048x2048x219): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x219): 81.151
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 256.002
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.2064
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 249.480
Elapsed time for mlp_fused_gelu (2048x4x112128): 0.0031
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.2140
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 240.693
Elapsed time for transformer_add_bias_dropout (2048x4x28032): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28032): 0.0011

Attention duration (in seconds): 0.2993
Attention throughput (in TFLOP/s): 178.336
MLP duration (in seconds): 0.4235
MLP throughput (in TFLOP/s): 243.225
Transformer duration (in seconds): 0.7291
Transformer throughput (in TFLOP/s): 214.463
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.1547
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 251.900
Elapsed time for attention_key_query_prob (512x2048x220x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x220x2048): 97.632
Elapsed time for attention_prob_times_values (512x2048x2048x220): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x220): 129.418
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0509
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 255.142
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.2079
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 249.978
Elapsed time for mlp_fused_gelu (2048x4x112640): 0.0031
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.2156
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 241.027
Elapsed time for transformer_add_bias_dropout (2048x4x28160): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28160): 0.0011

Attention duration (in seconds): 0.2925
Attention throughput (in TFLOP/s): 184.152
MLP duration (in seconds): 0.4266
MLP throughput (in TFLOP/s): 243.641
Transformer duration (in seconds): 0.7255
Transformer throughput (in TFLOP/s): 217.505
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.1571
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 250.359
Elapsed time for attention_key_query_prob (512x2048x221x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x221x2048): 67.861
Elapsed time for attention_prob_times_values (512x2048x2048x221): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x221): 82.049
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 255.662
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.2110
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 248.485
Elapsed time for mlp_fused_gelu (2048x4x113152): 0.0031
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.2178
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 240.819
Elapsed time for transformer_add_bias_dropout (2048x4x28288): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28288): 0.0011

Attention duration (in seconds): 0.3038
Attention throughput (in TFLOP/s): 178.888
MLP duration (in seconds): 0.4319
MLP throughput (in TFLOP/s): 242.832
Transformer duration (in seconds): 0.7421
Transformer throughput (in TFLOP/s): 214.556
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.1584
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 250.527
Elapsed time for attention_key_query_prob (512x2048x222x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x222x2048): 99.167
Elapsed time for attention_prob_times_values (512x2048x2048x222): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x222): 129.646
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 254.742
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.2132
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 248.193
Elapsed time for mlp_fused_gelu (2048x4x113664): 0.0031
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.2203
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 240.189
Elapsed time for transformer_add_bias_dropout (2048x4x28416): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28416): 0.0011

Attention duration (in seconds): 0.2972
Attention throughput (in TFLOP/s): 184.497
MLP duration (in seconds): 0.4367
MLP throughput (in TFLOP/s): 242.379
Transformer duration (in seconds): 0.7403
Transformer throughput (in TFLOP/s): 217.037
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.1601
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 250.161
Elapsed time for attention_key_query_prob (512x2048x223x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x223x2048): 69.038
Elapsed time for attention_prob_times_values (512x2048x2048x223): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x223): 81.487
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0526
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 253.819
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.2149
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 248.499
Elapsed time for mlp_fused_gelu (2048x4x114176): 0.0031
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.2209
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 241.768
Elapsed time for transformer_add_bias_dropout (2048x4x28544): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28544): 0.0011

Attention duration (in seconds): 0.3081
Attention throughput (in TFLOP/s): 179.499
MLP duration (in seconds): 0.4389
MLP throughput (in TFLOP/s): 243.334
Transformer duration (in seconds): 0.7535
Transformer throughput (in TFLOP/s): 215.139
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.1618
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 249.779
Elapsed time for attention_key_query_prob (512x2048x224x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x224x2048): 149.469
Elapsed time for attention_prob_times_values (512x2048x2048x224): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x224): 202.622
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0530
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 254.367
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.2169
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 248.435
Elapsed time for mlp_fused_gelu (2048x4x114688): 0.0031
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2237
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 240.885
Elapsed time for transformer_add_bias_dropout (2048x4x28672): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28672): 0.0012

Attention duration (in seconds): 0.2957
Attention throughput (in TFLOP/s): 188.674
MLP duration (in seconds): 0.4437
MLP throughput (in TFLOP/s): 242.865
Transformer duration (in seconds): 0.7459
Transformer throughput (in TFLOP/s): 219.259
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.1632
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 249.782
Elapsed time for attention_key_query_prob (512x2048x225x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x225x2048): 66.816
Elapsed time for attention_prob_times_values (512x2048x2048x225): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x225): 81.903
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0534
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 254.465
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.2196
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 247.548
Elapsed time for mlp_fused_gelu (2048x4x115200): 0.0032
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.2264
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 240.082
Elapsed time for transformer_add_bias_dropout (2048x4x28800): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28800): 0.0012

Attention duration (in seconds): 0.3127
Attention throughput (in TFLOP/s): 179.999
MLP duration (in seconds): 0.4492
MLP throughput (in TFLOP/s): 242.038
Transformer duration (in seconds): 0.7685
Transformer throughput (in TFLOP/s): 214.727
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.1652
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 249.004
Elapsed time for attention_key_query_prob (512x2048x226x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x226x2048): 94.996
Elapsed time for attention_prob_times_values (512x2048x2048x226): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x226): 131.786
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0540
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 254.097
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.2208
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 248.431
Elapsed time for mlp_fused_gelu (2048x4x115712): 0.0032
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.2284
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 240.065
Elapsed time for transformer_add_bias_dropout (2048x4x28928): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28928): 0.0012

Attention duration (in seconds): 0.3066
Attention throughput (in TFLOP/s): 185.222
MLP duration (in seconds): 0.4524
MLP throughput (in TFLOP/s): 242.460
Transformer duration (in seconds): 0.7655
Transformer throughput (in TFLOP/s): 217.462
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.1662
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 249.660
Elapsed time for attention_key_query_prob (512x2048x227x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x227x2048): 66.705
Elapsed time for attention_prob_times_values (512x2048x2048x227): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x227): 84.005
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0545
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 253.793
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.2234
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 247.721
Elapsed time for mlp_fused_gelu (2048x4x116224): 0.0032
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.2296
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 240.999
Elapsed time for transformer_add_bias_dropout (2048x4x29056): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29056): 0.0012

Attention duration (in seconds): 0.3168
Attention throughput (in TFLOP/s): 180.813
MLP duration (in seconds): 0.4561
MLP throughput (in TFLOP/s): 242.603
Transformer duration (in seconds): 0.7795
Transformer throughput (in TFLOP/s): 215.445
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.1678
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 249.433
Elapsed time for attention_key_query_prob (512x2048x228x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x228x2048): 95.374
Elapsed time for attention_prob_times_values (512x2048x2048x228): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x228): 132.694
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0551
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 253.433
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.2237
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 249.550
Elapsed time for mlp_fused_gelu (2048x4x116736): 0.0032
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.2323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 240.252
Elapsed time for transformer_add_bias_dropout (2048x4x29184): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29184): 0.0012

Attention duration (in seconds): 0.3104
Attention throughput (in TFLOP/s): 186.146
MLP duration (in seconds): 0.4592
MLP throughput (in TFLOP/s): 243.103
Transformer duration (in seconds): 0.7762
Transformer throughput (in TFLOP/s): 218.255
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.1696
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 248.962
Elapsed time for attention_key_query_prob (512x2048x229x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x229x2048): 67.169
Elapsed time for attention_prob_times_values (512x2048x2048x229): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x229): 84.543
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 253.953
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.2275
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 247.538
Elapsed time for mlp_fused_gelu (2048x4x117248): 0.0032
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2345
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 240.144
Elapsed time for transformer_add_bias_dropout (2048x4x29312): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29312): 0.0012

Attention duration (in seconds): 0.3212
Attention throughput (in TFLOP/s): 181.443
MLP duration (in seconds): 0.4652
MLP throughput (in TFLOP/s): 242.094
Transformer duration (in seconds): 0.7930
Transformer throughput (in TFLOP/s): 215.495
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.1708
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 249.438
Elapsed time for attention_key_query_prob (512x2048x230x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x230x2048): 96.471
Elapsed time for attention_prob_times_values (512x2048x2048x230): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x230): 133.406
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 252.507
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.2287
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 248.344
Elapsed time for mlp_fused_gelu (2048x4x117760): 0.0032
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2369
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 239.750
Elapsed time for transformer_add_bias_dropout (2048x4x29440): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29440): 0.0012

Attention duration (in seconds): 0.3145
Attention throughput (in TFLOP/s): 186.879
MLP duration (in seconds): 0.4689
MLP throughput (in TFLOP/s): 242.286
Transformer duration (in seconds): 0.7901
Transformer throughput (in TFLOP/s): 218.178
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.1725
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 249.080
Elapsed time for attention_key_query_prob (512x2048x231x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x231x2048): 67.233
Elapsed time for attention_prob_times_values (512x2048x2048x231): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x231): 83.172
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0565
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 253.461
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.2300
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 249.114
Elapsed time for mlp_fused_gelu (2048x4x118272): 0.0032
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2375
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 241.211
Elapsed time for transformer_add_bias_dropout (2048x4x29568): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29568): 0.0012

Attention duration (in seconds): 0.3256
Attention throughput (in TFLOP/s): 182.086
MLP duration (in seconds): 0.4708
MLP throughput (in TFLOP/s): 243.408
Transformer duration (in seconds): 0.8031
Transformer throughput (in TFLOP/s): 216.505
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.1743
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 248.622
Elapsed time for attention_key_query_prob (512x2048x232x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x232x2048): 130.747
Elapsed time for attention_prob_times_values (512x2048x2048x232): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x232): 204.369
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0570
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 253.566
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.2332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 247.861
Elapsed time for mlp_fused_gelu (2048x4x118784): 0.0033
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2408
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 240.020
Elapsed time for transformer_add_bias_dropout (2048x4x29696): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29696): 0.0012

Attention duration (in seconds): 0.3137
Attention throughput (in TFLOP/s): 190.610
MLP duration (in seconds): 0.4772
MLP throughput (in TFLOP/s): 242.211
Transformer duration (in seconds): 0.7976
Transformer throughput (in TFLOP/s): 219.871
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.1750
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 249.765
Elapsed time for attention_key_query_prob (512x2048x233x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x233x2048): 67.614
Elapsed time for attention_prob_times_values (512x2048x2048x233): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x233): 83.691
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0577
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 252.558
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.2355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 247.489
Elapsed time for mlp_fused_gelu (2048x4x119296): 0.0033
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.2436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 239.313
Elapsed time for transformer_add_bias_dropout (2048x4x29824): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29824): 0.0012

Attention duration (in seconds): 0.3293
Attention throughput (in TFLOP/s): 183.073
MLP duration (in seconds): 0.4824
MLP throughput (in TFLOP/s): 241.680
Transformer duration (in seconds): 0.8185
Transformer throughput (in TFLOP/s): 216.095
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.1777
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 248.166
Elapsed time for attention_key_query_prob (512x2048x234x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x234x2048): 98.617
Elapsed time for attention_prob_times_values (512x2048x2048x234): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x234): 136.283
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 252.893
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.2376
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 247.414
Elapsed time for mlp_fused_gelu (2048x4x119808): 0.0033
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.2451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 239.855
Elapsed time for transformer_add_bias_dropout (2048x4x29952): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29952): 0.0012

Attention duration (in seconds): 0.3232
Attention throughput (in TFLOP/s): 188.125
MLP duration (in seconds): 0.4860
MLP throughput (in TFLOP/s): 241.928
Transformer duration (in seconds): 0.8160
Transformer throughput (in TFLOP/s): 218.609
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.1781
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 249.666
Elapsed time for attention_key_query_prob (512x2048x235x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x235x2048): 68.463
Elapsed time for attention_prob_times_values (512x2048x2048x235): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x235): 87.091
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0584
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 253.867
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.2382
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 248.944
Elapsed time for mlp_fused_gelu (2048x4x120320): 0.0033
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2477
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 239.399
Elapsed time for transformer_add_bias_dropout (2048x4x30080): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30080): 0.0012

Attention duration (in seconds): 0.3327
Attention throughput (in TFLOP/s): 184.300
MLP duration (in seconds): 0.4892
MLP throughput (in TFLOP/s): 242.428
Transformer duration (in seconds): 0.8287
Transformer throughput (in TFLOP/s): 217.099
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.1802
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 248.969
Elapsed time for attention_key_query_prob (512x2048x236x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x236x2048): 98.941
Elapsed time for attention_prob_times_values (512x2048x2048x236): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x236): 138.059
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0590
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 253.439
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.2405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 248.625
Elapsed time for mlp_fused_gelu (2048x4x120832): 0.0033
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2494
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 239.784
Elapsed time for transformer_add_bias_dropout (2048x4x30208): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30208): 0.0012

Attention duration (in seconds): 0.3266
Attention throughput (in TFLOP/s): 189.333
MLP duration (in seconds): 0.4933
MLP throughput (in TFLOP/s): 242.483
Transformer duration (in seconds): 0.8267
Transformer throughput (in TFLOP/s): 219.473
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.1824
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 247.977
Elapsed time for attention_key_query_prob (512x2048x237x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x237x2048): 69.102
Elapsed time for attention_prob_times_values (512x2048x2048x237): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x237): 87.695
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0596
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 253.082
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.2436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 247.609
Elapsed time for mlp_fused_gelu (2048x4x121344): 0.0033
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2518
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 239.549
Elapsed time for transformer_add_bias_dropout (2048x4x30336): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30336): 0.0012

Attention duration (in seconds): 0.3382
Attention throughput (in TFLOP/s): 184.371
MLP duration (in seconds): 0.4987
MLP throughput (in TFLOP/s): 241.885
Transformer duration (in seconds): 0.8437
Transformer throughput (in TFLOP/s): 216.859
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.1834
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 248.771
Elapsed time for attention_key_query_prob (512x2048x238x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x238x2048): 99.610
Elapsed time for attention_prob_times_values (512x2048x2048x238): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x238): 138.900
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0600
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 253.459
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.2458
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 247.450
Elapsed time for mlp_fused_gelu (2048x4x121856): 0.0033
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2535
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 239.919
Elapsed time for transformer_add_bias_dropout (2048x4x30464): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30464): 0.0012

Attention duration (in seconds): 0.3308
Attention throughput (in TFLOP/s): 190.032
MLP duration (in seconds): 0.5026
MLP throughput (in TFLOP/s): 242.004
Transformer duration (in seconds): 0.8404
Transformer throughput (in TFLOP/s): 219.558
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.1854
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 248.154
Elapsed time for attention_key_query_prob (512x2048x239x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x239x2048): 69.051
Elapsed time for attention_prob_times_values (512x2048x2048x239): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x239): 86.775
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0604
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 253.877
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.2482
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 247.110
Elapsed time for mlp_fused_gelu (2048x4x122368): 0.0034
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2560
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 239.567
Elapsed time for transformer_add_bias_dropout (2048x4x30592): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30592): 0.0012

Attention duration (in seconds): 0.3423
Attention throughput (in TFLOP/s): 185.179
MLP duration (in seconds): 0.5076
MLP throughput (in TFLOP/s): 241.670
Transformer duration (in seconds): 0.8569
Transformer throughput (in TFLOP/s): 217.135
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.1867
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 248.426
Elapsed time for attention_key_query_prob (512x2048x240x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x240x2048): 145.615
Elapsed time for attention_prob_times_values (512x2048x2048x240): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x240): 212.517
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 252.840
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.2494
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 247.961
Elapsed time for mlp_fused_gelu (2048x4x122880): 0.0034
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2570
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 240.633
Elapsed time for transformer_add_bias_dropout (2048x4x30720): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30720): 0.0012

Attention duration (in seconds): 0.3296
Attention throughput (in TFLOP/s): 193.876
MLP duration (in seconds): 0.5098
MLP throughput (in TFLOP/s): 242.627
Transformer duration (in seconds): 0.8464
Transformer throughput (in TFLOP/s): 221.641
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.1881
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 248.699
Elapsed time for attention_key_query_prob (512x2048x241x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x241x2048): 69.714
Elapsed time for attention_prob_times_values (512x2048x2048x241): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x241): 87.547
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0614
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 253.963
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.2522
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 247.238
Elapsed time for mlp_fused_gelu (2048x4x123392): 0.0034
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.2590
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 240.833
Elapsed time for transformer_add_bias_dropout (2048x4x30848): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30848): 0.0012

Attention duration (in seconds): 0.3460
Attention throughput (in TFLOP/s): 186.242
MLP duration (in seconds): 0.5146
MLP throughput (in TFLOP/s): 242.387
Transformer duration (in seconds): 0.8675
Transformer throughput (in TFLOP/s): 218.047
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.1896
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 248.755
Elapsed time for attention_key_query_prob (512x2048x242x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x242x2048): 101.207
Elapsed time for attention_prob_times_values (512x2048x2048x242): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x242): 141.152
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0617
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 254.827
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.2538
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 247.804
Elapsed time for mlp_fused_gelu (2048x4x123904): 0.0034
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.2605
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 241.388
Elapsed time for transformer_add_bias_dropout (2048x4x30976): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30976): 0.0012

Attention duration (in seconds): 0.3388
Attention throughput (in TFLOP/s): 191.764
MLP duration (in seconds): 0.5177
MLP throughput (in TFLOP/s): 242.948
Transformer duration (in seconds): 0.8634
Transformer throughput (in TFLOP/s): 220.894
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x93312, b=2048): 0.1901
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x93312, b=2048): 250.088
Elapsed time for attention_key_query_prob (512x2048x243x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x243x2048): 70.542
Elapsed time for attention_prob_times_values (512x2048x2048x243): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x243): 90.245
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x31104x31104, b=2048): 0.0620
Throughput (in TFLOP/s) for attention_linear_projection (4x31104x31104, b=2048): 255.581
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x31104x124416, b=2048): 0.2557
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x124416, b=2048): 247.991
Elapsed time for mlp_fused_gelu (2048x4x124416): 0.0034
Elapsed time for mlp_4h_to_h (4x124416x31104, b=2048): 0.2626
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124416x31104, b=2048): 241.475
Elapsed time for transformer_add_bias_dropout (2048x4x31104): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31104): 0.0013

Attention duration (in seconds): 0.3484
Attention throughput (in TFLOP/s): 187.995
MLP duration (in seconds): 0.5216
MLP throughput (in TFLOP/s): 243.088
Transformer duration (in seconds): 0.8771
Transformer throughput (in TFLOP/s): 219.247
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x93696, b=2048): 0.1924
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x93696, b=2048): 249.193
Elapsed time for attention_key_query_prob (512x2048x244x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x244x2048): 102.534
Elapsed time for attention_prob_times_values (512x2048x2048x244): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x244): 143.701
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x31232x31232, b=2048): 0.0626
Throughput (in TFLOP/s) for attention_linear_projection (4x31232x31232, b=2048): 255.334
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x31232x124928, b=2048): 0.2571
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x124928, b=2048): 248.666
Elapsed time for mlp_fused_gelu (2048x4x124928): 0.0034
Elapsed time for mlp_4h_to_h (4x124928x31232, b=2048): 0.2646
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124928x31232, b=2048): 241.610
Elapsed time for transformer_add_bias_dropout (2048x4x31232): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31232): 0.0013

Attention duration (in seconds): 0.3423
Attention throughput (in TFLOP/s): 192.854
MLP duration (in seconds): 0.5251
MLP throughput (in TFLOP/s): 243.488
Transformer duration (in seconds): 0.8745
Transformer throughput (in TFLOP/s): 221.695
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x94080, b=2048): 0.1944
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x94080, b=2048): 248.637
Elapsed time for attention_key_query_prob (512x2048x245x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x245x2048): 71.349
Elapsed time for attention_prob_times_values (512x2048x2048x245): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x245): 90.950
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x31360x31360, b=2048): 0.0633
Throughput (in TFLOP/s) for attention_linear_projection (4x31360x31360, b=2048): 254.529
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x31360x125440, b=2048): 0.2603
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x125440, b=2048): 247.612
Elapsed time for mlp_fused_gelu (2048x4x125440): 0.0034
Elapsed time for mlp_4h_to_h (4x125440x31360, b=2048): 0.2673
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125440x31360, b=2048): 241.090
Elapsed time for transformer_add_bias_dropout (2048x4x31360): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31360): 0.0013

Attention duration (in seconds): 0.3539
Attention throughput (in TFLOP/s): 188.075
MLP duration (in seconds): 0.5311
MLP throughput (in TFLOP/s): 242.724
Transformer duration (in seconds): 0.8921
Transformer throughput (in TFLOP/s): 219.110
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x94464, b=2048): 0.1965
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x94464, b=2048): 248.019
Elapsed time for attention_key_query_prob (512x2048x246x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x246x2048): 103.071
Elapsed time for attention_prob_times_values (512x2048x2048x246): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x246): 143.881
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x31488x31488, b=2048): 0.0638
Throughput (in TFLOP/s) for attention_linear_projection (4x31488x31488, b=2048): 254.529
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x31488x125952, b=2048): 0.2626
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x125952, b=2048): 247.464
Elapsed time for mlp_fused_gelu (2048x4x125952): 0.0035
Elapsed time for mlp_4h_to_h (4x125952x31488, b=2048): 0.2680
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125952x31488, b=2048): 242.471
Elapsed time for transformer_add_bias_dropout (2048x4x31488): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31488): 0.0013

Attention duration (in seconds): 0.3478
Attention throughput (in TFLOP/s): 192.927
MLP duration (in seconds): 0.5340
MLP throughput (in TFLOP/s): 243.354
Transformer duration (in seconds): 0.8889
Transformer throughput (in TFLOP/s): 221.673
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x94848, b=2048): 0.1965
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x94848, b=2048): 250.030
Elapsed time for attention_key_query_prob (512x2048x247x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x247x2048): 70.635
Elapsed time for attention_prob_times_values (512x2048x2048x247): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x247): 90.449
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x31616x31616, b=2048): 0.0643
Throughput (in TFLOP/s) for attention_linear_projection (4x31616x31616, b=2048): 254.782
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x31616x126464, b=2048): 0.2645
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x126464, b=2048): 247.641
Elapsed time for mlp_fused_gelu (2048x4x126464): 0.0035
Elapsed time for mlp_4h_to_h (4x126464x31616, b=2048): 0.2708
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126464x31616, b=2048): 241.897
Elapsed time for transformer_add_bias_dropout (2048x4x31616): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31616): 0.0013

Attention duration (in seconds): 0.3574
Attention throughput (in TFLOP/s): 189.250
MLP duration (in seconds): 0.5388
MLP throughput (in TFLOP/s): 243.158
Transformer duration (in seconds): 0.9033
Transformer throughput (in TFLOP/s): 219.901
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x95232, b=2048): 0.1992
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x95232, b=2048): 248.701
Elapsed time for attention_key_query_prob (512x2048x248x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x248x2048): 131.655
Elapsed time for attention_prob_times_values (512x2048x2048x248): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x248): 216.715
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x31744x31744, b=2048): 0.0645
Throughput (in TFLOP/s) for attention_linear_projection (4x31744x31744, b=2048): 255.835
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x31744x126976, b=2048): 0.2672
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x126976, b=2048): 247.200
Elapsed time for mlp_fused_gelu (2048x4x126976): 0.0035
Elapsed time for mlp_4h_to_h (4x126976x31744, b=2048): 0.2731
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126976x31744, b=2048): 241.823
Elapsed time for transformer_add_bias_dropout (2048x4x31744): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31744): 0.0013

Attention duration (in seconds): 0.3465
Attention throughput (in TFLOP/s): 196.712
MLP duration (in seconds): 0.5437
MLP throughput (in TFLOP/s): 242.913
Transformer duration (in seconds): 0.8975
Transformer throughput (in TFLOP/s): 223.124
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x95616, b=2048): 0.2006
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x95616, b=2048): 248.905
Elapsed time for attention_key_query_prob (512x2048x249x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x249x2048): 71.395
Elapsed time for attention_prob_times_values (512x2048x2048x249): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x249): 91.351
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x31872x31872, b=2048): 0.0653
Throughput (in TFLOP/s) for attention_linear_projection (4x31872x31872, b=2048): 254.734
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x31872x127488, b=2048): 0.2684
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x127488, b=2048): 248.028
Elapsed time for mlp_fused_gelu (2048x4x127488): 0.0035
Elapsed time for mlp_4h_to_h (4x127488x31872, b=2048): 0.2754
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127488x31872, b=2048): 241.738
Elapsed time for transformer_add_bias_dropout (2048x4x31872): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31872): 0.0013

Attention duration (in seconds): 0.3625
Attention throughput (in TFLOP/s): 189.573
MLP duration (in seconds): 0.5473
MLP throughput (in TFLOP/s): 243.278
Transformer duration (in seconds): 0.9170
Transformer throughput (in TFLOP/s): 220.123
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x96000, b=2048): 0.2023
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x96000, b=2048): 248.800
Elapsed time for attention_key_query_prob (512x2048x250x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x250x2048): 105.271
Elapsed time for attention_prob_times_values (512x2048x2048x250): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x250): 147.340
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x32000x32000, b=2048): 0.0660
Throughput (in TFLOP/s) for attention_linear_projection (4x32000x32000, b=2048): 254.181
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x32000x128000, b=2048): 0.2700
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x128000, b=2048): 248.510
Elapsed time for mlp_fused_gelu (2048x4x128000): 0.0035
Elapsed time for mlp_4h_to_h (4x128000x32000, b=2048): 0.2780
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128000x32000, b=2048): 241.436
Elapsed time for transformer_add_bias_dropout (2048x4x32000): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32000): 0.0013

Attention duration (in seconds): 0.3556
Attention throughput (in TFLOP/s): 194.749
MLP duration (in seconds): 0.5515
MLP throughput (in TFLOP/s): 243.360
Transformer duration (in seconds): 0.9144
Transformer throughput (in TFLOP/s): 222.517
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x96384, b=2048): 0.2042
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x96384, b=2048): 248.463
Elapsed time for attention_key_query_prob (512x2048x251x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x251x2048): 72.802
Elapsed time for attention_prob_times_values (512x2048x2048x251): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x251): 94.383
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x32128x32128, b=2048): 0.0667
Throughput (in TFLOP/s) for attention_linear_projection (4x32128x32128, b=2048): 253.670
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x32128x128512, b=2048): 0.2723
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x128512, b=2048): 248.441
Elapsed time for mlp_fused_gelu (2048x4x128512): 0.0035
Elapsed time for mlp_4h_to_h (4x128512x32128, b=2048): 0.2798
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128512x32128, b=2048): 241.758
Elapsed time for transformer_add_bias_dropout (2048x4x32128): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32128): 0.0013

Attention duration (in seconds): 0.3669
Attention throughput (in TFLOP/s): 190.234
MLP duration (in seconds): 0.5556
MLP throughput (in TFLOP/s): 243.499
Transformer duration (in seconds): 0.9298
Transformer throughput (in TFLOP/s): 220.576
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x96768, b=2048): 0.2059
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x96768, b=2048): 248.363
Elapsed time for attention_key_query_prob (512x2048x252x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x252x2048): 106.245
Elapsed time for attention_prob_times_values (512x2048x2048x252): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x252): 149.256
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x32256x32256, b=2048): 0.0668
Throughput (in TFLOP/s) for attention_linear_projection (4x32256x32256, b=2048): 255.122
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x32256x129024, b=2048): 0.2755
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x129024, b=2048): 247.543
Elapsed time for mlp_fused_gelu (2048x4x129024): 0.0035
Elapsed time for mlp_4h_to_h (4x129024x32256, b=2048): 0.2829
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129024x32256, b=2048): 240.997
Elapsed time for transformer_add_bias_dropout (2048x4x32256): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32256): 0.0013

Attention duration (in seconds): 0.3600
Attention throughput (in TFLOP/s): 195.418
MLP duration (in seconds): 0.5619
MLP throughput (in TFLOP/s): 242.688
Transformer duration (in seconds): 0.9292
Transformer throughput (in TFLOP/s): 222.470
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x97152, b=2048): 0.2079
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x97152, b=2048): 247.906
Elapsed time for attention_key_query_prob (512x2048x253x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x253x2048): 73.580
Elapsed time for attention_prob_times_values (512x2048x2048x253): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x253): 95.241
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x32384x32384, b=2048): 0.0678
Throughput (in TFLOP/s) for attention_linear_projection (4x32384x32384, b=2048): 253.335
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x32384x129536, b=2048): 0.2780
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x129536, b=2048): 247.204
Elapsed time for mlp_fused_gelu (2048x4x129536): 0.0036
Elapsed time for mlp_4h_to_h (4x129536x32384, b=2048): 0.2842
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129536x32384, b=2048): 241.805
Elapsed time for transformer_add_bias_dropout (2048x4x32384): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32384): 0.0013

Attention duration (in seconds): 0.3718
Attention throughput (in TFLOP/s): 190.716
MLP duration (in seconds): 0.5658
MLP throughput (in TFLOP/s): 242.938
Transformer duration (in seconds): 0.9449
Transformer throughput (in TFLOP/s): 220.507
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x97536, b=2048): 0.2091
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x97536, b=2048): 248.502
Elapsed time for attention_key_query_prob (512x2048x254x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x254x2048): 107.952
Elapsed time for attention_prob_times_values (512x2048x2048x254): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x254): 150.808
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x32512x32512, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_linear_projection (4x32512x32512, b=2048): 253.875
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x32512x130048, b=2048): 0.2793
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x130048, b=2048): 248.001
Elapsed time for mlp_fused_gelu (2048x4x130048): 0.0036
Elapsed time for mlp_4h_to_h (4x130048x32512, b=2048): 0.2867
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130048x32512, b=2048): 241.654
Elapsed time for transformer_add_bias_dropout (2048x4x32512): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32512): 0.0013

Attention duration (in seconds): 0.3645
Attention throughput (in TFLOP/s): 196.056
MLP duration (in seconds): 0.5696
MLP throughput (in TFLOP/s): 243.253
Transformer duration (in seconds): 0.9414
Transformer throughput (in TFLOP/s): 223.081
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x97920, b=2048): 0.2104
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x97920, b=2048): 248.900
Elapsed time for attention_key_query_prob (512x2048x255x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x255x2048): 73.491
Elapsed time for attention_prob_times_values (512x2048x2048x255): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x255): 94.964
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x32640x32640, b=2048): 0.0688
Throughput (in TFLOP/s) for attention_linear_projection (4x32640x32640, b=2048): 253.600
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x32640x130560, b=2048): 0.2819
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x130560, b=2048): 247.641
Elapsed time for mlp_fused_gelu (2048x4x130560): 0.0036
Elapsed time for mlp_4h_to_h (4x130560x32640, b=2048): 0.2917
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130560x32640, b=2048): 239.330
Elapsed time for transformer_add_bias_dropout (2048x4x32640): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32640): 0.0013

Attention duration (in seconds): 0.3755
Attention throughput (in TFLOP/s): 191.778
MLP duration (in seconds): 0.5773
MLP throughput (in TFLOP/s): 241.905
Transformer duration (in seconds): 0.9601
Transformer throughput (in TFLOP/s): 220.444
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 128, hidden_size: 32768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32768x98304, b=2048): 0.2132
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32768x98304, b=2048): 247.544
Elapsed time for attention_key_query_prob (512x2048x256x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x256x2048): 158.810
Elapsed time for attention_prob_times_values (512x2048x2048x256): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x256): 226.240
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0608
(10807214080, 42481549312)
Elapsed time for attention_linear_projection (4x32768x32768, b=2048): 0.0695
Throughput (in TFLOP/s) for attention_linear_projection (4x32768x32768, b=2048): 253.259
(10807214080, 42481549312)
Elapsed time for mlp_h_to_4h (4x32768x131072, b=2048): 0.2830
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32768x131072, b=2048): 248.641
Elapsed time for mlp_fused_gelu (2048x4x131072): 0.0036
Elapsed time for mlp_4h_to_h (4x131072x32768, b=2048): 0.2916
Throughput (in TFLOP/s) for mlp_4h_to_h (4x131072x32768, b=2048): 241.332
Elapsed time for transformer_add_bias_dropout (2048x4x32768): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32768): 0.0013

Attention duration (in seconds): 0.3643
Attention throughput (in TFLOP/s): 199.206
MLP duration (in seconds): 0.5782
MLP throughput (in TFLOP/s): 243.408
Transformer duration (in seconds): 0.9499
Transformer throughput (in TFLOP/s): 224.554
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
