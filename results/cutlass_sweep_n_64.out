cupy is not loaded.
cupy is not loaded.
cupy is not loaded.
cupy is not loaded.
[2023-09-04 19:05:53,045] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-09-04 19:05:53,521] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.158.60, master_port=6000
[2023-09-04 19:05:53,521] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-04 19:05:53,525] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 251.487
b: 256, m: 2048, n: 128, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 94.239
b: 256, m: 2048, n: 2048, k: 128,
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 77.434
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 213.751
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0173
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 254.396
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0174
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 253.466

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 200.100
MLP duration (in seconds): 0.0346
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0594
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 244.929
b: 256, m: 2048, n: 130, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 83.561
b: 256, m: 2048, n: 2048, k: 130,
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 52.295
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 209.871
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0181
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 250.098
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0180
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 252.116

Attention duration (in seconds): 0.0280
Attention throughput (in TFLOP/s): 182.125
MLP duration (in seconds): 0.0361
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0641
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 249.086
b: 256, m: 2048, n: 132, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 84.822
b: 256, m: 2048, n: 2048, k: 132,
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 66.900
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 213.812
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 253.112
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 253.419

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 193.290
MLP duration (in seconds): 0.0369
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0641
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 251.559
b: 256, m: 2048, n: 134, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 85.558
b: 256, m: 2048, n: 2048, k: 134,
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 53.281
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 217.243
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0188
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 256.400
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0190
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 254.266

Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 188.120
MLP duration (in seconds): 0.0378
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0664
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 245.015
b: 256, m: 2048, n: 136, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 86.651
b: 256, m: 2048, n: 2048, k: 136,
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 73.581
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 213.376
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0199
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 249.245
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0195
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 255.116

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 195.702
MLP duration (in seconds): 0.0394
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0677
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 248.389
b: 256, m: 2048, n: 138, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 87.770
b: 256, m: 2048, n: 2048, k: 138,
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 54.291
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0059
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 216.679
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0202
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 252.626
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0200
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 255.949

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 189.094
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0704
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 251.898
b: 256, m: 2048, n: 140, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 89.713
b: 256, m: 2048, n: 2048, k: 140,
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 69.742
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0060
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 220.421
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0204
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 258.035
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0205
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 256.899

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 200.127
MLP duration (in seconds): 0.0409
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0702
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 246.339
b: 256, m: 2048, n: 142, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 90.339
b: 256, m: 2048, n: 2048, k: 142,
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 55.465
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 216.356
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 246.123
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 255.614

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 190.545
MLP duration (in seconds): 0.0432
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0748
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 249.979
b: 256, m: 2048, n: 144, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 91.728
b: 256, m: 2048, n: 2048, k: 144,
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 77.620
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 219.671
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 253.053
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0217
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 256.512

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 203.511
MLP duration (in seconds): 0.0437
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0741
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 252.094
b: 256, m: 2048, n: 146, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 92.407
b: 256, m: 2048, n: 2048, k: 146,
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 56.793
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 222.979
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0222
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 257.983
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0222
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 257.554

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 196.247
MLP duration (in seconds): 0.0444
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0767
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 256.857
b: 256, m: 2048, n: 148, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 94.510
b: 256, m: 2048, n: 2048, k: 148,
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 73.520
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 226.637
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 260.673
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 258.281

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 207.894
MLP duration (in seconds): 0.0453
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0767
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 250.833
b: 256, m: 2048, n: 150, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 94.839
b: 256, m: 2048, n: 2048, k: 150,
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 57.956
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 222.428
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0238
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 254.286
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0234
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 258.279

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 197.739
MLP duration (in seconds): 0.0471
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0809
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 252.636
b: 256, m: 2048, n: 152, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 96.429
b: 256, m: 2048, n: 2048, k: 152,
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 79.714
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 225.492
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0241
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 257.557
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0241
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 257.069

Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 209.194
MLP duration (in seconds): 0.0482
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0810
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 256.791
b: 256, m: 2048, n: 154, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 97.245
b: 256, m: 2048, n: 2048, k: 154,
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 59.561
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 228.526
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0245
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 259.973
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 257.072

Attention duration (in seconds): 0.0345
Attention throughput (in TFLOP/s): 203.633
MLP duration (in seconds): 0.0493
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0838
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 247.467
b: 256, m: 2048, n: 156, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 99.302
b: 256, m: 2048, n: 2048, k: 156,
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 76.724
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 224.619
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 253.332
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 257.503

Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 206.917
MLP duration (in seconds): 0.0512
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0860
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 253.058
b: 256, m: 2048, n: 158, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 99.792
b: 256, m: 2048, n: 2048, k: 158,
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 60.896
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 227.830
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 255.690
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0260
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 257.990

Attention duration (in seconds): 0.0362
Attention throughput (in TFLOP/s): 203.942
MLP duration (in seconds): 0.0522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0884
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 255.886
b: 256, m: 2048, n: 160, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 101.490
b: 256, m: 2048, n: 2048, k: 160,
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 86.266
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 230.725
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 259.321
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 259.000

Attention duration (in seconds): 0.0350
Attention throughput (in TFLOP/s): 216.246
MLP duration (in seconds): 0.0530
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0880
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 258.435
b: 256, m: 2048, n: 162, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 102.190
b: 256, m: 2048, n: 2048, k: 162,
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 62.307
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 233.530
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 261.268
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0271
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 259.542

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 209.352
MLP duration (in seconds): 0.0541
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0911
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 253.566
b: 256, m: 2048, n: 164, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 104.431
b: 256, m: 2048, n: 2048, k: 164,
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 80.527
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 229.668
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0283
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 255.346
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 258.053

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 214.400
MLP duration (in seconds): 0.0563
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0932
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 256.390
b: 256, m: 2048, n: 166, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 104.618
b: 256, m: 2048, n: 2048, k: 166,
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 64.004
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 232.846
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0286
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 258.794
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0286
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 258.635

Attention duration (in seconds): 0.0386
Attention throughput (in TFLOP/s): 210.337
MLP duration (in seconds): 0.0572
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0957
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 258.852
b: 256, m: 2048, n: 168, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 106.012
b: 256, m: 2048, n: 2048, k: 168,
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 87.814
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 236.622
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0292
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 259.376
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0292
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 259.532

Attention duration (in seconds): 0.0375
Attention throughput (in TFLOP/s): 221.466
MLP duration (in seconds): 0.0584
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0959
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 253.087
b: 256, m: 2048, n: 170, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 106.854
b: 256, m: 2048, n: 2048, k: 170,
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 65.636
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 231.749
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 254.597
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0301
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 257.674

Attention duration (in seconds): 0.0403
Attention throughput (in TFLOP/s): 210.426
MLP duration (in seconds): 0.0606
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1009
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0234
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 254.378
b: 256, m: 2048, n: 172, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 109.163
b: 256, m: 2048, n: 2048, k: 172,
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 84.354
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 234.535
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0309
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 256.682
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 258.164

Attention duration (in seconds): 0.0396
Attention throughput (in TFLOP/s): 218.965
MLP duration (in seconds): 0.0617
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 257.486
b: 256, m: 2048, n: 174, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 108.905
b: 256, m: 2048, n: 2048, k: 174,
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 67.201
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 237.658
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0314
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 259.063
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0316
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 257.101

Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 215.331
MLP duration (in seconds): 0.0630
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 252.168
b: 256, m: 2048, n: 176, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 111.216
b: 256, m: 2048, n: 2048, k: 176,
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 92.033
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 233.190
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 254.055
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 257.449

Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 220.435
MLP duration (in seconds): 0.0650
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 254.212
b: 256, m: 2048, n: 178, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 111.696
b: 256, m: 2048, n: 2048, k: 178,
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 68.684
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 235.858
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0331
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 256.648
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0329
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 258.165

Attention duration (in seconds): 0.0431
Attention throughput (in TFLOP/s): 215.095
MLP duration (in seconds): 0.0661
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 257.294
b: 256, m: 2048, n: 180, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 114.539
b: 256, m: 2048, n: 2048, k: 180,
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 87.557
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 238.982
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0335
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 259.317
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0336
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 258.679

Attention duration (in seconds): 0.0422
Attention throughput (in TFLOP/s): 224.204
MLP duration (in seconds): 0.0672
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 259.142
b: 256, m: 2048, n: 182, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 113.849
b: 256, m: 2048, n: 2048, k: 182,
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 70.053
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 241.713
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0341
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 261.059
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0343
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 259.421

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 220.135
MLP duration (in seconds): 0.0683
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 254.503
b: 256, m: 2048, n: 184, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 115.356
b: 256, m: 2048, n: 2048, k: 184,
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 94.867
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 237.386
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 255.834
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0352
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 257.935

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 224.798
MLP duration (in seconds): 0.0708
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 256.853
b: 256, m: 2048, n: 186, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 116.053
b: 256, m: 2048, n: 2048, k: 186,
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 71.461
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 240.057
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0360
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 257.639
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0359
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 258.497

Attention duration (in seconds): 0.0458
Attention throughput (in TFLOP/s): 220.116
MLP duration (in seconds): 0.0720
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0275
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 258.809
b: 256, m: 2048, n: 188, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 119.452
b: 256, m: 2048, n: 2048, k: 188,
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 91.143
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 242.971
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0366
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 259.558
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0366
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 259.142

Attention duration (in seconds): 0.0451
Attention throughput (in TFLOP/s): 228.447
MLP duration (in seconds): 0.0732
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 254.141
b: 256, m: 2048, n: 190, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 118.213
b: 256, m: 2048, n: 2048, k: 190,
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 72.861
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 238.667
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0378
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 256.240
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0373
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 259.717

Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 219.802
MLP duration (in seconds): 0.0751
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 257.115
b: 256, m: 2048, n: 192, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 121.063
b: 256, m: 2048, n: 2048, k: 192,
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 103.537
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 241.264
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0383
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 258.433
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0380
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 260.384

Attention duration (in seconds): 0.0465
Attention throughput (in TFLOP/s): 230.506
MLP duration (in seconds): 0.0763
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 259.299
b: 256, m: 2048, n: 194, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 120.444
b: 256, m: 2048, n: 2048, k: 194,
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 68.714
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 244.081
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0389
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 259.472
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0389
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 259.509

Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 222.770
MLP duration (in seconds): 0.0779
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 253.867
b: 256, m: 2048, n: 196, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 123.941
b: 256, m: 2048, n: 2048, k: 196,
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 86.009
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 239.123
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 254.650
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0400
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 257.638

Attention duration (in seconds): 0.0495
Attention throughput (in TFLOP/s): 225.168
MLP duration (in seconds): 0.0805
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 255.289
b: 256, m: 2048, n: 198, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 120.396
b: 256, m: 2048, n: 2048, k: 198,
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 69.141
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 241.634
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 257.115
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 259.643

Attention duration (in seconds): 0.0515
Attention throughput (in TFLOP/s): 220.914
MLP duration (in seconds): 0.0815
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1329
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 257.924
b: 256, m: 2048, n: 200, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 125.187
b: 256, m: 2048, n: 2048, k: 200,
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 97.081
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 244.119
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 258.927
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0414
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 259.671

Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 231.587
MLP duration (in seconds): 0.0828
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1329
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0317
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 258.884
b: 256, m: 2048, n: 202, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 120.886
b: 256, m: 2048, n: 2048, k: 202,
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 70.399
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 246.752
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0421
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 260.277
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0420
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 260.696

Attention duration (in seconds): 0.0526
Attention throughput (in TFLOP/s): 224.816
MLP duration (in seconds): 0.0841
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1367
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 254.489
b: 256, m: 2048, n: 204, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 128.145
b: 256, m: 2048, n: 2048, k: 204,
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 88.638
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 242.624
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 256.266
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0429
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 260.216

Attention duration (in seconds): 0.0528
Attention throughput (in TFLOP/s): 228.194
MLP duration (in seconds): 0.0865
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1393
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 256.417
b: 256, m: 2048, n: 206, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 123.321
b: 256, m: 2048, n: 2048, k: 206,
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 71.604
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 245.220
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 258.160
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 259.586

Attention duration (in seconds): 0.0547
Attention throughput (in TFLOP/s): 224.436
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1427
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 259.069
b: 256, m: 2048, n: 208, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 130.421
b: 256, m: 2048, n: 2048, k: 208,
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 100.558
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 247.600
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0447
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 260.018
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 260.605

Attention duration (in seconds): 0.0532
Attention throughput (in TFLOP/s): 235.031
MLP duration (in seconds): 0.0892
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1424
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 254.388
b: 256, m: 2048, n: 210, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 125.715
b: 256, m: 2048, n: 2048, k: 210,
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 72.975
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 243.795
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 256.166
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0455
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 260.093

Attention duration (in seconds): 0.0568
Attention throughput (in TFLOP/s): 224.264
MLP duration (in seconds): 0.0917
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1485
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0353
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 256.509
b: 256, m: 2048, n: 212, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 131.628
b: 256, m: 2048, n: 2048, k: 212,
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 91.278
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 245.786
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0468
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 257.709
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0462
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 261.111

Attention duration (in seconds): 0.0560
Attention throughput (in TFLOP/s): 231.727
MLP duration (in seconds): 0.0930
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1490
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 258.348
b: 256, m: 2048, n: 214, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 127.694
b: 256, m: 2048, n: 2048, k: 214,
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 73.946
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 248.941
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0472
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 260.489
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0472
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 260.660

Attention duration (in seconds): 0.0578
Attention throughput (in TFLOP/s): 228.400
MLP duration (in seconds): 0.0944
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1522
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 259.667
b: 256, m: 2048, n: 216, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 134.315
b: 256, m: 2048, n: 2048, k: 216,
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 102.140
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 251.037
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0478
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 261.763
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0480
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 261.125

Attention duration (in seconds): 0.0566
Attention throughput (in TFLOP/s): 237.494
MLP duration (in seconds): 0.0958
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1524
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 255.782
b: 256, m: 2048, n: 218, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 129.900
b: 256, m: 2048, n: 2048, k: 218,
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 75.155
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 246.841
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0495
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 257.581
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0489
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 260.819

Attention duration (in seconds): 0.0602
Attention throughput (in TFLOP/s): 227.618
MLP duration (in seconds): 0.0984
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1586
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0378
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 257.578
b: 256, m: 2048, n: 220, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 136.548
b: 256, m: 2048, n: 2048, k: 220,
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 94.867
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 248.738
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0500
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 260.095
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0500
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 259.978

Attention duration (in seconds): 0.0593
Attention throughput (in TFLOP/s): 234.916
MLP duration (in seconds): 0.0999
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1593
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0382
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 259.491
b: 256, m: 2048, n: 222, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 131.926
b: 256, m: 2048, n: 2048, k: 222,
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 76.988
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 249.747
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0507
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 261.053
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0506
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 261.449

Attention duration (in seconds): 0.0613
Attention throughput (in TFLOP/s): 231.423
MLP duration (in seconds): 0.1013
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1626
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0395
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 256.005
b: 256, m: 2048, n: 224, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 139.299
b: 256, m: 2048, n: 2048, k: 224,
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 108.160
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0136
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 247.894
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0523
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 257.482
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0515
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 261.518

Attention duration (in seconds): 0.0609
Attention throughput (in TFLOP/s): 236.795
MLP duration (in seconds): 0.1038
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1648
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0399
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 257.695
b: 256, m: 2048, n: 226, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 134.117
b: 256, m: 2048, n: 2048, k: 226,
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 77.862
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 249.919
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0529
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 259.059
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0525
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 261.115

Attention duration (in seconds): 0.0635
Attention throughput (in TFLOP/s): 231.308
MLP duration (in seconds): 0.1054
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1689
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0404
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 259.264
b: 256, m: 2048, n: 228, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 140.728
b: 256, m: 2048, n: 2048, k: 228,
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 99.114
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 252.005
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0534
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 261.434
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0535
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 260.969

Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 238.443
MLP duration (in seconds): 0.1068
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1695
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 255.333
b: 256, m: 2048, n: 230, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 136.266
b: 256, m: 2048, n: 2048, k: 230,
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 79.734
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0143
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 248.093
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 256.863
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0545
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 260.778

Attention duration (in seconds): 0.0658
Attention throughput (in TFLOP/s): 230.683
MLP duration (in seconds): 0.1097
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1756
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 257.119
b: 256, m: 2048, n: 232, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 143.167
b: 256, m: 2048, n: 2048, k: 232,
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 108.746
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 250.262
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 258.411
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0554
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 260.786

Attention duration (in seconds): 0.0646
Attention throughput (in TFLOP/s): 238.937
MLP duration (in seconds): 0.1113
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1760
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 259.155
b: 256, m: 2048, n: 234, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 138.300
b: 256, m: 2048, n: 2048, k: 234,
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 81.365
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 250.076
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 260.885
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 260.900

Attention duration (in seconds): 0.0670
Attention throughput (in TFLOP/s): 234.236
MLP duration (in seconds): 0.1127
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1797
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0432
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 259.708
b: 256, m: 2048, n: 236, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 144.527
b: 256, m: 2048, n: 2048, k: 236,
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 102.266
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 252.063
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0571
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 261.682
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 261.494

Attention duration (in seconds): 0.0665
Attention throughput (in TFLOP/s): 240.187
MLP duration (in seconds): 0.1143
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1808
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0443
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 257.149
b: 256, m: 2048, n: 238, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 140.719
b: 256, m: 2048, n: 2048, k: 238,
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 82.826
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 250.053
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0587
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 258.871
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0579
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 262.508

Attention duration (in seconds): 0.0694
Attention throughput (in TFLOP/s): 233.986
MLP duration (in seconds): 0.1167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1860
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0447
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 259.418
b: 256, m: 2048, n: 240, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 148.440
b: 256, m: 2048, n: 2048, k: 240,
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 112.707
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 251.723
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0593
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 260.553
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0589
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 262.516

Attention duration (in seconds): 0.0681
Attention throughput (in TFLOP/s): 242.174
MLP duration (in seconds): 0.1182
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1863
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0452
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 260.639
b: 256, m: 2048, n: 242, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 142.532
b: 256, m: 2048, n: 2048, k: 242,
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 84.703
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 252.951
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0602
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 261.324
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0603
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 260.793

Attention duration (in seconds): 0.0706
Attention throughput (in TFLOP/s): 237.543
MLP duration (in seconds): 0.1204
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1910
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0469
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 255.370
b: 256, m: 2048, n: 244, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 148.869
b: 256, m: 2048, n: 2048, k: 244,
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 105.509
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0161
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 248.729
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.0621
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 257.170
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0612
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 261.173

Attention duration (in seconds): 0.0715
Attention throughput (in TFLOP/s): 238.223
MLP duration (in seconds): 0.1233
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1948
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0474
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 257.297
b: 256, m: 2048, n: 246, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 144.992
b: 256, m: 2048, n: 2048, k: 246,
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 86.267
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 250.751
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0627
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 258.926
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0621
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 261.703

Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 235.984
MLP duration (in seconds): 0.1248
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1981
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 259.194
b: 256, m: 2048, n: 248, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 152.446
b: 256, m: 2048, n: 2048, k: 248,
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 114.730
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 253.008
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0640
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 257.892
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0631
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 261.457

Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 243.348
MLP duration (in seconds): 0.1272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1994
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 255.064
b: 256, m: 2048, n: 250, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 147.282
b: 256, m: 2048, n: 2048, k: 250,
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 88.230
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 249.012
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0654
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 256.647
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0643
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 260.777

Attention duration (in seconds): 0.0759
Attention throughput (in TFLOP/s): 235.171
MLP duration (in seconds): 0.1297
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0498
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 256.506
b: 256, m: 2048, n: 252, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 153.969
b: 256, m: 2048, n: 2048, k: 252,
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 109.346
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 251.093
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0661
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 258.070
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0654
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 260.834

Attention duration (in seconds): 0.0753
Attention throughput (in TFLOP/s): 240.823
MLP duration (in seconds): 0.1314
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 257.864
b: 256, m: 2048, n: 254, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 149.698
b: 256, m: 2048, n: 2048, k: 254,
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 90.143
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0171
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 253.142
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0666
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 259.866
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0662
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 261.510

Attention duration (in seconds): 0.0772
Attention throughput (in TFLOP/s): 238.558
MLP duration (in seconds): 0.1329
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0501
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 263.258
b: 256, m: 2048, n: 256, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 158.652
b: 256, m: 2048, n: 2048, k: 256,
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 122.612
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 256.006
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.0663
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 265.244
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0666
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 263.977

Attention duration (in seconds): 0.0752
Attention throughput (in TFLOP/s): 248.404
MLP duration (in seconds): 0.1330
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 259.642
b: 256, m: 2048, n: 258, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 126.023
b: 256, m: 2048, n: 2048, k: 258,
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 85.167
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 251.495
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.0685
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 260.820
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0674
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 264.934

Attention duration (in seconds): 0.0803
Attention throughput (in TFLOP/s): 236.383
MLP duration (in seconds): 0.1360
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0521
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 261.170
b: 256, m: 2048, n: 260, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 130.750
b: 256, m: 2048, n: 2048, k: 260,
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 103.536
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 254.211
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.1035
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 175.333
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0689
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 263.531

Attention duration (in seconds): 0.0796
Attention throughput (in TFLOP/s): 241.938
MLP duration (in seconds): 0.1724
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2520
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0535
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 258.245
b: 256, m: 2048, n: 262, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 127.617
b: 256, m: 2048, n: 2048, k: 262,
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 85.432
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 255.874
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.1101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 167.310
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0697
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 264.499

Attention duration (in seconds): 0.0825
Attention throughput (in TFLOP/s): 236.955
MLP duration (in seconds): 0.1798
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2623
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0541
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 259.360
b: 256, m: 2048, n: 264, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 130.539
b: 256, m: 2048, n: 2048, k: 264,
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 114.969
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 252.909
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.0717
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 260.911
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0708
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 264.364

Attention duration (in seconds): 0.0819
Attention throughput (in TFLOP/s): 242.372
MLP duration (in seconds): 0.1425
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0548
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 259.927
b: 256, m: 2048, n: 266, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 129.104
b: 256, m: 2048, n: 2048, k: 266,
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 85.682
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 255.017
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.0726
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 261.574
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 264.454

Attention duration (in seconds): 0.0845
Attention throughput (in TFLOP/s): 238.251
MLP duration (in seconds): 0.1444
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 261.076
b: 256, m: 2048, n: 268, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 134.219
b: 256, m: 2048, n: 2048, k: 268,
Elapsed time for attention_prob_times_values (256x2048x2048x268): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 105.104
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 256.066
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.0734
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 262.615
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0729
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 264.587

Attention duration (in seconds): 0.0840
Attention throughput (in TFLOP/s): 243.304
MLP duration (in seconds): 0.1463
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0558
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 262.824
b: 256, m: 2048, n: 270, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 130.623
b: 256, m: 2048, n: 2048, k: 270,
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 86.102
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 257.660
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.0989
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 197.961
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0739
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 264.908

Attention duration (in seconds): 0.0860
Attention throughput (in TFLOP/s): 241.023
MLP duration (in seconds): 0.1727
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2587
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0572
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 260.308
b: 256, m: 2048, n: 272, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 134.969
b: 256, m: 2048, n: 2048, k: 272,
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 118.780
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0196
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 253.264
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.1094
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 181.455
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0754
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 263.484

Attention duration (in seconds): 0.0861
Attention throughput (in TFLOP/s): 244.315
MLP duration (in seconds): 0.1848
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2709
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 260.942
b: 256, m: 2048, n: 274, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 132.445
b: 256, m: 2048, n: 2048, k: 274,
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 86.684
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 256.146
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.0767
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 262.594
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 264.858

Attention duration (in seconds): 0.0888
Attention throughput (in TFLOP/s): 240.136
MLP duration (in seconds): 0.1528
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2417
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0585
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 262.110
b: 256, m: 2048, n: 276, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 137.835
b: 256, m: 2048, n: 2048, k: 276,
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 107.210
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 257.557
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.0777
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 263.021
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0772
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 264.859

Attention duration (in seconds): 0.0882
Attention throughput (in TFLOP/s): 245.315
MLP duration (in seconds): 0.1549
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2431
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0601
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 258.969
b: 256, m: 2048, n: 278, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 134.176
b: 256, m: 2048, n: 2048, k: 278,
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 87.302
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 254.814
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 260.434
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0789
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 263.028

Attention duration (in seconds): 0.0917
Attention throughput (in TFLOP/s): 239.195
MLP duration (in seconds): 0.1585
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2503
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0610
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 258.896
b: 256, m: 2048, n: 280, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 138.572
b: 256, m: 2048, n: 2048, k: 280,
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 120.067
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 255.353
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.0808
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 260.513
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0801
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 262.819

Attention duration (in seconds): 0.0909
Attention throughput (in TFLOP/s): 244.704
MLP duration (in seconds): 0.1609
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2518
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0608
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 263.130
b: 256, m: 2048, n: 282, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 135.889
b: 256, m: 2048, n: 2048, k: 282,
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 88.592
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 259.477
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.0808
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 264.197
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0806
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 264.906

Attention duration (in seconds): 0.0927
Attention throughput (in TFLOP/s): 243.333
MLP duration (in seconds): 0.1614
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2541
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 260.057
b: 256, m: 2048, n: 284, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 142.676
b: 256, m: 2048, n: 2048, k: 284,
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 110.213
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 255.336
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.1321
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 163.946
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0814
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 265.921

Attention duration (in seconds): 0.0934
Attention throughput (in TFLOP/s): 244.743
MLP duration (in seconds): 0.2135
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0633
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 260.281
b: 256, m: 2048, n: 286, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 137.236
b: 256, m: 2048, n: 2048, k: 286,
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 88.943
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 255.999
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.0837
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 262.476
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0828
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 265.252

Attention duration (in seconds): 0.0961
Attention throughput (in TFLOP/s): 241.282
MLP duration (in seconds): 0.1664
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0635
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 263.027
b: 256, m: 2048, n: 288, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 142.110
b: 256, m: 2048, n: 2048, k: 288,
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 124.368
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 258.108
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.1332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 167.143
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0835
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 266.645

Attention duration (in seconds): 0.0944
Attention throughput (in TFLOP/s): 249.021
MLP duration (in seconds): 0.2167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.0867
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 195.337
b: 256, m: 2048, n: 290, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 138.565
b: 256, m: 2048, n: 2048, k: 290,
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 89.679
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 258.361
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.1316
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 171.522
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.0851
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 265.214

Attention duration (in seconds): 0.1200
Attention throughput (in TFLOP/s): 198.570
MLP duration (in seconds): 0.2167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3367
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.0658
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 260.915
b: 256, m: 2048, n: 292, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 145.277
b: 256, m: 2048, n: 2048, k: 292,
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 112.503
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 256.879
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.0874
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 261.907
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0866
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 264.263

Attention duration (in seconds): 0.0980
Attention throughput (in TFLOP/s): 246.457
MLP duration (in seconds): 0.1740
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2720
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.0665
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 261.802
b: 256, m: 2048, n: 294, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 141.565
b: 256, m: 2048, n: 2048, k: 294,
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 91.453
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 258.221
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 263.437
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 263.439

Attention duration (in seconds): 0.1003
Attention throughput (in TFLOP/s): 243.928
MLP duration (in seconds): 0.1762
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2764
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.0672
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 262.506
b: 256, m: 2048, n: 296, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 146.282
b: 256, m: 2048, n: 2048, k: 296,
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 125.820
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 257.397
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.0897
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 262.239
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.0894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 263.125

Attention duration (in seconds): 0.0994
Attention throughput (in TFLOP/s): 249.309
MLP duration (in seconds): 0.1791
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2785
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.0691
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 258.741
b: 256, m: 2048, n: 298, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 142.956
b: 256, m: 2048, n: 2048, k: 298,
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 92.312
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 253.647
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.0921
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 258.937
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0908
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 262.599

Attention duration (in seconds): 0.1040
Attention throughput (in TFLOP/s): 241.513
MLP duration (in seconds): 0.1828
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2868
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0699
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 259.216
b: 256, m: 2048, n: 300, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 149.315
b: 256, m: 2048, n: 2048, k: 300,
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 114.426
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 255.203
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.1407
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 171.757
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0919
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 262.760

Attention duration (in seconds): 0.1035
Attention throughput (in TFLOP/s): 245.842
MLP duration (in seconds): 0.2326
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3361
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.0706
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 259.995
b: 256, m: 2048, n: 302, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 144.875
b: 256, m: 2048, n: 2048, k: 302,
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 93.502
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 256.716
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.0936
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 261.497
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.0932
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 262.713

Attention duration (in seconds): 0.1059
Attention throughput (in TFLOP/s): 243.482
MLP duration (in seconds): 0.1868
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2927
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.0730
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 254.736
b: 256, m: 2048, n: 304, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 150.500
b: 256, m: 2048, n: 2048, k: 304,
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 129.951
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 253.746
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.0993
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 249.761
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.0946
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 262.152

Attention duration (in seconds): 0.1068
Attention throughput (in TFLOP/s): 244.410
MLP duration (in seconds): 0.1940
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.0729
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 258.461
b: 256, m: 2048, n: 306, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 146.218
b: 256, m: 2048, n: 2048, k: 306,
Elapsed time for attention_prob_times_values (256x2048x2048x306): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 94.832
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 255.120
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.0966
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 260.294
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.0956
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 262.939

Attention duration (in seconds): 0.1090
Attention throughput (in TFLOP/s): 242.674
MLP duration (in seconds): 0.1922
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.0731
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 261.255
b: 256, m: 2048, n: 308, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 152.430
b: 256, m: 2048, n: 2048, k: 308,
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 115.994
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0248
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 256.487
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.0979
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 260.121
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.0970
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 262.509

Attention duration (in seconds): 0.1080
Attention throughput (in TFLOP/s): 248.113
MLP duration (in seconds): 0.1949
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.0741
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 261.042
b: 256, m: 2048, n: 310, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 147.410
b: 256, m: 2048, n: 2048, k: 310,
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 94.141
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 257.870
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.0987
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 261.380
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.0980
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 263.166

Attention duration (in seconds): 0.1107
Attention throughput (in TFLOP/s): 245.030
MLP duration (in seconds): 0.1967
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.0759
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 258.150
b: 256, m: 2048, n: 312, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 154.155
b: 256, m: 2048, n: 2048, k: 312,
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 131.093
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 255.125
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.1584
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 164.969
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.0996
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 262.415

Attention duration (in seconds): 0.1110
Attention throughput (in TFLOP/s): 247.528
MLP duration (in seconds): 0.2580
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3690
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.0939
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 211.345
b: 256, m: 2048, n: 314, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 149.635
b: 256, m: 2048, n: 2048, k: 314,
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 96.501
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0258
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 256.552
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1537
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 172.229
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.1011
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 261.828

Attention duration (in seconds): 0.1312
Attention throughput (in TFLOP/s): 211.996
MLP duration (in seconds): 0.2548
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3860
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.0772
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 260.468
b: 256, m: 2048, n: 316, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 156.320
b: 256, m: 2048, n: 2048, k: 316,
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 119.773
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 257.907
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1036
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 258.734
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.1013
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 264.671

Attention duration (in seconds): 0.1132
Attention throughput (in TFLOP/s): 248.841
MLP duration (in seconds): 0.2049
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.0782
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 260.448
b: 256, m: 2048, n: 318, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 151.134
b: 256, m: 2048, n: 2048, k: 318,
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 97.787
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 257.669
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 215.549
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1019
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 266.514

Attention duration (in seconds): 0.1160
Attention throughput (in TFLOP/s): 245.768
MLP duration (in seconds): 0.2278
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3438
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.0785
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 262.480
b: 256, m: 2048, n: 320, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 158.142
b: 256, m: 2048, n: 2048, k: 320,
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 137.843
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 259.226
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1573
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 174.733
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1032
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 266.299

Attention duration (in seconds): 0.1144
Attention throughput (in TFLOP/s): 252.330
MLP duration (in seconds): 0.2605
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3749
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.0792
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 263.705
b: 256, m: 2048, n: 322, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 152.930
b: 256, m: 2048, n: 2048, k: 322,
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 92.211
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 261.121
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1049
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 265.220
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 265.929

Attention duration (in seconds): 0.1178
Attention throughput (in TFLOP/s): 247.955
MLP duration (in seconds): 0.2096
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.0803
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 263.332
b: 256, m: 2048, n: 324, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 159.725
b: 256, m: 2048, n: 2048, k: 324,
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 113.430
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 259.417
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1156
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 243.837
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1068
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 263.934

Attention duration (in seconds): 0.1179
Attention throughput (in TFLOP/s): 250.804
MLP duration (in seconds): 0.2223
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3402
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.0826
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 259.028
b: 256, m: 2048, n: 326, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 154.423
b: 256, m: 2048, n: 2048, k: 326,
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 93.052
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 254.900
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1349
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 211.514
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1086
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 262.789

Attention duration (in seconds): 0.1226
Attention throughput (in TFLOP/s): 244.037
MLP duration (in seconds): 0.2434
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3661
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.0832
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 260.341
b: 256, m: 2048, n: 328, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 160.939
b: 256, m: 2048, n: 2048, k: 328,
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 129.902
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 257.429
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1556
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 185.562
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1100
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 262.468

Attention duration (in seconds): 0.1210
Attention throughput (in TFLOP/s): 250.229
MLP duration (in seconds): 0.2657
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3867
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.0845
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 259.408
b: 256, m: 2048, n: 330, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 155.386
b: 256, m: 2048, n: 2048, k: 330,
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 92.904
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0282
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 258.842
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1782
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 164.022
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1108
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 263.949

Attention duration (in seconds): 0.1249
Attention throughput (in TFLOP/s): 245.318
MLP duration (in seconds): 0.2890
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.1173
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 189.253
b: 256, m: 2048, n: 332, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 163.106
b: 256, m: 2048, n: 2048, k: 332,
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 115.223
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 254.870
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1575
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 187.896
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1124
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 263.122

Attention duration (in seconds): 0.1568
Attention throughput (in TFLOP/s): 197.746
MLP duration (in seconds): 0.2699
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.1345
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 166.975
b: 256, m: 2048, n: 334, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 157.938
b: 256, m: 2048, n: 2048, k: 334,
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 94.908
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0288
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 260.328
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1780
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 168.260
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1125
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 266.109

Attention duration (in seconds): 0.1754
Attention throughput (in TFLOP/s): 178.944
MLP duration (in seconds): 0.2905
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4659
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.0861
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 263.844
b: 256, m: 2048, n: 336, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 165.713
b: 256, m: 2048, n: 2048, k: 336,
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 133.605
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 261.521
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1763
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 171.916
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1138
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 266.344

Attention duration (in seconds): 0.1249
Attention throughput (in TFLOP/s): 254.251
MLP duration (in seconds): 0.2901
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.1271
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 180.920
b: 256, m: 2048, n: 338, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 159.528
b: 256, m: 2048, n: 2048, k: 338,
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 96.054
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 258.029
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1894
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 161.932
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1155
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 265.584

Attention duration (in seconds): 0.1689
Attention throughput (in TFLOP/s): 190.109
MLP duration (in seconds): 0.3049
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4738
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.1161
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 200.530
b: 256, m: 2048, n: 340, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 168.229
b: 256, m: 2048, n: 2048, k: 340,
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 118.658
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 260.396
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1882
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 164.855
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1170
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 265.110

Attention duration (in seconds): 0.1563
Attention throughput (in TFLOP/s): 207.819
MLP duration (in seconds): 0.3053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4616
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.1327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 177.400
b: 256, m: 2048, n: 342, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 160.826
b: 256, m: 2048, n: 2048, k: 342,
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 96.846
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0300
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 261.266
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.1911
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 164.281
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.1178
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 266.442

Attention duration (in seconds): 0.1749
Attention throughput (in TFLOP/s): 187.878
MLP duration (in seconds): 0.3090
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4839
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.1222
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 194.980
b: 256, m: 2048, n: 344, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 168.560
b: 256, m: 2048, n: 2048, k: 344,
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 133.509
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0306
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 259.581
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.1451
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 218.954
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.1200
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 264.716

Attention duration (in seconds): 0.1627
Attention throughput (in TFLOP/s): 204.325
MLP duration (in seconds): 0.2651
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.1154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 208.899
b: 256, m: 2048, n: 346, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 163.253
b: 256, m: 2048, n: 2048, k: 346,
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 97.463
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 259.636
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.1900
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 169.179
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.1207
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 266.182

Attention duration (in seconds): 0.1585
Attention throughput (in TFLOP/s): 212.134
MLP duration (in seconds): 0.3107
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4692
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.1281
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 190.295
b: 256, m: 2048, n: 348, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 172.358
b: 256, m: 2048, n: 2048, k: 348,
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 121.272
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 260.423
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.1995
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 162.965
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.1225
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 265.386

Attention duration (in seconds): 0.1698
Attention throughput (in TFLOP/s): 200.219
MLP duration (in seconds): 0.3220
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4918
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.1440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 171.269
b: 256, m: 2048, n: 350, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 164.519
b: 256, m: 2048, n: 2048, k: 350,
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 97.889
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0316
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 260.200
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.2067
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 159.069
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 266.634

Attention duration (in seconds): 0.1878
Attention throughput (in TFLOP/s): 183.063
MLP duration (in seconds): 0.3301
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.0949
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 262.882
b: 256, m: 2048, n: 352, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 173.338
b: 256, m: 2048, n: 2048, k: 352,
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 139.690
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0320
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 259.567
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.1977
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 168.222
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1252
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 265.695

Attention duration (in seconds): 0.1367
Attention throughput (in TFLOP/s): 254.372
MLP duration (in seconds): 0.3229
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4596
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.1037
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 243.296
b: 256, m: 2048, n: 354, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 166.521
b: 256, m: 2048, n: 2048, k: 354,
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 99.533
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 258.619
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.2036
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 165.197
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1281
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 262.596

Attention duration (in seconds): 0.1484
Attention throughput (in TFLOP/s): 236.893
MLP duration (in seconds): 0.3317
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4802
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.1119
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 227.973
b: 256, m: 2048, n: 356, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 175.079
b: 256, m: 2048, n: 2048, k: 356,
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 123.610
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 260.015
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 208.224
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1292
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 263.254

Attention duration (in seconds): 0.1552
Attention throughput (in TFLOP/s): 229.079
MLP duration (in seconds): 0.2926
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4478
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.1467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 175.930
b: 256, m: 2048, n: 358, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 167.103
b: 256, m: 2048, n: 2048, k: 358,
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 100.844
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 258.156
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.2205
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 156.053
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.1309
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 262.901

Attention duration (in seconds): 0.1922
Attention throughput (in TFLOP/s): 186.993
MLP duration (in seconds): 0.3513
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5435
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.1007
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 259.113
b: 256, m: 2048, n: 360, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 175.342
b: 256, m: 2048, n: 2048, k: 360,
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 139.024
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0337
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 258.064
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.2209
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 157.495
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1328
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 261.963

Attention duration (in seconds): 0.1444
Attention throughput (in TFLOP/s): 251.684
MLP duration (in seconds): 0.3537
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4981
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.1611
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 163.791
b: 256, m: 2048, n: 362, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 168.255
b: 256, m: 2048, n: 2048, k: 362,
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 100.913
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 258.247
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.2231
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 157.697
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1335
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 263.577

Attention duration (in seconds): 0.2075
Attention throughput (in TFLOP/s): 177.061
MLP duration (in seconds): 0.3565
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5640
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1557
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 171.295
b: 256, m: 2048, n: 364, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 178.351
b: 256, m: 2048, n: 2048, k: 364,
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 125.425
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 264.255
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.2211
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 160.865
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.1340
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 265.327

Attention duration (in seconds): 0.2000
Attention throughput (in TFLOP/s): 185.661
MLP duration (in seconds): 0.3551
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1404
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 192.090
b: 256, m: 2048, n: 366, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 169.825
b: 256, m: 2048, n: 2048, k: 366,
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 101.379
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0345
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 260.833
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.2324
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 154.710
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1361
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 264.269

Attention duration (in seconds): 0.1872
Attention throughput (in TFLOP/s): 200.437
MLP duration (in seconds): 0.3685
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5557
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1219
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 223.611
b: 256, m: 2048, n: 368, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 180.605
b: 256, m: 2048, n: 2048, k: 368,
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 141.906
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 260.120
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.1992
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 182.511
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.1385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 262.400

Attention duration (in seconds): 0.1668
Attention throughput (in TFLOP/s): 227.402
MLP duration (in seconds): 0.3377
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1639
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 168.211
b: 256, m: 2048, n: 370, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 171.929
b: 256, m: 2048, n: 2048, k: 370,
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 104.640
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0352
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 261.320
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.2116
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 173.683
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.1397
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 262.980

Attention duration (in seconds): 0.2112
Attention throughput (in TFLOP/s): 181.504
MLP duration (in seconds): 0.3513
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1620
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 172.019
b: 256, m: 2048, n: 372, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 182.390
b: 256, m: 2048, n: 2048, k: 372,
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 127.548
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0358
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 259.092
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.2347
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 158.283
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1411
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 263.282

Attention duration (in seconds): 0.2084
Attention throughput (in TFLOP/s): 185.873
MLP duration (in seconds): 0.3758
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5842
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 262.293
b: 256, m: 2048, n: 374, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 174.273
b: 256, m: 2048, n: 2048, k: 374,
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 105.506
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 259.208
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.2441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 153.789
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.1426
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 263.309

Attention duration (in seconds): 0.1558
Attention throughput (in TFLOP/s): 251.311
MLP duration (in seconds): 0.3867
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1080
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 263.460
b: 256, m: 2048, n: 376, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 183.502
b: 256, m: 2048, n: 2048, k: 376,
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 144.085
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 261.073
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.2444
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 155.262
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.1438
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 264.000

Attention duration (in seconds): 0.1544
Attention throughput (in TFLOP/s): 256.285
MLP duration (in seconds): 0.3882
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5426
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1749
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 164.461
b: 256, m: 2048, n: 378, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 175.646
b: 256, m: 2048, n: 2048, k: 378,
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 105.547
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 261.911
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.2427
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 158.018
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 264.307

Attention duration (in seconds): 0.2238
Attention throughput (in TFLOP/s): 178.606
MLP duration (in seconds): 0.3878
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 201.839
b: 256, m: 2048, n: 380, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 183.998
b: 256, m: 2048, n: 2048, k: 380,
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 130.236
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 258.897
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.2432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 159.373
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1480
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 261.954

Attention duration (in seconds): 0.1922
Attention throughput (in TFLOP/s): 210.206
MLP duration (in seconds): 0.3912
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5834
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1793
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 163.880
b: 256, m: 2048, n: 382, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 176.033
b: 256, m: 2048, n: 2048, k: 382,
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 108.203
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0378
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 259.029
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.2547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 153.818
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.1496
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 261.919

Attention duration (in seconds): 0.2293
Attention throughput (in TFLOP/s): 177.973
MLP duration (in seconds): 0.4042
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.1859
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 159.667
b: 256, m: 2048, n: 384, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x384x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x384x2048): 188.777
b: 256, m: 2048, n: 2048, k: 384,
Elapsed time for attention_prob_times_values (256x2048x2048x384): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x384): 150.748
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0379
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 260.826
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.2569
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 154.104
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.1504
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 263.198

Attention duration (in seconds): 0.2337
Attention throughput (in TFLOP/s): 176.424
MLP duration (in seconds): 0.4072
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6410
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1858
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 161.489
b: 256, m: 2048, n: 386, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x386x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x386x2048): 150.191
b: 256, m: 2048, n: 2048, k: 386,
Elapsed time for attention_prob_times_values (256x2048x2048x386): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x386): 103.562
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0387
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 258.151
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.2615
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 152.937
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.1534
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 260.762

Attention duration (in seconds): 0.2380
Attention throughput (in TFLOP/s): 175.009
MLP duration (in seconds): 0.4149
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6529
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1888
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 160.573
b: 256, m: 2048, n: 388, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x388x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x388x2048): 155.691
b: 256, m: 2048, n: 2048, k: 388,
Elapsed time for attention_prob_times_values (256x2048x2048x388): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x388): 125.455
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0390
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 258.942
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.2636
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 153.334
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.1551
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 260.561

Attention duration (in seconds): 0.2398
Attention throughput (in TFLOP/s): 175.499
MLP duration (in seconds): 0.4186
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6584
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1883
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 162.658
b: 256, m: 2048, n: 390, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x390x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x390x2048): 151.767
b: 256, m: 2048, n: 2048, k: 390,
Elapsed time for attention_prob_times_values (256x2048x2048x390): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x390): 103.118
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 262.297
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.2676
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 152.561
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.1552
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 263.053

Attention duration (in seconds): 0.2408
Attention throughput (in TFLOP/s): 176.502
MLP duration (in seconds): 0.4228
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6636
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1907
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 162.268
b: 256, m: 2048, n: 392, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x392x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x392x2048): 154.956
b: 256, m: 2048, n: 2048, k: 392,
Elapsed time for attention_prob_times_values (256x2048x2048x392): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x392): 140.553
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 257.431
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.2656
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 155.277
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.1557
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 264.970

Attention duration (in seconds): 0.2421
Attention throughput (in TFLOP/s): 177.310
MLP duration (in seconds): 0.4213
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.1925
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 162.346
b: 256, m: 2048, n: 394, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x394x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x394x2048): 152.226
b: 256, m: 2048, n: 2048, k: 394,
Elapsed time for attention_prob_times_values (256x2048x2048x394): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x394): 103.056
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 259.500
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.2736
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 152.329
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.2432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 171.354

Attention duration (in seconds): 0.2464
Attention throughput (in TFLOP/s): 175.970
MLP duration (in seconds): 0.5167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7632
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.1922
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 164.245
b: 256, m: 2048, n: 396, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x396x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x396x2048): 158.958
b: 256, m: 2048, n: 2048, k: 396,
Elapsed time for attention_prob_times_values (256x2048x2048x396): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x396): 125.615
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0402
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 261.869
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.2810
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 149.826
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.1602
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 262.756

Attention duration (in seconds): 0.2445
Attention throughput (in TFLOP/s): 179.104
MLP duration (in seconds): 0.4412
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6857
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.1986
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 160.576
b: 256, m: 2048, n: 398, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x398x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x398x2048): 154.155
b: 256, m: 2048, n: 2048, k: 398,
Elapsed time for attention_prob_times_values (256x2048x2048x398): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x398): 104.720
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0408
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 260.810
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.2843
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 149.581
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.1629
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 261.018

Attention duration (in seconds): 0.2531
Attention throughput (in TFLOP/s): 174.777
MLP duration (in seconds): 0.4472
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7002
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.1247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 258.312
b: 256, m: 2048, n: 400, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x400x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x400x2048): 158.857
b: 256, m: 2048, n: 2048, k: 400,
Elapsed time for attention_prob_times_values (256x2048x2048x400): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x400): 143.221
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0414
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 259.048
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.2836
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 151.469
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.2369
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 181.279

Attention duration (in seconds): 0.1776
Attention throughput (in TFLOP/s): 251.568
MLP duration (in seconds): 0.5205
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6980
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.2048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 158.853
b: 256, m: 2048, n: 402, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x402x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x402x2048): 155.463
b: 256, m: 2048, n: 2048, k: 402,
Elapsed time for attention_prob_times_values (256x2048x2048x402): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x402): 105.366
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 260.244
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.2874
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 150.916
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.2647
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 163.887

Attention duration (in seconds): 0.2602
Attention throughput (in TFLOP/s): 173.333
MLP duration (in seconds): 0.5521
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.2105
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 156.072
b: 256, m: 2048, n: 404, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 161.909
b: 256, m: 2048, n: 2048, k: 404,
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 129.218
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 260.216
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.2844
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 154.071
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.2678
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 163.604

Attention duration (in seconds): 0.2647
Attention throughput (in TFLOP/s): 172.069
MLP duration (in seconds): 0.5522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.2127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 156.056
b: 256, m: 2048, n: 406, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 156.896
b: 256, m: 2048, n: 2048, k: 406,
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 106.192
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 260.331
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.2882
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 153.536
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.1688
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 262.082

Attention duration (in seconds): 0.2689
Attention throughput (in TFLOP/s): 171.028
MLP duration (in seconds): 0.4570
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1281
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 261.537
b: 256, m: 2048, n: 408, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 161.543
b: 256, m: 2048, n: 2048, k: 408,
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 145.629
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 262.007
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.2539
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 175.970
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1705
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 262.060

Attention duration (in seconds): 0.1822
Attention throughput (in TFLOP/s): 254.843
MLP duration (in seconds): 0.4244
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1293
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 261.694
b: 256, m: 2048, n: 410, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 158.475
b: 256, m: 2048, n: 2048, k: 410,
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 107.157
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0429
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 263.096
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.2938
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 153.595
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1705
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 264.611

Attention duration (in seconds): 0.1860
Attention throughput (in TFLOP/s): 252.106
MLP duration (in seconds): 0.4643
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6503
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.2131
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 160.334
b: 256, m: 2048, n: 412, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 164.990
b: 256, m: 2048, n: 2048, k: 412,
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 131.693
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 261.397
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.2604
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 175.001
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.2660
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 171.291

Attention duration (in seconds): 0.2688
Attention throughput (in TFLOP/s): 176.095
MLP duration (in seconds): 0.5264
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7952
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1947
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 177.267
b: 256, m: 2048, n: 414, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 159.283
b: 256, m: 2048, n: 2048, k: 414,
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 107.355
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0445
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 258.235
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.2970
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 154.904
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1746
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 263.474

Attention duration (in seconds): 0.2531
Attention throughput (in TFLOP/s): 188.834
MLP duration (in seconds): 0.4716
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.2073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 168.089
b: 256, m: 2048, n: 416, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 165.438
b: 256, m: 2048, n: 2048, k: 416,
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 151.149
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 262.627
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.3040
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 152.802
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1855
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 250.482

Attention duration (in seconds): 0.2628
Attention throughput (in TFLOP/s): 183.561
MLP duration (in seconds): 0.4895
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7523
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.2249
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 156.390
b: 256, m: 2048, n: 418, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 160.822
b: 256, m: 2048, n: 2048, k: 418,
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 108.652
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0445
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 263.300
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.2855
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 164.258
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.2708
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 173.173

Attention duration (in seconds): 0.2833
Attention throughput (in TFLOP/s): 171.890
MLP duration (in seconds): 0.5564
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8397
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.2107
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 168.573
b: 256, m: 2048, n: 420, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 168.539
b: 256, m: 2048, n: 2048, k: 420,
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 133.737
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0455
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 260.143
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.3133
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 151.132
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.2914
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 162.488

Attention duration (in seconds): 0.2683
Attention throughput (in TFLOP/s): 183.229
MLP duration (in seconds): 0.6047
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8730
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.2279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 157.342
b: 256, m: 2048, n: 422, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 162.674
b: 256, m: 2048, n: 2048, k: 422,
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 109.345
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 261.015
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.3229
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 148.042
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.1829
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 261.392

Attention duration (in seconds): 0.2875
Attention throughput (in TFLOP/s): 172.572
MLP duration (in seconds): 0.5058
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7933
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.2316
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 156.252
b: 256, m: 2048, n: 424, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 167.981
b: 256, m: 2048, n: 2048, k: 424,
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 150.540
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 261.442
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.3194
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 151.079
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.1879
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 256.826

Attention duration (in seconds): 0.2893
Attention throughput (in TFLOP/s): 173.134
MLP duration (in seconds): 0.5073
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7966
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.2353
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 155.305
b: 256, m: 2048, n: 426, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 162.995
b: 256, m: 2048, n: 2048, k: 426,
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 109.558
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0470
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 259.092
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.3252
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 149.796
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.3042
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 160.135

Attention duration (in seconds): 0.2962
Attention throughput (in TFLOP/s): 170.630
MLP duration (in seconds): 0.6294
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.2367
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 155.825
b: 256, m: 2048, n: 428, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 170.806
b: 256, m: 2048, n: 2048, k: 428,
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 135.190
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0472
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 260.514
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.3305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 148.770
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.2960
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 166.132

Attention duration (in seconds): 0.2960
Attention throughput (in TFLOP/s): 172.311
MLP duration (in seconds): 0.6265
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.2406
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 154.738
b: 256, m: 2048, n: 430, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 165.666
b: 256, m: 2048, n: 2048, k: 430,
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 111.437
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 263.439
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.3289
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 150.895
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.3106
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 159.779

Attention duration (in seconds): 0.3015
Attention throughput (in TFLOP/s): 170.730
MLP duration (in seconds): 0.6396
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9411
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.2366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 158.790
b: 256, m: 2048, n: 432, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 171.505
b: 256, m: 2048, n: 2048, k: 432,
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 153.272
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 263.101
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.3323
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 150.749
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.3147
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 159.200

Attention duration (in seconds): 0.2957
Attention throughput (in TFLOP/s): 175.703
MLP duration (in seconds): 0.6470
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9427
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.2416
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 156.932
b: 256, m: 2048, n: 434, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 166.807
b: 256, m: 2048, n: 2048, k: 434,
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 112.558
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0483
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 261.865
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.3366
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 150.212
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.3126
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 161.745

Attention duration (in seconds): 0.3038
Attention throughput (in TFLOP/s): 172.578
MLP duration (in seconds): 0.6492
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9530
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.2511
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 152.444
b: 256, m: 2048, n: 436, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 174.160
b: 256, m: 2048, n: 2048, k: 436,
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 136.501
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0485
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 263.056
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.3339
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 152.826
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.3211
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 158.906

Attention duration (in seconds): 0.3118
Attention throughput (in TFLOP/s): 169.672
MLP duration (in seconds): 0.6550
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9668
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.2474
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 156.102
b: 256, m: 2048, n: 438, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 168.323
b: 256, m: 2048, n: 2048, k: 438,
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 113.081
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0490
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 262.609
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.3438
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 149.780
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.3233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 159.268

Attention duration (in seconds): 0.3104
Attention throughput (in TFLOP/s): 171.993
MLP duration (in seconds): 0.6672
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9775
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.2546
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 153.090
b: 256, m: 2048, n: 440, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 174.113
b: 256, m: 2048, n: 2048, k: 440,
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 154.957
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0495
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 262.614
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.3502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 148.415
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.1970
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 263.749

Attention duration (in seconds): 0.3156
Attention throughput (in TFLOP/s): 170.657
MLP duration (in seconds): 0.5472
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8628
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.2492
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 157.834
b: 256, m: 2048, n: 442, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 169.621
b: 256, m: 2048, n: 2048, k: 442,
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 114.593
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 260.118
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.3527
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 148.699
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.3261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 160.833

Attention duration (in seconds): 0.3135
Attention throughput (in TFLOP/s): 173.347
MLP duration (in seconds): 0.6787
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9922
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.2571
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 154.369
b: 256, m: 2048, n: 444, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 177.230
b: 256, m: 2048, n: 2048, k: 444,
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 142.366
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0508
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 260.571
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.3588
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 147.489
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.3373
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 156.898

Attention duration (in seconds): 0.3200
Attention throughput (in TFLOP/s): 171.355
MLP duration (in seconds): 0.6961
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.2139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 187.216
b: 256, m: 2048, n: 446, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 171.383
b: 256, m: 2048, n: 2048, k: 446,
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 116.172
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0510
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 261.519
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.3752
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 142.329
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.3383
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 157.820

Attention duration (in seconds): 0.2788
Attention throughput (in TFLOP/s): 198.402
MLP duration (in seconds): 0.7135
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9923
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.2584
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 156.401
b: 256, m: 2048, n: 448, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 178.345
b: 256, m: 2048, n: 2048, k: 448,
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 161.609
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 262.689
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.3586
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 150.258
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2045
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 263.436

Attention duration (in seconds): 0.3210
Attention throughput (in TFLOP/s): 173.845
MLP duration (in seconds): 0.5631
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8840
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.2660
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 153.249
b: 256, m: 2048, n: 450, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 173.393
b: 256, m: 2048, n: 2048, k: 450,
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 111.555
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 261.896
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.3643
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 149.215
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.3418
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 159.057

Attention duration (in seconds): 0.3322
Attention throughput (in TFLOP/s): 169.472
MLP duration (in seconds): 0.7060
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0382
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.2515
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 163.533
b: 256, m: 2048, n: 452, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 179.842
b: 256, m: 2048, n: 2048, k: 452,
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 134.556
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 261.608
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.3745
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 146.435
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.3464
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 158.300

Attention duration (in seconds): 0.3165
Attention throughput (in TFLOP/s): 179.389
MLP duration (in seconds): 0.7210
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0375
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.1797
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 230.984
b: 256, m: 2048, n: 454, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 174.576
b: 256, m: 2048, n: 2048, k: 454,
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 111.680
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0527
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 262.452
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.3763
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 147.027
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.3406
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 162.461

Attention duration (in seconds): 0.2467
Attention throughput (in TFLOP/s): 232.208
MLP duration (in seconds): 0.7169
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9636
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.2659
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 157.423
b: 256, m: 2048, n: 456, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 179.326
b: 256, m: 2048, n: 2048, k: 456,
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 151.303
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0532
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 262.286
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.3688
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 151.341
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.3425
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 162.985

Attention duration (in seconds): 0.3311
Attention throughput (in TFLOP/s): 174.517
MLP duration (in seconds): 0.7113
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0424
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.2752
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 153.479
b: 256, m: 2048, n: 458, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 175.321
b: 256, m: 2048, n: 2048, k: 458,
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 111.979
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0536
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 262.695
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.3979
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 141.528
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2137
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 263.500

Attention duration (in seconds): 0.3431
Attention throughput (in TFLOP/s): 169.830
MLP duration (in seconds): 0.6116
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9547
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.2766
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 153.993
b: 256, m: 2048, n: 460, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 183.122
b: 256, m: 2048, n: 2048, k: 460,
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 136.770
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0547
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 259.564
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.3827
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 148.425
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2186
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 259.853

Attention duration (in seconds): 0.3440
Attention throughput (in TFLOP/s): 170.879
MLP duration (in seconds): 0.6013
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9452
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.2644
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 162.550
b: 256, m: 2048, n: 462, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 176.694
b: 256, m: 2048, n: 2048, k: 462,
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 112.951
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0545
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 262.702
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.3859
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 148.466
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2187
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 262.027

Attention duration (in seconds): 0.3333
Attention throughput (in TFLOP/s): 177.866
MLP duration (in seconds): 0.6046
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9379
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.2790
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 155.375
b: 256, m: 2048, n: 464, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 183.224
b: 256, m: 2048, n: 2048, k: 464,
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 153.828
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 263.176
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.3862
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 149.630
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 261.298

Attention duration (in seconds): 0.3458
Attention throughput (in TFLOP/s): 172.899
MLP duration (in seconds): 0.6074
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9532
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.2855
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 153.117
b: 256, m: 2048, n: 466, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 178.156
b: 256, m: 2048, n: 2048, k: 466,
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 113.851
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 260.739
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.4101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 142.132
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.3745
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 155.669

Attention duration (in seconds): 0.3558
Attention throughput (in TFLOP/s): 169.447
MLP duration (in seconds): 0.7846
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1404
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.2903
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 151.915
b: 256, m: 2048, n: 468, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 185.904
b: 256, m: 2048, n: 2048, k: 468,
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 139.452
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0565
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 259.980
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.3926
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 149.754
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.3695
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 159.096

Attention duration (in seconds): 0.3594
Attention throughput (in TFLOP/s): 169.175
MLP duration (in seconds): 0.7622
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.2883
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 154.251
b: 256, m: 2048, n: 470, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 178.609
b: 256, m: 2048, n: 2048, k: 470,
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 114.645
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0569
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 260.599
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.4022
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 147.419
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2270
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 261.270

Attention duration (in seconds): 0.3597
Attention throughput (in TFLOP/s): 170.485
MLP duration (in seconds): 0.6292
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9889
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.2916
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 153.840
b: 256, m: 2048, n: 472, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 185.691
b: 256, m: 2048, n: 2048, k: 472,
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 155.467
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0570
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 262.200
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.4143
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 144.344
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2285
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 261.700

Attention duration (in seconds): 0.3606
Attention throughput (in TFLOP/s): 171.489
MLP duration (in seconds): 0.6428
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.2996
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 151.002
b: 256, m: 2048, n: 474, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 179.583
b: 256, m: 2048, n: 2048, k: 474,
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 115.386
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 259.704
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.4021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 150.003
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 262.529

Attention duration (in seconds): 0.3721
Attention throughput (in TFLOP/s): 167.553
MLP duration (in seconds): 0.6318
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.2990
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 152.537
b: 256, m: 2048, n: 476, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 188.774
b: 256, m: 2048, n: 2048, k: 476,
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 139.888
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0583
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 260.722
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.4264
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 142.628
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2303
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 264.065

Attention duration (in seconds): 0.3701
Attention throughput (in TFLOP/s): 169.866
MLP duration (in seconds): 0.6568
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.3052
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 150.739
b: 256, m: 2048, n: 478, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 181.496
b: 256, m: 2048, n: 2048, k: 478,
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 116.615
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0592
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 258.937
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.4196
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 146.161
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2341
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 261.946

Attention duration (in seconds): 0.3788
Attention throughput (in TFLOP/s): 167.318
MLP duration (in seconds): 0.6538
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0326
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.3076
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 150.815
b: 256, m: 2048, n: 480, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 189.792
b: 256, m: 2048, n: 2048, k: 480,
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 160.735
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0598
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 258.707
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.4276
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 144.635
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2347
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 263.502

Attention duration (in seconds): 0.3792
Attention throughput (in TFLOP/s): 168.547
MLP duration (in seconds): 0.6623
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0415
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.2968
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 157.605
b: 256, m: 2048, n: 482, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 182.422
b: 256, m: 2048, n: 2048, k: 482,
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 117.751
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0598
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 260.651
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.4218
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 147.845
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.3899
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 159.938

Attention duration (in seconds): 0.3711
Attention throughput (in TFLOP/s): 173.651
MLP duration (in seconds): 0.8117
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1828
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.3096
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 152.313
b: 256, m: 2048, n: 484, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 192.438
b: 256, m: 2048, n: 2048, k: 484,
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 142.327
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0595
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 264.001
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.4311
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 145.882
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.3784
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 166.196

Attention duration (in seconds): 0.3819
Attention throughput (in TFLOP/s): 170.105
MLP duration (in seconds): 0.8094
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1913
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
