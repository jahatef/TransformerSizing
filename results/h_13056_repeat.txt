[2023-06-23 17:13:23,322] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-23 17:13:23,913] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.146.99, master_port=6000
[2023-06-23 17:13:23,913] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-23 17:13:25,551] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.623
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.341
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.418
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25248202752, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.150
(25248202752, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.637
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.284
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.705
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.113
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.391
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.217
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.753
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.391
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.058
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.384
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.748
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.658
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.222
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.400
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.157
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.392
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.414
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.150
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.247
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.672
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.615
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.604
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.521
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.443
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.797
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.393
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.169
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.117
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.172
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.721
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.779
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.651
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.287
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.756
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.355
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.104
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.173
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.501
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.665
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.474
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.498
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.948
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.356
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.447
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.688
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.584
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.585
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.611
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.236
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.372
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.133
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.645
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.394
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.813
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.589
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.302
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.684
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.099
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.369
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.140
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.448
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.460
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0321
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.601
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.453
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.975
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0888
Attention throughput (in TFLOP/s): 135.593
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.362
Transformer duration (in seconds): 0.1800
Transformer throughput (in TFLOP/s): 191.028
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.974
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.371
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.387
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.153
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.223
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.907
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.445
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.221
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.255
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.585
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.375
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.389
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.622
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.619
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.335
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.468
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.619
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.419
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.121
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.638
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.402
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.773
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.230
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.741
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.680
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.656
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.204
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.523
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.358
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.370
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.415
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.925
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.562
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.503
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 252.909
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.175
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.951
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.363
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.424
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.424
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.336
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.909
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.590
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.790
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.191
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.122
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.335
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.386
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.592
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.602
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.244
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.639
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.080
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.336
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.840
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.345
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.368
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0310
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.673
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.086
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.925
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.352
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.678
Transformer duration (in seconds): 0.1791
Transformer throughput (in TFLOP/s): 191.986
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.709
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.346
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.469
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.251
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.872
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.021
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.535
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.107
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.274
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.506
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.507
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.432
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.402
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.294
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0438
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.230
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.444
MLP duration (in seconds): 0.0885
MLP throughput (in TFLOP/s): 252.435
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 191.958
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.961
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.608
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.482
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0313
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.804
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.072
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.803
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0880
Attention throughput (in TFLOP/s): 136.977
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.097
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 191.886
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.420
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.304
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.392
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.821
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.085
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 254.358
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.505
MLP duration (in seconds): 0.0887
MLP throughput (in TFLOP/s): 251.909
Transformer duration (in seconds): 0.1793
Transformer throughput (in TFLOP/s): 191.801
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.115
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.510
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.551
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.828
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.621
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.186
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.593
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.054
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.291
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.826
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.359
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.513
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.672
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.038
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.809
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.526
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.083
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.256
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.374
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.781
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.578
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.478
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.801
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.400
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.664
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.257
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.419
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.467
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.538
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.416
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.767
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.958
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0440
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 253.760
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.414
MLP duration (in seconds): 0.0888
MLP throughput (in TFLOP/s): 251.557
Transformer duration (in seconds): 0.1795
Transformer throughput (in TFLOP/s): 191.608
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 256.546
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.767
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.440
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.080
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 256.280
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.384
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0880
Attention throughput (in TFLOP/s): 136.932
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 252.031
Transformer duration (in seconds): 0.1796
Transformer throughput (in TFLOP/s): 191.455
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.009
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.768
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.456
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.447
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.135
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.737
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.582
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.579
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.481
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.279
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.388
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.408
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.810
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.676
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.074
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.493
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.518
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.398
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.012
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.391
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.427
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.612
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.016
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.790
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.559
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.548
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.455
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.778
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.381
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.542
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.941
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.045
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.044
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.531
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 252.234
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 191.940
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.176
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.391
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.402
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.224
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.163
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.295
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.570
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.379
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.398
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.107
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.589
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.421
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.723
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.768
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.372
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.590
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.205
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.330
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.707
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.305
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.336
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.572
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.243
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.221
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.455
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.866
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.502
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.635
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.451
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.393
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.009
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.944
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.072
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.449
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.649
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.409
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.114
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.560
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.348
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.786
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.256
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.044
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.581
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.289
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.354
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.084
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.698
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.255
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.037
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.653
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.109
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.539
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.508
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.408
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.383
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.489
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.266
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.068
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.072
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.274
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.389
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.797
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.419
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.877
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.450
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.261
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.245
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.107
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.299
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.292
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.824
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.358
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.096
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.692
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.245
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 238.228
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.190
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.563
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0885
Attention throughput (in TFLOP/s): 136.075
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.521
Transformer duration (in seconds): 0.1797
Transformer throughput (in TFLOP/s): 191.413
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.987
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.612
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.282
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.653
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.662
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.308
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.281
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.612
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.271
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.943
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.336
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.251
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 253.434
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.050
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.030
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0879
Attention throughput (in TFLOP/s): 137.105
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.184
Transformer duration (in seconds): 0.1791
Transformer throughput (in TFLOP/s): 191.987
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.700
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.722
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.265
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.923
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.721
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.057
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.254
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.520
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.217
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.557
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.333
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.260
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.194
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.736
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0442
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 252.949
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.414
MLP duration (in seconds): 0.0890
MLP throughput (in TFLOP/s): 251.051
Transformer duration (in seconds): 0.1797
Transformer throughput (in TFLOP/s): 191.415
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.696
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.393
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.390
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.324
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.230
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.627
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.286
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.600
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 191.910
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.972
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.646
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.411
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.859
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.064
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.121
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.326
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.764
Transformer duration (in seconds): 0.1791
Transformer throughput (in TFLOP/s): 192.001
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.619
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.415
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.417
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.778
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.530
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.396
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.229
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.605
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.249
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.328
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.319
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.278
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.864
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.394
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.951
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.150
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.312
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.069
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.374
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.312
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.283
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.835
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.539
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.566
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.347
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.677
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.341
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.972
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.547
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.403
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.782
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.173
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.363
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.237
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.900
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.362
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.913
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.313
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.284
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.244
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.513
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.667
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.286
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.714
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.313
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.947
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.311
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.289
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.455
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.784
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.375
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.228
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.721
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.293
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.789
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.713
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.432
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.838
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.781
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.992
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.480
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.534
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.394
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.387
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.644
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.387
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.036
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.591
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.087
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.418
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.002
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.152
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.540
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.581
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.380
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.072
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.627
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.295
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.504
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.104
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.233
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.912
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.681
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.520
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.417
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.099
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.945
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.367
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.665
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.367
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.624
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.381
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.398
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.173
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.273
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.396
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.391
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.967
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.497
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.885
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.677
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.406
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.010
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.892
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.379
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.506
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.771
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.501
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.827
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.393
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.390
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.367
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.872
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.415
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.311
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.264
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.550
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.752
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.406
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.428
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.241
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.802
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.407
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.285
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.742
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.336
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.503
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.402
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.426
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.198
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.465
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.159
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.436
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.945
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.520
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.197
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.410
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.423
Elapsed time for attention_dropout (4x64x2048x2048): 0.0049
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.857
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.535
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.141
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0881
Attention throughput (in TFLOP/s): 136.691
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.963
Transformer duration (in seconds): 0.1791
Transformer throughput (in TFLOP/s): 192.011
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.186
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.395
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.420
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.545
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.645
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.019
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.402
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.958
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.498
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.990
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.414
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.428
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.565
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.064
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.391
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.291
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.345
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.567
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.409
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.622
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.385
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 253.633
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.564
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.449
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.265
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.132
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.470
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.078
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.637
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.418
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.160
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.037
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.236
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.373
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.771
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.409
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.142
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.374
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.394
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.605
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.909
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.326
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.577
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.755
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.544
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.201
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.634
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.443
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.504
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.020
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.437
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.359
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.346
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.615
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.552
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.470
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.314
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.261
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.486
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 254.294
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.357
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 252.055
Transformer duration (in seconds): 0.1793
Transformer throughput (in TFLOP/s): 191.747
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.622
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.401
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.477
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.561
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.749
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.100
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.491
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.568
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.415
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.665
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.772
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.395
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.861
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.958
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.265
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.468
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.779
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.104
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.810
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.448
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.435
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.216
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.882
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.124
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.363
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.644
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.354
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.831
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.384
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.435
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.062
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.208
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.364
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.354
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.920
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.449
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.438
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.363
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.430
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.103
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.832
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.445
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.404
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.777
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.434
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.201
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.425
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.427
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.811
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.776
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.382
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.349
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.202
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.562
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.205
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.680
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.415
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.250
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.560
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.226
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.407
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 254.021
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.526
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.820
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.491
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.456
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.736
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.233
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.437
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.474
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.450
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.734
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.688
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.629
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.461
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.117
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.654
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.411
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.429
MLP duration (in seconds): 0.0877
MLP throughput (in TFLOP/s): 254.638
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.773
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.889
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.779
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.437
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.001
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.370
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.220
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.466
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.411
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.713
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.798
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.412
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.424
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.555
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.868
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.429
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.451
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.784
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.470
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.484
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.401
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.440
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.495
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.229
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.446
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.452
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.970
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.538
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.432
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.393
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.448
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.052
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.205
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.385
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.348
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.410
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.629
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.946
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.411
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.432
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.224
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.959
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.402
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.257
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.300
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.527
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.598
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.397
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.444
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.973
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.094
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.405
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.239
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.882
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.359
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.267
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.437
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.447
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.132
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.603
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.874
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.313
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.874
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.406
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.279
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.417
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.447
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.731
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.401
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.194
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.163
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.444
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.143
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.483
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.630
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.440
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.940
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.813
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0022
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.422
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.439
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 252.205
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 191.870
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.721
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.776
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.447
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.066
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.849
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.454
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.374
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.272
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.597
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.365
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.417
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.467
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.854
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.611
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0438
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.057
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.396
MLP duration (in seconds): 0.0885
MLP throughput (in TFLOP/s): 252.505
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 191.951
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.698
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.395
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.438
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.056
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.510
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.439
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.394
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.103
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.546
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.951
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.537
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.420
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.471
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.153
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.425
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.561
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.402
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.774
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.710
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.429
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.429
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.278
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.414
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.454
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.480
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.063
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.592
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.140
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.332
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.449
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.034
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.605
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.365
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.274
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.112
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.468
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.250
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.339
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.283
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.715
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.759
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.383
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.311
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.181
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.505
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.572
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.316
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.331
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.648
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.572
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.344
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.380
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.073
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.519
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.347
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.390
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.447
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.946
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.159
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.457
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.379
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.422
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.662
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.397
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.413
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.451
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.630
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.976
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.503
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.376
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.358
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.631
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.782
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.557
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.450
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.328
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.655
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.421
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.514
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.682
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.473
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.702
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.556
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.455
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.209
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.537
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.341
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.425
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.537
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.716
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.581
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.405
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.324
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.483
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.875
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.398
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.377
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.243
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.570
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.510
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.324
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.291
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.453
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.080
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.346
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.359
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.312
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.584
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.846
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.700
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.331
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.443
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.320
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.499
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.458
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.522
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.749
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.620
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.392
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.292
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 253.094
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.158
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.468
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.209
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.945
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.362
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.002
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.380
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.430
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.957
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.899
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.386
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.439
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.264
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.639
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.725
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.474
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.440
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.638
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.080
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.381
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.446
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.347
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.675
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.515
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.390
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.416
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.166
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.004
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.455
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.431
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.347
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.663
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.569
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.396
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.459
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.195
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.474
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.350
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.376
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.522
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.685
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.286
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.399
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.450
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.397
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.270
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.412
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.595
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.454
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.817
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.868
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.393
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.446
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.382
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.342
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.385
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.510
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.475
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.768
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 256.986
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.407
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.446
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.157
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.281
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.315
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0879
Attention throughput (in TFLOP/s): 137.043
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.399
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.408
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.591
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.480
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.304
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 253.476
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.256
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.384
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.244
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.422
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.546
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.828
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.609
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.349
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.726
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.586
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.552
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.408
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.177
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.569
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.888
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.570
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.317
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.983
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.388
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.071
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.416
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.850
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.452
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0332
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 252.052
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.325
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.318
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.954
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.755
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.333
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0885
Attention throughput (in TFLOP/s): 136.097
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.155
Transformer duration (in seconds): 0.1794
Transformer throughput (in TFLOP/s): 191.654
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.335
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.614
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.295
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.581
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.757
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.758
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.422
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.877
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.467
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.882
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.356
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.135
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.571
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.212
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.334
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.495
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.377
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.703
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.589
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.324
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.366
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.264
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.086
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.639
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.362
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.977
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.462
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.757
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.320
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.314
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.019
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.563
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.037
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.433
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.919
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.504
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.723
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.543
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.317
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 253.736
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.052
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.468
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.304
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.377
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.588
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.737
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.791
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.452
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.588
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.518
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.454
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.469
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.595
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.782
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.932
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.770
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.436
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.801
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.478
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.497
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.530
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.599
Transformer duration (in seconds): 0.1783
Transformer throughput (in TFLOP/s): 192.827
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.862
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.391
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.429
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.386
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.856
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.446
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.449
MLP duration (in seconds): 0.0877
MLP throughput (in TFLOP/s): 254.756
Transformer duration (in seconds): 0.1783
Transformer throughput (in TFLOP/s): 192.827
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.894
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.626
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.456
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.708
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.615
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.335
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.500
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.619
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.441
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.641
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.450
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.434
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.892
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.449
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.381
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.378
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.525
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.695
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.583
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.401
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.452
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.055
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.372
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.508
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.435
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.067
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.564
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.758
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.407
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.431
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.382
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.881
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.472
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.421
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.296
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.638
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.189
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.406
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.444
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 253.509
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.418
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.430
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.380
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.537
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.699
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.821
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.767
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.451
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.606
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.595
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.708
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.488
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.271
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.677
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.029
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.636
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.469
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.340
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.608
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.992
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.499
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.448
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.374
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.898
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.526
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.434
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.729
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.761
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.615
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.422
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.304
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.643
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.917
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.747
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.426
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.970
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.937
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.481
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.469
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.328
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.682
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.127
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.382
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.443
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.557
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.651
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.459
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.510
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.179
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.657
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.740
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.400
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.450
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.314
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 256.906
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.454
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.421
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.331
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.277
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.846
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.414
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.483
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.581
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.573
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.186
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.396
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.516
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.331
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.766
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.421
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.486
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 244.914
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.175
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.367
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0881
Attention throughput (in TFLOP/s): 136.701
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.388
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.173
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.928
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.411
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.412
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.457
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.877
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.513
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.471
MLP duration (in seconds): 0.0877
MLP throughput (in TFLOP/s): 254.796
Transformer duration (in seconds): 0.1783
Transformer throughput (in TFLOP/s): 192.859
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.544
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.589
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.473
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.022
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.618
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.136
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.451
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 254.006
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.552
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.186
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.414
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.455
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.559
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.822
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.447
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.331
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.255
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.560
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.056
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.647
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.451
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.675
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.900
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.472
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.532
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.310
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.721
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.213
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.420
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.454
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.938
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.750
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.478
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.495
MLP duration (in seconds): 0.0877
MLP throughput (in TFLOP/s): 254.719
Transformer duration (in seconds): 0.1783
Transformer throughput (in TFLOP/s): 192.847
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.053
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.664
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.463
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.755
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.550
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.943
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.468
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.361
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.695
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.107
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.413
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.475
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.051
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.865
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.302
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.477
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.201
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.642
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.063
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.753
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.468
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.340
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.052
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.511
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.522
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.916
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.566
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 256.473
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.703
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.436
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.275
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.550
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.842
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0880
Attention throughput (in TFLOP/s): 136.945
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.312
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.316
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.841
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.610
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.457
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.536
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.808
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.447
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.479
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.756
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.476
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.971
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.546
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.487
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.493
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.706
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.878
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.498
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.919
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.557
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.915
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.409
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.477
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.550
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.422
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.543
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.410
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.593
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.742
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.997
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.705
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.474
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.611
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.693
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.275
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.452
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.106
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.590
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.991
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.778
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.452
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.158
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.780
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.949
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.495
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.989
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.576
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.895
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.781
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.457
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 252.619
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.569
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.410
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.305
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.117
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.493
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.040
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.774
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.442
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.343
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.364
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.010
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.520
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.304
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.711
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.055
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.570
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.443
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.730
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.811
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0441
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 253.449
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.463
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 252.283
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 191.912
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.189
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.548
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.446
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.700
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.814
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.512
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.550
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.792
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.543
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.189
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.425
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.471
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.974
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.804
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.638
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.484
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.848
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.515
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.039
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.399
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.467
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.075
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.415
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.719
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.468
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.704
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.450
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.189
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.639
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.469
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.140
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.795
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.465
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.522
MLP duration (in seconds): 0.0877
MLP throughput (in TFLOP/s): 254.733
Transformer duration (in seconds): 0.1783
Transformer throughput (in TFLOP/s): 192.870
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.814
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.769
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.458
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.148
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.294
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.509
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.461
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.514
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.749
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.620
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.407
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.484
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.063
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.799
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.734
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.392
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.879
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.444
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.697
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.727
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.324
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.754
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.862
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.444
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.403
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.289
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.243
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.853
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.430
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.462
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.738
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.450
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.077
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.477
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.892
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.526
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.173
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.563
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.444
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.766
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.382
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.715
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.362
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.683
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.371
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.048
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.404
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.426
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.867
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.465
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.022
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.517
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.359
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.730
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.620
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.406
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.379
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.429
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.665
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.480
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.412
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.197
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.596
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.995
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.770
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.428
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.039
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.405
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.174
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.479
MLP duration (in seconds): 0.0878
MLP throughput (in TFLOP/s): 254.403
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.716
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 256.846
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.414
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.437
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 253.075
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.860
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.507
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0880
Attention throughput (in TFLOP/s): 136.899
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.802
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.082
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.712
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.281
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.319
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.368
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0442
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 252.859
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.983
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.455
MLP duration (in seconds): 0.0890
MLP throughput (in TFLOP/s): 251.110
Transformer duration (in seconds): 0.1796
Transformer throughput (in TFLOP/s): 191.449
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.864
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.653
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.424
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.180
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.855
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.083
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.531
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.096
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.623
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.711
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.437
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.318
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.246
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.827
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.457
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.456
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.778
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.468
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.243
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.744
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.404
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.602
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.059
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.902
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.627
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.616
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.527
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.689
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.772
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.446
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.674
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.585
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.215
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.542
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.058
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.258
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.291
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.770
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.499
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.871
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.821
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.305
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.657
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.218
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.395
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.152
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.744
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.546
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.221
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.195
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.707
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.604
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.111
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.320
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.547
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.533
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.430
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.284
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.179
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.385
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.398
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 252.946
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.119
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.093
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.765
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.457
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 252.382
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.547
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.412
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.326
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.136
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.141
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.338
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.706
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.492
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.917
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.894
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.804
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.678
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.008
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.333
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.065
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.686
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.479
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.628
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.291
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.919
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.592
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.261
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.369
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.766
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.551
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.413
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.004
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.071
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.013
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.548
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.683
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.497
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.944
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.384
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.426
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.731
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.675
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.076
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.552
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.519
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.439
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.252
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.513
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.498
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.015
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.587
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.284
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.644
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.579
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.521
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.743
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.729
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.448
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.377
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.910
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.947
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.521
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.570
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.438
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.614
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.767
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.438
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 240.045
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.371
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.929
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0884
Attention throughput (in TFLOP/s): 136.353
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.303
Transformer duration (in seconds): 0.1796
Transformer throughput (in TFLOP/s): 191.533
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.644
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.773
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.441
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.231
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.186
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.234
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0874
Attention throughput (in TFLOP/s): 137.763
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.840
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.702
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.478
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.766
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.419
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.134
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.657
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.447
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.715
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.694
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.616
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.988
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.382
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.394
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.884
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.100
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.298
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.575
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.832
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.572
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.613
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.749
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.434
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.144
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.462
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.827
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.484
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.298
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.309
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.193
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.387
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.421
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.102
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.230
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.863
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.631
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.203
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.376
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.845
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.677
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.454
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.300
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.166
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.032
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.542
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.255
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.332
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.140
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.751
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.417
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.259
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.711
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.998
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.600
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.499
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.465
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.749
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.745
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.450
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.871
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.618
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.179
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.560
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.545
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.454
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.994
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.739
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.383
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.436
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.210
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.113
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.576
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.831
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.198
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.461
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.773
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.426
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.781
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.493
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.603
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.433
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.204
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.239
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.267
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.475
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.358
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.653
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.446
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.639
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.390
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.667
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.367
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.110
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.555
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.320
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.763
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.311
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.382
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.570
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 252.997
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.239
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.576
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.598
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.368
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.707
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.460
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.213
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.668
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.461
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.481
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 257.456
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.755
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.420
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.669
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.890
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.464
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 137.227
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.326
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.143
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.360
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.765
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.532
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.819
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 257.926
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.751
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.618
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.003
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.289
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.246
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.758
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.413
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.846
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.524
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.238
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.660
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 254.006
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.692
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.387
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.763
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.440
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.391
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.164
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.135
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.649
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.296
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.422
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.225
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.760
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.463
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.537
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.185
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.593
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.628
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.529
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.495
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.819
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.748
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.750
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.578
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.484
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.327
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.569
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.065
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.281
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.985
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.771
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.514
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.375
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.980
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.640
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.582
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.450
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.434
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.172
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.742
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.472
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.070
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.333
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.718
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.578
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.179
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.327
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.054
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.735
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.565
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.613
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.294
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.249
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.598
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 252.935
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.251
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.168
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.783
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.497
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.528
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.776
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.114
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.628
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.588
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.517
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.002
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.748
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.550
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.210
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.186
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.714
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.500
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.109
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.104
Transformer - MLP - Attention (in seconds): 0.0031


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.105
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.512
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.356
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.364
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.830
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.700
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.546
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.403
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.375
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.144
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.679
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.359
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.910
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.439
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.629
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.613
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.176
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.336
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.020
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.619
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.349
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.553
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.271
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.794
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.613
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.650
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.513
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.943
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.724
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.484
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.094
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.614
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.769
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.610
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.344
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.411
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.601
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.757
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.726
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.889
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.129
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.058
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.735
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.729
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.644
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0331
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 252.932
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.772
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.446
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.732
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.395
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.569
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0883
Attention throughput (in TFLOP/s): 136.390
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.106
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 191.857
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.353
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.753
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.461
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.180
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.937
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.448
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.705
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.344
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.481
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.300
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.768
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.456
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.406
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.689
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.225
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.637
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.114
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.346
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.514
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.779
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.429
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.062
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.731
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.241
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.719
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.629
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.590
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.575
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.756
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.501
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.920
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.189
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.157
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.717
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.319
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.477
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.382
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.753
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.707
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.035
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 260.036
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.096
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.704
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.183
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.792
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.547
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.760
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.419
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.949
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.256
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.515
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.716
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.527
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.552
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.757
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.837
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.409
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0310
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.042
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.849
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.582
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.595
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.364
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.409
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.683
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.745
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.536
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0311
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.623
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.979
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.386
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.367
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.324
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.239
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.615
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.770
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.655
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.393
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 260.001
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.278
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.710
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.764
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.638
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.506
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.736
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.405
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.002
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.451
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.884
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.711
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.800
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.650
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.165
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.754
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.500
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.159
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.859
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.715
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.664
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.435
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.485
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.543
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.540
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.426
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.828
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.198
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.186
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.682
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.827
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.642
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.771
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.476
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.525
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.688
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.019
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.249
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.718
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.289
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.472
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.917
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.765
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.421
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0317
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.599
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.911
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.071
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0882
Attention throughput (in TFLOP/s): 136.514
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.631
Transformer duration (in seconds): 0.1793
Transformer throughput (in TFLOP/s): 191.767
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.246
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.763
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.433
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.894
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.392
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.842
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.655
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.272
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.414
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.002
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.743
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.407
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.535
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.154
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.654
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.582
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.056
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.291
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.154
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.029
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.575
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.156
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.098
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.253
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.461
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.812
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.486
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.280
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.762
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.458
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.383
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.984
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.786
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.635
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.526
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.499
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.383
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.779
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.442
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.134
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.261
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.062
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.705
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.313
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.468
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.550
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.761
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.429
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.392
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.645
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.643
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.685
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.807
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.263
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.159
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.766
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.479
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.963
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.022
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.451
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.650
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.383
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.457
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.120
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.768
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.445
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.445
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.588
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.458
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.614
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.691
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.171
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.423
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.461
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.291
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.580
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.920
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.923
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.638
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.561
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.512
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.243
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.719
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.292
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.151
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.752
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.390
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.585
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.724
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.147
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.082
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.700
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.308
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 253.908
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.978
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.663
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.404
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.450
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.303
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.138
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.775
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.566
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.961
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.943
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.465
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.649
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.352
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.443
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.490
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.678
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.377
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.287
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.954
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.269
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.715
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.258
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.451
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.366
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.756
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.366
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.769
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.156
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.431
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.670
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.439
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.487
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.899
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.754
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.442
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.497
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.370
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.443
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.556
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.546
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.452
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.130
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.760
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.403
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.022
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.299
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.023
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.574
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.799
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.558
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.261
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.705
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.257
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.630
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.645
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.387
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.629
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.657
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.543
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.171
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.745
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.508
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.836
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.920
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.657
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.633
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.433
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.462
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.431
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.755
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.414
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.600
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.886
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.806
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.670
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.490
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.507
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.253
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.691
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.479
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.793
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.015
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.871
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.635
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.583
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.516
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 256.921
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.717
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.420
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.007
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.920
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.738
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0879
Attention throughput (in TFLOP/s): 137.056
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.947
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.234
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.947
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.701
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.332
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.112
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.940
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.363
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.516
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.775
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.477
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.070
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.704
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.301
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.639
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.730
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.366
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.505
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.161
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.629
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.558
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.716
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.298
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.150
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.996
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 258.064
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.374
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.154
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.553
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.042
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.768
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.395
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.754
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.889
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.055
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.609
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.611
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.515
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.524
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.763
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.440
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.866
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.492
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.275
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.572
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.528
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.455
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.498
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.760
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.630
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.933
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.650
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.221
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.720
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.092
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.391
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.294
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.762
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.376
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.299
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.626
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.175
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.686
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.056
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.357
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.197
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.783
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.430
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.870
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.755
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.735
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.581
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.396
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.411
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.121
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.739
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.422
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.866
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.536
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.586
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.622
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.216
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.373
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.927
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.746
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.565
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.541
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.293
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.082
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.561
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.339
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.377
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.390
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.747
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.387
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.961
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.659
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.235
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.678
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.100
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.365
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.071
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.760
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.378
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.530
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.855
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.235
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.662
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.192
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.389
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.915
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.689
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.402
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.035
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.217
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.657
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.580
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.094
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.296
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.056
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.748
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.530
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.425
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.687
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.779
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.588
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.865
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.591
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.917
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.769
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.384
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.002
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.642
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.767
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.518
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.865
Transformer duration (in seconds): 0.1790
Transformer throughput (in TFLOP/s): 192.168
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.845
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.785
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.614
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.531
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.214
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.513
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.567
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.992
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.626
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.299
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.779
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.581
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.609
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0439
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 254.651
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.431
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.661
MLP duration (in seconds): 0.0891
MLP throughput (in TFLOP/s): 250.782
Transformer duration (in seconds): 0.1796
Transformer throughput (in TFLOP/s): 191.487
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.195
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.781
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.515
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.100
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.251
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.756
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.677
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.159
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.390
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.623
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.775
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.419
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.174
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.339
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.736
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.747
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.672
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.630
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.353
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.763
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.423
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.847
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.349
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.137
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.540
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.874
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.564
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.779
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.772
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.416
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.739
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.625
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.858
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.555
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.866
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.569
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.349
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.763
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.631
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.736
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.809
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.941
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.744
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.030
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.383
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.520
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.748
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.367
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.866
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.896
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.393
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.706
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.779
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.640
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.742
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.767
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.423
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.146
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.431
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.069
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0874
Attention throughput (in TFLOP/s): 137.766
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.876
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.719
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.520
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.753
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.392
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.053
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.734
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.608
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.712
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.800
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.652
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.486
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.752
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.506
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.072
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.840
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.383
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.717
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.260
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.454
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.792
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.750
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.361
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0310
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.508
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.768
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.500
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.549
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.769
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.530
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.544
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.727
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.521
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.485
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.591
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.554
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.626
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.704
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.560
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.656
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.749
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.404
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.401
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.416
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.958
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.701
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.817
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.653
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.656
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.758
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.399
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0313
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 255.817
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.688
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.520
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0879
Attention throughput (in TFLOP/s): 137.082
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.257
Transformer duration (in seconds): 0.1791
Transformer throughput (in TFLOP/s): 192.018
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.195
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.768
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.398
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.364
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.194
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.498
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.679
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.974
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.696
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.481
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.751
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.401
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.450
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.516
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.753
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.738
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.800
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.294
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.557
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.710
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.361
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.620
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.679
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.391
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0874
Attention throughput (in TFLOP/s): 137.767
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.673
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.644
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.058
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.566
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.482
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.256
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.101
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.961
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.631
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.671
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.551
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.368
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.751
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.495
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0310
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.316
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.820
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.526
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.583
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.324
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.387
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.458
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.378
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.488
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.693
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.931
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.395
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.714
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.311
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.478
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.314
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.503
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.412
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.115
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.263
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.421
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.659
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.004
Transformer duration (in seconds): 0.1788
Transformer throughput (in TFLOP/s): 192.320
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.442
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.769
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.628
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.905
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.022
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.368
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.705
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.829
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.659
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.548
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.790
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.447
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.154
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.382
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.440
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.748
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 254.037
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 192.764
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.544
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.701
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.488
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.243
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.180
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.297
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.735
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.852
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.689
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.550
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.778
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.420
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.282
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.253
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.342
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.680
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.925
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.679
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.392
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.785
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.415
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.689
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.282
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 257.144
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.677
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.845
Transformer duration (in seconds): 0.1785
Transformer throughput (in TFLOP/s): 192.643
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.300
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.428
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.372
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 254.852
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 259.617
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.809
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.487
MLP duration (in seconds): 0.0880
MLP throughput (in TFLOP/s): 253.839
Transformer duration (in seconds): 0.1786
Transformer throughput (in TFLOP/s): 192.514
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.356
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.787
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.526
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 256.745
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.889
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.710
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.675
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.443
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.493
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.894
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.345
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.517
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.673
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.916
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.733
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0875
Attention throughput (in TFLOP/s): 137.610
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 253.469
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.457
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.013
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.419
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.498
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0310
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.423
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.138
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.849
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0877
Attention throughput (in TFLOP/s): 137.404
MLP duration (in seconds): 0.0883
MLP throughput (in TFLOP/s): 253.151
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 192.197
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 258.892
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.402
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.532
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.109
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.731
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 256.846
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0876
Attention throughput (in TFLOP/s): 137.574
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 253.434
Transformer duration (in seconds): 0.1787
Transformer throughput (in TFLOP/s): 192.423
Transformer - MLP - Attention (in seconds): 0.0030


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 259.267
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 85.415
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 116.480
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25246105600, 42481549312)
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.599
(25246105600, 42481549312)
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.403
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0015
