num_attention_heads: 16, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0386
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 85.379
Elapsed time for attention_key_query_prob (64x2048x512x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x512x2048): 64.517
Elapsed time for attention_prob_times_values (64x2048x2048x512): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x512): 68.782
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0121
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 91.115
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0518
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 84.873
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0506
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 86.861

Attention duration (in seconds): 0.0590
Attention throughput (in TFLOP/s): 83.921
MLP duration (in seconds): 0.1025
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1614
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0391
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 84.362
Elapsed time for attention_key_query_prob (128x2048x256x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x256x2048): 57.730
Elapsed time for attention_prob_times_values (128x2048x2048x256): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x256): 66.264
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 90.148
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0520
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 84.541
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0506
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 86.848

Attention duration (in seconds): 0.0602
Attention throughput (in TFLOP/s): 82.181
MLP duration (in seconds): 0.1027
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1629
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0391
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 84.330
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 47.391
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 60.709
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 90.261
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 84.485
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0506
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 86.846

Attention duration (in seconds): 0.0616
Attention throughput (in TFLOP/s): 80.290
MLP duration (in seconds): 0.1027
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1643
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0392
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 84.087
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 33.693
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 26.851
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 90.361
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0520
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 84.517
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0507
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 86.807

Attention duration (in seconds): 0.0698
Attention throughput (in TFLOP/s): 70.895
MLP duration (in seconds): 0.1027
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1725
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 80.482
Elapsed time for attention_key_query_prob (64x2048x520x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x520x2048): 58.605
Elapsed time for attention_prob_times_values (64x2048x2048x520): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x520): 60.560
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 88.204
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0551
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 82.391
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0531
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 85.477

Attention duration (in seconds): 0.0645
Attention throughput (in TFLOP/s): 78.982
MLP duration (in seconds): 0.1081
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1726
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 80.698
Elapsed time for attention_key_query_prob (128x2048x260x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x260x2048): 53.948
Elapsed time for attention_prob_times_values (128x2048x2048x260): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x260): 44.597
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 88.065
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0551
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 82.355
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0531
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 85.476

Attention duration (in seconds): 0.0665
Attention throughput (in TFLOP/s): 76.643
MLP duration (in seconds): 0.1082
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1746
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 80.334
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 40.501
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 43.329
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 88.170
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0550
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 82.415
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0531
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 85.479

Attention duration (in seconds): 0.0686
Attention throughput (in TFLOP/s): 74.321
MLP duration (in seconds): 0.1081
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1767
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 80.697
Elapsed time for attention_key_query_prob (512x2048x65x2048): 0.0805
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x65x2048): 3.468
Elapsed time for attention_prob_times_values (512x2048x2048x65): 0.0635
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x65): 4.399
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0128
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 88.284
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0550
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 82.479
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0531
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 85.484

Attention duration (in seconds): 0.1990
Attention throughput (in TFLOP/s): 25.606
MLP duration (in seconds): 0.1081
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0404
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 86.932
Elapsed time for attention_key_query_prob (64x2048x528x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x528x2048): 60.925
Elapsed time for attention_prob_times_values (64x2048x2048x528): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x528): 62.102
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 88.185
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0565
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 82.772
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0561
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 83.340

Attention duration (in seconds): 0.0628
Attention throughput (in TFLOP/s): 83.466
MLP duration (in seconds): 0.1126
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1755
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0403
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 87.003
Elapsed time for attention_key_query_prob (128x2048x264x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x264x2048): 52.409
Elapsed time for attention_prob_times_values (128x2048x2048x264): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x264): 54.773
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 88.446
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 83.030
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0561
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 83.322

Attention duration (in seconds): 0.0641
Attention throughput (in TFLOP/s): 81.781
MLP duration (in seconds): 0.1125
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1766
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0405
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 86.689
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 40.504
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 42.989
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 88.739
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 83.078
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0561
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 83.344

Attention duration (in seconds): 0.0672
Attention throughput (in TFLOP/s): 77.998
MLP duration (in seconds): 0.1124
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1797
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0403
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 87.010
Elapsed time for attention_key_query_prob (512x2048x66x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x66x2048): 27.422
Elapsed time for attention_prob_times_values (512x2048x2048x66): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x66): 36.670
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 88.670
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0562
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 83.248
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0561
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 83.349

Attention duration (in seconds): 0.0716
Attention throughput (in TFLOP/s): 73.272
MLP duration (in seconds): 0.1123
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1839
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0435
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 83.020
Elapsed time for attention_key_query_prob (64x2048x536x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x536x2048): 59.644
Elapsed time for attention_prob_times_values (64x2048x2048x536): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x536): 62.228
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 89.798
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0583
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 82.629
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 81.083

Attention duration (in seconds): 0.0664
Attention throughput (in TFLOP/s): 81.244
MLP duration (in seconds): 0.1178
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1842
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0437
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 82.775
Elapsed time for attention_key_query_prob (128x2048x268x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x268x2048): 54.825
Elapsed time for attention_prob_times_values (128x2048x2048x268): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x268): 45.976
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 89.798
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0582
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 82.797
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 81.086

Attention duration (in seconds): 0.0686
Attention throughput (in TFLOP/s): 78.652
MLP duration (in seconds): 0.1177
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1863
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 82.897
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 41.462
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 43.875
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 89.451
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0581
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 82.918
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 81.085

Attention duration (in seconds): 0.0706
Attention throughput (in TFLOP/s): 76.447
MLP duration (in seconds): 0.1176
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1882
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 82.850
Elapsed time for attention_key_query_prob (512x2048x67x2048): 0.0804
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x67x2048): 3.578
Elapsed time for attention_prob_times_values (512x2048x2048x67): 0.0603
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x67): 4.769
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 89.551
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0580
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 83.042
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 81.088

Attention duration (in seconds): 0.1979
Attention throughput (in TFLOP/s): 27.271
MLP duration (in seconds): 0.1175
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0455
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 81.924
Elapsed time for attention_key_query_prob (64x2048x544x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x544x2048): 64.256
Elapsed time for attention_prob_times_values (64x2048x2048x544): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x544): 64.175
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 87.371
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0606
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 81.928
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0602
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 82.528

Attention duration (in seconds): 0.0688
Attention throughput (in TFLOP/s): 80.706
MLP duration (in seconds): 0.1208
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1895
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0455
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 81.885
Elapsed time for attention_key_query_prob (128x2048x272x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x272x2048): 54.713
Elapsed time for attention_prob_times_values (128x2048x2048x272): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x272): 56.701
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 87.404
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0606
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 81.963
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0602
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 82.534

Attention duration (in seconds): 0.0702
Attention throughput (in TFLOP/s): 79.086
MLP duration (in seconds): 0.1207
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1909
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 81.690
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 41.877
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 45.117
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 87.396
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0605
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 82.034
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0602
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 82.539

Attention duration (in seconds): 0.0732
Attention throughput (in TFLOP/s): 75.773
MLP duration (in seconds): 0.1207
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1939
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0455
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 81.863
Elapsed time for attention_key_query_prob (512x2048x68x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x68x2048): 27.787
Elapsed time for attention_prob_times_values (512x2048x2048x68): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x68): 37.794
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 87.619
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0605
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 82.013
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0602
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 82.538

Attention duration (in seconds): 0.0779
Attention throughput (in TFLOP/s): 71.241
MLP duration (in seconds): 0.1207
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1986
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0452
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 84.838
Elapsed time for attention_key_query_prob (64x2048x552x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x552x2048): 58.994
Elapsed time for attention_prob_times_values (64x2048x2048x552): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x552): 63.649
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 88.894
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0603
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 84.711
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0632
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 80.836

Attention duration (in seconds): 0.0692
Attention throughput (in TFLOP/s): 82.381
MLP duration (in seconds): 0.1236
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1928
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0454
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 84.513
Elapsed time for attention_key_query_prob (128x2048x276x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x276x2048): 55.630
Elapsed time for attention_prob_times_values (128x2048x2048x276): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x276): 46.966
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 88.921
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0603
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 84.766
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0631
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 81.008

Attention duration (in seconds): 0.0714
Attention throughput (in TFLOP/s): 79.925
MLP duration (in seconds): 0.1234
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1948
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0453
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 84.614
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 42.468
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 44.979
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 88.801
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0604
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 84.617
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0631
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 80.959

Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 77.858
MLP duration (in seconds): 0.1236
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1968
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0453
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 84.647
Elapsed time for attention_key_query_prob (512x2048x69x2048): 0.0827
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x69x2048): 3.584
Elapsed time for attention_prob_times_values (512x2048x2048x69): 0.0592
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x69): 5.003
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 88.966
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0602
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 84.910
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0632
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 80.849

Attention duration (in seconds): 0.2016
Attention throughput (in TFLOP/s): 28.300
MLP duration (in seconds): 0.1234
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 82.539
Elapsed time for attention_key_query_prob (64x2048x560x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x560x2048): 61.309
Elapsed time for attention_prob_times_values (64x2048x2048x560): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x560): 65.398
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 89.249
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0641
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 82.065
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0632
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 83.258

Attention duration (in seconds): 0.0720
Attention throughput (in TFLOP/s): 81.373
MLP duration (in seconds): 0.1273
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1994
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 82.467
Elapsed time for attention_key_query_prob (128x2048x280x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x280x2048): 54.109
Elapsed time for attention_prob_times_values (128x2048x2048x280): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x280): 57.570
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 89.424
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0641
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 82.074
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0632
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 83.261

Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 79.941
MLP duration (in seconds): 0.1273
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2006
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0480
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 82.182
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 42.462
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 44.885
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 89.668
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0640
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 82.189
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0632
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 83.262

Attention duration (in seconds): 0.0765
Attention throughput (in TFLOP/s): 76.673
MLP duration (in seconds): 0.1272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 82.469
Elapsed time for attention_key_query_prob (512x2048x70x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x70x2048): 28.559
Elapsed time for attention_prob_times_values (512x2048x2048x70): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x70): 38.801
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 89.417
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0640
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 82.232
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0632
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 83.270

Attention duration (in seconds): 0.0808
Attention throughput (in TFLOP/s): 72.527
MLP duration (in seconds): 0.1272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 86.776
Elapsed time for attention_key_query_prob (64x2048x568x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x568x2048): 60.147
Elapsed time for attention_prob_times_values (64x2048x2048x568): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x568): 64.820
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 88.603
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0656
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 82.468
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 81.456

Attention duration (in seconds): 0.0718
Attention throughput (in TFLOP/s): 83.847
MLP duration (in seconds): 0.1321
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0469
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 86.645
Elapsed time for attention_key_query_prob (128x2048x284x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x284x2048): 56.871
Elapsed time for attention_prob_times_values (128x2048x2048x284): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x284): 47.405
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 88.538
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0655
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 82.689
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0665
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 81.437

Attention duration (in seconds): 0.0739
Attention throughput (in TFLOP/s): 81.462
MLP duration (in seconds): 0.1319
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 86.179
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 43.383
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 46.013
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 88.765
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0654
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 82.760
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 81.458

Attention duration (in seconds): 0.0760
Attention throughput (in TFLOP/s): 79.238
MLP duration (in seconds): 0.1319
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 86.670
Elapsed time for attention_key_query_prob (512x2048x71x2048): 0.0868
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x71x2048): 3.513
Elapsed time for attention_prob_times_values (512x2048x2048x71): 0.0661
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x71): 4.613
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 88.927
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0653
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 82.925
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 81.464

Attention duration (in seconds): 0.2150
Attention throughput (in TFLOP/s): 28.016
MLP duration (in seconds): 0.1317
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3467
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 84.683
Elapsed time for attention_key_query_prob (64x2048x576x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x576x2048): 63.890
Elapsed time for attention_prob_times_values (64x2048x2048x576): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x576): 67.667
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 88.919
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0675
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 82.445
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0699
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 79.621

Attention duration (in seconds): 0.0744
Attention throughput (in TFLOP/s): 83.175
MLP duration (in seconds): 0.1374
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 84.758
Elapsed time for attention_key_query_prob (128x2048x288x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x288x2048): 58.913
Elapsed time for attention_prob_times_values (128x2048x2048x288): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x288): 60.268
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 89.503
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0676
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 82.342
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0699
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 79.609

Attention duration (in seconds): 0.0752
Attention throughput (in TFLOP/s): 82.263
MLP duration (in seconds): 0.1375
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 84.707
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 45.067
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 48.420
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 89.610
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0674
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 82.573
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0699
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 79.621

Attention duration (in seconds): 0.0781
Attention throughput (in TFLOP/s): 79.229
MLP duration (in seconds): 0.1373
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0494
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 84.533
Elapsed time for attention_key_query_prob (512x2048x72x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x72x2048): 31.760
Elapsed time for attention_prob_times_values (512x2048x2048x72): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x72): 35.161
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 89.948
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0673
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 82.666
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0699
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 79.625

Attention duration (in seconds): 0.0834
Attention throughput (in TFLOP/s): 74.168
MLP duration (in seconds): 0.1372
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 82.710
Elapsed time for attention_key_query_prob (64x2048x584x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x584x2048): 59.427
Elapsed time for attention_prob_times_values (64x2048x2048x584): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x584): 61.414
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 87.818
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0691
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 82.782
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0716
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 79.965

Attention duration (in seconds): 0.0786
Attention throughput (in TFLOP/s): 80.822
MLP duration (in seconds): 0.1407
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 82.915
Elapsed time for attention_key_query_prob (128x2048x292x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x292x2048): 55.574
Elapsed time for attention_prob_times_values (128x2048x2048x292): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x292): 48.963
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 87.727
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0692
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 82.712
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0717
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 79.813

Attention duration (in seconds): 0.0801
Attention throughput (in TFLOP/s): 79.255
MLP duration (in seconds): 0.1409
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 82.836
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 44.272
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 47.190
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 88.257
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0691
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 82.778
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0716
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 79.968

Attention duration (in seconds): 0.0817
Attention throughput (in TFLOP/s): 77.673
MLP duration (in seconds): 0.1407
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 82.875
Elapsed time for attention_key_query_prob (512x2048x73x2048): 0.0889
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x73x2048): 3.527
Elapsed time for attention_prob_times_values (512x2048x2048x73): 0.0669
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x73): 4.684
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 88.120
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0688
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 83.109
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0715
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 79.980

Attention duration (in seconds): 0.2238
Attention throughput (in TFLOP/s): 28.363
MLP duration (in seconds): 0.1404
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3642
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0544
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 81.083
Elapsed time for attention_key_query_prob (64x2048x592x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x592x2048): 61.474
Elapsed time for attention_prob_times_values (64x2048x2048x592): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x592): 62.958
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 86.778
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0727
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 80.903
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0736
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 79.837

Attention duration (in seconds): 0.0815
Attention throughput (in TFLOP/s): 79.901
MLP duration (in seconds): 0.1463
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0545
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 80.982
Elapsed time for attention_key_query_prob (128x2048x296x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x296x2048): 53.240
Elapsed time for attention_prob_times_values (128x2048x2048x296): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x296): 60.448
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 86.725
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0725
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 81.100
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0736
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 79.837

Attention duration (in seconds): 0.0826
Attention throughput (in TFLOP/s): 78.849
MLP duration (in seconds): 0.1461
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 80.720
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 44.350
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 47.086
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 87.060
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0724
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 81.218
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0736
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 79.873

Attention duration (in seconds): 0.0854
Attention throughput (in TFLOP/s): 76.265
MLP duration (in seconds): 0.1460
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 80.785
Elapsed time for attention_key_query_prob (512x2048x74x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x74x2048): 29.963
Elapsed time for attention_prob_times_values (512x2048x2048x74): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x74): 40.799
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 87.215
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0722
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 81.389
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0730
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 80.539

Attention duration (in seconds): 0.0898
Attention throughput (in TFLOP/s): 72.524
MLP duration (in seconds): 0.1452
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0551
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 82.174
Elapsed time for attention_key_query_prob (64x2048x600x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x600x2048): 60.359
Elapsed time for attention_prob_times_values (64x2048x2048x600): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x600): 62.940
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 89.063
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0734
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 82.338
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 79.341

Attention duration (in seconds): 0.0825
Attention throughput (in TFLOP/s): 80.986
MLP duration (in seconds): 0.1495
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2320
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0551
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 82.145
Elapsed time for attention_key_query_prob (128x2048x300x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x300x2048): 56.523
Elapsed time for attention_prob_times_values (128x2048x2048x300): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x300): 49.592
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 89.510
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 82.364
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 79.339

Attention duration (in seconds): 0.0842
Attention throughput (in TFLOP/s): 79.375
MLP duration (in seconds): 0.1495
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0552
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 82.135
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 44.924
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 48.329
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 89.505
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 82.442
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 79.338

Attention duration (in seconds): 0.0859
Attention throughput (in TFLOP/s): 77.851
MLP duration (in seconds): 0.1494
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2352
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0552
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 82.112
Elapsed time for attention_key_query_prob (512x2048x75x2048): 0.0870
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x75x2048): 3.703
Elapsed time for attention_prob_times_values (512x2048x2048x75): 0.0623
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x75): 5.169
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 89.295
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0731
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 82.572
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 79.342

Attention duration (in seconds): 0.2214
Attention throughput (in TFLOP/s): 30.193
MLP duration (in seconds): 0.1493
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3706
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0560
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 83.107
Elapsed time for attention_key_query_prob (64x2048x608x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x608x2048): 65.170
Elapsed time for attention_prob_times_values (64x2048x2048x608): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x608): 65.108
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 88.690
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0747
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 83.037
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0775
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 80.014

Attention duration (in seconds): 0.0835
Attention throughput (in TFLOP/s): 82.119
MLP duration (in seconds): 0.1522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2357
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0560
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 82.999
Elapsed time for attention_key_query_prob (128x2048x304x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x304x2048): 55.661
Elapsed time for attention_prob_times_values (128x2048x2048x304): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x304): 62.550
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 88.799
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0745
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 83.235
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0775
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 80.023

Attention duration (in seconds): 0.0846
Attention throughput (in TFLOP/s): 81.039
MLP duration (in seconds): 0.1520
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2366
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0561
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 82.856
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 46.079
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 49.655
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 88.939
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0744
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 83.314
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0772
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 80.300

Attention duration (in seconds): 0.0872
Attention throughput (in TFLOP/s): 78.583
MLP duration (in seconds): 0.1517
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2389
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0561
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 82.886
Elapsed time for attention_key_query_prob (512x2048x76x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x76x2048): 30.434
Elapsed time for attention_prob_times_values (512x2048x2048x76): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x76): 41.766
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 89.262
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0744
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 83.335
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0769
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 80.693

Attention duration (in seconds): 0.0920
Attention throughput (in TFLOP/s): 74.484
MLP duration (in seconds): 0.1513
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2433
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 81.306
Elapsed time for attention_key_query_prob (64x2048x616x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x616x2048): 59.303
Elapsed time for attention_prob_times_values (64x2048x2048x616): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x616): 64.090
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 89.272
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0768
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 82.936
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 83.691

Attention duration (in seconds): 0.0873
Attention throughput (in TFLOP/s): 80.510
MLP duration (in seconds): 0.1528
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2401
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 81.359
Elapsed time for attention_key_query_prob (128x2048x308x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x308x2048): 57.321
Elapsed time for attention_prob_times_values (128x2048x2048x308): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x308): 50.258
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 89.279
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0769
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 82.780
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 83.691

Attention duration (in seconds): 0.0889
Attention throughput (in TFLOP/s): 79.084
MLP duration (in seconds): 0.1530
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2418
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 81.364
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 45.717
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 49.538
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 89.754
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0767
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 82.956
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 83.695

Attention duration (in seconds): 0.0903
Attention throughput (in TFLOP/s): 77.804
MLP duration (in seconds): 0.1528
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2431
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 81.386
Elapsed time for attention_key_query_prob (512x2048x77x2048): 0.0896
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x77x2048): 3.690
Elapsed time for attention_prob_times_values (512x2048x2048x77): 0.0631
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x77): 5.242
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 89.533
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0766
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 83.091
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 83.697

Attention duration (in seconds): 0.2292
Attention throughput (in TFLOP/s): 30.668
MLP duration (in seconds): 0.1527
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3818
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0591
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 82.897
Elapsed time for attention_key_query_prob (64x2048x624x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x624x2048): 61.780
Elapsed time for attention_prob_times_values (64x2048x2048x624): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x624): 65.981
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 87.121
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0788
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 82.905
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 81.399

Attention duration (in seconds): 0.0883
Attention throughput (in TFLOP/s): 81.525
MLP duration (in seconds): 0.1591
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2474
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0591
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 82.847
Elapsed time for attention_key_query_prob (128x2048x312x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x312x2048): 55.248
Elapsed time for attention_prob_times_values (128x2048x2048x312): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x312): 63.149
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 87.217
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0786
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 83.087
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 81.397

Attention duration (in seconds): 0.0892
Attention throughput (in TFLOP/s): 80.718
MLP duration (in seconds): 0.1589
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2481
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0592
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 82.756
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 45.731
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 49.106
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 87.402
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0786
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 83.122
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 81.396

Attention duration (in seconds): 0.0920
Attention throughput (in TFLOP/s): 78.258
MLP duration (in seconds): 0.1588
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2509
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0593
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 82.660
Elapsed time for attention_key_query_prob (512x2048x78x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x78x2048): 31.272
Elapsed time for attention_prob_times_values (512x2048x2048x78): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x78): 42.928
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 87.685
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0784
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 83.311
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 81.400

Attention duration (in seconds): 0.0964
Attention throughput (in TFLOP/s): 74.705
MLP duration (in seconds): 0.1587
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0606
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 82.949
Elapsed time for attention_key_query_prob (64x2048x632x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x632x2048): 60.724
Elapsed time for attention_prob_times_values (64x2048x2048x632): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x632): 66.112
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0192
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 87.213
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0802
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 83.591
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0795
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 84.294

Attention duration (in seconds): 0.0905
Attention throughput (in TFLOP/s): 81.527
MLP duration (in seconds): 0.1597
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2502
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0606
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 82.959
Elapsed time for attention_key_query_prob (128x2048x316x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x316x2048): 57.990
Elapsed time for attention_prob_times_values (128x2048x2048x316): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x316): 51.956
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 86.940
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0802
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 83.579
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0795
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 84.295

Attention duration (in seconds): 0.0922
Attention throughput (in TFLOP/s): 80.012
MLP duration (in seconds): 0.1597
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2519
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0604
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 83.240
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 46.569
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 50.674
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0192
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 87.272
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0801
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 83.653
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0795
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 84.294

Attention duration (in seconds): 0.0936
Attention throughput (in TFLOP/s): 78.881
MLP duration (in seconds): 0.1596
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2532
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0604
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 83.266
Elapsed time for attention_key_query_prob (512x2048x79x2048): 0.0929
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x79x2048): 3.652
Elapsed time for attention_prob_times_values (512x2048x2048x79): 0.0615
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x79): 5.517
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0192
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 87.384
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0799
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 83.832
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0795
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 84.300

Attention duration (in seconds): 0.2339
Attention throughput (in TFLOP/s): 31.546
MLP duration (in seconds): 0.1594
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3934
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 81.927
Elapsed time for attention_key_query_prob (64x2048x640x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x640x2048): 63.462
Elapsed time for attention_prob_times_values (64x2048x2048x640): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x640): 68.062
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 86.673
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0838
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 82.037
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 80.093

Attention duration (in seconds): 0.0932
Attention throughput (in TFLOP/s): 81.113
MLP duration (in seconds): 0.1696
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2628
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0630
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 81.835
Elapsed time for attention_key_query_prob (128x2048x320x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x320x2048): 58.219
Elapsed time for attention_prob_times_values (128x2048x2048x320): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x320): 66.229
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 86.673
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0838
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 81.969
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 80.094

Attention duration (in seconds): 0.0939
Attention throughput (in TFLOP/s): 80.509
MLP duration (in seconds): 0.1696
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0628
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 82.042
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 49.958
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 53.566
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 86.890
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0838
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 81.987
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 80.095

Attention duration (in seconds): 0.0959
Attention throughput (in TFLOP/s): 78.836
MLP duration (in seconds): 0.1696
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2655
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 81.992
Elapsed time for attention_key_query_prob (512x2048x80x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x80x2048): 34.790
Elapsed time for attention_prob_times_values (512x2048x2048x80): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x80): 39.247
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 87.051
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0835
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 82.271
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0853
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 80.598

Attention duration (in seconds): 0.1012
Attention throughput (in TFLOP/s): 74.676
MLP duration (in seconds): 0.1688
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2700
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0636
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 83.112
Elapsed time for attention_key_query_prob (64x2048x648x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x648x2048): 59.880
Elapsed time for attention_prob_times_values (64x2048x2048x648): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x648): 62.031
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 86.917
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0850
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 82.904
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0842
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 83.661

Attention duration (in seconds): 0.0953
Attention throughput (in TFLOP/s): 81.264
MLP duration (in seconds): 0.1692
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2644
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0636
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 83.085
Elapsed time for attention_key_query_prob (128x2048x324x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x324x2048): 56.874
Elapsed time for attention_prob_times_values (128x2048x2048x324): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x324): 52.708
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 87.239
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0850
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 82.875
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0842
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 83.660

Attention duration (in seconds): 0.0965
Attention throughput (in TFLOP/s): 80.215
MLP duration (in seconds): 0.1692
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2657
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0637
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 82.957
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 44.710
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 51.614
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 87.365
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0849
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 82.951
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0842
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 83.662

Attention duration (in seconds): 0.0984
Attention throughput (in TFLOP/s): 78.687
MLP duration (in seconds): 0.1691
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2675
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0636
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 83.036
Elapsed time for attention_key_query_prob (512x2048x81x2048): 0.0942
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x81x2048): 3.693
Elapsed time for attention_prob_times_values (512x2048x2048x81): 0.0750
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x81): 4.636
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 87.238
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0846
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 83.238
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0842
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 83.656

Attention duration (in seconds): 0.2531
Attention throughput (in TFLOP/s): 30.585
MLP duration (in seconds): 0.1688
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0647
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 83.715
Elapsed time for attention_key_query_prob (64x2048x656x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x656x2048): 62.250
Elapsed time for attention_prob_times_values (64x2048x2048x656): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x656): 63.529
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 87.174
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0867
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 83.247
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.1106
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 65.282

Attention duration (in seconds): 0.0966
Attention throughput (in TFLOP/s): 82.041
MLP duration (in seconds): 0.1973
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2939
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0648
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 83.548
Elapsed time for attention_key_query_prob (128x2048x328x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x328x2048): 54.173
Elapsed time for attention_prob_times_values (128x2048x2048x328): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x328): 56.545
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 87.332
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0868
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 83.225
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.1107
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 65.248

Attention duration (in seconds): 0.0982
Attention throughput (in TFLOP/s): 80.687
MLP duration (in seconds): 0.1974
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2956
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0649
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 83.453
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 44.794
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 50.902
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 87.399
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0867
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 83.265
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.1106
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 65.252

Attention duration (in seconds): 0.1003
Attention throughput (in TFLOP/s): 78.991
MLP duration (in seconds): 0.1974
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2977
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0648
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 83.522
Elapsed time for attention_key_query_prob (512x2048x82x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x82x2048): 32.614
Elapsed time for attention_prob_times_values (512x2048x2048x82): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x82): 44.873
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 87.765
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0863
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 83.666
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.1107
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 65.227

Attention duration (in seconds): 0.1040
Attention throughput (in TFLOP/s): 76.161
MLP duration (in seconds): 0.1970
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0665
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 83.411
Elapsed time for attention_key_query_prob (64x2048x664x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x664x2048): 61.291
Elapsed time for attention_prob_times_values (64x2048x2048x664): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x664): 63.035
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 87.163
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0886
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 83.461
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.1140
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 64.884

Attention duration (in seconds): 0.0992
Attention throughput (in TFLOP/s): 81.755
MLP duration (in seconds): 0.2026
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0666
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 83.239
Elapsed time for attention_key_query_prob (128x2048x332x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x332x2048): 58.025
Elapsed time for attention_prob_times_values (128x2048x2048x332): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x332): 53.643
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 87.199
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0886
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 83.495
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.1141
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 64.841

Attention duration (in seconds): 0.1006
Attention throughput (in TFLOP/s): 80.580
MLP duration (in seconds): 0.2027
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0665
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 83.380
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 45.559
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 52.524
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 87.354
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0885
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 83.598
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.1141
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 64.849

Attention duration (in seconds): 0.1023
Attention throughput (in TFLOP/s): 79.263
MLP duration (in seconds): 0.2025
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0665
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 83.421
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0938
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 3.801
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0783
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 4.550
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 87.712
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0883
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 83.794
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.1141
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 64.834

Attention duration (in seconds): 0.2597
Attention throughput (in TFLOP/s): 31.227
MLP duration (in seconds): 0.2024
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4621
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0681
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 83.424
Elapsed time for attention_key_query_prob (64x2048x672x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x672x2048): 65.660
Elapsed time for attention_prob_times_values (64x2048x2048x672): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x672): 64.922
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 86.929
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0887
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 85.449
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.1256
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 60.337

Attention duration (in seconds): 0.1010
Attention throughput (in TFLOP/s): 82.195
MLP duration (in seconds): 0.2142
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 83.278
Elapsed time for attention_key_query_prob (128x2048x336x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x336x2048): 56.425
Elapsed time for attention_prob_times_values (128x2048x2048x336): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x336): 58.613
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 87.043
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0886
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 85.518
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.1256
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 60.318

Attention duration (in seconds): 0.1025
Attention throughput (in TFLOP/s): 80.922
MLP duration (in seconds): 0.2142
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 83.276
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 45.470
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 54.436
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0217
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 87.259
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0885
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 85.654
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.1254
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 60.415

Attention duration (in seconds): 0.1045
Attention throughput (in TFLOP/s): 79.404
MLP duration (in seconds): 0.2139
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0681
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 83.384
Elapsed time for attention_key_query_prob (512x2048x84x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x84x2048): 33.141
Elapsed time for attention_prob_times_values (512x2048x2048x84): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x84): 45.797
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 87.561
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0886
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 85.542
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.1254
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 60.416

Attention duration (in seconds): 0.1085
Attention throughput (in TFLOP/s): 76.449
MLP duration (in seconds): 0.2140
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0699
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 83.223
Elapsed time for attention_key_query_prob (64x2048x680x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x680x2048): 60.833
Elapsed time for attention_prob_times_values (64x2048x2048x680): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x680): 64.367
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0222
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 87.468
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0933
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 83.158
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.1229
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 63.101

Attention duration (in seconds): 0.1038
Attention throughput (in TFLOP/s): 81.805
MLP duration (in seconds): 0.2162
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0700
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 83.174
Elapsed time for attention_key_query_prob (128x2048x340x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x340x2048): 59.052
Elapsed time for attention_prob_times_values (128x2048x2048x340): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x340): 54.871
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0222
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 87.435
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0934
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 83.097
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.1229
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 63.145

Attention duration (in seconds): 0.1050
Attention throughput (in TFLOP/s): 80.860
MLP duration (in seconds): 0.2162
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0700
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 83.167
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 46.556
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 53.721
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 87.800
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0932
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 83.204
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.1228
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 63.152

Attention duration (in seconds): 0.1067
Attention throughput (in TFLOP/s): 79.560
MLP duration (in seconds): 0.2161
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0699
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 83.199
Elapsed time for attention_key_query_prob (512x2048x85x2048): 0.0955
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x85x2048): 3.822
Elapsed time for attention_prob_times_values (512x2048x2048x85): 0.0800
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x85): 4.561
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 87.799
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0929
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 83.520
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.1229
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 63.107

Attention duration (in seconds): 0.2676
Attention throughput (in TFLOP/s): 31.721
MLP duration (in seconds): 0.2158
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4834
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0684
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 87.074
Elapsed time for attention_key_query_prob (64x2048x688x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x688x2048): 62.954
Elapsed time for attention_prob_times_values (64x2048x2048x688): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x688): 65.807
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 87.812
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0945
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 84.017
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.1267
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 62.665

Attention duration (in seconds): 0.1025
Attention throughput (in TFLOP/s): 84.692
MLP duration (in seconds): 0.2212
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0684
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 87.030
Elapsed time for attention_key_query_prob (128x2048x344x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x344x2048): 56.379
Elapsed time for attention_prob_times_values (128x2048x2048x344): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x344): 59.435
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 87.989
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0945
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 84.028
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.1268
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 62.629

Attention duration (in seconds): 0.1038
Attention throughput (in TFLOP/s): 83.650
MLP duration (in seconds): 0.2213
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0684
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 87.036
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 46.542
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 53.156
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 88.178
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0944
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 84.144
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.1268
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 62.614

Attention duration (in seconds): 0.1058
Attention throughput (in TFLOP/s): 82.018
MLP duration (in seconds): 0.2212
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0684
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 87.039
Elapsed time for attention_key_query_prob (512x2048x86x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x86x2048): 33.957
Elapsed time for attention_prob_times_values (512x2048x2048x86): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x86): 46.748
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 88.510
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0944
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 84.150
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.1268
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 62.643

Attention duration (in seconds): 0.1096
Attention throughput (in TFLOP/s): 79.170
MLP duration (in seconds): 0.2211
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0713
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 85.474
Elapsed time for attention_key_query_prob (64x2048x696x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x696x2048): 61.422
Elapsed time for attention_prob_times_values (64x2048x2048x696): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x696): 65.603
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 87.666
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0933
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 87.092
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.1230
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 66.064

Attention duration (in seconds): 0.1063
Attention throughput (in TFLOP/s): 83.510
MLP duration (in seconds): 0.2163
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0713
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 85.430
Elapsed time for attention_key_query_prob (128x2048x348x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x348x2048): 59.678
Elapsed time for attention_prob_times_values (128x2048x2048x348): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x348): 55.177
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 87.723
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0933
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 87.092
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.1230
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 66.085

Attention duration (in seconds): 0.1075
Attention throughput (in TFLOP/s): 82.519
MLP duration (in seconds): 0.2163
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0714
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 85.398
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 47.178
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 54.814
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 87.917
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0932
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 87.246
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.1229
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 66.148

Attention duration (in seconds): 0.1092
Attention throughput (in TFLOP/s): 81.251
MLP duration (in seconds): 0.2160
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0713
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 85.442
Elapsed time for attention_key_query_prob (512x2048x87x2048): 0.0981
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x87x2048): 3.811
Elapsed time for attention_prob_times_values (512x2048x2048x87): 0.0784
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x87): 4.768
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 87.990
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0928
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 87.590
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.1231
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 65.994

Attention duration (in seconds): 0.2709
Attention throughput (in TFLOP/s): 32.765
MLP duration (in seconds): 0.2159
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4868
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0745
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 83.717
Elapsed time for attention_key_query_prob (64x2048x704x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x704x2048): 65.202
Elapsed time for attention_prob_times_values (64x2048x2048x704): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x704): 68.701
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 87.393
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0993
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 83.717
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.1440
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 57.728

Attention duration (in seconds): 0.1096
Attention throughput (in TFLOP/s): 82.782
MLP duration (in seconds): 0.2434
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3529
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0745
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 83.690
Elapsed time for attention_key_query_prob (128x2048x352x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x352x2048): 60.419
Elapsed time for attention_prob_times_values (128x2048x2048x352): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x352): 61.328
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 87.326
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0993
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 83.734
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.1439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 57.789

Attention duration (in seconds): 0.1107
Attention throughput (in TFLOP/s): 81.913
MLP duration (in seconds): 0.2432
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3539
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0745
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 83.715
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 48.350
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 57.564
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 87.613
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0994
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 83.665
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.1440
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 57.748

Attention duration (in seconds): 0.1126
Attention throughput (in TFLOP/s): 80.557
MLP duration (in seconds): 0.2434
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3560
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0744
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 83.796
Elapsed time for attention_key_query_prob (512x2048x88x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x88x2048): 37.397
Elapsed time for attention_prob_times_values (512x2048x2048x88): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x88): 41.963
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 87.703
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0992
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 83.823
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.1440
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 57.753

Attention duration (in seconds): 0.1172
Attention throughput (in TFLOP/s): 77.372
MLP duration (in seconds): 0.2432
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3604
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0760
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 83.947
Elapsed time for attention_key_query_prob (64x2048x712x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x712x2048): 60.806
Elapsed time for attention_prob_times_values (64x2048x2048x712): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x712): 62.346
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 86.739
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.1047
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 81.259
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.1385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 61.414

Attention duration (in seconds): 0.1129
Attention throughput (in TFLOP/s): 82.092
MLP duration (in seconds): 0.2432
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3561
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0762
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 83.759
Elapsed time for attention_key_query_prob (128x2048x356x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x356x2048): 58.189
Elapsed time for attention_prob_times_values (128x2048x2048x356): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x356): 56.394
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 86.716
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.1046
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 81.277
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.1385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 61.418

Attention duration (in seconds): 0.1140
Attention throughput (in TFLOP/s): 81.295
MLP duration (in seconds): 0.2431
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3571
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0761
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 83.838
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 47.725
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 55.479
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 87.063
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.1045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 81.391
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.1385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 61.419

Attention duration (in seconds): 0.1154
Attention throughput (in TFLOP/s): 80.321
MLP duration (in seconds): 0.2430
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3584
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0761
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 83.850
Elapsed time for attention_key_query_prob (512x2048x89x2048): 0.0995
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x89x2048): 3.842
Elapsed time for attention_prob_times_values (512x2048x2048x89): 0.0792
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x89): 4.827
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 87.015
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.1045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 81.363
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.1385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 61.392

Attention duration (in seconds): 0.2792
Attention throughput (in TFLOP/s): 33.201
MLP duration (in seconds): 0.2431
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0781
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 83.500
Elapsed time for attention_key_query_prob (64x2048x720x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x720x2048): 62.899
Elapsed time for attention_prob_times_values (64x2048x2048x720): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x720): 63.836
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 86.929
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.1038
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 83.791
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.1395
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 62.332

Attention duration (in seconds): 0.1153
Attention throughput (in TFLOP/s): 82.113
MLP duration (in seconds): 0.2433
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3587
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0783
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 83.339
Elapsed time for attention_key_query_prob (128x2048x360x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x360x2048): 55.079
Elapsed time for attention_prob_times_values (128x2048x2048x360): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x360): 61.156
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 87.007
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.1037
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 83.882
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.1396
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 62.316

Attention duration (in seconds): 0.1166
Attention throughput (in TFLOP/s): 81.221
MLP duration (in seconds): 0.2433
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3599
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0783
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 83.335
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 47.778
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 55.216
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0249
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 87.221
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.1036
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 83.936
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.1398
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 62.225

Attention duration (in seconds): 0.1183
Attention throughput (in TFLOP/s): 80.058
MLP duration (in seconds): 0.2434
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3617
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0781
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 83.519
Elapsed time for attention_key_query_prob (512x2048x90x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x90x2048): 35.134
Elapsed time for attention_prob_times_values (512x2048x2048x90): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x90): 48.868
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0249
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 87.498
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.1038
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 83.803
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.1395
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 62.338

Attention duration (in seconds): 0.1219
Attention throughput (in TFLOP/s): 77.713
MLP duration (in seconds): 0.2433
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3652
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0765
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 87.217
Elapsed time for attention_key_query_prob (64x2048x728x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x728x2048): 61.709
Elapsed time for attention_prob_times_values (64x2048x2048x728): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x728): 63.510
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 84.171
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.1057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 84.108
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.1363
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 65.216

Attention duration (in seconds): 0.1154
Attention throughput (in TFLOP/s): 83.854
MLP duration (in seconds): 0.2421
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3574
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0765
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 87.140
Elapsed time for attention_key_query_prob (128x2048x364x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x364x2048): 58.991
Elapsed time for attention_prob_times_values (128x2048x2048x364): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x364): 57.518
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 84.004
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.1057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 84.142
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.1363
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 65.244

Attention duration (in seconds): 0.1164
Attention throughput (in TFLOP/s): 83.096
MLP duration (in seconds): 0.2420
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3584
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0765
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 87.161
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 48.506
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 56.602
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 84.387
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.1054
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 84.338
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.1364
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 65.206

Attention duration (in seconds): 0.1178
Attention throughput (in TFLOP/s): 82.106
MLP duration (in seconds): 0.2418
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3596
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0766
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 87.033
Elapsed time for attention_key_query_prob (512x2048x91x2048): 0.0998
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x91x2048): 3.918
Elapsed time for attention_prob_times_values (512x2048x2048x91): 0.0813
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x91): 4.808
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 84.554
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.1055
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 84.313
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.1364
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 65.174

Attention duration (in seconds): 0.2840
Attention throughput (in TFLOP/s): 34.066
MLP duration (in seconds): 0.2419
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0809
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 84.218
Elapsed time for attention_key_query_prob (64x2048x736x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x736x2048): 68.538
Elapsed time for attention_prob_times_values (64x2048x2048x736): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x736): 67.131
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 84.027
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.1009
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 90.082
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.1407
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 64.596

Attention duration (in seconds): 0.1196
Attention throughput (in TFLOP/s): 82.579
MLP duration (in seconds): 0.2416
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3612
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0819
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 83.203
Elapsed time for attention_key_query_prob (128x2048x368x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x368x2048): 59.038
Elapsed time for attention_prob_times_values (128x2048x2048x368): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x368): 64.375
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 83.637
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.1011
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 89.925
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.1407
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 64.582

Attention duration (in seconds): 0.1219
Attention throughput (in TFLOP/s): 81.025
MLP duration (in seconds): 0.2418
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3637
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0819
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 83.231
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 49.555
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 59.970
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 83.932
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.1011
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 89.935
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.1408
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 64.568

Attention duration (in seconds): 0.1235
Attention throughput (in TFLOP/s): 79.970
MLP duration (in seconds): 0.2418
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3653
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0819
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 83.260
Elapsed time for attention_key_query_prob (512x2048x92x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x92x2048): 35.963
Elapsed time for attention_prob_times_values (512x2048x2048x92): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x92): 49.786
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 83.984
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.1010
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 90.008
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.1406
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 64.633

Attention duration (in seconds): 0.1278
Attention throughput (in TFLOP/s): 77.270
MLP duration (in seconds): 0.2416
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3694
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0811
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 85.926
Elapsed time for attention_key_query_prob (64x2048x744x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x744x2048): 62.471
Elapsed time for attention_prob_times_values (64x2048x2048x744): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x744): 66.005
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 89.337
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.1079
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 86.077
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.1398
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 66.412

Attention duration (in seconds): 0.1195
Attention throughput (in TFLOP/s): 84.404
MLP duration (in seconds): 0.2477
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3672
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0812
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 85.778
Elapsed time for attention_key_query_prob (128x2048x372x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x372x2048): 60.888
Elapsed time for attention_prob_times_values (128x2048x2048x372): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x372): 60.175
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 89.337
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.1079
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 86.053
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.1401
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 66.299

Attention duration (in seconds): 0.1204
Attention throughput (in TFLOP/s): 83.778
MLP duration (in seconds): 0.2480
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3684
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0811
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 85.847
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 50.157
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 58.706
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 89.312
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.1079
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 86.069
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.1399
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 66.363

Attention duration (in seconds): 0.1219
Attention throughput (in TFLOP/s): 82.740
MLP duration (in seconds): 0.2478
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3697
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0812
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 85.796
Elapsed time for attention_key_query_prob (512x2048x93x2048): 0.1009
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x93x2048): 3.960
Elapsed time for attention_prob_times_values (512x2048x2048x93): 0.0801
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x93): 4.987
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 89.356
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.1079
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 86.063
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.1399
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 66.397

Attention duration (in seconds): 0.2881
Attention throughput (in TFLOP/s): 35.006
MLP duration (in seconds): 0.2478
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5359
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0824
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 86.334
Elapsed time for attention_key_query_prob (64x2048x752x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x752x2048): 64.558
Elapsed time for attention_prob_times_values (64x2048x2048x752): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x752): 67.303
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 89.752
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.1102
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 86.090
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.1532
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 61.933

Attention duration (in seconds): 0.1211
Attention throughput (in TFLOP/s): 85.012
MLP duration (in seconds): 0.2634
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3845
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0824
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 86.317
Elapsed time for attention_key_query_prob (128x2048x376x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x376x2048): 58.153
Elapsed time for attention_prob_times_values (128x2048x2048x376): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x376): 65.167
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 89.728
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.1102
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 86.114
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.1533
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 61.872

Attention duration (in seconds): 0.1220
Attention throughput (in TFLOP/s): 84.380
MLP duration (in seconds): 0.2635
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3855
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0825
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 86.287
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 49.965
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 57.944
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 89.717
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.1102
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 86.104
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.1535
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 61.801

Attention duration (in seconds): 0.1240
Attention throughput (in TFLOP/s): 83.058
MLP duration (in seconds): 0.2637
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3877
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0825
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 86.270
Elapsed time for attention_key_query_prob (512x2048x94x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x94x2048): 36.905
Elapsed time for attention_prob_times_values (512x2048x2048x94): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x94): 51.002
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 89.728
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.1101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 86.173
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.1532
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 61.923

Attention duration (in seconds): 0.1278
Attention throughput (in TFLOP/s): 80.574
MLP duration (in seconds): 0.2633
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3911
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0845
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 85.978
Elapsed time for attention_key_query_prob (64x2048x760x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x760x2048): 62.962
Elapsed time for attention_prob_times_values (64x2048x2048x760): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x760): 67.131
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 82.861
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.1084
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 89.366
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.1511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 64.140

Attention duration (in seconds): 0.1263
Attention throughput (in TFLOP/s): 83.169
MLP duration (in seconds): 0.2595
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3858
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0844
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 86.112
Elapsed time for attention_key_query_prob (128x2048x380x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x380x2048): 61.438
Elapsed time for attention_prob_times_values (128x2048x2048x380): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x380): 61.629
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0293
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 82.731
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.1084
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 89.371
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.1513
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 64.039

Attention duration (in seconds): 0.1269
Attention throughput (in TFLOP/s): 82.764
MLP duration (in seconds): 0.2598
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3867
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0844
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 86.115
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 50.610
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 59.884
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 82.969
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.1084
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 89.371
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.1512
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 64.109

Attention duration (in seconds): 0.1285
Attention throughput (in TFLOP/s): 81.781
MLP duration (in seconds): 0.2596
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3881
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0844
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 86.108
Elapsed time for attention_key_query_prob (512x2048x95x2048): 0.1004
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x95x2048): 4.065
Elapsed time for attention_prob_times_values (512x2048x2048x95): 0.0817
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x95): 4.994
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 83.060
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.1084
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 89.366
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.1513
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 64.045

Attention duration (in seconds): 0.2957
Attention throughput (in TFLOP/s): 35.537
MLP duration (in seconds): 0.2597
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5554
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0848
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 87.556
Elapsed time for attention_key_query_prob (64x2048x768x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x768x2048): 67.075
Elapsed time for attention_prob_times_values (64x2048x2048x768): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x768): 69.809
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 85.561
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.1190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 83.179
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.1690
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 58.549

Attention duration (in seconds): 0.1257
Attention throughput (in TFLOP/s): 85.262
MLP duration (in seconds): 0.2880
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0848
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 87.553
Elapsed time for attention_key_query_prob (128x2048x384x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x384x2048): 61.914
Elapsed time for attention_prob_times_values (128x2048x2048x384): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x384): 68.944
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 85.560
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.1191
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 83.117
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.1690
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 58.550

Attention duration (in seconds): 0.1263
Attention throughput (in TFLOP/s): 84.864
MLP duration (in seconds): 0.2881
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0847
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 87.580
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 52.902
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 64.795
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 85.563
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.1191
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 83.113
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.1689
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 58.582

Attention duration (in seconds): 0.1278
Attention throughput (in TFLOP/s): 83.875
MLP duration (in seconds): 0.2880
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0848
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 87.536
Elapsed time for attention_key_query_prob (512x2048x96x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x96x2048): 42.058
Elapsed time for attention_prob_times_values (512x2048x2048x96): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x96): 46.585
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0288
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 85.771
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.1188
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 83.288
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.1689
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 58.582

Attention duration (in seconds): 0.1323
Attention throughput (in TFLOP/s): 81.041
MLP duration (in seconds): 0.2877
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0887
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 85.432
Elapsed time for attention_key_query_prob (64x2048x776x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x776x2048): 61.745
Elapsed time for attention_prob_times_values (64x2048x2048x776): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x776): 64.056
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 85.003
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.1133
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 89.145
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.1571
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 64.290

Attention duration (in seconds): 0.1317
Attention throughput (in TFLOP/s): 83.065
MLP duration (in seconds): 0.2705
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0887
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 85.429
Elapsed time for attention_key_query_prob (128x2048x388x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x388x2048): 60.482
Elapsed time for attention_prob_times_values (128x2048x2048x388): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x388): 62.119
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0296
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 85.192
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.1133
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 89.187
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.1572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 64.285

Attention duration (in seconds): 0.1319
Attention throughput (in TFLOP/s): 82.888
MLP duration (in seconds): 0.2704
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0886
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 85.521
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 49.095
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 60.686
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0296
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 85.322
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.1131
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 89.360
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.1573
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 64.237

Attention duration (in seconds): 0.1336
Attention throughput (in TFLOP/s): 81.886
MLP duration (in seconds): 0.2703
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0887
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 85.438
Elapsed time for attention_key_query_prob (512x2048x97x2048): 0.1098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x97x2048): 3.794
Elapsed time for attention_prob_times_values (512x2048x2048x97): 0.0921
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x97): 4.522
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0296
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 85.428
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.1129
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 89.518
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.1570
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 64.335

Attention duration (in seconds): 0.3202
Attention throughput (in TFLOP/s): 34.154
MLP duration (in seconds): 0.2699
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5901
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0895
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 86.458
Elapsed time for attention_key_query_prob (64x2048x784x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x784x2048): 64.714
Elapsed time for attention_prob_times_values (64x2048x2048x784): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x784): 65.436
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 89.689
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.1189
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 86.698
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.1613
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 63.945

Attention duration (in seconds): 0.1311
Attention throughput (in TFLOP/s): 85.057
MLP duration (in seconds): 0.2802
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0893
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 86.610
Elapsed time for attention_key_query_prob (128x2048x392x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x392x2048): 57.239
Elapsed time for attention_prob_times_values (128x2048x2048x392): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x392): 59.769
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 89.701
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.1189
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 86.743
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.1614
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 63.876

Attention duration (in seconds): 0.1324
Attention throughput (in TFLOP/s): 84.222
MLP duration (in seconds): 0.2803
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0894
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 86.468
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 48.913
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 59.608
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 89.866
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.1189
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 86.744
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.1614
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 63.892

Attention duration (in seconds): 0.1338
Attention throughput (in TFLOP/s): 83.364
MLP duration (in seconds): 0.2803
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0895
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 86.457
Elapsed time for attention_key_query_prob (512x2048x98x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x98x2048): 35.443
Elapsed time for attention_prob_times_values (512x2048x2048x98): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x98): 53.039
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 89.960
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.1189
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 86.712
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.1613
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 63.935

Attention duration (in seconds): 0.1379
Attention throughput (in TFLOP/s): 80.870
MLP duration (in seconds): 0.2802
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0915
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 86.285
Elapsed time for attention_key_query_prob (64x2048x792x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x792x2048): 62.598
Elapsed time for attention_prob_times_values (64x2048x2048x792): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x792): 65.203
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0295
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 89.251
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.1196
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 87.995
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.1581
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 66.573

Attention duration (in seconds): 0.1343
Attention throughput (in TFLOP/s): 84.714
MLP duration (in seconds): 0.2777
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0915
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 86.213
Elapsed time for attention_key_query_prob (128x2048x396x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x396x2048): 61.129
Elapsed time for attention_prob_times_values (128x2048x2048x396): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x396): 62.649
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0295
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 89.244
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.1194
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 88.133
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.1581
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 66.570

Attention duration (in seconds): 0.1348
Attention throughput (in TFLOP/s): 84.395
MLP duration (in seconds): 0.2775
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0916
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 86.205
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 49.658
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 61.440
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0294
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 89.342
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.1195
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 88.038
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.1581
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 66.546

Attention duration (in seconds): 0.1365
Attention throughput (in TFLOP/s): 83.333
MLP duration (in seconds): 0.2777
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0916
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 86.206
Elapsed time for attention_key_query_prob (512x2048x99x2048): 0.1135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x99x2048): 3.746
Elapsed time for attention_prob_times_values (512x2048x2048x99): 0.0920
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x99): 4.621
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0295
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 89.292
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.1195
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 88.084
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.1581
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 66.557

Attention duration (in seconds): 0.3265
Attention throughput (in TFLOP/s): 34.831
MLP duration (in seconds): 0.2776
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0935
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 86.110
Elapsed time for attention_key_query_prob (64x2048x800x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x800x2048): 67.843
Elapsed time for attention_prob_times_values (64x2048x2048x800): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x800): 66.525
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 85.713
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.1233
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 87.108
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.1728
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 62.123

Attention duration (in seconds): 0.1376
Attention throughput (in TFLOP/s): 84.261
MLP duration (in seconds): 0.2961
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0935
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 86.119
Elapsed time for attention_key_query_prob (128x2048x400x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x400x2048): 59.807
Elapsed time for attention_prob_times_values (128x2048x2048x400): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x400): 61.138
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 85.948
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.1233
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 87.063
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.1729
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 62.115

Attention duration (in seconds): 0.1389
Attention throughput (in TFLOP/s): 83.458
MLP duration (in seconds): 0.2962
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0934
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 86.187
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 49.119
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 51.922
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 86.071
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.1233
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 87.065
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.1728
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 62.146

Attention duration (in seconds): 0.1416
Attention throughput (in TFLOP/s): 81.872
MLP duration (in seconds): 0.2961
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4377
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0934
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 86.194
Elapsed time for attention_key_query_prob (512x2048x100x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x100x2048): 35.605
Elapsed time for attention_prob_times_values (512x2048x2048x100): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x100): 53.935
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 86.168
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.1232
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 87.140
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.1727
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 62.157

Attention duration (in seconds): 0.1446
Attention throughput (in TFLOP/s): 80.192
MLP duration (in seconds): 0.2960
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4406
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0921
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 89.211
Elapsed time for attention_key_query_prob (64x2048x808x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x808x2048): 61.844
Elapsed time for attention_prob_times_values (64x2048x2048x808): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x808): 65.689
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0328
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 83.499
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.1262
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 86.772
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.1684
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 65.038

Attention duration (in seconds): 0.1385
Attention throughput (in TFLOP/s): 85.351
MLP duration (in seconds): 0.2946
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4331
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0920
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 89.259
Elapsed time for attention_key_query_prob (128x2048x404x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x404x2048): 62.178
Elapsed time for attention_prob_times_values (128x2048x2048x404): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x404): 63.666
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 83.613
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.1261
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 86.872
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.1682
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 65.107

Attention duration (in seconds): 0.1386
Attention throughput (in TFLOP/s): 85.303
MLP duration (in seconds): 0.2943
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4329
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0921
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 89.241
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 50.407
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 62.368
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 83.737
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.1260
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 86.923
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.1683
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 65.076

Attention duration (in seconds): 0.1403
Attention throughput (in TFLOP/s): 84.245
MLP duration (in seconds): 0.2943
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0920
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 89.291
Elapsed time for attention_key_query_prob (512x2048x101x2048): 0.1163
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x101x2048): 3.729
Elapsed time for attention_prob_times_values (512x2048x2048x101): 0.0936
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x101): 4.635
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 84.123
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.1260
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 86.960
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.1684
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 65.042

Attention duration (in seconds): 0.3345
Attention throughput (in TFLOP/s): 35.342
MLP duration (in seconds): 0.2944
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0959
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 87.403
Elapsed time for attention_key_query_prob (64x2048x816x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x816x2048): 64.761
Elapsed time for attention_prob_times_values (64x2048x2048x816): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x816): 67.326
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 87.624
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.1257
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 88.903
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.1804
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 61.934

Attention duration (in seconds): 0.1410
Attention throughput (in TFLOP/s): 85.440
MLP duration (in seconds): 0.3060
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4470
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0958
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 87.418
Elapsed time for attention_key_query_prob (128x2048x408x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x408x2048): 59.373
Elapsed time for attention_prob_times_values (128x2048x2048x408): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x408): 61.602
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0318
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 87.734
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.1257
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 88.902
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.1803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 61.969

Attention duration (in seconds): 0.1422
Attention throughput (in TFLOP/s): 84.742
MLP duration (in seconds): 0.3059
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4481
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0957
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 87.505
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 50.527
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 61.378
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0318
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 87.735
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.1257
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 88.903
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.1802
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 61.980

Attention duration (in seconds): 0.1434
Attention throughput (in TFLOP/s): 84.020
MLP duration (in seconds): 0.3059
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4493
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0958
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 87.420
Elapsed time for attention_key_query_prob (512x2048x102x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x102x2048): 36.320
Elapsed time for attention_prob_times_values (512x2048x2048x102): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x102): 54.729
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0317
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 88.082
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.1256
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 88.912
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.1803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 61.952

Attention duration (in seconds): 0.1476
Attention throughput (in TFLOP/s): 81.614
MLP duration (in seconds): 0.3060
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4536
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0985
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 86.719
Elapsed time for attention_key_query_prob (64x2048x824x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x824x2048): 63.180
Elapsed time for attention_prob_times_values (64x2048x2048x824): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x824): 67.031
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0330
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 86.228
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.1315
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 86.644
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.1911
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 59.621

Attention duration (in seconds): 0.1451
Attention throughput (in TFLOP/s): 84.577
MLP duration (in seconds): 0.3225
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4677
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0985
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 86.760
Elapsed time for attention_key_query_prob (128x2048x412x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x412x2048): 62.340
Elapsed time for attention_prob_times_values (128x2048x2048x412): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x412): 64.273
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0330
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 86.316
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.1314
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 86.719
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.1912
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 59.587

Attention duration (in seconds): 0.1454
Attention throughput (in TFLOP/s): 84.403
MLP duration (in seconds): 0.3225
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4680
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0986
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 86.646
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 51.074
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 63.614
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 86.440
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.1314
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 86.703
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.1911
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 59.612

Attention duration (in seconds): 0.1472
Attention throughput (in TFLOP/s): 83.418
MLP duration (in seconds): 0.3225
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4696
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0985
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 86.706
Elapsed time for attention_key_query_prob (512x2048x103x2048): 0.1210
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x103x2048): 3.655
Elapsed time for attention_prob_times_values (512x2048x2048x103): 0.0939
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x103): 4.710
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 86.587
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.1313
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 86.772
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.1911
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 59.616

Attention duration (in seconds): 0.3464
Attention throughput (in TFLOP/s): 35.442
MLP duration (in seconds): 0.3224
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.1018
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 85.555
Elapsed time for attention_key_query_prob (64x2048x832x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x832x2048): 67.119
Elapsed time for attention_prob_times_values (64x2048x2048x832): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x832): 69.841
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0337
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 86.165
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.1401
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 82.921
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.2015
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 57.624

Attention duration (in seconds): 0.1486
Attention throughput (in TFLOP/s): 84.191
MLP duration (in seconds): 0.3416
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4902
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.1016
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 85.697
Elapsed time for attention_key_query_prob (128x2048x416x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x416x2048): 63.580
Elapsed time for attention_prob_times_values (128x2048x2048x416): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x416): 64.005
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 86.311
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.1400
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 82.946
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.2015
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 57.641

Attention duration (in seconds): 0.1493
Attention throughput (in TFLOP/s): 83.781
MLP duration (in seconds): 0.3415
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4908
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.1018
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 85.550
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 51.780
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 54.010
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 86.474
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.1399
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 83.041
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.2016
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 57.619

Attention duration (in seconds): 0.1523
Attention throughput (in TFLOP/s): 82.128
MLP duration (in seconds): 0.3414
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4937
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.1016
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 85.728
Elapsed time for attention_key_query_prob (512x2048x104x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x104x2048): 38.267
Elapsed time for attention_prob_times_values (512x2048x2048x104): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x104): 49.187
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0335
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 86.645
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.1396
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 83.188
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.2015
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 57.635

Attention duration (in seconds): 0.1559
Attention throughput (in TFLOP/s): 80.242
MLP duration (in seconds): 0.3411
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4970
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.1028
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 86.335
Elapsed time for attention_key_query_prob (64x2048x840x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x840x2048): 62.438
Elapsed time for attention_prob_times_values (64x2048x2048x840): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x840): 63.991
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 86.746
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.1347
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 87.912
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.1821
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 65.018

Attention duration (in seconds): 0.1512
Attention throughput (in TFLOP/s): 84.245
MLP duration (in seconds): 0.3167
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4680
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.1027
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 86.480
Elapsed time for attention_key_query_prob (128x2048x420x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x420x2048): 61.812
Elapsed time for attention_prob_times_values (128x2048x2048x420): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x420): 65.220
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 86.732
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.1340
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 88.340
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.1821
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 65.010

Attention duration (in seconds): 0.1510
Attention throughput (in TFLOP/s): 84.371
MLP duration (in seconds): 0.3161
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4671
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.1028
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 86.333
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 51.407
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 64.202
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 86.854
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.1338
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 88.499
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.1822
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 64.957

Attention duration (in seconds): 0.1527
Attention throughput (in TFLOP/s): 83.425
MLP duration (in seconds): 0.3160
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.1027
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 86.420
Elapsed time for attention_key_query_prob (512x2048x105x2048): 0.1231
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x105x2048): 3.663
Elapsed time for attention_prob_times_values (512x2048x2048x105): 0.0964
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x105): 4.680
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0339
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 87.297
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.1337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 88.535
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.1821
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 65.000

Attention duration (in seconds): 0.3561
Attention throughput (in TFLOP/s): 35.775
MLP duration (in seconds): 0.3158
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6719
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.1010
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 89.615
Elapsed time for attention_key_query_prob (64x2048x848x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x848x2048): 63.966
Elapsed time for attention_prob_times_values (64x2048x2048x848): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x848): 65.466
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0350
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 86.293
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.1384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 87.141
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.1818
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 66.363

Attention duration (in seconds): 0.1500
Attention throughput (in TFLOP/s): 86.504
MLP duration (in seconds): 0.3202
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4702
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.1010
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 89.588
Elapsed time for attention_key_query_prob (128x2048x424x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x424x2048): 57.620
Elapsed time for attention_prob_times_values (128x2048x2048x424): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x424): 63.551
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 86.306
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.1385
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 87.103
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.1818
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 66.374

Attention duration (in seconds): 0.1510
Attention throughput (in TFLOP/s): 85.921
MLP duration (in seconds): 0.3203
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4713
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.1010
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 89.595
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 51.730
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 63.787
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 86.404
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.1384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 87.183
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.1819
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 66.326

Attention duration (in seconds): 0.1518
Attention throughput (in TFLOP/s): 85.454
MLP duration (in seconds): 0.3203
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4721
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.1010
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 89.575
Elapsed time for attention_key_query_prob (512x2048x106x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x106x2048): 37.517
Elapsed time for attention_prob_times_values (512x2048x2048x106): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x106): 56.085
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0348
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 86.608
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.1385
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 87.101
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.1818
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 66.364

Attention duration (in seconds): 0.1561
Attention throughput (in TFLOP/s): 83.124
MLP duration (in seconds): 0.3203
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4764
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.1099
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 83.932
Elapsed time for attention_key_query_prob (64x2048x856x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x856x2048): 63.024
Elapsed time for attention_prob_times_values (64x2048x2048x856): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x856): 65.476
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0367
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 83.776
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.1364
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 90.098
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.1939
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 63.407

Attention duration (in seconds): 0.1608
Attention throughput (in TFLOP/s): 82.143
MLP duration (in seconds): 0.3303
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4912
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.1100
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 83.809
Elapsed time for attention_key_query_prob (128x2048x428x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x428x2048): 62.368
Elapsed time for attention_prob_times_values (128x2048x2048x428): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x428): 66.134
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0367
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 83.818
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.1364
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 90.101
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.1939
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 63.397

Attention duration (in seconds): 0.1610
Attention throughput (in TFLOP/s): 82.067
MLP duration (in seconds): 0.3303
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4913
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.1099
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 83.863
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 52.209
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 65.150
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 83.887
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.1363
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 90.181
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.1940
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 63.370

Attention duration (in seconds): 0.1624
Attention throughput (in TFLOP/s): 81.340
MLP duration (in seconds): 0.3303
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4927
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.1099
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 83.875
Elapsed time for attention_key_query_prob (512x2048x107x2048): 0.1232
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x107x2048): 3.729
Elapsed time for attention_prob_times_values (512x2048x2048x107): 0.1002
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x107): 4.587
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0365
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 84.252
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.1357
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 90.614
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.1940
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 63.378

Attention duration (in seconds): 0.3698
Attention throughput (in TFLOP/s): 35.727
MLP duration (in seconds): 0.3296
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6995
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.1083
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 86.732
Elapsed time for attention_key_query_prob (64x2048x864x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x864x2048): 68.179
Elapsed time for attention_prob_times_values (64x2048x2048x864): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x864): 67.121
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 88.195
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.1443
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 86.773
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.1991
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 62.904

Attention duration (in seconds): 0.1575
Attention throughput (in TFLOP/s): 85.400
MLP duration (in seconds): 0.3434
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5009
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.1082
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 86.844
Elapsed time for attention_key_query_prob (128x2048x432x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x432x2048): 60.232
Elapsed time for attention_prob_times_values (128x2048x2048x432): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x432): 65.551
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 88.167
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.1441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 86.932
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.1993
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 62.843

Attention duration (in seconds): 0.1584
Attention throughput (in TFLOP/s): 84.896
MLP duration (in seconds): 0.3434
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.1081
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 86.855
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 52.372
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 55.217
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 88.295
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.1441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 86.884
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.1991
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 62.905

Attention duration (in seconds): 0.1609
Attention throughput (in TFLOP/s): 83.621
MLP duration (in seconds): 0.3432
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.1083
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 86.727
Elapsed time for attention_key_query_prob (512x2048x108x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x108x2048): 37.913
Elapsed time for attention_prob_times_values (512x2048x2048x108): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x108): 57.259
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0354
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 88.368
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.1441
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 86.901
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.1992
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 62.874

Attention duration (in seconds): 0.1641
Attention throughput (in TFLOP/s): 81.986
MLP duration (in seconds): 0.3433
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.1100
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 87.017
Elapsed time for attention_key_query_prob (64x2048x872x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x872x2048): 62.119
Elapsed time for attention_prob_times_values (64x2048x2048x872): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x872): 66.314
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0367
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 87.008
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.1417
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 90.040
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.2028
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 62.918

Attention duration (in seconds): 0.1612
Attention throughput (in TFLOP/s): 84.944
MLP duration (in seconds): 0.3444
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.1100
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 87.000
Elapsed time for attention_key_query_prob (128x2048x436x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x436x2048): 63.192
Elapsed time for attention_prob_times_values (128x2048x2048x436): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x436): 66.727
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 87.034
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.1416
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 90.064
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.2029
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 62.872

Attention duration (in seconds): 0.1610
Attention throughput (in TFLOP/s): 85.029
MLP duration (in seconds): 0.3446
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.1099
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 87.035
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 52.834
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 66.145
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0367
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 87.005
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.1416
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 90.094
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.2026
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 62.980

Attention duration (in seconds): 0.1625
Attention throughput (in TFLOP/s): 84.254
MLP duration (in seconds): 0.3442
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.1100
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 87.016
Elapsed time for attention_key_query_prob (512x2048x109x2048): 0.1238
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x109x2048): 3.780
Elapsed time for attention_prob_times_values (512x2048x2048x109): 0.0989
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x109): 4.735
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0365
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 87.292
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.1414
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 90.250
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.2025
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 63.007

Attention duration (in seconds): 0.3692
Attention throughput (in TFLOP/s): 37.090
MLP duration (in seconds): 0.3438
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.1088
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 89.597
Elapsed time for attention_key_query_prob (64x2048x880x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x880x2048): 65.184
Elapsed time for attention_prob_times_values (64x2048x2048x880): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x880): 68.437
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0382
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 85.038
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.1465
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 88.655
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.2165
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 60.017

Attention duration (in seconds): 0.1611
Attention throughput (in TFLOP/s): 86.511
MLP duration (in seconds): 0.3630
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.1097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 88.834
Elapsed time for attention_key_query_prob (128x2048x440x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x440x2048): 59.452
Elapsed time for attention_prob_times_values (128x2048x2048x440): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x440): 65.862
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0385
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 84.424
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.1474
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 88.166
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.2161
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 60.119

Attention duration (in seconds): 0.1633
Attention throughput (in TFLOP/s): 85.355
MLP duration (in seconds): 0.3635
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.1097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 88.832
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 52.837
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 64.987
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0384
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 84.591
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.1473
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 88.199
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.2161
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 60.119

Attention duration (in seconds): 0.1643
Attention throughput (in TFLOP/s): 84.827
MLP duration (in seconds): 0.3634
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.1097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 88.837
Elapsed time for attention_key_query_prob (512x2048x110x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x110x2048): 38.643
Elapsed time for attention_prob_times_values (512x2048x2048x110): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x110): 57.572
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0382
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 85.008
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.1476
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 88.017
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.2162
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 60.081

Attention duration (in seconds): 0.1683
Attention throughput (in TFLOP/s): 82.798
MLP duration (in seconds): 0.3639
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.1108
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 89.526
Elapsed time for attention_key_query_prob (64x2048x888x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x888x2048): 63.310
Elapsed time for attention_prob_times_values (64x2048x2048x888): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x888): 67.611
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 88.456
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.1471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 89.917
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.2125
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 62.264

Attention duration (in seconds): 0.1628
Attention throughput (in TFLOP/s): 87.118
MLP duration (in seconds): 0.3596
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.1109
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 89.493
Elapsed time for attention_key_query_prob (128x2048x444x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x444x2048): 64.081
Elapsed time for attention_prob_times_values (128x2048x2048x444): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x444): 67.993
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 88.432
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.1471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 89.915
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.2122
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 62.353

Attention duration (in seconds): 0.1627
Attention throughput (in TFLOP/s): 87.161
MLP duration (in seconds): 0.3593
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.1109
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 89.492
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 53.899
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 67.376
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 88.371
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.1471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 89.918
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.2122
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 62.358

Attention duration (in seconds): 0.1642
Attention throughput (in TFLOP/s): 86.366
MLP duration (in seconds): 0.3593
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.1109
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 89.501
Elapsed time for attention_key_query_prob (512x2048x111x2048): 0.1275
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x111x2048): 3.738
Elapsed time for attention_prob_times_values (512x2048x2048x111): 0.0962
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x111): 4.955
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 88.488
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.1471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 89.921
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.2122
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 62.330

Attention duration (in seconds): 0.3720
Attention throughput (in TFLOP/s): 38.127
MLP duration (in seconds): 0.3594
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.1162
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 86.913
Elapsed time for attention_key_query_prob (64x2048x896x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x896x2048): 66.879
Elapsed time for attention_prob_times_values (64x2048x2048x896): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x896): 70.190
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0385
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 87.513
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.1496
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 90.033
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.2314
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 58.205

Attention duration (in seconds): 0.1688
Attention throughput (in TFLOP/s): 85.517
MLP duration (in seconds): 0.3810
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5498
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.1161
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 87.039
Elapsed time for attention_key_query_prob (128x2048x448x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x448x2048): 63.268
Elapsed time for attention_prob_times_values (128x2048x2048x448): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x448): 69.025
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0385
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 87.432
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.1496
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 90.053
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.2312
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 58.250

Attention duration (in seconds): 0.1691
Attention throughput (in TFLOP/s): 85.318
MLP duration (in seconds): 0.3808
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5499
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.1163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 86.879
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 56.056
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 58.570
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0385
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 87.493
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.1494
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 90.143
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.2314
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 58.199

Attention duration (in seconds): 0.1716
Attention throughput (in TFLOP/s): 84.119
MLP duration (in seconds): 0.3808
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5524
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.1162
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 86.960
Elapsed time for attention_key_query_prob (512x2048x112x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x112x2048): 41.198
Elapsed time for attention_prob_times_values (512x2048x2048x112): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x112): 52.543
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0384
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 87.668
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.1493
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 90.240
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.2316
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 58.153

Attention duration (in seconds): 0.1754
Attention throughput (in TFLOP/s): 82.272
MLP duration (in seconds): 0.3809
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5563
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.1178
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 87.267
Elapsed time for attention_key_query_prob (64x2048x904x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x904x2048): 61.613
Elapsed time for attention_prob_times_values (64x2048x2048x904): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x904): 65.301
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0394
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 87.091
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.1517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 90.370
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.2102
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 65.235

Attention duration (in seconds): 0.1725
Attention throughput (in TFLOP/s): 85.109
MLP duration (in seconds): 0.3619
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5344
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.1178
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 87.277
Elapsed time for attention_key_query_prob (128x2048x452x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x452x2048): 62.812
Elapsed time for attention_prob_times_values (128x2048x2048x452): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x452): 68.521
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 87.243
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.1514
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 90.557
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.2101
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 65.266

Attention duration (in seconds): 0.1719
Attention throughput (in TFLOP/s): 85.397
MLP duration (in seconds): 0.3615
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5334
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.1178
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 87.314
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 52.331
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 68.535
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 87.118
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.1510
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 90.786
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.2102
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 65.230

Attention duration (in seconds): 0.1735
Attention throughput (in TFLOP/s): 84.633
MLP duration (in seconds): 0.3612
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.1177
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 87.353
Elapsed time for attention_key_query_prob (512x2048x113x2048): 0.1284
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x113x2048): 3.781
Elapsed time for attention_prob_times_values (512x2048x2048x113): 0.1031
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x113): 4.705
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0392
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 87.403
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.1510
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 90.799
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.2101
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 65.272

Attention duration (in seconds): 0.3884
Attention throughput (in TFLOP/s): 37.795
MLP duration (in seconds): 0.3611
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7495
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.1198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 87.393
Elapsed time for attention_key_query_prob (64x2048x912x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x912x2048): 64.792
Elapsed time for attention_prob_times_values (64x2048x2048x912): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x912): 66.759
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 87.077
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.1547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 90.227
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.2227
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 62.670

Attention duration (in seconds): 0.1747
Attention throughput (in TFLOP/s): 85.477
MLP duration (in seconds): 0.3773
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5520
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.1198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 87.346
Elapsed time for attention_key_query_prob (128x2048x456x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x456x2048): 58.938
Elapsed time for attention_prob_times_values (128x2048x2048x456): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x456): 60.901
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0400
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 87.154
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.1547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 90.220
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.2224
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 62.737

Attention duration (in seconds): 0.1762
Attention throughput (in TFLOP/s): 84.756
MLP duration (in seconds): 0.3771
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5533
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.1199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 87.292
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 52.007
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 67.397
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0401
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 86.998
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.1547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 90.215
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.2222
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 62.799

Attention duration (in seconds): 0.1767
Attention throughput (in TFLOP/s): 84.527
MLP duration (in seconds): 0.3769
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5536
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.1199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 87.266
Elapsed time for attention_key_query_prob (512x2048x114x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x114x2048): 39.786
Elapsed time for attention_prob_times_values (512x2048x2048x114): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x114): 59.885
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0399
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 87.441
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.1547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 90.227
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.2226
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 62.679

Attention duration (in seconds): 0.1803
Attention throughput (in TFLOP/s): 82.823
MLP duration (in seconds): 0.3773
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5576
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.1201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 88.674
Elapsed time for attention_key_query_prob (64x2048x920x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x920x2048): 62.326
Elapsed time for attention_prob_times_values (64x2048x2048x920): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x920): 66.248
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0409
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 86.821
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.1598
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 88.886
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.2436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 58.290

Attention duration (in seconds): 0.1764
Attention throughput (in TFLOP/s): 86.113
MLP duration (in seconds): 0.4034
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5797
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.1201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 88.675
Elapsed time for attention_key_query_prob (128x2048x460x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x460x2048): 63.530
Elapsed time for attention_prob_times_values (128x2048x2048x460): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x460): 69.367
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0408
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 86.957
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.1598
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 88.888
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.2436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 58.305

Attention duration (in seconds): 0.1758
Attention throughput (in TFLOP/s): 86.382
MLP duration (in seconds): 0.4033
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5791
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.1201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 88.671
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 52.694
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 69.535
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0409
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 86.779
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.1597
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 88.891
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.2436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 58.296

Attention duration (in seconds): 0.1775
Attention throughput (in TFLOP/s): 85.569
MLP duration (in seconds): 0.4033
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5808
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.1201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 88.671
Elapsed time for attention_key_query_prob (512x2048x115x2048): 0.1265
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x115x2048): 3.905
Elapsed time for attention_prob_times_values (512x2048x2048x115): 0.1050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x115): 4.702
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0407
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 87.229
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.1597
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 88.903
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.2436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 58.298

Attention duration (in seconds): 0.3923
Attention throughput (in TFLOP/s): 38.713
MLP duration (in seconds): 0.4033
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7956
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.1207
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 89.778
Elapsed time for attention_key_query_prob (64x2048x928x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x928x2048): 67.937
Elapsed time for attention_prob_times_values (64x2048x2048x928): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x928): 68.103
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 83.713
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.1601
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 90.218
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.2288
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 63.139

Attention duration (in seconds): 0.1785
Attention throughput (in TFLOP/s): 86.526
MLP duration (in seconds): 0.3890
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5675
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.1207
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 89.795
Elapsed time for attention_key_query_prob (128x2048x464x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x464x2048): 61.044
Elapsed time for attention_prob_times_values (128x2048x2048x464): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x464): 62.664
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 83.801
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.1601
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 90.247
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.2287
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 63.174

Attention duration (in seconds): 0.1799
Attention throughput (in TFLOP/s): 85.855
MLP duration (in seconds): 0.3888
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.1207
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 89.795
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 51.312
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 58.841
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 83.832
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.1593
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 90.683
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.2287
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 63.168

Attention duration (in seconds): 0.1819
Attention throughput (in TFLOP/s): 84.889
MLP duration (in seconds): 0.3881
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5700
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.1207
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 89.800
Elapsed time for attention_key_query_prob (512x2048x116x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x116x2048): 40.012
Elapsed time for attention_prob_times_values (512x2048x2048x116): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x116): 60.369
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0429
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 84.132
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.1588
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 90.956
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.2286
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 63.214

Attention duration (in seconds): 0.1843
Attention throughput (in TFLOP/s): 83.799
MLP duration (in seconds): 0.3874
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5717
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.1226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 89.946
Elapsed time for attention_key_query_prob (64x2048x936x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x936x2048): 61.956
Elapsed time for attention_prob_times_values (64x2048x2048x936): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x936): 67.216
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 87.228
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.1634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 89.978
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.2473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 59.425

Attention duration (in seconds): 0.1803
Attention throughput (in TFLOP/s): 87.109
MLP duration (in seconds): 0.4107
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5910
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.1226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 89.943
Elapsed time for attention_key_query_prob (128x2048x468x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x468x2048): 64.347
Elapsed time for attention_prob_times_values (128x2048x2048x468): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x468): 70.645
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 87.069
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.1634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 89.979
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.2472
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 59.455

Attention duration (in seconds): 0.1797
Attention throughput (in TFLOP/s): 87.392
MLP duration (in seconds): 0.4106
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5903
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.1226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 89.946
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 53.651
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 70.048
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 87.208
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.1634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 89.981
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.2475
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 59.378

Attention duration (in seconds): 0.1812
Attention throughput (in TFLOP/s): 86.646
MLP duration (in seconds): 0.4109
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5921
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.1226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 89.934
Elapsed time for attention_key_query_prob (512x2048x117x2048): 0.1265
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x117x2048): 3.972
Elapsed time for attention_prob_times_values (512x2048x2048x117): 0.1062
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x117): 4.734
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 87.219
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.1634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 89.980
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.2472
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 59.462

Attention duration (in seconds): 0.3974
Attention throughput (in TFLOP/s): 39.517
MLP duration (in seconds): 0.4105
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.1282
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 87.471
Elapsed time for attention_key_query_prob (64x2048x944x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x944x2048): 63.983
Elapsed time for attention_prob_times_values (64x2048x2048x944): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x944): 68.733
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 87.692
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.1656
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 90.272
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.1969
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 75.946

Attention duration (in seconds): 0.1861
Attention throughput (in TFLOP/s): 85.780
MLP duration (in seconds): 0.3625
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5486
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.1282
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 87.457
Elapsed time for attention_key_query_prob (128x2048x472x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x472x2048): 60.369
Elapsed time for attention_prob_times_values (128x2048x2048x472): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x472): 62.273
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 87.750
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.1656
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 90.274
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.1967
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 75.989

Attention duration (in seconds): 0.1873
Attention throughput (in TFLOP/s): 85.215
MLP duration (in seconds): 0.3624
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5497
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.1283
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 87.408
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 53.479
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 68.864
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 87.645
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.1655
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 90.353
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.1969
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 75.943

Attention duration (in seconds): 0.1878
Attention throughput (in TFLOP/s): 85.023
MLP duration (in seconds): 0.3623
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5501
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.1283
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 87.420
Elapsed time for attention_key_query_prob (512x2048x118x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x118x2048): 40.942
Elapsed time for attention_prob_times_values (512x2048x2048x118): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x118): 60.954
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 87.688
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.1644
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 90.955
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.1969
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 75.946

Attention duration (in seconds): 0.1916
Attention throughput (in TFLOP/s): 83.328
MLP duration (in seconds): 0.3612
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5528
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.1307
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 87.260
Elapsed time for attention_key_query_prob (64x2048x952x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x952x2048): 62.267
Elapsed time for attention_prob_times_values (64x2048x2048x952): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x952): 68.262
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0435
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 87.346
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.1690
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 89.975
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.2025
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 75.092

Attention duration (in seconds): 0.1899
Attention throughput (in TFLOP/s): 85.450
MLP duration (in seconds): 0.3715
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5614
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.1308
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 87.189
Elapsed time for attention_key_query_prob (128x2048x476x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x476x2048): 64.692
Elapsed time for attention_prob_times_values (128x2048x2048x476): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x476): 71.066
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0435
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 87.317
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.1690
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 89.976
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.2027
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 75.026

Attention duration (in seconds): 0.1894
Attention throughput (in TFLOP/s): 85.668
MLP duration (in seconds): 0.3717
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5611
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.1307
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 87.220
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 54.323
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 71.175
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0435
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 87.334
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.1690
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 89.975
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.2025
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 75.099

Attention duration (in seconds): 0.1909
Attention throughput (in TFLOP/s): 85.021
MLP duration (in seconds): 0.3715
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5623
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.1307
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 87.274
Elapsed time for attention_key_query_prob (512x2048x119x2048): 0.1329
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x119x2048): 3.847
Elapsed time for attention_prob_times_values (512x2048x2048x119): 0.1139
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x119): 4.486
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0435
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 87.471
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.1690
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 89.975
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.2025
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 75.076

Attention duration (in seconds): 0.4209
Attention throughput (in TFLOP/s): 38.553
MLP duration (in seconds): 0.3715
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7924
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.1394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 83.178
Elapsed time for attention_key_query_prob (64x2048x960x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x960x2048): 65.547
Elapsed time for attention_prob_times_values (64x2048x2048x960): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x960): 70.797
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0447
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 86.547
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.1865
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 82.912
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.1948
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 79.363

Attention duration (in seconds): 0.1992
Attention throughput (in TFLOP/s): 82.785
MLP duration (in seconds): 0.3813
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5805
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.1394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 83.212
Elapsed time for attention_key_query_prob (128x2048x480x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x480x2048): 64.326
Elapsed time for attention_prob_times_values (128x2048x2048x480): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x480): 65.183
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 86.575
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.1866
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 82.876
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.1948
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 79.372

Attention duration (in seconds): 0.1999
Attention throughput (in TFLOP/s): 82.493
MLP duration (in seconds): 0.3814
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5813
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.1394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 83.175
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 54.223
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 61.677
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0445
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 86.799
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.1867
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 82.823
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.1948
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 79.365

Attention duration (in seconds): 0.2018
Attention throughput (in TFLOP/s): 81.721
MLP duration (in seconds): 0.3815
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5833
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.1394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 83.198
Elapsed time for attention_key_query_prob (512x2048x120x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x120x2048): 43.116
Elapsed time for attention_prob_times_values (512x2048x2048x120): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x120): 55.726
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0445
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 86.866
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.1865
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 82.895
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.1948
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 79.361

Attention duration (in seconds): 0.2051
Attention throughput (in TFLOP/s): 80.419
MLP duration (in seconds): 0.3814
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5864
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.1315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 89.681
Elapsed time for attention_key_query_prob (64x2048x968x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x968x2048): 61.997
Elapsed time for attention_prob_times_values (64x2048x2048x968): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x968): 64.842
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 86.003
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.1735
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 90.597
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.2023
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 77.691

Attention duration (in seconds): 0.1936
Attention throughput (in TFLOP/s): 86.585
MLP duration (in seconds): 0.3759
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5694
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.1315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 89.670
Elapsed time for attention_key_query_prob (128x2048x484x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x484x2048): 63.637
Elapsed time for attention_prob_times_values (128x2048x2048x484): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x484): 72.256
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 86.040
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.1734
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 90.643
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.2027
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 77.537

Attention duration (in seconds): 0.1925
Attention throughput (in TFLOP/s): 87.054
MLP duration (in seconds): 0.3762
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.1315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 89.692
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 55.084
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 72.460
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 86.130
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.1734
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 90.646
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.2026
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 77.589

Attention duration (in seconds): 0.1937
Attention throughput (in TFLOP/s): 86.529
MLP duration (in seconds): 0.3760
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5697
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.1315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 89.694
Elapsed time for attention_key_query_prob (512x2048x121x2048): 0.1339
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x121x2048): 3.882
Elapsed time for attention_prob_times_values (512x2048x2048x121): 0.1142
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x121): 4.551
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0455
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 86.343
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.1734
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 90.648
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.2033
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 77.320

Attention duration (in seconds): 0.4251
Attention throughput (in TFLOP/s): 39.431
MLP duration (in seconds): 0.3767
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.1330
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 90.104
Elapsed time for attention_key_query_prob (64x2048x976x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x976x2048): 64.260
Elapsed time for attention_prob_times_values (64x2048x2048x976): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x976): 66.245
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 87.441
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.1773
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 90.146
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.2089
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 76.503

Attention duration (in seconds): 0.1948
Attention throughput (in TFLOP/s): 87.429
MLP duration (in seconds): 0.3862
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5810
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.1330
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 90.089
Elapsed time for attention_key_query_prob (128x2048x488x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x488x2048): 59.181
Elapsed time for attention_prob_times_values (128x2048x2048x488): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x488): 64.428
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 87.412
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.1772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 90.178
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.2090
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 76.465

Attention duration (in seconds): 0.1957
Attention throughput (in TFLOP/s): 87.000
MLP duration (in seconds): 0.3862
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5820
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.1330
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 90.094
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 54.643
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 71.358
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 87.390
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.1771
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 90.246
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.2089
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 76.521

Attention duration (in seconds): 0.1957
Attention throughput (in TFLOP/s): 87.022
MLP duration (in seconds): 0.3859
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5816
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.1331
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 90.085
Elapsed time for attention_key_query_prob (512x2048x122x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x122x2048): 41.883
Elapsed time for attention_prob_times_values (512x2048x2048x122): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x122): 62.566
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 87.540
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.1769
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 90.365
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.2089
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 76.496

Attention duration (in seconds): 0.1996
Attention throughput (in TFLOP/s): 85.327
MLP duration (in seconds): 0.3858
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5854
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.1396
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 87.287
Elapsed time for attention_key_query_prob (64x2048x984x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x984x2048): 63.181
Elapsed time for attention_prob_times_values (64x2048x2048x984): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x984): 65.645
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0464
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 87.535
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.1803
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 90.120
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.2122
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 76.548

Attention duration (in seconds): 0.2024
Attention throughput (in TFLOP/s): 85.488
MLP duration (in seconds): 0.3925
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5949
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.1396
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 87.289
Elapsed time for attention_key_query_prob (128x2048x492x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x492x2048): 64.817
Elapsed time for attention_prob_times_values (128x2048x2048x492): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x492): 73.316
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0464
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 87.491
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.1803
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 90.120
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.2123
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 76.525

Attention duration (in seconds): 0.2014
Attention throughput (in TFLOP/s): 85.926
MLP duration (in seconds): 0.3925
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5939
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.1394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 87.395
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 55.599
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 73.709
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0464
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 87.539
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.1803
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 90.121
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.2123
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 76.512

Attention duration (in seconds): 0.2025
Attention throughput (in TFLOP/s): 85.452
MLP duration (in seconds): 0.3926
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5950
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.1394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 87.375
Elapsed time for attention_key_query_prob (512x2048x123x2048): 0.1313
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x123x2048): 4.022
Elapsed time for attention_prob_times_values (512x2048x2048x123): 0.1060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x123): 4.983
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0464
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 87.573
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.1803
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 90.122
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.2120
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 76.637

Attention duration (in seconds): 0.4232
Attention throughput (in TFLOP/s): 40.885
MLP duration (in seconds): 0.3922
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.1453
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 85.228
Elapsed time for attention_key_query_prob (64x2048x992x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x992x2048): 67.435
Elapsed time for attention_prob_times_values (64x2048x2048x992): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x992): 66.450
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0487
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 84.692
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.1932
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 85.451
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.2191
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 75.357

Attention duration (in seconds): 0.2099
Attention throughput (in TFLOP/s): 83.717
MLP duration (in seconds): 0.4123
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.1468
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 84.359
Elapsed time for attention_key_query_prob (128x2048x496x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x496x2048): 59.969
Elapsed time for attention_prob_times_values (128x2048x2048x496): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x496): 64.333
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0488
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 84.615
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.1941
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 85.038
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.2191
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 75.339

Attention duration (in seconds): 0.2127
Attention throughput (in TFLOP/s): 82.620
MLP duration (in seconds): 0.4133
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.1477
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 83.823
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 52.955
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 61.017
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0488
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 84.638
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.1940
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 85.104
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.2192
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 75.335

Attention duration (in seconds): 0.2153
Attention throughput (in TFLOP/s): 81.641
MLP duration (in seconds): 0.4132
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.1478
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 83.800
Elapsed time for attention_key_query_prob (512x2048x124x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x124x2048): 39.776
Elapsed time for attention_prob_times_values (512x2048x2048x124): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x124): 60.694
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0487
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 84.671
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.1938
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 85.179
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.2192
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 75.326

Attention duration (in seconds): 0.2187
Attention throughput (in TFLOP/s): 80.371
MLP duration (in seconds): 0.4130
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.1483
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 84.820
Elapsed time for attention_key_query_prob (64x2048x1000x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1000x2048): 60.531
Elapsed time for attention_prob_times_values (64x2048x2048x1000): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1000): 65.117
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 83.930
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.1990
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 84.328
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.2201
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 76.231

Attention duration (in seconds): 0.2154
Attention throughput (in TFLOP/s): 82.860
MLP duration (in seconds): 0.4190
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6345
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.1483
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 84.872
Elapsed time for attention_key_query_prob (128x2048x500x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x500x2048): 62.933
Elapsed time for attention_prob_times_values (128x2048x2048x500): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x500): 73.447
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 83.941
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.1990
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 84.299
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.2200
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 76.245

Attention duration (in seconds): 0.2141
Attention throughput (in TFLOP/s): 83.391
MLP duration (in seconds): 0.4191
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6331
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.1483
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 84.842
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 53.512
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 73.303
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0499
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 83.987
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.1990
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 84.311
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.2201
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 76.228

Attention duration (in seconds): 0.2156
Attention throughput (in TFLOP/s): 82.794
MLP duration (in seconds): 0.4191
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.1484
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 84.787
Elapsed time for attention_key_query_prob (512x2048x125x2048): 0.0212
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x125x2048): 25.287
Elapsed time for attention_prob_times_values (512x2048x2048x125): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x125): 33.666
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0499
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 83.988
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.1991
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 84.272
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.2202
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 76.203

Attention duration (in seconds): 0.2355
Attention throughput (in TFLOP/s): 75.793
MLP duration (in seconds): 0.4192
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6548
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.2294
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 55.744
Elapsed time for attention_key_query_prob (64x2048x1008x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1008x2048): 62.945
Elapsed time for attention_prob_times_values (64x2048x2048x1008): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1008): 66.359
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0508
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 83.853
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.2032
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 83.886
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.2301
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 74.094

Attention duration (in seconds): 0.2969
Attention throughput (in TFLOP/s): 61.056
MLP duration (in seconds): 0.4333
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.2278
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 56.118
Elapsed time for attention_key_query_prob (128x2048x504x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x504x2048): 58.990
Elapsed time for attention_prob_times_values (128x2048x2048x504): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x504): 64.526
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0508
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 83.850
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.2032
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 83.902
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.2300
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 74.113

Attention duration (in seconds): 0.2962
Attention throughput (in TFLOP/s): 61.203
MLP duration (in seconds): 0.4332
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.2294
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 55.736
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 53.318
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 72.248
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0508
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 83.870
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.2030
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 83.957
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.2300
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 74.125

Attention duration (in seconds): 0.2978
Attention throughput (in TFLOP/s): 60.869
MLP duration (in seconds): 0.4330
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.2281
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 56.046
Elapsed time for attention_key_query_prob (512x2048x126x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x126x2048): 40.743
Elapsed time for attention_prob_times_values (512x2048x2048x126): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x126): 61.974
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 84.051
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.2026
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 84.141
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.2297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 74.202

Attention duration (in seconds): 0.3008
Attention throughput (in TFLOP/s): 60.263
MLP duration (in seconds): 0.4323
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7332
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.1534
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 84.679
Elapsed time for attention_key_query_prob (64x2048x1016x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1016x2048): 61.680
Elapsed time for attention_prob_times_values (64x2048x2048x1016): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1016): 65.831
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 84.401
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.2045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 84.705
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.2333
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 74.239

Attention duration (in seconds): 0.2218
Attention throughput (in TFLOP/s): 82.994
MLP duration (in seconds): 0.4377
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6596
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.1533
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 84.712
Elapsed time for attention_key_query_prob (128x2048x508x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x508x2048): 63.457
Elapsed time for attention_prob_times_values (128x2048x2048x508): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x508): 74.554
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0512
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 84.483
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.2047
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 84.619
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.2333
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 74.225

Attention duration (in seconds): 0.2205
Attention throughput (in TFLOP/s): 83.493
MLP duration (in seconds): 0.4380
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6585
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.1535
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 84.631
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 53.924
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 74.602
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0512
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 84.523
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.2042
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 84.818
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.2333
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 74.220

Attention duration (in seconds): 0.2221
Attention throughput (in TFLOP/s): 82.877
MLP duration (in seconds): 0.4375
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6596
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.1534
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 84.661
Elapsed time for attention_key_query_prob (512x2048x127x2048): 0.0217
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x127x2048): 25.132
Elapsed time for attention_prob_times_values (512x2048x2048x127): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x127): 33.667
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0512
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 84.509
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.2043
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 84.780
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.2332
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 74.277

Attention duration (in seconds): 0.2426
Attention throughput (in TFLOP/s): 75.896
MLP duration (in seconds): 0.4374
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6800
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.2475
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 53.307
Elapsed time for attention_key_query_prob (64x2048x1024x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1024x2048): 66.367
Elapsed time for attention_prob_times_values (64x2048x2048x1024): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1024): 68.679
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 83.897
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.2055
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 85.595
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.2288
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 76.898

Attention duration (in seconds): 0.3162
Attention throughput (in TFLOP/s): 59.109
MLP duration (in seconds): 0.4343
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7505
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.2473
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 53.358
Elapsed time for attention_key_query_prob (128x2048x512x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x512x2048): 63.490
Elapsed time for attention_prob_times_values (128x2048x2048x512): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x512): 68.055
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 83.926
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.2054
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 85.656
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.2304
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 76.369

Attention duration (in seconds): 0.3164
Attention throughput (in TFLOP/s): 59.073
MLP duration (in seconds): 0.4357
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7522
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.2485
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 53.091
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 56.833
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 65.470
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 84.012
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.2051
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 85.788
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.2303
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 76.375

Attention duration (in seconds): 0.3189
Attention throughput (in TFLOP/s): 58.606
MLP duration (in seconds): 0.4354
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7543
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.2503
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 52.708
Elapsed time for attention_key_query_prob (512x2048x128x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x128x2048): 46.599
Elapsed time for attention_prob_times_values (512x2048x2048x128): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x128): 60.446
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0522
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 84.230
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.2053
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 85.681
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.2304
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 76.342

Attention duration (in seconds): 0.3234
Attention throughput (in TFLOP/s): 57.792
MLP duration (in seconds): 0.4358
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7592
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.1581
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 84.750
Elapsed time for attention_key_query_prob (64x2048x1032x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1032x2048): 60.946
Elapsed time for attention_prob_times_values (64x2048x2048x1032): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1032): 63.756
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 83.806
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.2107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 84.786
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.2386
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 74.899

Attention duration (in seconds): 0.2292
Attention throughput (in TFLOP/s): 82.790
MLP duration (in seconds): 0.4493
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6785
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.1582
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 84.736
Elapsed time for attention_key_query_prob (128x2048x516x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x516x2048): 62.788
Elapsed time for attention_prob_times_values (128x2048x2048x516): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x516): 58.124
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0534
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 83.681
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.2107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 84.802
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.2384
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 74.945

Attention duration (in seconds): 0.2299
Attention throughput (in TFLOP/s): 82.545
MLP duration (in seconds): 0.4491
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6790
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.1581
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 84.769
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 52.725
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 45.261
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 83.784
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.2109
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 84.730
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.2385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 74.911

Attention duration (in seconds): 0.2342
Attention throughput (in TFLOP/s): 81.041
MLP duration (in seconds): 0.4494
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6836
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.1582
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 84.727
Elapsed time for attention_key_query_prob (512x2048x129x2048): 0.0237
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x129x2048): 23.362
Elapsed time for attention_prob_times_values (512x2048x2048x129): 0.0227
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x129): 24.356
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 83.829
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.2105
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 84.867
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.2384
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 74.957

Attention duration (in seconds): 0.2579
Attention throughput (in TFLOP/s): 73.574
MLP duration (in seconds): 0.4489
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.1629
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 83.530
Elapsed time for attention_key_query_prob (64x2048x1040x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1040x2048): 62.987
Elapsed time for attention_prob_times_values (64x2048x2048x1040): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1040): 64.802
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 82.637
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.2163
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 83.901
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.2403
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 75.512

Attention duration (in seconds): 0.2353
Attention throughput (in TFLOP/s): 81.862
MLP duration (in seconds): 0.4566
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6919
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.1634
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 83.295
Elapsed time for attention_key_query_prob (128x2048x520x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x520x2048): 58.192
Elapsed time for attention_prob_times_values (128x2048x2048x520): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x520): 60.724
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 82.614
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.2160
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 84.002
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.2403
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 75.523

Attention duration (in seconds): 0.2371
Attention throughput (in TFLOP/s): 81.246
MLP duration (in seconds): 0.4563
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6934
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.1629
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 83.529
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 51.811
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 45.272
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 82.657
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.2161
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 83.968
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.2402
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 75.543

Attention duration (in seconds): 0.2409
Attention throughput (in TFLOP/s): 79.953
MLP duration (in seconds): 0.4563
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6972
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.1630
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 83.482
Elapsed time for attention_key_query_prob (512x2048x130x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x130x2048): 39.027
Elapsed time for attention_prob_times_values (512x2048x2048x130): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x130): 43.576
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0547
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 82.957
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.2152
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 84.333
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.2403
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 75.520

Attention duration (in seconds): 0.2448
Attention throughput (in TFLOP/s): 78.679
MLP duration (in seconds): 0.4555
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7003
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.1664
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 83.041
Elapsed time for attention_key_query_prob (64x2048x1048x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1048x2048): 61.497
Elapsed time for attention_prob_times_values (64x2048x2048x1048): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1048): 64.472
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 83.109
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.2217
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 83.097
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.2465
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 74.744

Attention duration (in seconds): 0.2397
Attention throughput (in TFLOP/s): 81.558
MLP duration (in seconds): 0.4683
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.1664
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 83.041
Elapsed time for attention_key_query_prob (128x2048x524x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x524x2048): 62.984
Elapsed time for attention_prob_times_values (128x2048x2048x524): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x524): 58.456
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 83.108
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.2217
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 83.097
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.2466
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 74.730

Attention duration (in seconds): 0.2404
Attention throughput (in TFLOP/s): 81.327
MLP duration (in seconds): 0.4683
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.1664
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 83.040
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 52.963
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 46.087
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 83.154
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.2217
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 83.106
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.2465
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 74.759

Attention duration (in seconds): 0.2447
Attention throughput (in TFLOP/s): 79.916
MLP duration (in seconds): 0.4682
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.1664
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 83.039
Elapsed time for attention_key_query_prob (512x2048x131x2048): 0.0234
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x131x2048): 24.062
Elapsed time for attention_prob_times_values (512x2048x2048x131): 0.0224
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x131): 25.067
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0550
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 83.785
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.2217
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 83.103
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.2466
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 74.726

Attention duration (in seconds): 0.2672
Attention throughput (in TFLOP/s): 73.163
MLP duration (in seconds): 0.4683
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7356
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.1751
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 80.148
Elapsed time for attention_key_query_prob (64x2048x1056x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1056x2048): 66.752
Elapsed time for attention_prob_times_values (64x2048x2048x1056): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1056): 66.060
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0582
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 80.362
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.2340
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 79.961
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.2530
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 73.954

Attention duration (in seconds): 0.2503
Attention throughput (in TFLOP/s): 79.260
MLP duration (in seconds): 0.4870
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7373
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.1749
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 80.211
Elapsed time for attention_key_query_prob (128x2048x528x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x528x2048): 60.573
Elapsed time for attention_prob_times_values (128x2048x2048x528): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x528): 62.285
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 80.490
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.2337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 80.050
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.2529
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 73.990

Attention duration (in seconds): 0.2515
Attention throughput (in TFLOP/s): 78.896
MLP duration (in seconds): 0.4866
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7381
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.1750
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 80.200
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 52.025
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 54.603
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 80.485
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.2335
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 80.119
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.2529
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 73.984

Attention duration (in seconds): 0.2544
Attention throughput (in TFLOP/s): 78.013
MLP duration (in seconds): 0.4864
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7407
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.1750
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 80.193
Elapsed time for attention_key_query_prob (512x2048x132x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x132x2048): 39.108
Elapsed time for attention_prob_times_values (512x2048x2048x132): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x132): 43.290
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0578
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 80.877
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.2337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 80.065
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.2528
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 74.004

Attention duration (in seconds): 0.2604
Attention throughput (in TFLOP/s): 76.201
MLP duration (in seconds): 0.4865
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7469
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.1758
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 81.018
Elapsed time for attention_key_query_prob (64x2048x1064x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1064x2048): 60.881
Elapsed time for attention_prob_times_values (64x2048x2048x1064): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1064): 65.154
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0582
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 81.599
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.2345
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 81.004
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.2494
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 76.165

Attention duration (in seconds): 0.2522
Attention throughput (in TFLOP/s): 79.851
MLP duration (in seconds): 0.4838
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7360
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.1758
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 81.017
Elapsed time for attention_key_query_prob (128x2048x532x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x532x2048): 63.588
Elapsed time for attention_prob_times_values (128x2048x2048x532): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x532): 58.757
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0582
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 81.557
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.2343
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 81.081
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.2492
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 76.212

Attention duration (in seconds): 0.2528
Attention throughput (in TFLOP/s): 79.666
MLP duration (in seconds): 0.4835
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7362
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.1759
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 80.965
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 53.538
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 46.646
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0582
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 81.537
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.2339
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 81.214
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.2493
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 76.191

Attention duration (in seconds): 0.2571
Attention throughput (in TFLOP/s): 78.322
MLP duration (in seconds): 0.4832
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7402
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.1760
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 80.918
Elapsed time for attention_key_query_prob (512x2048x133x2048): 0.0238
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x133x2048): 23.980
Elapsed time for attention_prob_times_values (512x2048x2048x133): 0.0226
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x133): 25.305
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0582
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 81.644
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.2343
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 81.068
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.2493
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 76.187

Attention duration (in seconds): 0.2806
Attention throughput (in TFLOP/s): 71.761
MLP duration (in seconds): 0.4836
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7642
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.1811
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 79.857
Elapsed time for attention_key_query_prob (64x2048x1072x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1072x2048): 62.876
Elapsed time for attention_prob_times_values (64x2048x2048x1072): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1072): 66.392
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0600
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 80.276
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.2409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 80.027
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.2572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 74.973

Attention duration (in seconds): 0.2589
Attention throughput (in TFLOP/s): 78.903
MLP duration (in seconds): 0.4981
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7570
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.1811
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 79.864
Elapsed time for attention_key_query_prob (128x2048x536x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x536x2048): 59.402
Elapsed time for attention_prob_times_values (128x2048x2048x536): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x536): 62.448
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0602
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 80.116
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.2409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 80.036
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.2571
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 74.984

Attention duration (in seconds): 0.2601
Attention throughput (in TFLOP/s): 78.543
MLP duration (in seconds): 0.4980
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7581
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.1812
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 79.812
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 52.997
Elapsed time for attention_prob_times_values (256x2048x2048x268): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 46.212
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0599
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 80.490
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.2409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 80.024
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.2564
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 75.204

Attention duration (in seconds): 0.2644
Attention throughput (in TFLOP/s): 77.281
MLP duration (in seconds): 0.4973
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7617
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.1814
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 79.735
Elapsed time for attention_key_query_prob (512x2048x134x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x134x2048): 39.800
Elapsed time for attention_prob_times_values (512x2048x2048x134): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x134): 44.491
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0598
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 80.624
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.2405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 80.167
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.2569
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 75.035

Attention duration (in seconds): 0.2685
Attention throughput (in TFLOP/s): 76.085
MLP duration (in seconds): 0.4974
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7660
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.1779
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 82.477
Elapsed time for attention_key_query_prob (64x2048x1080x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1080x2048): 61.031
Elapsed time for attention_prob_times_values (64x2048x2048x1080): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1080): 65.980
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0590
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 82.978
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.2376
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 82.348
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.2639
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 74.159

Attention duration (in seconds): 0.2552
Attention throughput (in TFLOP/s): 81.226
MLP duration (in seconds): 0.5015
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7567
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.1788
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 82.101
Elapsed time for attention_key_query_prob (128x2048x540x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x540x2048): 63.625
Elapsed time for attention_prob_times_values (128x2048x2048x540): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x540): 59.871
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0589
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 83.011
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.2369
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 82.614
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.2639
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 74.141

Attention duration (in seconds): 0.2565
Attention throughput (in TFLOP/s): 80.814
MLP duration (in seconds): 0.5008
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7573
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.1783
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 82.306
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 53.960
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 47.194
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0589
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 83.030
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.2376
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 82.344
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.2638
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 74.179

Attention duration (in seconds): 0.2603
Attention throughput (in TFLOP/s): 79.642
MLP duration (in seconds): 0.5015
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7617
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.1783
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 82.326
Elapsed time for attention_key_query_prob (512x2048x135x2048): 0.0246
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x135x2048): 23.569
Elapsed time for attention_prob_times_values (512x2048x2048x135): 0.0231
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x135): 25.150
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0589
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 83.015
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.2364
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 82.769
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.2638
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 74.188

Attention duration (in seconds): 0.2849
Attention throughput (in TFLOP/s): 72.767
MLP duration (in seconds): 0.5002
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7851
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.1797
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 82.903
Elapsed time for attention_key_query_prob (64x2048x1088x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1088x2048): 64.498
Elapsed time for attention_prob_times_values (64x2048x2048x1088): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1088): 68.438
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0597
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 83.106
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.2395
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 82.939
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.2695
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 73.694

Attention duration (in seconds): 0.2570
Attention throughput (in TFLOP/s): 81.821
MLP duration (in seconds): 0.5089
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7659
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.1763
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 84.484
Elapsed time for attention_key_query_prob (64x2048x1088x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1088x2048): 66.301
Elapsed time for attention_prob_times_values (64x2048x2048x1088): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1088): 69.919
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 84.567
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.2350
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 84.516
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.2664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 74.562

Attention duration (in seconds): 0.2522
Attention throughput (in TFLOP/s): 83.386
MLP duration (in seconds): 0.5013
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7535
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.1765
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 84.386
Elapsed time for attention_key_query_prob (128x2048x544x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x544x2048): 64.530
Elapsed time for attention_prob_times_values (128x2048x2048x544): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x544): 65.757
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 84.568
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.2353
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 84.400
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.2669
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 74.415

Attention duration (in seconds): 0.2532
Attention throughput (in TFLOP/s): 83.064
MLP duration (in seconds): 0.5022
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7553
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.1779
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 83.740
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 54.320
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 56.740
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 84.572
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.2352
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 84.421
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.2670
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 74.387

Attention duration (in seconds): 0.2576
Attention throughput (in TFLOP/s): 81.623
MLP duration (in seconds): 0.5022
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7599
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.1779
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 83.733
Elapsed time for attention_key_query_prob (512x2048x136x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x136x2048): 41.938
Elapsed time for attention_prob_times_values (512x2048x2048x136): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x136): 45.595
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 84.570
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.2370
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 83.794
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.2667
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 74.457

Attention duration (in seconds): 0.2633
Attention throughput (in TFLOP/s): 79.854
MLP duration (in seconds): 0.5037
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7671
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.1851
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 81.673
Elapsed time for attention_key_query_prob (64x2048x1096x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1096x2048): 61.302
Elapsed time for attention_prob_times_values (64x2048x2048x1096): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1096): 63.616
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0617
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 81.722
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.2464
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 81.800
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.2644
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 76.232

Attention duration (in seconds): 0.2656
Attention throughput (in TFLOP/s): 80.320
MLP duration (in seconds): 0.5107
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7763
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.1851
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 81.674
Elapsed time for attention_key_query_prob (128x2048x548x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x548x2048): 63.379
Elapsed time for attention_prob_times_values (128x2048x2048x548): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x548): 61.208
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0617
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 81.717
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.2463
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 81.808
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.2644
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 76.208

Attention duration (in seconds): 0.2656
Attention throughput (in TFLOP/s): 80.304
MLP duration (in seconds): 0.5108
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7764
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.1852
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 81.626
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 55.436
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 48.310
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0615
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 81.931
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.2464
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 81.792
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.2644
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 76.235

Attention duration (in seconds): 0.2695
Attention throughput (in TFLOP/s): 79.158
MLP duration (in seconds): 0.5107
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7802
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.1851
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 81.675
Elapsed time for attention_key_query_prob (512x2048x137x2048): 0.0242
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x137x2048): 24.286
Elapsed time for attention_prob_times_values (512x2048x2048x137): 0.0231
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x137): 25.467
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 82.277
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.2463
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 81.814
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.2644
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 76.229

Attention duration (in seconds): 0.2936
Attention throughput (in TFLOP/s): 72.642
MLP duration (in seconds): 0.5107
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.1856
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 82.638
Elapsed time for attention_key_query_prob (64x2048x1104x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1104x2048): 63.175
Elapsed time for attention_prob_times_values (64x2048x2048x1104): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1104): 65.068
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0614
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 83.227
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.2475
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 82.609
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.2643
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 77.371

Attention duration (in seconds): 0.2655
Attention throughput (in TFLOP/s): 81.484
MLP duration (in seconds): 0.5118
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7773
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.1856
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 82.643
Elapsed time for attention_key_query_prob (128x2048x552x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x552x2048): 59.859
Elapsed time for attention_prob_times_values (128x2048x2048x552): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x552): 64.618
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0614
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 83.278
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.2474
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 82.653
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.2644
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 77.324

Attention duration (in seconds): 0.2660
Attention throughput (in TFLOP/s): 81.320
MLP duration (in seconds): 0.5118
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7779
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.1856
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 82.649
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 55.270
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 48.190
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0614
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 83.264
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.2474
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 82.639
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.2645
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 77.321

Attention duration (in seconds): 0.2700
Attention throughput (in TFLOP/s): 80.132
MLP duration (in seconds): 0.5119
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7819
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.1856
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 82.629
Elapsed time for attention_key_query_prob (512x2048x138x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x138x2048): 41.445
Elapsed time for attention_prob_times_values (512x2048x2048x138): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x138): 46.068
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0614
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 83.274
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.2474
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 82.658
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.2643
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 77.374

Attention duration (in seconds): 0.2742
Attention throughput (in TFLOP/s): 78.910
MLP duration (in seconds): 0.5117
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7858
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.1914
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 81.310
Elapsed time for attention_key_query_prob (64x2048x1112x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1112x2048): 62.182
Elapsed time for attention_prob_times_values (64x2048x2048x1112): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1112): 64.604
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0637
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 81.434
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.2548
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 81.429
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.2737
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 75.803

Attention duration (in seconds): 0.2739
Attention throughput (in TFLOP/s): 80.105
MLP duration (in seconds): 0.5285
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.1915
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 81.262
Elapsed time for attention_key_query_prob (128x2048x556x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x556x2048): 64.279
Elapsed time for attention_prob_times_values (128x2048x2048x556): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x556): 61.756
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0636
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 81.488
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.2548
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 81.416
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.2748
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 75.493

Attention duration (in seconds): 0.2741
Attention throughput (in TFLOP/s): 80.051
MLP duration (in seconds): 0.5296
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.1915
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 81.260
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 55.804
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 48.871
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0635
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 81.710
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.2547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 81.438
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.2752
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 75.393

Attention duration (in seconds): 0.2779
Attention throughput (in TFLOP/s): 78.959
MLP duration (in seconds): 0.5299
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.1914
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 81.278
Elapsed time for attention_key_query_prob (512x2048x139x2048): 0.0235
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x139x2048): 25.370
Elapsed time for attention_prob_times_values (512x2048x2048x139): 0.0228
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x139): 26.205
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0631
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 82.141
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.2547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 81.437
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.2741
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 75.694

Attention duration (in seconds): 0.3009
Attention throughput (in TFLOP/s): 72.917
MLP duration (in seconds): 0.5288
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.1875
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 84.170
Elapsed time for attention_key_query_prob (64x2048x1120x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1120x2048): 66.658
Elapsed time for attention_prob_times_values (64x2048x2048x1120): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1120): 67.738
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0619
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 84.960
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.2502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 84.125
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.2794
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 75.319

Attention duration (in seconds): 0.2674
Attention throughput (in TFLOP/s): 83.216
MLP duration (in seconds): 0.5296
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7969
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.1875
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 84.169
Elapsed time for attention_key_query_prob (128x2048x560x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x560x2048): 62.195
Elapsed time for attention_prob_times_values (128x2048x2048x560): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x560): 66.339
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0625
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 84.221
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.2502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 84.124
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.2788
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 75.480

Attention duration (in seconds): 0.2687
Attention throughput (in TFLOP/s): 82.789
MLP duration (in seconds): 0.5290
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7977
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.1875
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 84.166
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 54.240
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 57.571
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0619
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 84.975
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.2502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 84.122
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.2787
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 75.507

Attention duration (in seconds): 0.2710
Attention throughput (in TFLOP/s): 82.102
MLP duration (in seconds): 0.5289
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7999
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.1875
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 84.180
Elapsed time for attention_key_query_prob (512x2048x140x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x140x2048): 41.676
Elapsed time for attention_prob_times_values (512x2048x2048x140): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x140): 45.842
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0619
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 84.980
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.2482
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 84.808
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.2786
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 75.547

Attention duration (in seconds): 0.2770
Attention throughput (in TFLOP/s): 80.329
MLP duration (in seconds): 0.5267
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.1886
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 84.872
Elapsed time for attention_key_query_prob (64x2048x1128x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1128x2048): 61.638
Elapsed time for attention_prob_times_values (64x2048x2048x1128): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1128): 66.897
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 84.881
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.2521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 84.664
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.2768
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 77.119

Attention duration (in seconds): 0.2704
Attention throughput (in TFLOP/s): 83.428
MLP duration (in seconds): 0.5289
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7993
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.1886
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 84.870
Elapsed time for attention_key_query_prob (128x2048x564x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x564x2048): 65.076
Elapsed time for attention_prob_times_values (128x2048x2048x564): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x564): 61.123
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 84.882
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.2521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 84.665
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.2768
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 77.115

Attention duration (in seconds): 0.2707
Attention throughput (in TFLOP/s): 83.323
MLP duration (in seconds): 0.5290
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7997
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.1886
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 84.871
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 56.154
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 49.444
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 84.886
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.2521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 84.667
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.2766
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 77.181

Attention duration (in seconds): 0.2745
Attention throughput (in TFLOP/s): 82.166
MLP duration (in seconds): 0.5287
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.1886
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 84.871
Elapsed time for attention_key_query_prob (512x2048x141x2048): 0.0239
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x141x2048): 25.308
Elapsed time for attention_prob_times_values (512x2048x2048x141): 0.0228
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x141): 26.507
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 84.884
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.2521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 84.666
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.2767
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 77.136

Attention duration (in seconds): 0.2983
Attention throughput (in TFLOP/s): 75.626
MLP duration (in seconds): 0.5289
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.1986
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 81.783
Elapsed time for attention_key_query_prob (64x2048x1136x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1136x2048): 63.046
Elapsed time for attention_prob_times_values (64x2048x2048x1136): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1136): 68.124
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0661
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 81.899
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.2647
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 81.794
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.2854
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 75.862

Attention duration (in seconds): 0.2833
Attention throughput (in TFLOP/s): 80.739
MLP duration (in seconds): 0.5501
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8334
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.1984
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 81.841
Elapsed time for attention_key_query_prob (128x2048x568x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x568x2048): 60.440
Elapsed time for attention_prob_times_values (128x2048x2048x568): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x568): 66.346
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0661
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 81.844
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.2645
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 81.844
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.2852
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 75.903

Attention duration (in seconds): 0.2838
Attention throughput (in TFLOP/s): 80.579
MLP duration (in seconds): 0.5498
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8336
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.1985
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 81.805
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 55.994
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 49.136
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0659
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 82.194
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.2646
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 81.827
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.2848
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 76.018

Attention duration (in seconds): 0.2877
Attention throughput (in TFLOP/s): 79.507
MLP duration (in seconds): 0.5494
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8371
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.1984
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 81.845
Elapsed time for attention_key_query_prob (512x2048x142x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x142x2048): 42.346
Elapsed time for attention_prob_times_values (512x2048x2048x142): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x142): 47.236
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0657
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 82.400
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.2645
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 81.843
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.2853
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 75.887

Attention duration (in seconds): 0.2914
Attention throughput (in TFLOP/s): 78.485
MLP duration (in seconds): 0.5498
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8413
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.1994
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 82.570
Elapsed time for attention_key_query_prob (64x2048x1144x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1144x2048): 61.675
Elapsed time for attention_prob_times_values (64x2048x2048x1144): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1144): 67.782
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0661
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 83.015
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.2660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 82.529
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.2852
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 76.986

Attention duration (in seconds): 0.2846
Attention throughput (in TFLOP/s): 81.471
MLP duration (in seconds): 0.5513
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.1993
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 82.609
Elapsed time for attention_key_query_prob (128x2048x572x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x572x2048): 65.419
Elapsed time for attention_prob_times_values (128x2048x2048x572): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x572): 61.816
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0661
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 83.071
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.2658
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 82.611
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.2848
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 77.098

Attention duration (in seconds): 0.2847
Attention throughput (in TFLOP/s): 81.424
MLP duration (in seconds): 0.5506
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8353
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.1995
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 82.534
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 57.069
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 50.024
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0660
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 83.157
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.2660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 82.551
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.2847
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 77.125

Attention duration (in seconds): 0.2886
Attention throughput (in TFLOP/s): 80.344
MLP duration (in seconds): 0.5507
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8392
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.1993
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 82.619
Elapsed time for attention_key_query_prob (512x2048x143x2048): 0.0245
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x143x2048): 25.113
Elapsed time for attention_prob_times_values (512x2048x2048x143): 0.0233
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x143): 26.389
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0660
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 83.152
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.2658
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 82.602
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.2867
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 76.590

Attention duration (in seconds): 0.3131
Attention throughput (in TFLOP/s): 74.059
MLP duration (in seconds): 0.5525
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8656
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.2048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 81.542
Elapsed time for attention_key_query_prob (64x2048x1152x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1152x2048): 66.523
Elapsed time for attention_prob_times_values (64x2048x2048x1152): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1152): 69.606
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 81.626
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.2731
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 81.524
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.2931
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 75.954

Attention duration (in seconds): 0.2912
Attention throughput (in TFLOP/s): 80.718
MLP duration (in seconds): 0.5663
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8574
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.2048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 81.523
Elapsed time for attention_key_query_prob (128x2048x576x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x576x2048): 63.602
Elapsed time for attention_prob_times_values (128x2048x2048x576): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x576): 68.641
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 81.633
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.2734
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 81.440
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.2929
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 76.004

Attention duration (in seconds): 0.2918
Attention throughput (in TFLOP/s): 80.554
MLP duration (in seconds): 0.5663
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8581
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.2048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 81.550
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 58.642
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 60.372
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0676
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 82.288
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.2730
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 81.556
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.2931
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 75.971

Attention duration (in seconds): 0.2932
Attention throughput (in TFLOP/s): 80.156
MLP duration (in seconds): 0.5661
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8593
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.2048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 81.524
Elapsed time for attention_key_query_prob (512x2048x144x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x144x2048): 45.136
Elapsed time for attention_prob_times_values (512x2048x2048x144): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x144): 48.577
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0676
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 82.311
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.2728
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 81.620
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.2935
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 75.854

Attention duration (in seconds): 0.2989
Attention throughput (in TFLOP/s): 78.630
MLP duration (in seconds): 0.5663
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8652
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.2003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 84.518
Elapsed time for attention_key_query_prob (64x2048x1160x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1160x2048): 61.671
Elapsed time for attention_prob_times_values (64x2048x2048x1160): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1160): 65.185
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0667
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 84.631
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.2669
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 84.582
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.3003
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 75.182

Attention duration (in seconds): 0.2867
Attention throughput (in TFLOP/s): 83.095
MLP duration (in seconds): 0.5672
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8539
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.2003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 84.520
Elapsed time for attention_key_query_prob (128x2048x580x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x580x2048): 64.137
Elapsed time for attention_prob_times_values (128x2048x2048x580): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x580): 63.071
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0667
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 84.645
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.2669
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 84.582
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.3002
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 75.206

Attention duration (in seconds): 0.2866
Attention throughput (in TFLOP/s): 83.119
MLP duration (in seconds): 0.5671
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8537
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.2003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 84.520
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 55.201
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 50.565
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0661
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 85.362
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.2669
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 84.582
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.3001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 75.221

Attention duration (in seconds): 0.2900
Attention throughput (in TFLOP/s): 82.129
MLP duration (in seconds): 0.5670
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8571
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.2003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 84.519
Elapsed time for attention_key_query_prob (512x2048x145x2048): 0.0245
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x145x2048): 25.413
Elapsed time for attention_prob_times_values (512x2048x2048x145): 0.0233
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x145): 26.747
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0661
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 85.386
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.2669
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 84.583
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.3002
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 75.194

Attention duration (in seconds): 0.3142
Attention throughput (in TFLOP/s): 75.811
MLP duration (in seconds): 0.5671
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8813
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.2048
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 83.809
Elapsed time for attention_key_query_prob (64x2048x1168x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1168x2048): 62.360
Elapsed time for attention_prob_times_values (64x2048x2048x1168): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1168): 64.419
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0689
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 82.988
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.2760
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 82.923
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.3058
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 74.845

Attention duration (in seconds): 0.2936
Attention throughput (in TFLOP/s): 82.238
MLP duration (in seconds): 0.5818
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8754
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.2069
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 82.970
Elapsed time for attention_key_query_prob (128x2048x584x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x584x2048): 58.431
Elapsed time for attention_prob_times_values (128x2048x2048x584): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x584): 60.314
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0690
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 82.962
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.2761
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 82.910
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.3063
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 74.713

Attention duration (in seconds): 0.2970
Attention throughput (in TFLOP/s): 81.288
MLP duration (in seconds): 0.5824
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8794
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.2069
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 82.972
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 53.298
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 48.361
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0690
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 82.969
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.2760
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 82.924
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.3082
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 74.273

Attention duration (in seconds): 0.3006
Attention throughput (in TFLOP/s): 80.317
MLP duration (in seconds): 0.5842
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8848
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.2069
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 82.976
Elapsed time for attention_key_query_prob (512x2048x146x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x146x2048): 42.024
Elapsed time for attention_prob_times_values (512x2048x2048x146): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x146): 46.891
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0689
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 83.001
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.2760
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 82.922
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.3080
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 74.319

Attention duration (in seconds): 0.3041
Attention throughput (in TFLOP/s): 79.386
MLP duration (in seconds): 0.5840
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8881
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.2213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 78.640
Elapsed time for attention_key_query_prob (64x2048x1176x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1176x2048): 60.055
Elapsed time for attention_prob_times_values (64x2048x2048x1176): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1176): 62.860
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0741
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 78.315
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.2968
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 78.165
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.3209
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 72.306

Attention duration (in seconds): 0.3159
Attention throughput (in TFLOP/s): 77.444
MLP duration (in seconds): 0.6177
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9336
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.2233
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 77.937
Elapsed time for attention_key_query_prob (128x2048x588x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x588x2048): 62.098
Elapsed time for attention_prob_times_values (128x2048x2048x588): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x588): 60.876
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0741
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 78.249
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.2980
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 77.858
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.3230
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 71.842

Attention duration (in seconds): 0.3179
Attention throughput (in TFLOP/s): 76.947
MLP duration (in seconds): 0.6210
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9389
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.2230
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 78.018
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 53.211
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 48.662
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0741
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 78.290
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.2981
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 77.829
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.3216
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 72.156

Attention duration (in seconds): 0.3220
Attention throughput (in TFLOP/s): 75.983
MLP duration (in seconds): 0.6197
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9417
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.2231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 77.990
Elapsed time for attention_key_query_prob (512x2048x147x2048): 0.0256
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x147x2048): 24.663
Elapsed time for attention_prob_times_values (512x2048x2048x147): 0.0237
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x147): 26.691
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0741
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 78.267
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.2980
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 77.858
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.3223
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 71.999

Attention duration (in seconds): 0.3465
Attention throughput (in TFLOP/s): 70.608
MLP duration (in seconds): 0.6203
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9668
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.2223
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 79.350
Elapsed time for attention_key_query_prob (64x2048x1184x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1184x2048): 64.204
Elapsed time for attention_prob_times_values (64x2048x2048x1184): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1184): 64.222
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0739
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 79.566
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.2964
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 79.354
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.3283
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 71.641

Attention duration (in seconds): 0.3160
Attention throughput (in TFLOP/s): 78.452
MLP duration (in seconds): 0.6247
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9407
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.2224
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 79.298
Elapsed time for attention_key_query_prob (128x2048x592x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x592x2048): 59.241
Elapsed time for attention_prob_times_values (128x2048x2048x592): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x592): 60.573
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0738
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 79.675
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.2964
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 79.356
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.3285
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 71.590

Attention duration (in seconds): 0.3175
Attention throughput (in TFLOP/s): 78.089
MLP duration (in seconds): 0.6249
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9424
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.2224
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 79.301
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 51.689
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 58.105
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0736
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 79.841
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.2964
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 79.350
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.3282
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 71.651

Attention duration (in seconds): 0.3193
Attention throughput (in TFLOP/s): 77.636
MLP duration (in seconds): 0.6246
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9440
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.2223
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 79.340
Elapsed time for attention_key_query_prob (512x2048x148x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x148x2048): 41.310
Elapsed time for attention_prob_times_values (512x2048x2048x148): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x148): 45.976
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0736
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 79.904
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.2963
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 79.377
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.3289
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 71.518

Attention duration (in seconds): 0.3251
Attention throughput (in TFLOP/s): 76.249
MLP duration (in seconds): 0.6252
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9503
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.2296
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 77.880
Elapsed time for attention_key_query_prob (64x2048x1192x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1192x2048): 58.872
Elapsed time for attention_prob_times_values (64x2048x2048x1192): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1192): 63.293
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0762
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 78.226
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.3058
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 77.942
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.3325
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 71.691

Attention duration (in seconds): 0.3267
Attention throughput (in TFLOP/s): 76.877
MLP duration (in seconds): 0.6384
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9651
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.2297
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 77.849
Elapsed time for attention_key_query_prob (128x2048x596x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x596x2048): 62.816
Elapsed time for attention_prob_times_values (128x2048x2048x596): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x596): 61.480
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0762
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 78.205
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.3057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 77.975
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.3323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 71.747

Attention duration (in seconds): 0.3265
Attention throughput (in TFLOP/s): 76.941
MLP duration (in seconds): 0.6380
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9644
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.2296
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 77.881
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 53.822
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 48.745
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0761
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 78.317
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.3057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 77.969
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.3323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 71.744

Attention duration (in seconds): 0.3307
Attention throughput (in TFLOP/s): 75.960
MLP duration (in seconds): 0.6380
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.2296
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 77.882
Elapsed time for attention_key_query_prob (512x2048x149x2048): 0.0260
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x149x2048): 24.570
Elapsed time for attention_prob_times_values (512x2048x2048x149): 0.0237
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x149): 26.991
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0760
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 78.433
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.3057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 77.990
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.3322
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 71.753

Attention duration (in seconds): 0.3553
Attention throughput (in TFLOP/s): 70.696
MLP duration (in seconds): 0.6379
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9932
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.2257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 80.282
Elapsed time for attention_key_query_prob (64x2048x1200x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1200x2048): 60.742
Elapsed time for attention_prob_times_values (64x2048x2048x1200): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1200): 64.515
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0745
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 81.037
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.2996
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 80.625
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.3356
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 71.983

Attention duration (in seconds): 0.3208
Attention throughput (in TFLOP/s): 79.320
MLP duration (in seconds): 0.6353
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9561
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.2257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 80.284
Elapsed time for attention_key_query_prob (128x2048x600x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x600x2048): 58.246
Elapsed time for attention_prob_times_values (128x2048x2048x600): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x600): 60.561
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0745
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 81.086
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.3010
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 80.252
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.3362
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 71.857

Attention duration (in seconds): 0.3219
Attention throughput (in TFLOP/s): 79.061
MLP duration (in seconds): 0.6373
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9591
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.2257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 80.283
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 53.210
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 48.897
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0744
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 81.154
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.3004
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 80.414
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.3362
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 71.859

Attention duration (in seconds): 0.3254
Attention throughput (in TFLOP/s): 78.204
MLP duration (in seconds): 0.6366
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9620
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.2257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 80.291
Elapsed time for attention_key_query_prob (512x2048x150x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x150x2048): 41.772
Elapsed time for attention_prob_times_values (512x2048x2048x150): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x150): 47.153
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0744
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 81.190
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.3003
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 80.460
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.3364
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 71.813

Attention duration (in seconds): 0.3291
Attention throughput (in TFLOP/s): 77.314
MLP duration (in seconds): 0.6367
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9658
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.2277
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 80.630
Elapsed time for attention_key_query_prob (64x2048x1208x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1208x2048): 59.164
Elapsed time for attention_prob_times_values (64x2048x2048x1208): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1208): 63.995
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0754
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 81.133
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.3021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 81.031
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.3375
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 72.535

Attention duration (in seconds): 0.3243
Attention throughput (in TFLOP/s): 79.501
MLP duration (in seconds): 0.6397
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9639
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.2277
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 80.630
Elapsed time for attention_key_query_prob (128x2048x604x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x604x2048): 62.999
Elapsed time for attention_prob_times_values (128x2048x2048x604): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x604): 61.767
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0756
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 80.995
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.3021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 81.032
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.3374
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 72.551

Attention duration (in seconds): 0.3241
Attention throughput (in TFLOP/s): 79.544
MLP duration (in seconds): 0.6396
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9637
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.2277
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 80.632
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 54.037
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 49.709
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0754
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 81.128
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.3021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 81.030
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.3372
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 72.611

Attention duration (in seconds): 0.3282
Attention throughput (in TFLOP/s): 78.544
MLP duration (in seconds): 0.6393
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9675
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.2277
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 80.632
Elapsed time for attention_key_query_prob (512x2048x151x2048): 0.0265
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x151x2048): 24.493
Elapsed time for attention_prob_times_values (512x2048x2048x151): 0.0240
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x151): 26.995
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0753
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 81.307
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.3021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 81.033
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.3372
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 72.603

Attention duration (in seconds): 0.3535
Attention throughput (in TFLOP/s): 72.925
MLP duration (in seconds): 0.6393
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9928
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.2369
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 78.523
Elapsed time for attention_key_query_prob (64x2048x1216x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1216x2048): 62.481
Elapsed time for attention_prob_times_values (64x2048x2048x1216): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1216): 66.488
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0791
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 78.431
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.3158
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 78.562
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.3485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 71.187

Attention duration (in seconds): 0.3363
Attention throughput (in TFLOP/s): 77.652
MLP duration (in seconds): 0.6643
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0005
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.2369
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 78.525
Elapsed time for attention_key_query_prob (128x2048x608x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x608x2048): 62.765
Elapsed time for attention_prob_times_values (128x2048x2048x608): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x608): 62.626
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0787
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 78.767
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.3158
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 78.550
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.3485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 71.193

Attention duration (in seconds): 0.3365
Attention throughput (in TFLOP/s): 77.602
MLP duration (in seconds): 0.6643
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0008
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.2369
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 78.547
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 54.056
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 60.078
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0787
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 78.831
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.3158
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 78.562
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.3485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 71.189

Attention duration (in seconds): 0.3385
Attention throughput (in TFLOP/s): 77.146
MLP duration (in seconds): 0.6642
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.2369
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 78.526
Elapsed time for attention_key_query_prob (512x2048x152x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x152x2048): 44.557
Elapsed time for attention_prob_times_values (512x2048x2048x152): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x152): 48.558
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0786
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 78.884
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.3157
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 78.587
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.3485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 71.175

Attention duration (in seconds): 0.3437
Attention throughput (in TFLOP/s): 75.987
MLP duration (in seconds): 0.6642
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.2385
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 79.058
Elapsed time for attention_key_query_prob (64x2048x1224x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1224x2048): 59.187
Elapsed time for attention_prob_times_values (64x2048x2048x1224): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1224): 61.825
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0792
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 79.335
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.3182
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 78.992
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.3540
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 71.007

Attention duration (in seconds): 0.3394
Attention throughput (in TFLOP/s): 77.933
MLP duration (in seconds): 0.6722
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.2384
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 79.079
Elapsed time for attention_key_query_prob (128x2048x612x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x612x2048): 61.576
Elapsed time for attention_prob_times_values (128x2048x2048x612): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x612): 62.338
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0792
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 79.378
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.3179
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 79.061
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.3539
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 71.030

Attention duration (in seconds): 0.3388
Attention throughput (in TFLOP/s): 78.076
MLP duration (in seconds): 0.6718
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.2384
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 79.061
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 54.268
Elapsed time for attention_prob_times_values (256x2048x2048x306): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 50.017
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0791
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 79.453
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.3178
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 79.083
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.3539
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 71.018

Attention duration (in seconds): 0.3428
Attention throughput (in TFLOP/s): 77.162
MLP duration (in seconds): 0.6718
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.2385
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 79.039
Elapsed time for attention_key_query_prob (512x2048x153x2048): 0.0266
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x153x2048): 24.685
Elapsed time for attention_prob_times_values (512x2048x2048x153): 0.0241
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x153): 27.259
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0791
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 79.451
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.3178
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 79.093
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.3539
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 71.027

Attention duration (in seconds): 0.3683
Attention throughput (in TFLOP/s): 71.810
MLP duration (in seconds): 0.6717
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0400
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.2442
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 78.201
Elapsed time for attention_key_query_prob (64x2048x1232x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1232x2048): 60.572
Elapsed time for attention_prob_times_values (64x2048x2048x1232): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1232): 63.147
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0811
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 78.500
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.3255
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 78.226
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.3483
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 73.110

Attention duration (in seconds): 0.3467
Attention throughput (in TFLOP/s): 77.261
MLP duration (in seconds): 0.6738
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.2442
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 78.196
Elapsed time for attention_key_query_prob (128x2048x616x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x616x2048): 57.181
Elapsed time for attention_prob_times_values (128x2048x2048x616): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x616): 62.073
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0811
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 78.542
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.3255
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 78.228
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.3487
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 73.036

Attention duration (in seconds): 0.3475
Attention throughput (in TFLOP/s): 77.083
MLP duration (in seconds): 0.6742
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.2443
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 78.183
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 54.056
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 49.540
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0809
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 78.680
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.3255
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 78.234
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.3488
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 73.007

Attention duration (in seconds): 0.3508
Attention throughput (in TFLOP/s): 76.366
MLP duration (in seconds): 0.6743
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.2442
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 78.208
Elapsed time for attention_key_query_prob (512x2048x154x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x154x2048): 42.885
Elapsed time for attention_prob_times_values (512x2048x2048x154): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x154): 48.227
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0808
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 78.838
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.3255
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 78.226
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.3488
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 73.012

Attention duration (in seconds): 0.3541
Attention throughput (in TFLOP/s): 75.652
MLP duration (in seconds): 0.6743
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.2394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 80.814
Elapsed time for attention_key_query_prob (64x2048x1240x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1240x2048): 59.331
Elapsed time for attention_prob_times_values (64x2048x2048x1240): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1240): 62.515
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0798
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 80.790
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.3195
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 80.749
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.3584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 71.972

Attention duration (in seconds): 0.3411
Attention throughput (in TFLOP/s): 79.530
MLP duration (in seconds): 0.6779
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.2394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 80.814
Elapsed time for attention_key_query_prob (128x2048x620x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x620x2048): 62.259
Elapsed time for attention_prob_times_values (128x2048x2048x620): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x620): 62.992
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0797
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 80.948
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.3195
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 80.751
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.3589
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 71.873

Attention duration (in seconds): 0.3403
Attention throughput (in TFLOP/s): 79.709
MLP duration (in seconds): 0.6784
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.2394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 80.814
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 54.320
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 50.382
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0794
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 81.273
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.3195
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 80.750
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.3585
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 71.961

Attention duration (in seconds): 0.3442
Attention throughput (in TFLOP/s): 78.809
MLP duration (in seconds): 0.6779
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.2394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 80.814
Elapsed time for attention_key_query_prob (512x2048x155x2048): 0.0269
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x155x2048): 24.790
Elapsed time for attention_prob_times_values (512x2048x2048x155): 0.0238
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x155): 27.971
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0791
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 81.484
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.3195
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 80.751
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.3595
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 71.758

Attention duration (in seconds): 0.3692
Attention throughput (in TFLOP/s): 73.476
MLP duration (in seconds): 0.6790
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0482
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.2306
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 84.976
Elapsed time for attention_key_query_prob (64x2048x1248x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1248x2048): 66.077
Elapsed time for attention_prob_times_values (64x2048x2048x1248): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1248): 67.061
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0769
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 84.918
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.3102
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 84.246
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.3471
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 75.282

Attention duration (in seconds): 0.3277
Attention throughput (in TFLOP/s): 83.831
MLP duration (in seconds): 0.6573
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9850
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.2327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 84.231
Elapsed time for attention_key_query_prob (128x2048x624x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x624x2048): 61.927
Elapsed time for attention_prob_times_values (128x2048x2048x624): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x624): 65.534
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0776
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 84.166
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.3101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 84.254
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.3470
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 75.298

Attention duration (in seconds): 0.3313
Attention throughput (in TFLOP/s): 82.910
MLP duration (in seconds): 0.6572
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9885
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.2327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 84.238
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 54.837
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 63.061
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0776
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 84.173
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.3101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 84.256
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.3471
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 75.281

Attention duration (in seconds): 0.3331
Attention throughput (in TFLOP/s): 82.469
MLP duration (in seconds): 0.6572
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9903
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.2327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 84.234
Elapsed time for attention_key_query_prob (512x2048x156x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x156x2048): 44.234
Elapsed time for attention_prob_times_values (512x2048x2048x156): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x156): 49.681
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0775
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 84.266
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.3101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 84.257
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.3472
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 75.253

Attention duration (in seconds): 0.3388
Attention throughput (in TFLOP/s): 81.078
MLP duration (in seconds): 0.6574
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9962
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.2445
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 81.201
Elapsed time for attention_key_query_prob (64x2048x1256x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1256x2048): 61.521
Elapsed time for attention_prob_times_values (64x2048x2048x1256): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1256): 65.474
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0818
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 80.930
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.8617
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 30.713
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.3586
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 73.813

Attention duration (in seconds): 0.3475
Attention throughput (in TFLOP/s): 80.050
MLP duration (in seconds): 1.2203
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5678
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.2445
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 81.200
Elapsed time for attention_key_query_prob (128x2048x628x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x628x2048): 64.605
Elapsed time for attention_prob_times_values (128x2048x2048x628): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x628): 65.850
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0817
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 81.023
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.8602
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 30.767
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.3585
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 73.816

Attention duration (in seconds): 0.3468
Attention throughput (in TFLOP/s): 80.206
MLP duration (in seconds): 1.2188
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5656
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.2445
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 81.183
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 56.702
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 52.949
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0816
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 81.077
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.8602
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 30.767
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.3586
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 73.810

Attention duration (in seconds): 0.3507
Attention throughput (in TFLOP/s): 79.303
MLP duration (in seconds): 1.2188
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5696
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.2446
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 81.158
Elapsed time for attention_key_query_prob (512x2048x157x2048): 0.0261
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x157x2048): 25.837
Elapsed time for attention_prob_times_values (512x2048x2048x157): 0.0233
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x157): 28.976
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0817
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 81.013
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.8601
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 30.770
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.3585
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 73.820

Attention duration (in seconds): 0.3756
Attention throughput (in TFLOP/s): 74.050
MLP duration (in seconds): 1.2187
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5943
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.2470
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 81.404
Elapsed time for attention_key_query_prob (64x2048x1264x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1264x2048): 62.394
Elapsed time for attention_prob_times_values (64x2048x2048x1264): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1264): 65.967
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0824
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 81.359
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.3307
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 81.065
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.4405
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 60.853

Attention duration (in seconds): 0.3505
Attention throughput (in TFLOP/s): 80.351
MLP duration (in seconds): 0.7711
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.2487
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 80.828
Elapsed time for attention_key_query_prob (128x2048x632x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x632x2048): 59.697
Elapsed time for attention_prob_times_values (128x2048x2048x632): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x632): 64.343
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0825
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 81.215
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.3311
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 80.958
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.4404
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 60.861

Attention duration (in seconds): 0.3531
Attention throughput (in TFLOP/s): 79.746
MLP duration (in seconds): 0.7715
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.2488
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 80.792
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 55.690
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 51.804
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0825
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 81.228
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.3311
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 80.952
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.4404
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 60.863

Attention duration (in seconds): 0.3566
Attention throughput (in TFLOP/s): 78.970
MLP duration (in seconds): 0.7715
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.2486
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 80.880
Elapsed time for attention_key_query_prob (512x2048x158x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x158x2048): 44.342
Elapsed time for attention_prob_times_values (512x2048x2048x158): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x158): 49.850
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0825
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 81.250
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.3309
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 80.995
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.4404
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 60.866

Attention duration (in seconds): 0.3600
Attention throughput (in TFLOP/s): 78.238
MLP duration (in seconds): 0.7713
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.2567
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 79.307
Elapsed time for attention_key_query_prob (64x2048x1272x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1272x2048): 60.141
Elapsed time for attention_prob_times_values (64x2048x2048x1272): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1272): 64.984
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0855
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 79.345
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.8756
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 31.001
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.4511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 60.175

Attention duration (in seconds): 0.3641
Attention throughput (in TFLOP/s): 78.305
MLP duration (in seconds): 1.3267
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.6908
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.2567
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 79.304
Elapsed time for attention_key_query_prob (128x2048x636x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x636x2048): 63.716
Elapsed time for attention_prob_times_values (128x2048x2048x636): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x636): 65.381
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0855
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 79.349
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.8766
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 30.968
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.4511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 60.172

Attention duration (in seconds): 0.3634
Attention throughput (in TFLOP/s): 78.455
MLP duration (in seconds): 1.3277
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.6911
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.2568
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 79.294
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 56.214
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 52.469
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0853
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 79.538
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.8747
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 31.034
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.4511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 60.175

Attention duration (in seconds): 0.3672
Attention throughput (in TFLOP/s): 77.636
MLP duration (in seconds): 1.3258
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.6930
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.2567
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 79.295
Elapsed time for attention_key_query_prob (512x2048x159x2048): 0.0263
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x159x2048): 25.958
Elapsed time for attention_prob_times_values (512x2048x2048x159): 0.0240
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x159): 28.510
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0855
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 79.339
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.8757
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 30.998
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.4512
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 60.169

Attention duration (in seconds): 0.3925
Attention throughput (in TFLOP/s): 72.631
MLP duration (in seconds): 1.3269
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.2518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 81.865
Elapsed time for attention_key_query_prob (64x2048x1280x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1280x2048): 63.098
Elapsed time for attention_prob_times_values (64x2048x2048x1280): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1280): 67.193
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0849
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 80.969
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.3367
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 81.643
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.4735
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 58.052

Attention duration (in seconds): 0.3578
Attention throughput (in TFLOP/s): 80.662
MLP duration (in seconds): 0.8102
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1680
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.2518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 81.867
Elapsed time for attention_key_query_prob (128x2048x640x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x640x2048): 62.362
Elapsed time for attention_prob_times_values (128x2048x2048x640): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x640): 66.785
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0849
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 80.967
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.3367
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 81.642
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.4734
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 58.059

Attention duration (in seconds): 0.3580
Attention throughput (in TFLOP/s): 80.620
MLP duration (in seconds): 0.8101
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1681
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.2519
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 81.835
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 57.278
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 65.430
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0849
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 80.983
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.3367
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 81.639
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.4735
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 58.048

Attention duration (in seconds): 0.3593
Attention throughput (in TFLOP/s): 80.334
MLP duration (in seconds): 0.8102
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1695
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.2518
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 81.861
Elapsed time for attention_key_query_prob (512x2048x160x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x160x2048): 49.021
Elapsed time for attention_prob_times_values (512x2048x2048x160): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x160): 52.725
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0842
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 81.645
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.3367
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 81.641
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.4734
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 58.067

Attention duration (in seconds): 0.3631
Attention throughput (in TFLOP/s): 79.497
MLP duration (in seconds): 0.8101
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1731
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.2514
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 83.022
Elapsed time for attention_key_query_prob (64x2048x1288x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1288x2048): 59.208
Elapsed time for attention_prob_times_values (64x2048x2048x1288): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1288): 63.470
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0836
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 83.197
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.3345
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 83.201
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.4554
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 61.117

Attention duration (in seconds): 0.3576
Attention throughput (in TFLOP/s): 81.689
MLP duration (in seconds): 0.7899
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1476
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.2514
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 83.023
Elapsed time for attention_key_query_prob (128x2048x644x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x644x2048): 62.657
Elapsed time for attention_prob_times_values (128x2048x2048x644): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x644): 65.331
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0836
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 83.189
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.3345
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 83.203
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.4554
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 61.110

Attention duration (in seconds): 0.3567
Attention throughput (in TFLOP/s): 81.907
MLP duration (in seconds): 0.7900
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1467
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.2514
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 83.023
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 54.481
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 52.914
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0836
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 83.196
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.3345
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 83.202
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.4555
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 61.098

Attention duration (in seconds): 0.3608
Attention throughput (in TFLOP/s): 80.969
MLP duration (in seconds): 0.7901
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1509
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.2514
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 83.022
Elapsed time for attention_key_query_prob (512x2048x161x2048): 0.0281
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x161x2048): 24.646
Elapsed time for attention_prob_times_values (512x2048x2048x161): 0.0238
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x161): 29.018
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0836
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 83.193
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.3345
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 83.202
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.4553
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 61.128

Attention duration (in seconds): 0.3870
Attention throughput (in TFLOP/s): 75.500
MLP duration (in seconds): 0.7898
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1768
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.2643
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 79.963
Elapsed time for attention_key_query_prob (64x2048x1296x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1296x2048): 62.129
Elapsed time for attention_prob_times_values (64x2048x2048x1296): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1296): 63.949
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0882
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 79.838
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.9366
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 30.087
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.4600
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 61.260

Attention duration (in seconds): 0.3746
Attention throughput (in TFLOP/s): 78.935
MLP duration (in seconds): 1.3966
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7712
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.2644
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 79.920
Elapsed time for attention_key_query_prob (128x2048x648x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x648x2048): 58.904
Elapsed time for attention_prob_times_values (128x2048x2048x648): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x648): 60.690
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0882
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 79.893
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.9357
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 30.116
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.4601
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 61.250

Attention duration (in seconds): 0.3759
Attention throughput (in TFLOP/s): 78.667
MLP duration (in seconds): 1.3958
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7717
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.2644
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 79.940
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 54.459
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 52.573
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0881
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 79.943
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.9356
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 30.117
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.4601
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 61.249

Attention duration (in seconds): 0.3785
Attention throughput (in TFLOP/s): 78.124
MLP duration (in seconds): 1.3957
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7742
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.2642
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 79.986
Elapsed time for attention_key_query_prob (512x2048x162x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x162x2048): 42.603
Elapsed time for attention_prob_times_values (512x2048x2048x162): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x162): 50.981
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0882
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 79.898
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.9359
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 30.108
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.4600
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 61.259

Attention duration (in seconds): 0.3824
Attention throughput (in TFLOP/s): 77.334
MLP duration (in seconds): 1.3959
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7783
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.2652
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 80.674
Elapsed time for attention_key_query_prob (64x2048x1304x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1304x2048): 60.403
Elapsed time for attention_prob_times_values (64x2048x2048x1304): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1304): 63.590
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0882
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 80.817
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.3531
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 80.801
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.4698
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 60.721

Attention duration (in seconds): 0.3761
Attention throughput (in TFLOP/s): 79.583
MLP duration (in seconds): 0.8229
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1990
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.2654
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 80.625
Elapsed time for attention_key_query_prob (128x2048x652x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x652x2048): 64.069
Elapsed time for attention_prob_times_values (128x2048x2048x652): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x652): 65.696
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0881
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 80.926
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.3531
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 80.797
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.4701
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 60.688

Attention duration (in seconds): 0.3751
Attention throughput (in TFLOP/s): 79.789
MLP duration (in seconds): 0.8232
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1983
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.2651
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 80.706
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 55.232
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 53.473
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0879
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 81.118
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.3531
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 80.794
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.4699
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 60.716

Attention duration (in seconds): 0.3788
Attention throughput (in TFLOP/s): 79.008
MLP duration (in seconds): 0.8230
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.2653
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 80.656
Elapsed time for attention_key_query_prob (512x2048x163x2048): 0.0286
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x163x2048): 24.514
Elapsed time for attention_prob_times_values (512x2048x2048x163): 0.0237
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x163): 29.479
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0877
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 81.339
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.3529
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 80.833
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.4699
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 60.705

Attention duration (in seconds): 0.4053
Attention throughput (in TFLOP/s): 73.848
MLP duration (in seconds): 0.8229
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.2660
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 81.430
Elapsed time for attention_key_query_prob (64x2048x1312x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1312x2048): 66.333
Elapsed time for attention_prob_times_values (64x2048x2048x1312): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1312): 66.633
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0885
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 81.625
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.9358
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 30.861
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.4711
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 61.308

Attention duration (in seconds): 0.3756
Attention throughput (in TFLOP/s): 80.633
MLP duration (in seconds): 1.4069
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7825
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.2680
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 80.813
Elapsed time for attention_key_query_prob (128x2048x656x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x656x2048): 61.895
Elapsed time for attention_prob_times_values (128x2048x2048x656): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x656): 63.126
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0890
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 81.118
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.9367
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 30.832
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.4712
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 61.289

Attention duration (in seconds): 0.3796
Attention throughput (in TFLOP/s): 79.798
MLP duration (in seconds): 1.4079
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7874
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.2680
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 80.811
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 54.281
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 56.751
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0888
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 81.319
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.9351
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 30.884
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.4713
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 61.279

Attention duration (in seconds): 0.3822
Attention throughput (in TFLOP/s): 79.247
MLP duration (in seconds): 1.4064
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7886
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.2682
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 80.745
Elapsed time for attention_key_query_prob (512x2048x164x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x164x2048): 43.025
Elapsed time for attention_prob_times_values (512x2048x2048x164): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x164): 51.606
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0885
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 81.567
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.9351
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 30.884
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.4715
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 61.256

Attention duration (in seconds): 0.3868
Attention throughput (in TFLOP/s): 78.308
MLP duration (in seconds): 1.4066
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7933
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.2619
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 83.713
Elapsed time for attention_key_query_prob (64x2048x1320x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1320x2048): 60.556
Elapsed time for attention_prob_times_values (64x2048x2048x1320): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1320): 65.576
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0869
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 84.109
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.3495
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 83.651
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.4756
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 61.461

Attention duration (in seconds): 0.3713
Attention throughput (in TFLOP/s): 82.548
MLP duration (in seconds): 0.8251
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1964
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.2615
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 83.826
Elapsed time for attention_key_query_prob (128x2048x660x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x660x2048): 65.653
Elapsed time for attention_prob_times_values (128x2048x2048x660): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x660): 67.459
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0869
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 84.094
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.3485
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 83.875
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.4756
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 61.459

Attention duration (in seconds): 0.3698
Attention throughput (in TFLOP/s): 82.893
MLP duration (in seconds): 0.8242
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1939
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.2616
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 83.801
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 56.430
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 54.952
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0869
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 84.119
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.3490
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 83.756
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.4758
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 61.441

Attention duration (in seconds): 0.3740
Attention throughput (in TFLOP/s): 81.961
MLP duration (in seconds): 0.8248
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1988
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.2616
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 83.820
Elapsed time for attention_key_query_prob (512x2048x165x2048): 0.0283
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x165x2048): 25.082
Elapsed time for attention_prob_times_values (512x2048x2048x165): 0.0235
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x165): 30.149
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0869
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 84.123
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.3486
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 83.859
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.4757
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 61.447

Attention duration (in seconds): 0.4002
Attention throughput (in TFLOP/s): 76.587
MLP duration (in seconds): 0.8243
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.2634
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 84.247
Elapsed time for attention_key_query_prob (64x2048x1328x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1328x2048): 62.143
Elapsed time for attention_prob_times_values (64x2048x2048x1328): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1328): 66.808
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0879
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 84.164
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.3517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 84.121
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.4829
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 61.273

Attention duration (in seconds): 0.3734
Attention throughput (in TFLOP/s): 83.050
MLP duration (in seconds): 0.8346
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.2634
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 84.244
Elapsed time for attention_key_query_prob (128x2048x664x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x664x2048): 60.948
Elapsed time for attention_prob_times_values (128x2048x2048x664): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x664): 63.084
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0872
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 84.808
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.3517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 84.122
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.4832
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 61.232

Attention duration (in seconds): 0.3736
Attention throughput (in TFLOP/s): 83.006
MLP duration (in seconds): 0.8349
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.2634
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 84.242
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 56.669
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 54.530
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0876
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 84.478
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.3517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 84.125
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.4830
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 61.262

Attention duration (in seconds): 0.3766
Attention throughput (in TFLOP/s): 82.345
MLP duration (in seconds): 0.8347
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.2634
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 84.251
Elapsed time for attention_key_query_prob (512x2048x166x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x166x2048): 43.951
Elapsed time for attention_prob_times_values (512x2048x2048x166): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x166): 53.008
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0872
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 84.817
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.3517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 84.118
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.4831
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 61.242

Attention duration (in seconds): 0.3803
Attention throughput (in TFLOP/s): 81.556
MLP duration (in seconds): 0.8349
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.2775
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 80.942
Elapsed time for attention_key_query_prob (64x2048x1336x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1336x2048): 60.759
Elapsed time for attention_prob_times_values (64x2048x2048x1336): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1336): 66.222
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0926
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 80.848
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.9962
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 30.061
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.4817
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 62.171

Attention duration (in seconds): 0.3927
Attention throughput (in TFLOP/s): 79.907
MLP duration (in seconds): 1.4778
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.8705
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.2775
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 80.942
Elapsed time for attention_key_query_prob (128x2048x668x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x668x2048): 66.055
Elapsed time for attention_prob_times_values (128x2048x2048x668): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x668): 67.597
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0924
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 81.001
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.9960
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 30.066
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.4816
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 62.179

Attention duration (in seconds): 0.3914
Attention throughput (in TFLOP/s): 80.181
MLP duration (in seconds): 1.4776
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.8690
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.2775
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 80.927
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 57.121
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 55.616
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0925
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 80.976
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.9975
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 30.022
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.4816
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 62.179

Attention duration (in seconds): 0.3954
Attention throughput (in TFLOP/s): 79.357
MLP duration (in seconds): 1.4791
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.8745
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.2775
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 80.947
Elapsed time for attention_key_query_prob (512x2048x167x2048): 0.0282
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x167x2048): 25.394
Elapsed time for attention_prob_times_values (512x2048x2048x167): 0.0241
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x167): 29.786
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0923
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 81.097
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.9968
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 30.042
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.4816
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 62.174

Attention duration (in seconds): 0.4221
Attention throughput (in TFLOP/s): 74.343
MLP duration (in seconds): 1.4784
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9005
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.2767
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 82.130
Elapsed time for attention_key_query_prob (64x2048x1344x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1344x2048): 65.000
Elapsed time for attention_prob_times_values (64x2048x2048x1344): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1344): 68.728
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0923
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 82.072
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.3695
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 82.014
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.5133
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 59.043

Attention duration (in seconds): 0.3907
Attention throughput (in TFLOP/s): 81.269
MLP duration (in seconds): 0.8828
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2734
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.2770
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 82.059
Elapsed time for attention_key_query_prob (128x2048x672x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x672x2048): 65.292
Elapsed time for attention_prob_times_values (128x2048x2048x672): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x672): 65.091
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0923
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 82.092
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.3692
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 82.079
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.5132
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 59.049

Attention duration (in seconds): 0.3914
Attention throughput (in TFLOP/s): 81.113
MLP duration (in seconds): 0.8824
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2739
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.2769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 82.080
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 56.456
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 58.540
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0923
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 82.099
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.3695
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 82.020
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.5132
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 59.056

Attention duration (in seconds): 0.3943
Attention throughput (in TFLOP/s): 80.518
MLP duration (in seconds): 0.8826
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2769
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.2769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 82.095
Elapsed time for attention_key_query_prob (512x2048x168x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x168x2048): 45.486
Elapsed time for attention_prob_times_values (512x2048x2048x168): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x168): 54.314
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0923
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 82.124
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.3694
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 82.050
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.5133
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 59.042

Attention duration (in seconds): 0.3983
Attention throughput (in TFLOP/s): 79.717
MLP duration (in seconds): 0.8826
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2809
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.2857
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 80.507
Elapsed time for attention_key_query_prob (64x2048x1352x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1352x2048): 61.375
Elapsed time for attention_prob_times_values (64x2048x2048x1352): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1352): 64.373
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0950
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 80.724
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 1.0605
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 28.918
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.5789
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 52.976

Attention duration (in seconds): 0.4038
Attention throughput (in TFLOP/s): 79.547
MLP duration (in seconds): 1.6394
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0431
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.2855
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 80.571
Elapsed time for attention_key_query_prob (128x2048x676x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x676x2048): 65.134
Elapsed time for attention_prob_times_values (128x2048x2048x676): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x676): 68.267
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0946
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 81.077
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 1.0596
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 28.942
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.5793
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 52.937

Attention duration (in seconds): 0.4018
Attention throughput (in TFLOP/s): 79.936
MLP duration (in seconds): 1.6389
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0407
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.2857
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 80.507
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 57.955
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 56.331
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0945
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 81.153
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 1.0594
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 28.948
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.5790
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 52.964

Attention duration (in seconds): 0.4056
Attention throughput (in TFLOP/s): 79.193
MLP duration (in seconds): 1.6384
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0440
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.2856
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 80.542
Elapsed time for attention_key_query_prob (512x2048x169x2048): 0.0282
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x169x2048): 25.699
Elapsed time for attention_prob_times_values (512x2048x2048x169): 0.0239
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x169): 30.341
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0944
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 81.178
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 1.0608
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 28.910
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.5788
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 52.981

Attention duration (in seconds): 0.4322
Attention throughput (in TFLOP/s): 74.318
MLP duration (in seconds): 1.6396
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0718
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.2768
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 84.075
Elapsed time for attention_key_query_prob (64x2048x1360x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1360x2048): 63.320
Elapsed time for attention_prob_times_values (64x2048x2048x1360): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1360): 65.735
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0924
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 84.001
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.3711
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 83.617
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.5115
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 60.666

Attention duration (in seconds): 0.3918
Attention throughput (in TFLOP/s): 82.927
MLP duration (in seconds): 0.8826
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2744
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.2769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 84.048
Elapsed time for attention_key_query_prob (128x2048x680x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x680x2048): 60.463
Elapsed time for attention_prob_times_values (128x2048x2048x680): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x680): 64.545
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0924
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 83.977
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.3711
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 83.618
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.5115
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 60.667

Attention duration (in seconds): 0.3927
Attention throughput (in TFLOP/s): 82.744
MLP duration (in seconds): 0.8826
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2753
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.2769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 84.050
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 57.441
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 55.282
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0924
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 83.983
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.3711
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 83.612
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.5115
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 60.667

Attention duration (in seconds): 0.3952
Attention throughput (in TFLOP/s): 82.217
MLP duration (in seconds): 0.8826
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2778
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.2769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 84.047
Elapsed time for attention_key_query_prob (512x2048x170x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x170x2048): 44.788
Elapsed time for attention_prob_times_values (512x2048x2048x170): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x170): 54.100
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0924
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 83.993
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.3708
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 83.696
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.5115
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 60.666

Attention duration (in seconds): 0.3991
Attention throughput (in TFLOP/s): 81.418
MLP duration (in seconds): 0.8823
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2813
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.2790
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 84.404
Elapsed time for attention_key_query_prob (64x2048x1368x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1368x2048): 61.791
Elapsed time for attention_prob_times_values (64x2048x2048x1368): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1368): 65.267
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0923
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 85.011
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.3722
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 84.357
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.5141
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 61.073

Attention duration (in seconds): 0.3945
Attention throughput (in TFLOP/s): 83.319
MLP duration (in seconds): 0.8863
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2807
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.2790
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 84.405
Elapsed time for attention_key_query_prob (128x2048x684x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x684x2048): 66.402
Elapsed time for attention_prob_times_values (128x2048x2048x684): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x684): 69.036
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0931
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 84.324
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.3722
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 84.358
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.5137
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 61.120

Attention duration (in seconds): 0.3938
Attention throughput (in TFLOP/s): 83.465
MLP duration (in seconds): 0.8859
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2797
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.2790
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 84.402
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 58.264
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 56.992
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0922
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 85.114
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.3722
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 84.362
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.5129
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 61.213

Attention duration (in seconds): 0.3967
Attention throughput (in TFLOP/s): 82.847
MLP duration (in seconds): 0.8851
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2818
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.2790
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 84.405
Elapsed time for attention_key_query_prob (512x2048x171x2048): 0.0284
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x171x2048): 25.853
Elapsed time for attention_prob_times_values (512x2048x2048x171): 0.0235
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x171): 31.231
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0922
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 85.106
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.3722
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 84.359
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.5136
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 61.134

Attention duration (in seconds): 0.4231
Attention throughput (in TFLOP/s): 77.672
MLP duration (in seconds): 0.8858
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.2924
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 81.475
Elapsed time for attention_key_query_prob (64x2048x1376x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1376x2048): 66.173
Elapsed time for attention_prob_times_values (64x2048x2048x1376): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1376): 66.933
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0979
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 81.158
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 1.0443
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 30.418
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.5153
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 61.640

Attention duration (in seconds): 0.4125
Attention throughput (in TFLOP/s): 80.597
MLP duration (in seconds): 1.5597
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9721
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.2925
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 81.464
Elapsed time for attention_key_query_prob (128x2048x688x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x688x2048): 62.585
Elapsed time for attention_prob_times_values (128x2048x2048x688): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x688): 65.967
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0972
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 81.740
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 1.0434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 30.444
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.5152
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 61.652

Attention duration (in seconds): 0.4126
Attention throughput (in TFLOP/s): 80.568
MLP duration (in seconds): 1.5587
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9713
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.2927
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 81.385
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 56.474
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 59.121
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0972
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 81.700
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 1.0440
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 30.427
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.5152
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 61.652

Attention duration (in seconds): 0.4155
Attention throughput (in TFLOP/s): 80.005
MLP duration (in seconds): 1.5592
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9747
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.2925
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 81.440
Elapsed time for attention_key_query_prob (512x2048x172x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x172x2048): 44.854
Elapsed time for attention_prob_times_values (512x2048x2048x172): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x172): 53.714
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0974
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 81.521
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 1.0437
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 30.436
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.5148
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 61.700

Attention duration (in seconds): 0.4202
Attention throughput (in TFLOP/s): 79.117
MLP duration (in seconds): 1.5585
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9787
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.2958
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 81.478
Elapsed time for attention_key_query_prob (64x2048x1384x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1384x2048): 60.729
Elapsed time for attention_prob_times_values (64x2048x2048x1384): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1384): 65.057
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0983
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 81.755
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.3961
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 81.132
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.5304
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 60.583

Attention duration (in seconds): 0.4177
Attention throughput (in TFLOP/s): 80.486
MLP duration (in seconds): 0.9265
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3443
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.2979
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 80.893
Elapsed time for attention_key_query_prob (128x2048x692x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x692x2048): 65.977
Elapsed time for attention_prob_times_values (128x2048x2048x692): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x692): 68.376
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0986
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 81.445
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.3962
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 81.102
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.5307
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 60.560

Attention duration (in seconds): 0.4187
Attention throughput (in TFLOP/s): 80.297
MLP duration (in seconds): 0.9269
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3456
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.2980
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 80.870
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 58.029
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 56.549
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0986
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 81.449
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.3963
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 81.090
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.5305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 60.572

Attention duration (in seconds): 0.4226
Attention throughput (in TFLOP/s): 79.557
MLP duration (in seconds): 0.9268
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3495
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.2981
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 80.850
Elapsed time for attention_key_query_prob (512x2048x173x2048): 0.0290
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x173x2048): 25.626
Elapsed time for attention_prob_times_values (512x2048x2048x173): 0.0239
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x173): 31.116
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0985
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 81.562
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.3962
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 81.118
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.5305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 60.579

Attention duration (in seconds): 0.4495
Attention throughput (in TFLOP/s): 74.801
MLP duration (in seconds): 0.9266
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3761
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.2940
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 82.930
Elapsed time for attention_key_query_prob (64x2048x1392x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1392x2048): 64.017
Elapsed time for attention_prob_times_values (64x2048x2048x1392): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1392): 68.512
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0979
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 83.015
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 1.0845
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 29.976
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.5210
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 62.393

Attention duration (in seconds): 0.4145
Attention throughput (in TFLOP/s): 82.038
MLP duration (in seconds): 1.6055
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.2946
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 82.767
Elapsed time for attention_key_query_prob (128x2048x696x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x696x2048): 62.877
Elapsed time for attention_prob_times_values (128x2048x2048x696): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x696): 67.117
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0979
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 82.999
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 1.0851
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 29.958
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.5211
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 62.386

Attention duration (in seconds): 0.4155
Attention throughput (in TFLOP/s): 81.833
MLP duration (in seconds): 1.6062
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.2949
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 82.676
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 59.234
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 57.259
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0979
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 83.011
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 1.0870
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 29.907
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.5211
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 62.380

Attention duration (in seconds): 0.4185
Attention throughput (in TFLOP/s): 81.255
MLP duration (in seconds): 1.6081
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.2949
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 82.684
Elapsed time for attention_key_query_prob (512x2048x174x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x174x2048): 46.426
Elapsed time for attention_prob_times_values (512x2048x2048x174): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x174): 56.237
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0979
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 83.019
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 1.0873
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 29.899
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.5210
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 62.397

Attention duration (in seconds): 0.4222
Attention throughput (in TFLOP/s): 80.547
MLP duration (in seconds): 1.6083
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.2959
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 83.349
Elapsed time for attention_key_query_prob (64x2048x1400x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1400x2048): 60.430
Elapsed time for attention_prob_times_values (64x2048x2048x1400): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1400): 66.028
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0986
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 83.398
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.3978
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 82.657
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.5468
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 60.142

Attention duration (in seconds): 0.4183
Attention throughput (in TFLOP/s): 82.208
MLP duration (in seconds): 0.9446
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3629
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.2980
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 82.761
Elapsed time for attention_key_query_prob (128x2048x700x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x700x2048): 65.734
Elapsed time for attention_prob_times_values (128x2048x2048x700): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x700): 69.143
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0993
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 82.771
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.3978
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 82.654
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.5467
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 60.152

Attention duration (in seconds): 0.4196
Attention throughput (in TFLOP/s): 81.947
MLP duration (in seconds): 0.9445
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3641
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.2980
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 82.763
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 58.129
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 57.235
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0987
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 83.311
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.3978
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 82.656
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.5467
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 60.152

Attention duration (in seconds): 0.4227
Attention throughput (in TFLOP/s): 81.344
MLP duration (in seconds): 0.9445
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3672
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.2980
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 82.764
Elapsed time for attention_key_query_prob (512x2048x175x2048): 0.0292
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x175x2048): 25.745
Elapsed time for attention_prob_times_values (512x2048x2048x175): 0.0244
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x175): 30.805
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0987
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 83.290
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.3978
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 82.656
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.5467
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 60.147

Attention duration (in seconds): 0.4503
Attention throughput (in TFLOP/s): 76.367
MLP duration (in seconds): 0.9446
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3948
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.8673
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 28.763
Elapsed time for attention_key_query_prob (64x2048x1408x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1408x2048): 64.312
Elapsed time for attention_prob_times_values (64x2048x2048x1408): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1408): 68.313
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0997
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 83.389
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.3997
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 83.207
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.5588
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 59.525

Attention duration (in seconds): 0.9898
Attention throughput (in TFLOP/s): 35.130
MLP duration (in seconds): 0.9585
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9483
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.8679
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 28.741
Elapsed time for attention_key_query_prob (128x2048x704x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x704x2048): 64.785
Elapsed time for attention_prob_times_values (128x2048x2048x704): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x704): 67.438
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0997
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 83.377
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.4000
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 83.154
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.5587
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 59.527

Attention duration (in seconds): 0.9905
Attention throughput (in TFLOP/s): 35.104
MLP duration (in seconds): 0.9587
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9493
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.8673
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 28.763
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 59.687
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 59.886
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0997
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 83.385
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.3996
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 83.232
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.5589
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 59.512

Attention duration (in seconds): 0.9923
Attention throughput (in TFLOP/s): 35.042
MLP duration (in seconds): 0.9585
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9508
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.8671
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 28.770
Elapsed time for attention_key_query_prob (512x2048x176x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x176x2048): 46.970
Elapsed time for attention_prob_times_values (512x2048x2048x176): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x176): 55.750
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0997
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 83.389
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.3998
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 83.188
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.5588
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 59.525

Attention duration (in seconds): 0.9964
Attention throughput (in TFLOP/s): 34.897
MLP duration (in seconds): 0.9586
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9550
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.8562
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 29.467
Elapsed time for attention_key_query_prob (64x2048x1416x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1416x2048): 59.585
Elapsed time for attention_prob_times_values (64x2048x2048x1416): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1416): 63.863
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.1052
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 79.911
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 1.1485
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 29.290
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.5489
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 61.287

Attention duration (in seconds): 0.9861
Attention throughput (in TFLOP/s): 35.655
MLP duration (in seconds): 1.6974
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6835
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.8556
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 29.489
Elapsed time for attention_key_query_prob (128x2048x708x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x708x2048): 65.034
Elapsed time for attention_prob_times_values (128x2048x2048x708): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x708): 69.692
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.1053
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 79.902
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 1.1479
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 29.305
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.5488
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 61.295

Attention duration (in seconds): 0.9834
Attention throughput (in TFLOP/s): 35.753
MLP duration (in seconds): 1.6967
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6801
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.8545
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 29.526
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 56.922
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 57.292
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.1053
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 79.887
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 1.1491
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 29.274
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.5488
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 61.293

Attention duration (in seconds): 0.9864
Attention throughput (in TFLOP/s): 35.645
MLP duration (in seconds): 1.6979
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6843
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.8552
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 29.500
Elapsed time for attention_key_query_prob (512x2048x177x2048): 0.0292
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x177x2048): 26.021
Elapsed time for attention_prob_times_values (512x2048x2048x177): 0.0243
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x177): 31.281
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.1052
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 79.949
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 1.1504
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 29.240
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.5488
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 61.293

Attention duration (in seconds): 1.0139
Attention throughput (in TFLOP/s): 34.677
MLP duration (in seconds): 1.6993
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.7132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.3144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 81.158
Elapsed time for attention_key_query_prob (64x2048x1424x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1424x2048): 61.607
Elapsed time for attention_prob_times_values (64x2048x2048x1424): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1424): 64.961
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.1042
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 81.589
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.4186
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 81.277
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.5710
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 59.578

Attention duration (in seconds): 0.4428
Attention throughput (in TFLOP/s): 80.281
MLP duration (in seconds): 0.9896
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.4324
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.3145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 81.138
Elapsed time for attention_key_query_prob (128x2048x712x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x712x2048): 59.878
Elapsed time for attention_prob_times_values (128x2048x2048x712): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x712): 61.383
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.1043
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 81.577
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.4187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 81.243
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.5711
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 59.575

Attention duration (in seconds): 0.4439
Attention throughput (in TFLOP/s): 80.075
MLP duration (in seconds): 0.9898
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.4337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.3145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 81.140
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 56.185
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 56.759
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.1045
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 81.425
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.4187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 81.245
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.5712
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 59.562

Attention duration (in seconds): 0.4460
Attention throughput (in TFLOP/s): 79.709
MLP duration (in seconds): 0.9899
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.4359
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.3144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 81.164
Elapsed time for attention_key_query_prob (512x2048x178x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x178x2048): 45.511
Elapsed time for attention_prob_times_values (512x2048x2048x178): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x178): 55.560
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.1042
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 81.597
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.4189
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 81.211
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.5711
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 59.575

Attention duration (in seconds): 0.4492
Attention throughput (in TFLOP/s): 79.146
MLP duration (in seconds): 0.9900
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.4391
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.3210
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 80.382
Elapsed time for attention_key_query_prob (64x2048x1432x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1432x2048): 59.874
Elapsed time for attention_prob_times_values (64x2048x2048x1432): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1432): 64.408
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.1070
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 80.365
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 1.1575
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 29.722
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.5884
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 58.468

Attention duration (in seconds): 0.4528
Attention throughput (in TFLOP/s): 79.375
MLP duration (in seconds): 1.7459
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1987
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.3229
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 79.910
Elapsed time for attention_key_query_prob (128x2048x716x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x716x2048): 65.574
Elapsed time for attention_prob_times_values (128x2048x2048x716): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x716): 69.828
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.1074
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 80.099
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 1.1565
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 29.747
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.5886
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 58.449

Attention duration (in seconds): 0.4530
Attention throughput (in TFLOP/s): 79.339
MLP duration (in seconds): 1.7452
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1982
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.3230
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 79.882
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 56.890
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 57.787
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.1073
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 80.137
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 1.1567
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 29.743
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.5885
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 58.459

Attention duration (in seconds): 0.4572
Attention throughput (in TFLOP/s): 78.619
MLP duration (in seconds): 1.7452
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.2024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.3230
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 79.881
Elapsed time for attention_key_query_prob (512x2048x179x2048): 0.0296
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x179x2048): 25.959
Elapsed time for attention_prob_times_values (512x2048x2048x179): 0.0240
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x179): 32.089
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.1073
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 80.176
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 1.1565
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 29.747
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.5886
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 58.449

Attention duration (in seconds): 0.4839
Attention throughput (in TFLOP/s): 74.280
MLP duration (in seconds): 1.7451
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.2290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.3130
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 83.367
Elapsed time for attention_key_query_prob (64x2048x1440x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1440x2048): 64.131
Elapsed time for attention_prob_times_values (64x2048x2048x1440): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1440): 65.846
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.1042
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 83.435
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.4185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 83.132
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.5693
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 61.112

Attention duration (in seconds): 0.4410
Attention throughput (in TFLOP/s): 82.391
MLP duration (in seconds): 0.9877
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.4288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.3161
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 82.541
Elapsed time for attention_key_query_prob (128x2048x720x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x720x2048): 61.818
Elapsed time for attention_prob_times_values (128x2048x2048x720): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x720): 62.755
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.1043
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 83.349
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.4217
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 82.496
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.5694
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 61.096

Attention duration (in seconds): 0.4453
Attention throughput (in TFLOP/s): 81.601
MLP duration (in seconds): 0.9911
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.4364
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.3161
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 82.542
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 54.515
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 60.204
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.1042
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 83.444
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.4221
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 82.423
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.5694
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 61.095

Attention duration (in seconds): 0.4474
Attention throughput (in TFLOP/s): 81.222
MLP duration (in seconds): 0.9915
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.4389
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.3161
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 82.540
Elapsed time for attention_key_query_prob (512x2048x180x2048): 0.0169
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x180x2048): 45.702
Elapsed time for attention_prob_times_values (512x2048x2048x180): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x180): 54.499
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.1042
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 83.456
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.4219
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 82.457
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.5694
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 61.097

Attention duration (in seconds): 0.4514
Attention throughput (in TFLOP/s): 80.490
MLP duration (in seconds): 0.9913
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.4427
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.8918
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 29.583
Elapsed time for attention_key_query_prob (64x2048x1448x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1448x2048): 58.802
Elapsed time for attention_prob_times_values (64x2048x2048x1448): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1448): 64.738
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.1053
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 83.548
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.4211
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 83.530
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.5777
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 60.889

Attention duration (in seconds): 1.0223
Attention throughput (in TFLOP/s): 35.930
MLP duration (in seconds): 0.9988
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.8914
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 29.595
Elapsed time for attention_key_query_prob (128x2048x724x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x724x2048): 65.790
Elapsed time for attention_prob_times_values (128x2048x2048x724): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x724): 70.285
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.1052
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 83.575
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.4211
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 83.527
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.5776
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 60.900

Attention duration (in seconds): 1.0196
Attention throughput (in TFLOP/s): 36.027
MLP duration (in seconds): 0.9988
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.8916
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 29.589
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 57.247
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 58.123
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.1052
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 83.588
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.4211
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 83.532
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.5777
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 60.888

Attention duration (in seconds): 1.0238
Attention throughput (in TFLOP/s): 35.877
MLP duration (in seconds): 0.9989
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.8910
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 29.610
Elapsed time for attention_key_query_prob (512x2048x181x2048): 0.0298
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x181x2048): 26.092
Elapsed time for attention_prob_times_values (512x2048x2048x181): 0.0240
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x181): 32.345
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.1044
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 84.237
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.4211
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 83.532
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.5777
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 60.889

Attention duration (in seconds): 1.0492
Attention throughput (in TFLOP/s): 35.008
MLP duration (in seconds): 0.9988
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0481
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.8945
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 29.822
Elapsed time for attention_key_query_prob (64x2048x1456x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1456x2048): 61.318
Elapsed time for attention_prob_times_values (64x2048x2048x1456): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1456): 66.067
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.1105
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 80.494
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 1.2063
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 29.485
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.5831
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 60.992

Attention duration (in seconds): 1.0295
Attention throughput (in TFLOP/s): 36.066
MLP duration (in seconds): 1.7894
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.8961
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 29.767
Elapsed time for attention_key_query_prob (128x2048x728x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x728x2048): 60.674
Elapsed time for attention_prob_times_values (128x2048x2048x728): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x728): 62.451
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.1102
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 80.660
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 1.2057
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 29.499
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.5833
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 60.972

Attention duration (in seconds): 1.0318
Attention throughput (in TFLOP/s): 35.987
MLP duration (in seconds): 1.7890
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.8958
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 29.778
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 57.077
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 56.750
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.1102
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 80.714
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 1.2072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 29.463
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.5833
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 60.978

Attention duration (in seconds): 1.0334
Attention throughput (in TFLOP/s): 35.929
MLP duration (in seconds): 1.7904
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.8968
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 29.746
Elapsed time for attention_key_query_prob (512x2048x182x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x182x2048): 46.395
Elapsed time for attention_prob_times_values (512x2048x2048x182): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x182): 56.727
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.1098
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 80.997
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 1.2074
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 29.458
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.5835
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 60.959

Attention duration (in seconds): 1.0372
Attention throughput (in TFLOP/s): 35.799
MLP duration (in seconds): 1.7908
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.3334
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 80.901
Elapsed time for attention_key_query_prob (64x2048x1464x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1464x2048): 58.668
Elapsed time for attention_prob_times_values (64x2048x2048x1464): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1464): 65.355
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.1102
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 81.576
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.4437
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 81.036
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.5921
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 60.733

Attention duration (in seconds): 0.4690
Attention throughput (in TFLOP/s): 80.026
MLP duration (in seconds): 1.0358
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.3336
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 80.830
Elapsed time for attention_key_query_prob (128x2048x732x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x732x2048): 65.415
Elapsed time for attention_prob_times_values (128x2048x2048x732): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x732): 70.540
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.1099
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 81.771
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.4437
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 81.037
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.5920
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 60.738

Attention duration (in seconds): 0.4667
Attention throughput (in TFLOP/s): 80.409
MLP duration (in seconds): 1.0358
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.3335
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 80.873
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 57.039
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 58.725
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.1100
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 81.720
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.4436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 81.062
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.5919
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 60.748

Attention duration (in seconds): 0.4706
Attention throughput (in TFLOP/s): 79.744
MLP duration (in seconds): 1.0355
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.3334
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 80.899
Elapsed time for attention_key_query_prob (512x2048x183x2048): 0.0307
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x183x2048): 25.590
Elapsed time for attention_prob_times_values (512x2048x2048x183): 0.0246
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x183): 31.963
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.1098
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 81.849
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.4433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 81.107
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.5919
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 60.747

Attention duration (in seconds): 0.4985
Attention throughput (in TFLOP/s): 75.287
MLP duration (in seconds): 1.0353
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.3359
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 81.168
Elapsed time for attention_key_query_prob (64x2048x1472x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1472x2048): 64.500
Elapsed time for attention_prob_times_values (64x2048x2048x1472): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1472): 67.884
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.1120
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 81.169
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 1.2592
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 28.869
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.6250
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 58.163

Attention duration (in seconds): 0.4718
Attention throughput (in TFLOP/s): 80.407
MLP duration (in seconds): 1.8842
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.3560
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.3374
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 80.798
Elapsed time for attention_key_query_prob (128x2048x736x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x736x2048): 65.950
Elapsed time for attention_prob_times_values (128x2048x2048x736): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x736): 64.626
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.1122
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 80.983
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 1.2601
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 28.849
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.6250
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 58.166

Attention duration (in seconds): 0.4739
Attention throughput (in TFLOP/s): 80.049
MLP duration (in seconds): 1.8851
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.3590
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.3371
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 80.869
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 56.279
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 62.206
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.1119
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 81.223
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 1.2608
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 28.833
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.6250
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 58.160

Attention duration (in seconds): 0.4758
Attention throughput (in TFLOP/s): 79.729
MLP duration (in seconds): 1.8858
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.3616
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.3376
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 80.771
Elapsed time for attention_key_query_prob (512x2048x184x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x184x2048): 47.779
Elapsed time for attention_prob_times_values (512x2048x2048x184): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x184): 57.572
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.1118
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 81.280
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 1.2615
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 28.817
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.6249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 58.176

Attention duration (in seconds): 0.4796
Attention throughput (in TFLOP/s): 79.088
MLP duration (in seconds): 1.8864
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.3660
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.3292
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 83.727
Elapsed time for attention_key_query_prob (64x2048x1480x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1480x2048): 58.658
Elapsed time for attention_prob_times_values (64x2048x2048x1480): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1480): 63.724
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.1099
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 83.587
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.4404
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 83.435
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.6011
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 61.131

Attention duration (in seconds): 0.4651
Attention throughput (in TFLOP/s): 82.428
MLP duration (in seconds): 1.0416
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.3290
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 83.771
Elapsed time for attention_key_query_prob (128x2048x740x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x740x2048): 65.506
Elapsed time for attention_prob_times_values (128x2048x2048x740): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x740): 71.411
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.1096
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 83.811
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.4400
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 83.526
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.6012
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 61.124

Attention duration (in seconds): 0.4619
Attention throughput (in TFLOP/s): 83.003
MLP duration (in seconds): 1.0412
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.3301
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 83.505
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 58.112
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 59.876
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.1095
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 83.881
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.4394
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 83.635
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.6013
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 61.120

Attention duration (in seconds): 0.4665
Attention throughput (in TFLOP/s): 82.177
MLP duration (in seconds): 1.0406
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.3294
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 83.673
Elapsed time for attention_key_query_prob (512x2048x185x2048): 0.0304
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x185x2048): 26.179
Elapsed time for attention_prob_times_values (512x2048x2048x185): 0.0243
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x185): 32.716
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.1095
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 83.888
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.4400
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 83.511
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.6013
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 61.114

Attention duration (in seconds): 0.4936
Attention throughput (in TFLOP/s): 77.677
MLP duration (in seconds): 1.0414
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.9552
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 29.168
Elapsed time for attention_key_query_prob (64x2048x1488x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1488x2048): 61.153
Elapsed time for attention_prob_times_values (64x2048x2048x1488): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1488): 65.434
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.1103
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 84.215
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 1.2824
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 28.968
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.6353
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 58.472

Attention duration (in seconds): 1.0907
Attention throughput (in TFLOP/s): 35.522
MLP duration (in seconds): 1.9177
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.9554
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 29.160
Elapsed time for attention_key_query_prob (128x2048x744x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x744x2048): 60.785
Elapsed time for attention_prob_times_values (128x2048x2048x744): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x744): 64.076
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.1102
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 84.274
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 1.2811
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 28.996
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.6354
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 58.460

Attention duration (in seconds): 1.0913
Attention throughput (in TFLOP/s): 35.505
MLP duration (in seconds): 1.9165
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.9527
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 29.242
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 58.409
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 58.281
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.1102
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 84.280
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 1.2833
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 28.947
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.6357
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 58.439

Attention duration (in seconds): 1.0903
Attention throughput (in TFLOP/s): 35.535
MLP duration (in seconds): 1.9189
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.9558
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 29.149
Elapsed time for attention_key_query_prob (512x2048x186x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x186x2048): 47.607
Elapsed time for attention_prob_times_values (512x2048x2048x186): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x186): 57.397
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.1102
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 84.269
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 1.2827
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 28.961
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.6354
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 58.458

Attention duration (in seconds): 1.0967
Attention throughput (in TFLOP/s): 35.329
MLP duration (in seconds): 1.9181
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.9512
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 29.604
Elapsed time for attention_key_query_prob (64x2048x1496x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1496x2048): 59.339
Elapsed time for attention_prob_times_values (64x2048x2048x1496): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1496): 64.604
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.1163
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 80.691
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 1.2732
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 29.491
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.6351
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 59.117

Attention duration (in seconds): 1.0935
Attention throughput (in TFLOP/s): 35.805
MLP duration (in seconds): 1.9083
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.9511
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 29.607
Elapsed time for attention_key_query_prob (128x2048x748x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x748x2048): 66.070
Elapsed time for attention_prob_times_values (128x2048x2048x748): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x748): 72.333
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.1163
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 80.742
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 1.2721
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 29.515
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.6352
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 59.110

Attention duration (in seconds): 1.0907
Attention throughput (in TFLOP/s): 35.899
MLP duration (in seconds): 1.9074
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.9980
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.9531
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 29.545
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 58.805
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 60.285
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.1163
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 80.739
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 1.2706
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 29.551
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.6352
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 59.108

Attention duration (in seconds): 1.0964
Attention throughput (in TFLOP/s): 35.712
MLP duration (in seconds): 1.9058
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.9523
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 29.572
Elapsed time for attention_key_query_prob (512x2048x187x2048): 0.0302
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x187x2048): 26.608
Elapsed time for attention_prob_times_values (512x2048x2048x187): 0.0240
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x187): 33.442
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.1163
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 80.740
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 1.2720
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 29.518
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.6352
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 59.114

Attention duration (in seconds): 1.1227
Attention throughput (in TFLOP/s): 34.874
MLP duration (in seconds): 1.9072
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.3454
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 82.408
Elapsed time for attention_key_query_prob (64x2048x1504x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1504x2048): 64.209
Elapsed time for attention_prob_times_values (64x2048x2048x1504): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1504): 66.522
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.1152
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 82.351
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 1.3230
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 28.685
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.6470
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 58.654

Attention duration (in seconds): 0.4853
Attention throughput (in TFLOP/s): 81.526
MLP duration (in seconds): 1.9700
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4553
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.3471
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 82.000
Elapsed time for attention_key_query_prob (128x2048x752x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x752x2048): 62.918
Elapsed time for attention_prob_times_values (128x2048x2048x752): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x752): 65.297
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.1157
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 82.009
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 1.3243
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 28.658
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.6469
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 58.665

Attention duration (in seconds): 0.4880
Attention throughput (in TFLOP/s): 81.077
MLP duration (in seconds): 1.9712
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4592
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.3469
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 82.054
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 56.288
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 62.771
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.1158
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 81.922
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 1.3241
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 28.660
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.6472
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 58.637

Attention duration (in seconds): 0.4899
Attention throughput (in TFLOP/s): 80.762
MLP duration (in seconds): 1.9714
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4613
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.3470
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 82.031
Elapsed time for attention_key_query_prob (512x2048x188x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x188x2048): 47.044
Elapsed time for attention_prob_times_values (512x2048x2048x188): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x188): 56.699
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.1152
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 82.378
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 1.3251
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 28.639
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.6470
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 58.652

Attention duration (in seconds): 0.4936
Attention throughput (in TFLOP/s): 80.165
MLP duration (in seconds): 1.9722
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4657
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.3585
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 80.235
Elapsed time for attention_key_query_prob (64x2048x1512x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1512x2048): 59.289
Elapsed time for attention_prob_times_values (64x2048x2048x1512): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1512): 64.916
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.1190
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 80.559
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 1.2951
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 29.615
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.6533
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 58.712

Attention duration (in seconds): 0.5037
Attention throughput (in TFLOP/s): 79.362
MLP duration (in seconds): 1.9484
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4521
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.3587
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 80.190
Elapsed time for attention_key_query_prob (128x2048x756x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x756x2048): 66.587
Elapsed time for attention_prob_times_values (128x2048x2048x756): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x756): 73.450
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.1190
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 80.599
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 1.2943
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 29.633
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.6532
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 58.719

Attention duration (in seconds): 0.5009
Attention throughput (in TFLOP/s): 79.807
MLP duration (in seconds): 1.9475
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4485
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.3585
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 80.240
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 58.991
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 60.832
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.1190
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 80.610
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 1.2941
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 29.638
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.6533
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 58.708

Attention duration (in seconds): 0.5046
Attention throughput (in TFLOP/s): 79.234
MLP duration (in seconds): 1.9474
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4520
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.3587
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 80.186
Elapsed time for attention_key_query_prob (512x2048x189x2048): 0.0305
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x189x2048): 26.606
Elapsed time for attention_prob_times_values (512x2048x2048x189): 0.0241
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x189): 33.684
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.1189
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 80.617
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 1.2955
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 29.607
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.6532
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 58.721

Attention duration (in seconds): 0.5323
Attention throughput (in TFLOP/s): 75.106
MLP duration (in seconds): 1.9487
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4810
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.3523
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 82.515
Elapsed time for attention_key_query_prob (64x2048x1520x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1520x2048): 59.836
Elapsed time for attention_prob_times_values (64x2048x2048x1520): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1520): 65.399
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.1175
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 82.466
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 1.3360
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 29.014
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.6690
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 57.939

Attention duration (in seconds): 0.4959
Attention throughput (in TFLOP/s): 81.449
MLP duration (in seconds): 2.0050
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.5009
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.3525
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 82.482
Elapsed time for attention_key_query_prob (128x2048x760x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x760x2048): 60.037
Elapsed time for attention_prob_times_values (128x2048x2048x760): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x760): 63.843
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.1177
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 82.365
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 1.3361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 29.012
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.6692
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 57.922

Attention duration (in seconds): 0.4965
Attention throughput (in TFLOP/s): 81.360
MLP duration (in seconds): 2.0053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.5018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.3524
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 82.495
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 57.540
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 58.949
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.1177
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 82.357
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 1.3358
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 29.019
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.6690
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 57.937

Attention duration (in seconds): 0.4981
Attention throughput (in TFLOP/s): 81.098
MLP duration (in seconds): 2.0048
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.5029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.3524
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 82.493
Elapsed time for attention_key_query_prob (512x2048x190x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x190x2048): 46.994
Elapsed time for attention_prob_times_values (512x2048x2048x190): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x190): 57.848
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.1175
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 82.465
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 1.3352
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 29.031
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.6693
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 57.915

Attention duration (in seconds): 0.5014
Attention throughput (in TFLOP/s): 80.564
MLP duration (in seconds): 2.0045
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.5059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 1.0314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 28.483
Elapsed time for attention_key_query_prob (64x2048x1528x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1528x2048): 58.768
Elapsed time for attention_prob_times_values (64x2048x2048x1528): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1528): 64.471
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.1181
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 82.930
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 1.3746
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 28.496
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.6717
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 58.317

Attention duration (in seconds): 1.1762
Attention throughput (in TFLOP/s): 34.698
MLP duration (in seconds): 2.0463
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.2225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 1.0312
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 28.490
Elapsed time for attention_key_query_prob (128x2048x764x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x764x2048): 65.448
Elapsed time for attention_prob_times_values (128x2048x2048x764): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x764): 73.088
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.1181
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 82.930
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 1.3727
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 28.535
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.6714
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 58.344

Attention duration (in seconds): 1.1730
Attention throughput (in TFLOP/s): 34.792
MLP duration (in seconds): 2.0441
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.2171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 1.0308
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 28.500
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 58.490
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 60.597
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.1181
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 82.931
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 1.3743
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 28.502
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.6715
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 58.334

Attention duration (in seconds): 1.1765
Attention throughput (in TFLOP/s): 34.690
MLP duration (in seconds): 2.0458
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.2223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 1.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 28.460
Elapsed time for attention_key_query_prob (512x2048x191x2048): 0.0314
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x191x2048): 26.154
Elapsed time for attention_prob_times_values (512x2048x2048x191): 0.0250
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x191): 32.749
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.1181
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 82.928
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 1.3752
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 28.484
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.6716
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 58.328

Attention duration (in seconds): 1.2068
Attention throughput (in TFLOP/s): 33.819
MLP duration (in seconds): 2.0468
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.2535
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.9845
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 30.155
Elapsed time for attention_key_query_prob (512x2048x192x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x192x2048): 53.399
Elapsed time for attention_prob_times_values (512x2048x2048x192): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x192): 65.585
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.1185
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 83.512
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 1.3099
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 30.217
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.6675
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 59.300

Attention duration (in seconds): 1.1310
Attention throughput (in TFLOP/s): 36.457
MLP duration (in seconds): 1.9774
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.1084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.3554
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 84.414
Elapsed time for attention_key_query_prob (512x2048x193x2048): 0.0319
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x193x2048): 25.999
Elapsed time for attention_prob_times_values (512x2048x2048x193): 0.0311
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x193): 26.693
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.1180
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 84.758
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 1.3751
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 29.085
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.6622
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 60.394

Attention duration (in seconds): 0.5363
Attention throughput (in TFLOP/s): 77.674
MLP duration (in seconds): 2.0374
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.5736
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.3635
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 83.390
Elapsed time for attention_key_query_prob (512x2048x194x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x194x2048): 47.673
Elapsed time for attention_prob_times_values (512x2048x2048x194): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x194): 61.458
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.1208
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 83.632
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 1.4483
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 27.902
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.6815
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 59.300

Attention duration (in seconds): 0.5153
Attention throughput (in TFLOP/s): 81.658
MLP duration (in seconds): 2.1298
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6451
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.3542
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 86.453
Elapsed time for attention_key_query_prob (512x2048x195x2048): 0.0318
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x195x2048): 26.355
Elapsed time for attention_prob_times_values (512x2048x2048x195): 0.0308
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x195): 27.175
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.1181
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 86.421
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 1.3772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 29.645
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.6776
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 60.256

Attention duration (in seconds): 0.5349
Attention throughput (in TFLOP/s): 79.460
MLP duration (in seconds): 2.0548
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.5897
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 1.0440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 29.634
Elapsed time for attention_key_query_prob (512x2048x196x2048): 0.0176
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x196x2048): 47.721
Elapsed time for attention_prob_times_values (512x2048x2048x196): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x196): 60.350
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.1175
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 87.727
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 1.4085
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 29.285
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.6865
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 60.084

Attention duration (in seconds): 1.1931
Attention throughput (in TFLOP/s): 35.984
MLP duration (in seconds): 2.0951
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.2882
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 1.0878
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 28.732
Elapsed time for attention_key_query_prob (512x2048x197x2048): 0.0321
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x197x2048): 26.363
Elapsed time for attention_prob_times_values (512x2048x2048x197): 0.0309
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x197): 27.396
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.1251
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 83.254
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 1.4673
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 28.399
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.7140
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 58.363

Attention duration (in seconds): 1.2759
Attention throughput (in TFLOP/s): 33.987
MLP duration (in seconds): 2.1813
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.4572
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.3733
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 84.576
Elapsed time for attention_key_query_prob (512x2048x198x2048): 0.0176
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x198x2048): 48.267
Elapsed time for attention_prob_times_values (512x2048x2048x198): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x198): 62.095
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.1240
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 84.842
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 1.4412
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 29.208
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.6986
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 60.260

Attention duration (in seconds): 0.5286
Attention throughput (in TFLOP/s): 82.846
MLP duration (in seconds): 2.1398
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6684
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.3826
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 83.353
Elapsed time for attention_key_query_prob (512x2048x199x2048): 0.0325
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x199x2048): 26.281
Elapsed time for attention_prob_times_values (512x2048x2048x199): 0.0316
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x199): 27.033
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.1270
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 83.691
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 1.4653
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 29.019
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.7069
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 60.151

Attention duration (in seconds): 0.5738
Attention throughput (in TFLOP/s): 77.089
MLP duration (in seconds): 2.1722
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.7460
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.3731
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 86.334
Elapsed time for attention_key_query_prob (512x2048x200x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x200x2048): 49.183
Elapsed time for attention_prob_times_values (512x2048x2048x200): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x200): 52.040
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.1240
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 86.596
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 1.5186
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 28.282
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.7245
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 59.283

Attention duration (in seconds): 0.5311
Attention throughput (in TFLOP/s): 84.108
MLP duration (in seconds): 2.2431
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.7742
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 1.1329
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 28.718
Elapsed time for attention_key_query_prob (512x2048x201x2048): 0.0326
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x201x2048): 26.441
Elapsed time for attention_prob_times_values (512x2048x2048x201): 0.0316
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x201): 27.278
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.1243
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 87.276
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 1.5107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 28.715
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.7261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 59.740

Attention duration (in seconds): 1.3215
Attention throughput (in TFLOP/s): 34.133
MLP duration (in seconds): 2.2369
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.5584
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 1.1313
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 29.046
Elapsed time for attention_key_query_prob (512x2048x202x2048): 0.0176
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x202x2048): 49.172
Elapsed time for attention_prob_times_values (512x2048x2048x202): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x202): 63.134
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.1307
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 83.825
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 1.5119
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 28.978
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.7260
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 60.351

Attention duration (in seconds): 1.2934
Attention throughput (in TFLOP/s): 35.217
MLP duration (in seconds): 2.2379
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.5313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
