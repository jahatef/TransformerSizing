[2023-11-03 17:41:41,862] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-03 17:41:42,660] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.144.211, master_port=6000
[2023-11-03 17:41:42,660] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-03 17:41:45,772] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
latency for (v=51136): 0.003165006637573242
latency for (v=51137): 0.0031561851501464844
latency for (v=51138): 0.0030832290649414062
latency for (v=51139): 0.003123760223388672
latency for (v=51140): 0.003110647201538086
latency for (v=51141): 0.003156423568725586
latency for (v=51142): 0.0030858516693115234
latency for (v=51143): 0.0030701160430908203
latency for (v=51144): 0.0030927658081054688
latency for (v=51145): 0.003108978271484375
latency for (v=51146): 0.003083467483520508
latency for (v=51147): 0.003103017807006836
latency for (v=51148): 0.003076314926147461
latency for (v=51149): 0.003082752227783203
latency for (v=51150): 0.0030722618103027344
latency for (v=51151): 0.003160715103149414
latency for (v=51152): 0.0031685829162597656
latency for (v=51153): 0.0030777454376220703
latency for (v=51154): 0.003069639205932617
latency for (v=51155): 0.0030927658081054688
latency for (v=51156): 0.003065824508666992
latency for (v=51157): 0.0030641555786132812
latency for (v=51158): 0.0031452178955078125
latency for (v=51159): 0.0030655860900878906
latency for (v=51160): 0.0030972957611083984
latency for (v=51161): 0.0030710697174072266
latency for (v=51162): 0.0030813217163085938
latency for (v=51163): 0.0031235218048095703
latency for (v=51164): 0.0030689239501953125
latency for (v=51165): 0.0030858516693115234
latency for (v=51166): 0.0030868053436279297
latency for (v=51167): 0.0031099319458007812
latency for (v=51168): 0.003038644790649414
latency for (v=51169): 0.0030324459075927734
latency for (v=51170): 0.003069639205932617
latency for (v=51171): 0.002995729446411133
latency for (v=51172): 0.0030100345611572266
latency for (v=51173): 0.0030753612518310547
latency for (v=51174): 0.0030298233032226562
latency for (v=51175): 0.005327463150024414
latency for (v=51176): 0.003025054931640625
latency for (v=51177): 0.00304412841796875
latency for (v=51178): 0.002995729446411133
latency for (v=51179): 0.003047943115234375
latency for (v=51180): 0.0031270980834960938
latency for (v=51181): 0.0029997825622558594
latency for (v=51182): 0.003006458282470703
latency for (v=51183): 0.002973794937133789
latency for (v=51184): 0.002989053726196289
latency for (v=51185): 0.003038167953491211
latency for (v=51186): 0.0029659271240234375
latency for (v=51187): 0.0029578208923339844
latency for (v=51188): 0.0029697418212890625
latency for (v=51189): 0.0029668807983398438
latency for (v=51190): 0.002962350845336914
latency for (v=51191): 0.0029993057250976562
latency for (v=51192): 0.002988576889038086
latency for (v=51193): 0.0029816627502441406
latency for (v=51194): 0.0030350685119628906
latency for (v=51195): 0.0030770301818847656
latency for (v=51196): 0.002979755401611328
latency for (v=51197): 0.002956867218017578
latency for (v=51198): 0.0030732154846191406
latency for (v=51199): 0.003019571304321289
latency for (v=51200): 0.0029609203338623047
latency for (v=51201): 0.003032207489013672
latency for (v=51202): 0.003047466278076172
latency for (v=51203): 0.0031032562255859375
latency for (v=51204): 0.0031375885009765625
latency for (v=51205): 0.003168344497680664
latency for (v=51206): 0.0030689239501953125
latency for (v=51207): 0.003177642822265625
latency for (v=51208): 0.0030574798583984375
latency for (v=51209): 0.003041505813598633
latency for (v=51210): 0.0030469894409179688
latency for (v=51211): 0.003108978271484375
latency for (v=51212): 0.003069639205932617
latency for (v=51213): 0.003118753433227539
latency for (v=51214): 0.003086090087890625
latency for (v=51215): 0.003046274185180664
latency for (v=51216): 0.0030426979064941406
latency for (v=51217): 0.003083467483520508
latency for (v=51218): 0.0031430721282958984
latency for (v=51219): 0.003143310546875
latency for (v=51220): 0.0030624866485595703
latency for (v=51221): 0.003076791763305664
latency for (v=51222): 0.003045320510864258
latency for (v=51223): 0.0030524730682373047
latency for (v=51224): 0.0030975341796875
latency for (v=51225): 0.003042459487915039
latency for (v=51226): 0.0030739307403564453
latency for (v=51227): 0.0031189918518066406
latency for (v=51228): 0.0030875205993652344
latency for (v=51229): 0.003081083297729492
latency for (v=51230): 0.0030622482299804688
latency for (v=51231): 0.003014802932739258
latency for (v=51232): 0.0030579566955566406
latency for (v=51233): 0.0031905174255371094
latency for (v=51234): 0.0030057430267333984
latency for (v=51235): 0.0030417442321777344
latency for (v=51236): 0.0030257701873779297
latency for (v=51237): 0.0029888153076171875
latency for (v=51238): 0.0029785633087158203
latency for (v=51239): 0.0029845237731933594
latency for (v=51240): 0.002981424331665039
latency for (v=51241): 0.0030138492584228516
latency for (v=51242): 0.0029761791229248047
latency for (v=51243): 0.002966165542602539
latency for (v=51244): 0.002996683120727539
latency for (v=51245): 0.0029387474060058594
latency for (v=51246): 0.0029518604278564453
latency for (v=51247): 0.0029363632202148438
latency for (v=51248): 0.0029816627502441406
latency for (v=51249): 0.00296783447265625
latency for (v=51250): 0.0029747486114501953
latency for (v=51251): 0.002979755401611328
latency for (v=51252): 0.002970457077026367
latency for (v=51253): 0.0029332637786865234
latency for (v=51254): 0.0030622482299804688
latency for (v=51255): 0.0030312538146972656
latency for (v=51256): 0.0029795169830322266
latency for (v=51257): 0.0030562877655029297
latency for (v=51258): 0.003034830093383789
latency for (v=51259): 0.0029549598693847656
latency for (v=51260): 0.003004312515258789
latency for (v=51261): 0.003050088882446289
latency for (v=51262): 0.0029745101928710938
latency for (v=51263): 0.00301361083984375
latency for (v=51264): 0.0030367374420166016
[2023-11-03 17:48:36,880] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-03 17:48:37,414] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.153.24, master_port=6000
[2023-11-03 17:48:37,414] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-03 17:48:38,665] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
51136: 0.0030133724212646484
51137: 0.0029747486114501953
51138: 0.0029740333557128906
51139: 0.003002166748046875
51140: 0.0030088424682617188
51141: 0.0029566287994384766
51142: 0.0029883384704589844
51143: 0.0030045509338378906
51144: 0.0029077529907226562
51145: 0.0029439926147460938
51146: 0.0029566287994384766
51147: 0.002938985824584961
51148: 0.0029554367065429688
51149: 0.002997159957885742
51150: 0.002960205078125
51151: 0.0030372142791748047
51152: 0.0029468536376953125
51153: 0.0029861927032470703
51154: 0.0029761791229248047
51155: 0.0030121803283691406
51156: 0.003002166748046875
51157: 0.0029850006103515625
51158: 0.0030040740966796875
51159: 0.0030059814453125
51160: 0.0029854774475097656
51161: 0.0029900074005126953
51162: 0.0030205249786376953
51163: 0.0030257701873779297
51164: 0.0030431747436523438
51165: 0.0030040740966796875
51166: 0.003063678741455078
51167: 0.0030722618103027344
51168: 0.0030362606048583984
51169: 0.003041982650756836
51170: 0.0030333995819091797
51171: 0.0030515193939208984
51172: 0.0030586719512939453
51173: 0.003056049346923828
51174: 0.003036975860595703
51175: 0.0030651092529296875
51176: 0.003098011016845703
51177: 0.0030608177185058594
51178: 0.003043651580810547
51179: 0.003079652786254883
51180: 0.003059864044189453
51181: 0.003034353256225586
51182: 0.003059864044189453
51183: 0.003037691116333008
51184: 0.0030388832092285156
51185: 0.0030670166015625
51186: 0.003067493438720703
51187: 0.0030345916748046875
51188: 0.0030601024627685547
51189: 0.0030269622802734375
51190: 0.0030410289764404297
51191: 0.0030639171600341797
51192: 0.003050565719604492
51193: 0.0030517578125
51194: 0.003056764602661133
51195: 0.003053426742553711
51196: 0.0031366348266601562
51197: 0.003044605255126953
51198: 0.0030410289764404297
51199: 0.0030100345611572266
51200: 0.0029904842376708984
51201: 0.0029561519622802734
51202: 0.0029735565185546875
51203: 0.002944469451904297
51204: 0.0030536651611328125
51205: 0.0029754638671875
51206: 0.002953767776489258
51207: 0.002957582473754883
51208: 0.002982616424560547
51209: 0.002972841262817383
51210: 0.0029900074005126953
51211: 0.002976655960083008
51212: 0.003001689910888672
51213: 0.002980947494506836
51214: 0.0029861927032470703
51215: 0.003023386001586914
51216: 0.0030341148376464844
51217: 0.003046274185180664
51218: 0.003011941909790039
51219: 0.0030603408813476562
51220: 0.0030281543731689453
51221: 0.0030755996704101562
51222: 0.0030291080474853516
51223: 0.003050088882446289
51224: 0.0030210018157958984
51225: 0.003039121627807617
51226: 0.0030345916748046875
51227: 0.003038167953491211
51228: 0.0030660629272460938
51229: 0.0030465126037597656
51230: 0.0030422210693359375
51231: 0.0030221939086914062
51232: 0.0030651092529296875
51233: 0.003052949905395508
51234: 0.0030395984649658203
51235: 0.0030260086059570312
51236: 0.0030481815338134766
51237: 0.003059864044189453
51238: 0.0030672550201416016
51239: 0.003025531768798828
51240: 0.0030553340911865234
51241: 0.0030410289764404297
51242: 0.0030584335327148438
51243: 0.003044605255126953
51244: 0.0030641555786132812
51245: 0.003056049346923828
51246: 0.0030584335327148438
51247: 0.003053426742553711
51248: 0.0030519962310791016
51249: 0.003062725067138672
51250: 0.003068208694458008
51251: 0.0030820369720458984
51252: 0.003067493438720703
51253: 0.002991914749145508
51254: 0.003027200698852539
51255: 0.0030045509338378906
51256: 0.0030126571655273438
51257: 0.002964496612548828
51258: 0.0029735565185546875
51259: 0.0030257701873779297
51260: 0.003071308135986328
51261: 0.002958536148071289
51262: 0.0029838085174560547
51263: 0.002933979034423828
51264: 0.002959012985229492
[2023-11-03 17:54:41,256] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-03 17:54:41,721] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.153.24, master_port=6000
[2023-11-03 17:54:41,721] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-03 17:54:42,984] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
51136: 0.0232851505279541
51137: 0.023244619369506836
51138: 0.023194551467895508
51139: 0.02324080467224121
51140: 0.023287057876586914
51141: 0.02321791648864746
51142: 0.023209571838378906
51143: 0.023238182067871094
51144: 0.02319622039794922
51145: 0.023251056671142578
51146: 0.023215532302856445
51147: 0.02319025993347168
51148: 0.023244619369506836
51149: 0.023210525512695312
51150: 0.023276329040527344
51151: 0.023246049880981445
51152: 0.0231783390045166
51153: 0.02324080467224121
51154: 0.023176193237304688
51155: 0.023172378540039062
51156: 0.023221254348754883
51157: 0.023245811462402344
51158: 0.023190975189208984
51159: 0.02322697639465332
51160: 0.023186683654785156
51161: 0.023257970809936523
51162: 0.02319955825805664
51163: 0.023215293884277344
51164: 0.023293256759643555
51165: 0.02325129508972168
51166: 0.023273944854736328
51167: 0.023241043090820312
51168: 0.023242473602294922
51169: 0.02325272560119629
51170: 0.023163557052612305
51171: 0.02322697639465332
51172: 0.023186206817626953
51173: 0.023175477981567383
51174: 0.023178577423095703
51175: 0.02317976951599121
51176: 0.023219823837280273
51177: 0.02324700355529785
51178: 0.02321791648864746
51179: 0.023209095001220703
51180: 0.023200273513793945
51181: 0.023259401321411133
51182: 0.023237228393554688
51183: 0.02320075035095215
51184: 0.02324509620666504
51185: 0.023215532302856445
51186: 0.02320384979248047
51187: 0.02326345443725586
51188: 0.023167848587036133
51189: 0.023229598999023438
51190: 0.023224830627441406
51191: 0.023289918899536133
51192: 0.02324080467224121
51193: 0.02323174476623535
51194: 0.02326035499572754
51195: 0.023225069046020508
51196: 0.023229598999023438
51197: 0.023224830627441406
51198: 0.02322864532470703
51199: 0.023241758346557617
51200: 0.023238182067871094
51201: 0.023208141326904297
51202: 0.02323460578918457
51203: 0.027394771575927734
51204: 0.0232393741607666
51205: 0.02321910858154297
51206: 0.023236989974975586
51207: 0.02324700355529785
51208: 0.023245573043823242
51209: 0.02318739891052246
51210: 0.023207426071166992
51211: 0.023194074630737305
51212: 0.023182153701782227
51213: 0.023219585418701172
51214: 0.023244857788085938
51215: 0.02323293685913086
51216: 0.023290634155273438
51217: 0.023198366165161133
51218: 0.023252248764038086
51219: 0.023283958435058594
51220: 0.023233652114868164
51221: 0.023216962814331055
51222: 0.023282527923583984
51223: 0.02324700355529785
51224: 0.02324533462524414
51225: 0.023189783096313477
51226: 0.023215055465698242
51227: 0.023236989974975586
51228: 0.02323317527770996
51229: 0.0232393741607666
51230: 0.023223161697387695
51231: 0.023230791091918945
51232: 0.023244619369506836
51233: 0.023253679275512695
51234: 0.023179054260253906
51235: 0.023266077041625977
51236: 0.02322101593017578
51237: 0.02329540252685547
51238: 0.023235321044921875
51239: 0.02323603630065918
51240: 0.023236751556396484
51241: 0.02324056625366211
51242: 0.023248910903930664
51243: 0.027332544326782227
51244: 0.02323293685913086
51245: 0.023172378540039062
51246: 0.02325129508972168
51247: 0.02323436737060547
51248: 0.0232083797454834
51249: 0.023260116577148438
51250: 0.023245811462402344
51251: 0.023218393325805664
51252: 0.023235082626342773
51253: 0.02324366569519043
51254: 0.023250818252563477
51255: 0.023229122161865234
51256: 0.023275136947631836
51257: 0.023264169692993164
51258: 0.023189783096313477
51259: 0.023229598999023438
51260: 0.023192167282104492
51261: 0.02319192886352539
51262: 0.023276805877685547
51263: 0.0232698917388916
51264: 0.023262739181518555
