1.13.1 

num_attention_heads: 64, hidden_size: 64, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x1x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x1x2048): 0.936
Elapsed time for attention_prob_times_values (256x2048x2048x1): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x1): 1.429

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 1.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x2x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x2x2048): 1.193
Elapsed time for attention_prob_times_values (256x2048x2048x2): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x2): 2.774

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 1.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x3x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x3x2048): 2.826
Elapsed time for attention_prob_times_values (256x2048x2048x3): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x3): 3.319

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 3.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x4x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x4x2048): 2.408
Elapsed time for attention_prob_times_values (256x2048x2048x4): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x4): 5.530

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 4.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x5x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x5x2048): 4.514
Elapsed time for attention_prob_times_values (256x2048x2048x5): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x5): 5.107

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 6.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x6x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x6x2048): 3.468
Elapsed time for attention_prob_times_values (256x2048x2048x6): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x6): 8.085

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 6.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x7x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x7x2048): 6.066
Elapsed time for attention_prob_times_values (256x2048x2048x7): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x7): 6.922

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 9.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x8x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x8x2048): 10.159
Elapsed time for attention_prob_times_values (256x2048x2048x8): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x8): 11.038

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 15.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x9x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x9x2048): 6.248
Elapsed time for attention_prob_times_values (256x2048x2048x9): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x9): 8.636

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 11.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x10x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x10x2048): 6.063
Elapsed time for attention_prob_times_values (256x2048x2048x10): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x10): 13.470

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 13.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x11x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x11x2048): 7.198
Elapsed time for attention_prob_times_values (256x2048x2048x11): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x11): 10.840

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 14.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x12x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x12x2048): 6.948
Elapsed time for attention_prob_times_values (256x2048x2048x12): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x12): 15.756

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 16.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x13x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x13x2048): 7.704
Elapsed time for attention_prob_times_values (256x2048x2048x13): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x13): 12.067

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 17.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x14x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x14x2048): 8.449
Elapsed time for attention_prob_times_values (256x2048x2048x14): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x14): 17.260

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 21.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x15x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x15x2048): 10.348
Elapsed time for attention_prob_times_values (256x2048x2048x15): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x15): 14.240

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 23.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x16x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x16x2048): 17.685
Elapsed time for attention_prob_times_values (256x2048x2048x16): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x16): 21.685

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 38.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x17x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x17x2048): 11.532
Elapsed time for attention_prob_times_values (256x2048x2048x17): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x17): 16.366

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 27.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x18x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x18x2048): 10.694
Elapsed time for attention_prob_times_values (256x2048x2048x18): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x18): 22.546

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 30.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x19x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x19x2048): 12.664
Elapsed time for attention_prob_times_values (256x2048x2048x19): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x19): 17.214

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 31.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x20x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x20x2048): 12.006
Elapsed time for attention_prob_times_values (256x2048x2048x20): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x20): 24.999

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 36.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x21x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x21x2048): 13.995
Elapsed time for attention_prob_times_values (256x2048x2048x21): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x21): 18.149

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 36.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x22x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x22x2048): 13.116
Elapsed time for attention_prob_times_values (256x2048x2048x22): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x22): 25.026

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 40.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x23x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x23x2048): 15.626
Elapsed time for attention_prob_times_values (256x2048x2048x23): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x23): 22.199

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 44.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x24x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x24x2048): 26.771
Elapsed time for attention_prob_times_values (256x2048x2048x24): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x24): 31.389

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 72.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x25x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x25x2048): 16.722
Elapsed time for attention_prob_times_values (256x2048x2048x25): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x25): 22.868

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 49.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x26x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x26x2048): 15.698
Elapsed time for attention_prob_times_values (256x2048x2048x26): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x26): 29.023

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 53.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x27x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x27x2048): 17.335
Elapsed time for attention_prob_times_values (256x2048x2048x27): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x27): 22.842

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 52.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x28x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x28x2048): 15.991
Elapsed time for attention_prob_times_values (256x2048x2048x28): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x28): 30.903

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 57.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x29x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x29x2048): 18.614
Elapsed time for attention_prob_times_values (256x2048x2048x29): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x29): 25.223

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 60.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x30x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x30x2048): 17.741
Elapsed time for attention_prob_times_values (256x2048x2048x30): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x30): 33.438

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 66.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 1984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x31x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x31x2048): 19.453
Elapsed time for attention_prob_times_values (256x2048x2048x31): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x31): 28.250

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 67.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x32x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x32x2048): 39.087
Elapsed time for attention_prob_times_values (256x2048x2048x32): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x32): 41.821

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 121.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x33x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x33x2048): 18.460
Elapsed time for attention_prob_times_values (256x2048x2048x33): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x33): 22.320

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 61.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x34x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x34x2048): 14.888
Elapsed time for attention_prob_times_values (256x2048x2048x34): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x34): 36.928

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 66.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x35x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x35x2048): 19.578
Elapsed time for attention_prob_times_values (256x2048x2048x35): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x35): 25.150

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 70.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x36x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x36x2048): 14.935
Elapsed time for attention_prob_times_values (256x2048x2048x36): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x36): 39.743

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 70.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x37x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x37x2048): 19.917
Elapsed time for attention_prob_times_values (256x2048x2048x37): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x37): 26.308

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 75.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x38x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x38x2048): 16.158
Elapsed time for attention_prob_times_values (256x2048x2048x38): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x38): 40.583

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 78.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x39x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x39x2048): 19.469
Elapsed time for attention_prob_times_values (256x2048x2048x39): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x39): 25.221

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 75.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x40x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x40x2048): 35.975
Elapsed time for attention_prob_times_values (256x2048x2048x40): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x40): 51.294

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 148.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x41x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x41x2048): 21.014
Elapsed time for attention_prob_times_values (256x2048x2048x41): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x41): 27.307

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 84.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x42x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x42x2048): 17.178
Elapsed time for attention_prob_times_values (256x2048x2048x42): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x42): 44.844

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 90.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x43x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x43x2048): 22.414
Elapsed time for attention_prob_times_values (256x2048x2048x43): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x43): 30.221

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 94.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x44x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x44x2048): 17.681
Elapsed time for attention_prob_times_values (256x2048x2048x44): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x44): 48.255

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 97.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x45x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x45x2048): 23.252
Elapsed time for attention_prob_times_values (256x2048x2048x45): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x45): 30.235

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 100.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x46x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x46x2048): 18.194
Elapsed time for attention_prob_times_values (256x2048x2048x46): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x46): 47.994

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 102.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x47x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x47x2048): 23.702
Elapsed time for attention_prob_times_values (256x2048x2048x47): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x47): 30.732

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 105.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x48x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x48x2048): 46.379
Elapsed time for attention_prob_times_values (256x2048x2048x48): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x48): 61.640

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 211.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x49x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x49x2048): 23.893
Elapsed time for attention_prob_times_values (256x2048x2048x49): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x49): 32.105

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 111.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x50x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x50x2048): 19.280
Elapsed time for attention_prob_times_values (256x2048x2048x50): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x50): 54.391

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 117.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x51x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x51x2048): 25.816
Elapsed time for attention_prob_times_values (256x2048x2048x51): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x51): 35.015

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 124.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x52x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x52x2048): 19.406
Elapsed time for attention_prob_times_values (256x2048x2048x52): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x52): 55.012

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 121.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x53x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x53x2048): 26.178
Elapsed time for attention_prob_times_values (256x2048x2048x53): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x53): 36.469

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 131.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x54x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x54x2048): 20.450
Elapsed time for attention_prob_times_values (256x2048x2048x54): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x54): 56.642

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 131.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x55x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x55x2048): 26.962
Elapsed time for attention_prob_times_values (256x2048x2048x55): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x55): 35.733

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 136.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x56x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x56x2048): 49.931
Elapsed time for attention_prob_times_values (256x2048x2048x56): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x56): 70.232

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 262.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x57x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x57x2048): 27.231
Elapsed time for attention_prob_times_values (256x2048x2048x57): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x57): 36.271

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 141.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x58x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x58x2048): 22.114
Elapsed time for attention_prob_times_values (256x2048x2048x58): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x58): 59.440

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 149.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x59x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x59x2048): 28.624
Elapsed time for attention_prob_times_values (256x2048x2048x59): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x59): 40.278

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 156.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x60x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x60x2048): 20.960
Elapsed time for attention_prob_times_values (256x2048x2048x60): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x60): 61.455

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 148.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x61x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x61x2048): 25.422
Elapsed time for attention_prob_times_values (256x2048x2048x61): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x61): 41.536

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 151.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x62x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x62x2048): 22.241
Elapsed time for attention_prob_times_values (256x2048x2048x62): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x62): 62.487

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 159.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x63x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x63x2048): 30.089
Elapsed time for attention_prob_times_values (256x2048x2048x63): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x63): 40.733

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 170.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x64x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x64x2048): 56.616
Elapsed time for attention_prob_times_values (256x2048x2048x64): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x64): 76.141

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 324.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x65x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x65x2048): 26.747
Elapsed time for attention_prob_times_values (256x2048x2048x65): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x65): 41.065

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 163.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x66x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x66x2048): 19.805
Elapsed time for attention_prob_times_values (256x2048x2048x66): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x66): 70.236

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 158.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x67x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x67x2048): 27.179
Elapsed time for attention_prob_times_values (256x2048x2048x67): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x67): 41.092

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 169.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x68x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x68x2048): 19.199
Elapsed time for attention_prob_times_values (256x2048x2048x68): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x68): 71.427

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 158.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x69x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x69x2048): 26.884
Elapsed time for attention_prob_times_values (256x2048x2048x69): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x69): 40.557

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 171.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x70x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x70x2048): 19.741
Elapsed time for attention_prob_times_values (256x2048x2048x70): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x70): 69.769

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 165.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x71x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x71x2048): 27.696
Elapsed time for attention_prob_times_values (256x2048x2048x71): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x71): 42.623

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 182.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x72x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x72x2048): 63.593
Elapsed time for attention_prob_times_values (256x2048x2048x72): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x72): 82.740

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 395.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x73x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x73x2048): 27.820
Elapsed time for attention_prob_times_values (256x2048x2048x73): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x73): 43.441

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 188.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x74x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x74x2048): 49.322
Elapsed time for attention_prob_times_values (256x2048x2048x74): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x74): 76.569

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 337.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x75x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x75x2048): 29.201
Elapsed time for attention_prob_times_values (256x2048x2048x75): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x75): 46.776

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 204.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x76x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x76x2048): 51.416
Elapsed time for attention_prob_times_values (256x2048x2048x76): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x76): 78.485

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 357.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x77x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x77x2048): 28.801
Elapsed time for attention_prob_times_values (256x2048x2048x77): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x77): 47.027

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 207.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x78x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x78x2048): 51.926
Elapsed time for attention_prob_times_values (256x2048x2048x78): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x78): 80.265

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 370.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x79x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x79x2048): 29.960
Elapsed time for attention_prob_times_values (256x2048x2048x79): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x79): 48.855

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 220.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x80x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x80x2048): 72.690
Elapsed time for attention_prob_times_values (256x2048x2048x80): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x80): 89.368

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 481.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x81x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x81x2048): 30.029
Elapsed time for attention_prob_times_values (256x2048x2048x81): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x81): 49.598

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 226.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x82x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x82x2048): 54.280
Elapsed time for attention_prob_times_values (256x2048x2048x82): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x82): 83.579

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 403.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x83x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x83x2048): 31.133
Elapsed time for attention_prob_times_values (256x2048x2048x83): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x83): 51.353

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 239.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x84x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x84x2048): 56.531
Elapsed time for attention_prob_times_values (256x2048x2048x84): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x84): 86.046

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 426.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x85x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x85x2048): 30.714
Elapsed time for attention_prob_times_values (256x2048x2048x85): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x85): 51.308

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 242.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x86x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x86x2048): 58.203
Elapsed time for attention_prob_times_values (256x2048x2048x86): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x86): 83.508

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 437.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x87x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x87x2048): 31.809
Elapsed time for attention_prob_times_values (256x2048x2048x87): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x87): 49.760

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 249.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x88x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x88x2048): 74.622
Elapsed time for attention_prob_times_values (256x2048x2048x88): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x88): 95.086

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 543.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x89x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x89x2048): 32.284
Elapsed time for attention_prob_times_values (256x2048x2048x89): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x89): 54.485

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 266.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x90x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x90x2048): 58.556
Elapsed time for attention_prob_times_values (256x2048x2048x90): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x90): 87.828

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 465.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x91x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x91x2048): 32.464
Elapsed time for attention_prob_times_values (256x2048x2048x91): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x91): 39.907

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 239.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x92x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x92x2048): 61.246
Elapsed time for attention_prob_times_values (256x2048x2048x92): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x92): 93.592

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 499.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x93x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x93x2048): 32.755
Elapsed time for attention_prob_times_values (256x2048x2048x93): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x93): 57.241

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 283.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x94x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x94x2048): 61.581
Elapsed time for attention_prob_times_values (256x2048x2048x94): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x94): 90.331

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 503.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x95x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x95x2048): 34.244
Elapsed time for attention_prob_times_values (256x2048x2048x95): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x95): 56.653

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 296.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x96x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x96x2048): 68.938
Elapsed time for attention_prob_times_values (256x2048x2048x96): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x96): 106.568

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 586.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x97x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x97x2048): 31.126
Elapsed time for attention_prob_times_values (256x2048x2048x97): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x97): 58.379

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 286.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x98x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x98x2048): 59.330
Elapsed time for attention_prob_times_values (256x2048x2048x98): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x98): 99.471

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 529.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x99x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x99x2048): 30.698
Elapsed time for attention_prob_times_values (256x2048x2048x99): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x99): 59.979

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 291.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x100x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x100x2048): 58.280
Elapsed time for attention_prob_times_values (256x2048x2048x100): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x100): 101.353

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 536.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x101x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x101x2048): 31.116
Elapsed time for attention_prob_times_values (256x2048x2048x101): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x101): 61.756

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 302.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x102x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x102x2048): 58.705
Elapsed time for attention_prob_times_values (256x2048x2048x102): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x102): 103.009

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 551.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x103x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x103x2048): 30.780
Elapsed time for attention_prob_times_values (256x2048x2048x103): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x103): 60.518

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 303.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x104x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x104x2048): 85.353
Elapsed time for attention_prob_times_values (256x2048x2048x104): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x104): 116.613

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 739.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x105x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x105x2048): 30.484
Elapsed time for attention_prob_times_values (256x2048x2048x105): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x105): 61.786

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 308.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x106x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x106x2048): 63.503
Elapsed time for attention_prob_times_values (256x2048x2048x106): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x106): 107.277

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 608.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x107x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x107x2048): 31.434
Elapsed time for attention_prob_times_values (256x2048x2048x107): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x107): 58.766

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 314.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x108x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x108x2048): 64.053
Elapsed time for attention_prob_times_values (256x2048x2048x108): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x108): 105.829

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 618.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 6976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x109x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x109x2048): 32.125
Elapsed time for attention_prob_times_values (256x2048x2048x109): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x109): 64.877

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 335.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x110x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x110x2048): 65.425
Elapsed time for attention_prob_times_values (256x2048x2048x110): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x110): 110.703

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 647.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x111x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x111x2048): 32.328
Elapsed time for attention_prob_times_values (256x2048x2048x111): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x111): 54.154

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 321.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x112x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x112x2048): 92.070
Elapsed time for attention_prob_times_values (256x2048x2048x112): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x112): 125.116

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 848.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x113x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x113x2048): 33.255
Elapsed time for attention_prob_times_values (256x2048x2048x113): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x113): 15.464

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 170.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x114x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x114x2048): 64.890
Elapsed time for attention_prob_times_values (256x2048x2048x114): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x114): 107.871

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 658.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x115x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x115x2048): 33.667
Elapsed time for attention_prob_times_values (256x2048x2048x115): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x115): 69.894

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 372.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x116x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x116x2048): 68.161
Elapsed time for attention_prob_times_values (256x2048x2048x116): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x116): 115.823

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 707.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x117x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x117x2048): 34.534
Elapsed time for attention_prob_times_values (256x2048x2048x117): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x117): 67.330

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 379.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x118x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x118x2048): 67.870
Elapsed time for attention_prob_times_values (256x2048x2048x118): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x118): 116.362

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 718.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x119x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x119x2048): 33.923
Elapsed time for attention_prob_times_values (256x2048x2048x119): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x119): 69.143

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 384.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x120x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x120x2048): 90.449
Elapsed time for attention_prob_times_values (256x2048x2048x120): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x120): 123.584

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 887.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x121x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x121x2048): 33.904
Elapsed time for attention_prob_times_values (256x2048x2048x121): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x121): 70.378

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 391.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x122x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x122x2048): 18.414
Elapsed time for attention_prob_times_values (256x2048x2048x122): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x122): 118.439

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 274.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x123x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x123x2048): 35.595
Elapsed time for attention_prob_times_values (256x2048x2048x123): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x123): 74.400

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 418.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x124x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x124x2048): 72.876
Elapsed time for attention_prob_times_values (256x2048x2048x124): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x124): 118.115

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 788.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x125x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x125x2048): 36.497
Elapsed time for attention_prob_times_values (256x2048x2048x125): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x125): 68.232

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 419.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x126x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x126x2048): 70.308
Elapsed time for attention_prob_times_values (256x2048x2048x126): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x126): 125.004

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 798.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x127x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x127x2048): 36.435
Elapsed time for attention_prob_times_values (256x2048x2048x127): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x127): 72.968

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 434.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 107.169
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 141.915

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 1099.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x129x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x129x2048): 34.070
Elapsed time for attention_prob_times_values (256x2048x2048x129): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x129): 55.348

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 382.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 71.359
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 76.064

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 671.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x131x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x131x2048): 34.370
Elapsed time for attention_prob_times_values (256x2048x2048x131): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x131): 57.088

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 394.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 69.343
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 89.435

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 722.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x133x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x133x2048): 33.806
Elapsed time for attention_prob_times_values (256x2048x2048x133): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x133): 55.851

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 392.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 72.876
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 93.556

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 768.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x135x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x135x2048): 33.179
Elapsed time for attention_prob_times_values (256x2048x2048x135): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x135): 57.449

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 396.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 99.165
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 124.756

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 1049.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x137x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x137x2048): 32.956
Elapsed time for attention_prob_times_values (256x2048x2048x137): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x137): 57.307

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 400.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 72.037
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 90.955

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 773.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x139x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x139x2048): 34.636
Elapsed time for attention_prob_times_values (256x2048x2048x139): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x139): 60.574

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 426.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 75.356
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 92.837

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 811.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x141x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x141x2048): 34.670
Elapsed time for attention_prob_times_values (256x2048x2048x141): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x141): 58.588

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 427.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 75.785
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 96.421

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 838.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x143x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x143x2048): 34.766
Elapsed time for attention_prob_times_values (256x2048x2048x143): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x143): 58.911

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 434.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 102.882
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 135.801

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 1170.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x145x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x145x2048): 56.814
Elapsed time for attention_prob_times_values (256x2048x2048x145): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x145): 57.833

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 576.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 75.979
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 97.977

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 866.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x147x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x147x2048): 56.273
Elapsed time for attention_prob_times_values (256x2048x2048x147): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x147): 58.462

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 584.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 72.406
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 94.813

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 841.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x149x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x149x2048): 55.531
Elapsed time for attention_prob_times_values (256x2048x2048x149): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x149): 60.104

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 595.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 79.145
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 98.310

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 909.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x151x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x151x2048): 56.699
Elapsed time for attention_prob_times_values (256x2048x2048x151): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x151): 59.431

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 605.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 103.063
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 140.061

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1246.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x153x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x153x2048): 56.493
Elapsed time for attention_prob_times_values (256x2048x2048x153): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x153): 59.645

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 612.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 78.961
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 100.435

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 939.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x155x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x155x2048): 58.138
Elapsed time for attention_prob_times_values (256x2048x2048x155): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x155): 61.271

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 637.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 82.243
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 102.579

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 981.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x157x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x157x2048): 59.620
Elapsed time for attention_prob_times_values (256x2048x2048x157): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x157): 62.426

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 659.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 81.837
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 104.297

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 997.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x159x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x159x2048): 59.825
Elapsed time for attention_prob_times_values (256x2048x2048x159): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x159): 62.698

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 669.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 122.256
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 149.995

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 1481.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x161x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x161x2048): 57.697
Elapsed time for attention_prob_times_values (256x2048x2048x161): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x161): 61.727

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 659.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 79.814
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 105.082

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1009.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x163x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x163x2048): 57.695
Elapsed time for attention_prob_times_values (256x2048x2048x163): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x163): 64.101

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 679.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 78.707
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 104.174

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 1008.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x165x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x165x2048): 57.032
Elapsed time for attention_prob_times_values (256x2048x2048x165): 0.0183
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x165): 19.408

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 327.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 80.154
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 108.741

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 1049.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x167x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x167x2048): 57.907
Elapsed time for attention_prob_times_values (256x2048x2048x167): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x167): 63.083

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 690.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 112.598
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 153.828

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 1495.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x169x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x169x2048): 58.115
Elapsed time for attention_prob_times_values (256x2048x2048x169): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x169): 64.613

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 707.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 81.129
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 110.327

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1086.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x171x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x171x2048): 58.752
Elapsed time for attention_prob_times_values (256x2048x2048x171): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x171): 68.034

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 736.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 81.338
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 112.101

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1107.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x173x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x173x2048): 59.078
Elapsed time for attention_prob_times_values (256x2048x2048x173): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x173): 68.416

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 748.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 84.506
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 111.803

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1143.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x175x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x175x2048): 59.644
Elapsed time for attention_prob_times_values (256x2048x2048x175): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x175): 65.806

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 746.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 120.014
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 155.312

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 1624.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x177x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x177x2048): 59.708
Elapsed time for attention_prob_times_values (256x2048x2048x177): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x177): 67.343

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 763.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 86.073
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 115.187

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1194.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x179x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x179x2048): 59.948
Elapsed time for attention_prob_times_values (256x2048x2048x179): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x179): 68.916

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 781.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 83.931
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 114.409

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 1186.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x181x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x181x2048): 60.451
Elapsed time for attention_prob_times_values (256x2048x2048x181): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x181): 71.059

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 804.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 86.420
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 116.586

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 1228.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x183x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x183x2048): 60.440
Elapsed time for attention_prob_times_values (256x2048x2048x183): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x183): 69.374

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 803.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 114.377
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 165.882

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 1692.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x185x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x185x2048): 61.053
Elapsed time for attention_prob_times_values (256x2048x2048x185): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x185): 69.439

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 816.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 89.475
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 119.080

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1289.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x187x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x187x2048): 61.184
Elapsed time for attention_prob_times_values (256x2048x2048x187): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x187): 72.324

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 841.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 90.336
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 120.498

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 1316.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x189x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x189x2048): 62.262
Elapsed time for attention_prob_times_values (256x2048x2048x189): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x189): 74.397

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 868.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 88.566
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 121.356

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 1318.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x191x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x191x2048): 62.516
Elapsed time for attention_prob_times_values (256x2048x2048x191): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x191): 72.151

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 866.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 138.973
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 176.162

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 2019.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x193x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x193x2048): 61.015
Elapsed time for attention_prob_times_values (256x2048x2048x193): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x193): 73.248

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 869.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 85.077
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 118.203

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 1298.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x195x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x195x2048): 59.723
Elapsed time for attention_prob_times_values (256x2048x2048x195): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x195): 74.191

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 872.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 87.043
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 117.295

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 1324.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x197x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x197x2048): 60.962
Elapsed time for attention_prob_times_values (256x2048x2048x197): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x197): 72.946

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 884.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 87.406
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 120.746

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 1356.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x199x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x199x2048): 60.664
Elapsed time for attention_prob_times_values (256x2048x2048x199): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x199): 73.454

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 892.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 120.740
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 178.431

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 1944.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x201x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x201x2048): 62.533
Elapsed time for attention_prob_times_values (256x2048x2048x201): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x201): 72.846

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 912.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 89.903
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 120.263

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 1401.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x203x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x203x2048): 62.938
Elapsed time for attention_prob_times_values (256x2048x2048x203): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x203): 74.900

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 936.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 88.141
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 122.376

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1409.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x205x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x205x2048): 63.892
Elapsed time for attention_prob_times_values (256x2048x2048x205): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x205): 74.905

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 952.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 91.498
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 122.741

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 1454.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x207x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x207x2048): 64.311
Elapsed time for attention_prob_times_values (256x2048x2048x207): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x207): 75.516

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 968.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 134.998
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 180.683

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 2163.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x209x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x209x2048): 64.031
Elapsed time for attention_prob_times_values (256x2048x2048x209): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x209): 75.363

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 973.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 90.184
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 122.778

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 1468.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x211x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x211x2048): 63.123
Elapsed time for attention_prob_times_values (256x2048x2048x211): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x211): 77.506

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 987.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 93.243
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 125.618

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 1525.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x213x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x213x2048): 64.079
Elapsed time for attention_prob_times_values (256x2048x2048x213): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x213): 74.998

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 989.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 58.030
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 122.151

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1131.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x215x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x215x2048): 63.953
Elapsed time for attention_prob_times_values (256x2048x2048x215): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x215): 77.113

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1009.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 124.795
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 189.293

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 2181.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x217x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x217x2048): 64.202
Elapsed time for attention_prob_times_values (256x2048x2048x217): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x217): 75.697

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1011.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 93.924
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 125.060

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 1568.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x219x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x219x2048): 63.104
Elapsed time for attention_prob_times_values (256x2048x2048x219): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x219): 78.442

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1027.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 38.591
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 124.824

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 869.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x221x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x221x2048): 62.706
Elapsed time for attention_prob_times_values (256x2048x2048x221): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x221): 79.313

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1037.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 97.394
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 129.206

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1652.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x223x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x223x2048): 68.748
Elapsed time for attention_prob_times_values (256x2048x2048x223): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x223): 78.893

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1097.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 147.346
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 196.400

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 2525.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x225x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x225x2048): 66.582
Elapsed time for attention_prob_times_values (256x2048x2048x225): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x225): 81.169

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1101.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 93.842
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 131.671

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1657.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x227x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x227x2048): 64.530
Elapsed time for attention_prob_times_values (256x2048x2048x227): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x227): 82.111

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1097.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 94.381
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 132.844

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1682.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x229x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x229x2048): 66.772
Elapsed time for attention_prob_times_values (256x2048x2048x229): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x229): 83.427

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1135.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 95.706
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 133.940

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 1716.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x231x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x231x2048): 66.866
Elapsed time for attention_prob_times_values (256x2048x2048x231): 0.0197
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x231): 25.237

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 565.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 132.291
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 187.852

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 2406.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x233x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x233x2048): 65.692
Elapsed time for attention_prob_times_values (256x2048x2048x233): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x233): 81.724

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1133.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 97.551
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 135.910

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 1774.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x235x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x235x2048): 66.408
Elapsed time for attention_prob_times_values (256x2048x2048x235): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x235): 86.174

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1176.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 98.108
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 136.376

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1797.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x237x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x237x2048): 67.807
Elapsed time for attention_prob_times_values (256x2048x2048x237): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x237): 85.745

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1197.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 97.846
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 138.905

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1822.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x239x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x239x2048): 67.285
Elapsed time for attention_prob_times_values (256x2048x2048x239): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x239): 83.650

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1188.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 140.518
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 207.440

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 2680.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x241x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x241x2048): 68.327
Elapsed time for attention_prob_times_values (256x2048x2048x241): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x241): 87.094

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1230.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 98.484
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 140.792

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1868.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x243x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x243x2048): 68.920
Elapsed time for attention_prob_times_values (256x2048x2048x243): 0.0180
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x243): 28.950

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 660.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 101.447
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 143.120

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 1929.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x245x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x245x2048): 70.319
Elapsed time for attention_prob_times_values (256x2048x2048x245): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x245): 90.286

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1289.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 99.291
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 139.532

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 1899.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x247x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x247x2048): 69.295
Elapsed time for attention_prob_times_values (256x2048x2048x247): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x247): 87.486

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1271.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 128.033
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 211.253

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 2630.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x249x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x249x2048): 69.550
Elapsed time for attention_prob_times_values (256x2048x2048x249): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x249): 88.188

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1288.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 101.716
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 143.903

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1981.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x251x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x251x2048): 72.348
Elapsed time for attention_prob_times_values (256x2048x2048x251): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x251): 91.206

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1346.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 51.766
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 149.350

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1287.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x253x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x253x2048): 73.098
Elapsed time for attention_prob_times_values (256x2048x2048x253): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x253): 94.174

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1383.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 104.999
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 150.932

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 2089.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x255x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x255x2048): 72.920
Elapsed time for attention_prob_times_values (256x2048x2048x255): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x255): 91.076

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1371.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 148.496
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 221.418

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 3022.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x257x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x257x2048): 69.735
Elapsed time for attention_prob_times_values (256x2048x2048x257): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x257): 55.037

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1049.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 101.991
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 95.338

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1687.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x259x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x259x2048): 70.439
Elapsed time for attention_prob_times_values (256x2048x2048x259): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x259): 56.283

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1075.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 98.897
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 97.517

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1693.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x261x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x261x2048): 67.807
Elapsed time for attention_prob_times_values (256x2048x2048x261): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x261): 55.422

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1055.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 102.058
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 95.376

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1713.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x263x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x263x2048): 69.290
Elapsed time for attention_prob_times_values (256x2048x2048x263): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x263): 55.444

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1074.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 137.459
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 130.783

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 2345.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x265x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x265x2048): 70.045
Elapsed time for attention_prob_times_values (256x2048x2048x265): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x265): 55.863

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1091.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 101.539
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 96.914

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1747.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x267x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x267x2048): 70.393
Elapsed time for attention_prob_times_values (256x2048x2048x267): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x267): 56.653

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1110.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 104.409
Elapsed time for attention_prob_times_values (256x2048x2048x268): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 95.875

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1774.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x269x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x269x2048): 70.088
Elapsed time for attention_prob_times_values (256x2048x2048x269): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x269): 57.371

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1123.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 100.814
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 96.143

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1759.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x271x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x271x2048): 70.495
Elapsed time for attention_prob_times_values (256x2048x2048x271): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x271): 56.356

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1123.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 151.860
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 137.995

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 2602.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x273x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x273x2048): 69.865
Elapsed time for attention_prob_times_values (256x2048x2048x273): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x273): 56.449

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1127.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 99.829
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 97.932

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1792.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x275x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x275x2048): 72.276
Elapsed time for attention_prob_times_values (256x2048x2048x275): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x275): 57.677

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1166.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 105.769
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 97.612

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1852.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x277x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x277x2048): 71.500
Elapsed time for attention_prob_times_values (256x2048x2048x277): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x277): 58.413

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1177.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 107.459
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 98.808

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1891.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x279x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x279x2048): 72.484
Elapsed time for attention_prob_times_values (256x2048x2048x279): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x279): 57.502

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1182.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 134.071
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 140.229

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 2535.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x281x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x281x2048): 71.806
Elapsed time for attention_prob_times_values (256x2048x2048x281): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x281): 57.973

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1190.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 105.989
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 98.826

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1905.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x283x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x283x2048): 73.565
Elapsed time for attention_prob_times_values (256x2048x2048x283): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x283): 58.852

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1221.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 108.012
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 98.428

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1931.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x285x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x285x2048): 72.676
Elapsed time for attention_prob_times_values (256x2048x2048x285): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x285): 59.516

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1231.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 48.753
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 100.294

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1238.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x287x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x287x2048): 75.021
Elapsed time for attention_prob_times_values (256x2048x2048x287): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x287): 58.806

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1248.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 162.404
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 145.874

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 2920.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x289x2048): 0.0216
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x289x2048): 28.714
Elapsed time for attention_prob_times_values (256x2048x2048x289): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x289): 58.483

Attention duration (in seconds): 0.0322
Attention throughput (in TFLOP/s): 734.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 105.055
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 102.206

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1981.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x291x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x291x2048): 70.861
Elapsed time for attention_prob_times_values (256x2048x2048x291): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x291): 60.169

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1248.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 103.052
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 98.757

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1941.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x293x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x293x2048): 70.254
Elapsed time for attention_prob_times_values (256x2048x2048x293): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x293): 59.894

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1248.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 105.625
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 100.647

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1997.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x295x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x295x2048): 71.534
Elapsed time for attention_prob_times_values (256x2048x2048x295): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x295): 60.338

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1272.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 140.737
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 145.111

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 2786.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x297x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x297x2048): 70.435
Elapsed time for attention_prob_times_values (256x2048x2048x297): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x297): 60.466

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1272.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 105.913
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 104.813

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 2067.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x299x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x299x2048): 69.540
Elapsed time for attention_prob_times_values (256x2048x2048x299): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x299): 61.352

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1283.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 107.016
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 103.266

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 2075.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x301x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x301x2048): 69.958
Elapsed time for attention_prob_times_values (256x2048x2048x301): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x301): 60.720

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1288.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 105.302
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 103.867

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 2078.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x303x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x303x2048): 70.001
Elapsed time for attention_prob_times_values (256x2048x2048x303): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x303): 60.941

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1299.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 155.235
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 150.178

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 3053.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x305x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x305x2048): 69.166
Elapsed time for attention_prob_times_values (256x2048x2048x305): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x305): 61.095

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1301.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 106.660
Elapsed time for attention_prob_times_values (256x2048x2048x306): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 106.871

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 2148.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x307x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x307x2048): 69.521
Elapsed time for attention_prob_times_values (256x2048x2048x307): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x307): 61.997

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1323.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 107.549
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 94.016

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 2031.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x309x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x309x2048): 69.830
Elapsed time for attention_prob_times_values (256x2048x2048x309): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x309): 62.304

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1337.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 109.658
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 105.999

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 2196.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x311x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x311x2048): 69.924
Elapsed time for attention_prob_times_values (256x2048x2048x311): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x311): 61.958

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1342.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 138.627
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 154.783

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 2998.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x313x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x313x2048): 70.761
Elapsed time for attention_prob_times_values (256x2048x2048x313): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x313): 62.005

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1359.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 110.864
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 98.825

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 2155.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x315x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x315x2048): 71.337
Elapsed time for attention_prob_times_values (256x2048x2048x315): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x315): 63.515

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1390.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 109.007
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 108.063

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 2252.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x317x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x317x2048): 69.217
Elapsed time for attention_prob_times_values (256x2048x2048x317): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x317): 64.548

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1390.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 112.462
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0258
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 26.480

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 894.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x319x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x319x2048): 69.997
Elapsed time for attention_prob_times_values (256x2048x2048x319): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x319): 64.666

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1407.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 165.697
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 161.474

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 3434.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x321x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x321x2048): 44.418
Elapsed time for attention_prob_times_values (256x2048x2048x321): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x321): 63.462

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1100.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 107.098
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 108.904

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 2281.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x323x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x323x2048): 69.026
Elapsed time for attention_prob_times_values (256x2048x2048x323): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x323): 63.209

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1398.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 108.816
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 110.014

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 2324.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x325x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x325x2048): 69.601
Elapsed time for attention_prob_times_values (256x2048x2048x325): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x325): 64.540

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1427.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 109.146
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 108.786

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 2329.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x327x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x327x2048): 70.488
Elapsed time for attention_prob_times_values (256x2048x2048x327): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x327): 63.469

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1431.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 139.818
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 161.001

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 3217.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x329x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x329x2048): 70.838
Elapsed time for attention_prob_times_values (256x2048x2048x329): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x329): 64.266

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1453.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 110.374
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 110.039

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 2383.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x331x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x331x2048): 71.264
Elapsed time for attention_prob_times_values (256x2048x2048x331): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x331): 65.443

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1479.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 109.759
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 111.578

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 2406.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x333x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x333x2048): 72.176
Elapsed time for attention_prob_times_values (256x2048x2048x333): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x333): 66.127

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1505.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 109.468
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 110.587

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 2406.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x335x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x335x2048): 72.609
Elapsed time for attention_prob_times_values (256x2048x2048x335): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x335): 65.068

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1505.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 157.094
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 162.623

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 3515.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x337x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x337x2048): 73.900
Elapsed time for attention_prob_times_values (256x2048x2048x337): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x337): 66.677

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1546.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 112.903
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 112.694

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 2495.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x339x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x339x2048): 73.935
Elapsed time for attention_prob_times_values (256x2048x2048x339): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x339): 67.738

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1568.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 111.337
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 112.190

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 2486.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x341x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x341x2048): 75.146
Elapsed time for attention_prob_times_values (256x2048x2048x341): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x341): 68.645

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1600.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 115.205
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 115.807

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 2584.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x343x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x343x2048): 75.936
Elapsed time for attention_prob_times_values (256x2048x2048x343): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x343): 67.752

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1606.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 181.505
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 163.301

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 3868.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x345x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x345x2048): 76.207
Elapsed time for attention_prob_times_values (256x2048x2048x345): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x345): 67.473

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1614.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 115.855
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 113.811

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 2597.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x347x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x347x2048): 74.830
Elapsed time for attention_prob_times_values (256x2048x2048x347): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x347): 68.797

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1626.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 111.608
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 114.277

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 2569.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x349x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x349x2048): 74.967
Elapsed time for attention_prob_times_values (256x2048x2048x349): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x349): 68.349

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1631.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 115.287
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 112.650

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 2606.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x351x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x351x2048): 76.426
Elapsed time for attention_prob_times_values (256x2048x2048x351): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x351): 69.262

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1666.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 196.294
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 170.486

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 4197.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x353x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x353x2048): 74.957
Elapsed time for attention_prob_times_values (256x2048x2048x353): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x353): 69.485

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1663.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 111.950
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 117.828

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 2655.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x355x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x355x2048): 73.512
Elapsed time for attention_prob_times_values (256x2048x2048x355): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x355): 70.339

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1666.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 109.424
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 117.754

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 2637.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x357x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x357x2048): 71.787
Elapsed time for attention_prob_times_values (256x2048x2048x357): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x357): 70.021

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1652.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 112.665
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 117.690

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 2690.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x359x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x359x2048): 73.331
Elapsed time for attention_prob_times_values (256x2048x2048x359): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x359): 68.863

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1664.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 182.548
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 175.114

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 4200.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x361x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x361x2048): 72.645
Elapsed time for attention_prob_times_values (256x2048x2048x361): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x361): 70.753

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1689.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 114.111
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 119.603

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 2759.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x363x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x363x2048): 72.214
Elapsed time for attention_prob_times_values (256x2048x2048x363): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x363): 72.287

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1711.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 111.837
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 120.431

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 2754.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x365x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x365x2048): 71.867
Elapsed time for attention_prob_times_values (256x2048x2048x365): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x365): 72.206

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1715.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 111.701
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 120.078

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 2763.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x367x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x367x2048): 73.353
Elapsed time for attention_prob_times_values (256x2048x2048x367): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x367): 71.335

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1731.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 183.841
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 180.409

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 4370.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x369x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x369x2048): 73.457
Elapsed time for attention_prob_times_values (256x2048x2048x369): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x369): 71.849

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1748.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 111.144
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 122.416

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 2810.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x371x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x371x2048): 73.799
Elapsed time for attention_prob_times_values (256x2048x2048x371): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x371): 73.321

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1779.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 111.805
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 120.424

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 2811.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x373x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x373x2048): 74.422
Elapsed time for attention_prob_times_values (256x2048x2048x373): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x373): 73.360

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1796.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 113.043
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 125.361

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 2897.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x375x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x375x2048): 73.707
Elapsed time for attention_prob_times_values (256x2048x2048x375): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x375): 73.066

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1793.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 180.048
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 180.144

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 4412.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x377x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x377x2048): 74.065
Elapsed time for attention_prob_times_values (256x2048x2048x377): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x377): 72.712

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1802.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 113.449
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 123.853

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 2916.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x379x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x379x2048): 73.769
Elapsed time for attention_prob_times_values (256x2048x2048x379): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x379): 76.118

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1849.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 116.186
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 126.007

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 2992.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x381x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x381x2048): 74.870
Elapsed time for attention_prob_times_values (256x2048x2048x381): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x381): 76.797

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1881.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 116.838
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 127.021

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 3027.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x383x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x383x2048): 75.475
Elapsed time for attention_prob_times_values (256x2048x2048x383): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x383): 75.094

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1877.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x384x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x384x2048): 192.925
Elapsed time for attention_prob_times_values (256x2048x2048x384): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x384): 189.097

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 4774.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x385x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x385x2048): 74.804
Elapsed time for attention_prob_times_values (256x2048x2048x385): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x385): 76.168

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1891.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x386x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x386x2048): 107.321
Elapsed time for attention_prob_times_values (256x2048x2048x386): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x386): 127.411

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 2927.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x387x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x387x2048): 74.164
Elapsed time for attention_prob_times_values (256x2048x2048x387): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x387): 75.897

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1889.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x388x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x388x2048): 109.610
Elapsed time for attention_prob_times_values (256x2048x2048x388): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x388): 126.230

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 2962.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x389x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x389x2048): 75.297
Elapsed time for attention_prob_times_values (256x2048x2048x389): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x389): 75.297

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1905.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x390x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x390x2048): 108.635
Elapsed time for attention_prob_times_values (256x2048x2048x390): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x390): 123.974

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 2938.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x391x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x391x2048): 75.431
Elapsed time for attention_prob_times_values (256x2048x2048x391): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x391): 74.067

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1901.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x392x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x392x2048): 184.300
Elapsed time for attention_prob_times_values (256x2048x2048x392): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x392): 187.222

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 4736.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x393x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x393x2048): 75.665
Elapsed time for attention_prob_times_values (256x2048x2048x393): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x393): 73.765

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1909.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x394x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x394x2048): 109.566
Elapsed time for attention_prob_times_values (256x2048x2048x394): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x394): 125.649

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 2999.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x395x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x395x2048): 76.144
Elapsed time for attention_prob_times_values (256x2048x2048x395): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x395): 76.174

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1956.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x396x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x396x2048): 111.461
Elapsed time for attention_prob_times_values (256x2048x2048x396): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x396): 125.100

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 3035.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x397x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x397x2048): 76.163
Elapsed time for attention_prob_times_values (256x2048x2048x397): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x397): 77.092

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1977.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x398x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x398x2048): 112.052
Elapsed time for attention_prob_times_values (256x2048x2048x398): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x398): 125.157

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 3059.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x399x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x399x2048): 77.140
Elapsed time for attention_prob_times_values (256x2048x2048x399): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x399): 75.622

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1980.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x400x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x400x2048): 188.613
Elapsed time for attention_prob_times_values (256x2048x2048x400): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x400): 72.828

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2732.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x401x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x401x2048): 78.269
Elapsed time for attention_prob_times_values (256x2048x2048x401): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x401): 75.742

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2006.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x402x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x402x2048): 111.453
Elapsed time for attention_prob_times_values (256x2048x2048x402): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x402): 122.659

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 3051.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x403x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x403x2048): 77.688
Elapsed time for attention_prob_times_values (256x2048x2048x403): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x403): 77.398

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2030.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 111.667
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 126.377

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 3112.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x405x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x405x2048): 78.438
Elapsed time for attention_prob_times_values (256x2048x2048x405): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x405): 78.605

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2066.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 111.325
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 125.693

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 3114.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x407x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x407x2048): 78.023
Elapsed time for attention_prob_times_values (256x2048x2048x407): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x407): 77.025

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2049.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 189.733
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 187.545

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 4998.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x409x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x409x2048): 78.779
Elapsed time for attention_prob_times_values (256x2048x2048x409): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x409): 76.402

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2060.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 111.576
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 127.563

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 3169.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x411x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x411x2048): 79.151
Elapsed time for attention_prob_times_values (256x2048x2048x411): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x411): 78.698

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2106.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 113.385
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 121.392

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 3136.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x413x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x413x2048): 78.975
Elapsed time for attention_prob_times_values (256x2048x2048x413): 0.0266
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x413): 33.327

Attention duration (in seconds): 0.0378
Attention throughput (in TFLOP/s): 1256.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0378
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 103.828
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 122.757

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 3023.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x415x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x415x2048): 79.489
Elapsed time for attention_prob_times_values (256x2048x2048x415): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x415): 79.971

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2147.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 202.365
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 199.584

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 5426.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x417x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x417x2048): 78.172
Elapsed time for attention_prob_times_values (256x2048x2048x417): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x417): 79.715

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2136.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 115.576
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 128.978

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 3306.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x419x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x419x2048): 76.875
Elapsed time for attention_prob_times_values (256x2048x2048x419): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x419): 79.532

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2125.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 115.378
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 131.014

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 3343.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x421x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x421x2048): 59.281
Elapsed time for attention_prob_times_values (256x2048x2048x421): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x421): 79.869

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1858.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 117.041
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 128.866

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 3358.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x423x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x423x2048): 76.398
Elapsed time for attention_prob_times_values (256x2048x2048x423): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x423): 79.376

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2136.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 181.109
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 195.059

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 5165.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x425x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x425x2048): 77.098
Elapsed time for attention_prob_times_values (256x2048x2048x425): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x425): 78.700

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2146.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 117.352
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 130.154

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 3409.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x427x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x427x2048): 76.882
Elapsed time for attention_prob_times_values (256x2048x2048x427): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x427): 81.367

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2188.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 116.987
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 133.671

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 3462.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x429x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x429x2048): 78.232
Elapsed time for attention_prob_times_values (256x2048x2048x429): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x429): 81.144

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2215.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 118.509
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 131.639

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 3476.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x431x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x431x2048): 78.501
Elapsed time for attention_prob_times_values (256x2048x2048x431): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x431): 80.345

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2218.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 190.797
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 206.195

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 5549.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x433x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x433x2048): 77.915
Elapsed time for attention_prob_times_values (256x2048x2048x433): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x433): 81.013

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2229.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 118.714
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 132.693

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 3524.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x435x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x435x2048): 78.381
Elapsed time for attention_prob_times_values (256x2048x2048x435): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x435): 83.377

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2277.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 120.243
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 134.796

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 3590.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x437x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x437x2048): 79.536
Elapsed time for attention_prob_times_values (256x2048x2048x437): 0.0325
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x437): 28.916

Attention duration (in seconds): 0.0443
Attention throughput (in TFLOP/s): 1200.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0443
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 121.195
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 133.467

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 3604.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x439x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x439x2048): 79.637
Elapsed time for attention_prob_times_values (256x2048x2048x439): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x439): 81.872

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2296.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 189.091
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 205.825

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 5617.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x441x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x441x2048): 79.673
Elapsed time for attention_prob_times_values (256x2048x2048x441): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x441): 81.729

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2304.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 119.674
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 134.236

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 3622.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x443x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x443x2048): 80.884
Elapsed time for attention_prob_times_values (256x2048x2048x443): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x443): 84.390

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2369.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 82.133
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 133.565

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2924.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x445x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x445x2048): 81.413
Elapsed time for attention_prob_times_values (256x2048x2048x445): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x445): 84.499

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2389.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 121.509
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 137.524

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 3725.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x447x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x447x2048): 81.557
Elapsed time for attention_prob_times_values (256x2048x2048x447): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x447): 85.272

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2412.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 202.186
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 211.092

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 5989.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x449x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x449x2048): 79.754
Elapsed time for attention_prob_times_values (256x2048x2048x449): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x449): 83.624

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2372.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 114.073
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 137.687

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 3634.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x451x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x451x2048): 80.180
Elapsed time for attention_prob_times_values (256x2048x2048x451): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x451): 86.090

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2423.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 116.886
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 135.614

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 3672.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x453x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x453x2048): 80.331
Elapsed time for attention_prob_times_values (256x2048x2048x453): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x453): 86.919

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2447.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 116.963
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 135.987

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 3694.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x455x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x455x2048): 79.742
Elapsed time for attention_prob_times_values (256x2048x2048x455): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x455): 84.152

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2410.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 185.330
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 210.209

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 5811.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x457x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x457x2048): 80.384
Elapsed time for attention_prob_times_values (256x2048x2048x457): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x457): 83.131

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2416.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 115.100
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 137.878

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 3716.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x459x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x459x2048): 81.310
Elapsed time for attention_prob_times_values (256x2048x2048x459): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x459): 87.325

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2499.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 116.245
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 138.318

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 3758.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x461x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x461x2048): 79.539
Elapsed time for attention_prob_times_values (256x2048x2048x461): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x461): 87.637

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2486.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 117.601
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 134.797

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 3752.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x463x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x463x2048): 76.611
Elapsed time for attention_prob_times_values (256x2048x2048x463): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x463): 85.260

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2416.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 194.696
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 62.302

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2831.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x465x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x465x2048): 80.449
Elapsed time for attention_prob_times_values (256x2048x2048x465): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x465): 85.205

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2487.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 119.617
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 138.981

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 3873.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x467x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x467x2048): 81.405
Elapsed time for attention_prob_times_values (256x2048x2048x467): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x467): 86.944

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2538.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 120.819
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 136.345

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 3875.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x469x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x469x2048): 80.356
Elapsed time for attention_prob_times_values (256x2048x2048x469): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x469): 85.431

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2510.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 120.616
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 129.675

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 3796.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x471x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x471x2048): 81.517
Elapsed time for attention_prob_times_values (256x2048x2048x471): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x471): 86.968

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2561.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 192.180
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 217.496

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 6223.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x473x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x473x2048): 81.444
Elapsed time for attention_prob_times_values (256x2048x2048x473): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x473): 86.984

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2571.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 121.515
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 137.466

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 3950.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x475x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x475x2048): 80.532
Elapsed time for attention_prob_times_values (256x2048x2048x475): 0.0228
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x475): 44.713

Attention duration (in seconds): 0.0355
Attention throughput (in TFLOP/s): 1764.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0355
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 119.730
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 139.870

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 3967.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x477x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x477x2048): 81.319
Elapsed time for attention_prob_times_values (256x2048x2048x477): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x477): 89.750

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2629.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 120.002
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 140.866

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 4001.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x479x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x479x2048): 82.019
Elapsed time for attention_prob_times_values (256x2048x2048x479): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x479): 88.275

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2630.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 197.816
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 223.978

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 6512.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x481x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x481x2048): 80.259
Elapsed time for attention_prob_times_values (256x2048x2048x481): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x481): 87.762

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2604.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 123.975
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 142.003

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 4120.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x483x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x483x2048): 80.479
Elapsed time for attention_prob_times_values (256x2048x2048x483): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x483): 89.945

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2649.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 121.927
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 143.004

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 4113.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x485x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x485x2048): 81.246
Elapsed time for attention_prob_times_values (256x2048x2048x485): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x485): 90.336

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2678.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x486x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x486x2048): 124.287
Elapsed time for attention_prob_times_values (256x2048x2048x486): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x486): 143.121

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 4174.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x487x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x487x2048): 79.960
Elapsed time for attention_prob_times_values (256x2048x2048x487): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x487): 88.137

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2636.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x488x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x488x2048): 189.103
Elapsed time for attention_prob_times_values (256x2048x2048x488): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x488): 219.721

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 6402.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x489x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x489x2048): 80.310
Elapsed time for attention_prob_times_values (256x2048x2048x489): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x489): 88.950

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2664.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x490x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x490x2048): 125.370
Elapsed time for attention_prob_times_values (256x2048x2048x490): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x490): 141.382

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 4202.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x491x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x491x2048): 80.386
Elapsed time for attention_prob_times_values (256x2048x2048x491): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x491): 87.775

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2659.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x492x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x492x2048): 123.918
Elapsed time for attention_prob_times_values (256x2048x2048x492): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x492): 144.355

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 4234.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x493x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x493x2048): 82.569
Elapsed time for attention_prob_times_values (256x2048x2048x493): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x493): 91.633

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2763.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x494x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x494x2048): 127.072
Elapsed time for attention_prob_times_values (256x2048x2048x494): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x494): 147.561

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 4352.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x495x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x495x2048): 82.077
Elapsed time for attention_prob_times_values (256x2048x2048x495): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x495): 91.302

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2760.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x496x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x496x2048): 203.432
Elapsed time for attention_prob_times_values (256x2048x2048x496): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x496): 225.396

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 6843.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x497x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x497x2048): 83.374
Elapsed time for attention_prob_times_values (256x2048x2048x497): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x497): 90.257

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2779.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x498x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x498x2048): 128.600
Elapsed time for attention_prob_times_values (256x2048x2048x498): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x498): 150.447

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 4454.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x499x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x499x2048): 84.312
Elapsed time for attention_prob_times_values (256x2048x2048x499): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x499): 94.516

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2868.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x500x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x500x2048): 127.440
Elapsed time for attention_prob_times_values (256x2048x2048x500): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x500): 151.209

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 4460.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x501x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x501x2048): 84.283
Elapsed time for attention_prob_times_values (256x2048x2048x501): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x501): 94.572

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2880.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x502x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x502x2048): 126.362
Elapsed time for attention_prob_times_values (256x2048x2048x502): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x502): 150.731

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 4450.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x503x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x503x2048): 84.297
Elapsed time for attention_prob_times_values (256x2048x2048x503): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x503): 93.311

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2873.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x504x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x504x2048): 194.567
Elapsed time for attention_prob_times_values (256x2048x2048x504): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x504): 226.608

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 6804.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x505x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x505x2048): 83.708
Elapsed time for attention_prob_times_values (256x2048x2048x505): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x505): 93.897

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2882.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x506x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x506x2048): 129.394
Elapsed time for attention_prob_times_values (256x2048x2048x506): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x506): 154.047

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 4588.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x507x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x507x2048): 85.735
Elapsed time for attention_prob_times_values (256x2048x2048x507): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x507): 97.020

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2975.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x508x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x508x2048): 132.061
Elapsed time for attention_prob_times_values (256x2048x2048x508): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x508): 151.668

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 4623.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x509x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x509x2048): 87.379
Elapsed time for attention_prob_times_values (256x2048x2048x509): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x509): 95.169

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2989.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x510x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x510x2048): 130.070
Elapsed time for attention_prob_times_values (256x2048x2048x510): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x510): 153.219

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 4625.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x511x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x511x2048): 84.475
Elapsed time for attention_prob_times_values (256x2048x2048x511): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x511): 96.921

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2973.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 32768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (256x2048x512x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x512x2048): 210.791
Elapsed time for attention_prob_times_values (256x2048x2048x512): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x512): 236.376

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 7354.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
