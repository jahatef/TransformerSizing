cupy is not loaded.
cupy is not loaded.
cupy is not loaded.
cupy is not loaded.
[2023-09-06 22:46:59,814] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-09-06 22:47:00,681] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.157.112, master_port=6000
[2023-09-06 22:47:00,681] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-06 22:47:00,698] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0208
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 158.892
b: 256, m: 2048, n: 128, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 29.572
b: 256, m: 2048, n: 2048, k: 128,
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 61.680
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.1858
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 5.917
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0178
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 247.067
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 219.075

Attention duration (in seconds): 0.2203
Attention throughput (in TFLOP/s): 22.455
MLP duration (in seconds): 0.0379
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2582
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 224.963
b: 256, m: 2048, n: 130, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 53.595
b: 256, m: 2048, n: 2048, k: 130,
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 36.065
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 147.087
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0311
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 145.875
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0192
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 235.745

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 142.377
MLP duration (in seconds): 0.0503
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0861
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 224.678
b: 256, m: 2048, n: 132, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.1929
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 1.470
b: 256, m: 2048, n: 2048, k: 132,
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 58.419
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 185.633
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0203
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 229.952
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 188.192

Attention duration (in seconds): 0.2196
Attention throughput (in TFLOP/s): 23.878
MLP duration (in seconds): 0.0452
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2648
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 242.643
b: 256, m: 2048, n: 134, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 62.007
b: 256, m: 2048, n: 2048, k: 134,
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 40.114
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0060
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 200.300
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0196
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 246.304
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0217
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 222.261

Attention duration (in seconds): 0.0327
Attention throughput (in TFLOP/s): 164.856
MLP duration (in seconds): 0.0413
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0740
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 212.972
b: 256, m: 2048, n: 136, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 77.081
b: 256, m: 2048, n: 2048, k: 136,
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 51.524
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 123.302
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0214
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 232.470
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0218
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 227.552

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 149.940
MLP duration (in seconds): 0.0432
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0802
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 230.169
b: 256, m: 2048, n: 138, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 57.434
b: 256, m: 2048, n: 2048, k: 138,
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 39.089
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 185.110
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0246
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 207.883
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 232.125

Attention duration (in seconds): 0.0363
Attention throughput (in TFLOP/s): 157.144
MLP duration (in seconds): 0.0466
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0829
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 218.806
b: 256, m: 2048, n: 140, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 28.325
b: 256, m: 2048, n: 2048, k: 140,
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 46.467
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 204.991
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0267
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 197.362
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.2861
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 18.391

Attention duration (in seconds): 0.0415
Attention throughput (in TFLOP/s): 141.149
MLP duration (in seconds): 0.3127
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3543
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 199.540
b: 256, m: 2048, n: 142, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 57.985
b: 256, m: 2048, n: 2048, k: 142,
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 40.332
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 201.579
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 237.017
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0236
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 229.128

Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 151.029
MLP duration (in seconds): 0.0465
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0863
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 219.748
b: 256, m: 2048, n: 144, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 78.623
b: 256, m: 2048, n: 2048, k: 144,
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 69.617
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 113.197
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0243
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 228.718
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0236
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 236.281

Attention duration (in seconds): 0.0397
Attention throughput (in TFLOP/s): 155.920
MLP duration (in seconds): 0.0479
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0876
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 225.801
b: 256, m: 2048, n: 146, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 77.846
b: 256, m: 2048, n: 2048, k: 146,
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 49.715
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 180.210
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0255
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 224.018
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0235
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 243.091

Attention duration (in seconds): 0.0373
Attention throughput (in TFLOP/s): 170.317
MLP duration (in seconds): 0.0491
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0864
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 234.216
b: 256, m: 2048, n: 148, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 72.801
b: 256, m: 2048, n: 2048, k: 148,
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 58.746
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 97.715
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 237.022
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 245.710

Attention duration (in seconds): 0.0436
Attention throughput (in TFLOP/s): 149.275
MLP duration (in seconds): 0.0487
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0924
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 232.005
b: 256, m: 2048, n: 150, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 78.373
b: 256, m: 2048, n: 2048, k: 150,
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 52.789
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 184.566
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0260
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 232.153
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0245
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 246.190

Attention duration (in seconds): 0.0379
Attention throughput (in TFLOP/s): 176.276
MLP duration (in seconds): 0.0505
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0885
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 236.566
b: 256, m: 2048, n: 152, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 85.890
b: 256, m: 2048, n: 2048, k: 152,
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 65.074
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 211.631
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 235.014
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 235.199

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 191.446
MLP duration (in seconds): 0.0528
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0886
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 224.285
b: 256, m: 2048, n: 154, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 60.567
b: 256, m: 2048, n: 2048, k: 154,
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 51.453
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 196.406
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 250.179
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 235.797

Attention duration (in seconds): 0.0413
Attention throughput (in TFLOP/s): 170.245
MLP duration (in seconds): 0.0524
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0937
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 240.448
b: 256, m: 2048, n: 156, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 79.256
b: 256, m: 2048, n: 2048, k: 156,
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 54.521
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 102.984
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 232.709
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0267
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 244.819

Attention duration (in seconds): 0.0466
Attention throughput (in TFLOP/s): 154.543
MLP duration (in seconds): 0.0548
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 231.016
b: 256, m: 2048, n: 158, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 79.786
b: 256, m: 2048, n: 2048, k: 158,
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 45.773
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 164.023
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0286
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 234.504
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0277
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 241.901

Attention duration (in seconds): 0.0436
Attention throughput (in TFLOP/s): 169.127
MLP duration (in seconds): 0.0563
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0999
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 243.941
b: 256, m: 2048, n: 160, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 91.310
b: 256, m: 2048, n: 2048, k: 160,
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 77.465
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 188.081
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0296
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 232.208
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0284
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 242.050

Attention duration (in seconds): 0.0385
Attention throughput (in TFLOP/s): 196.542
MLP duration (in seconds): 0.0580
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0964
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 242.558
b: 256, m: 2048, n: 162, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 39.570
b: 256, m: 2048, n: 2048, k: 162,
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 43.582
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 195.870
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0282
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 249.678
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 236.944

Attention duration (in seconds): 0.0475
Attention throughput (in TFLOP/s): 162.792
MLP duration (in seconds): 0.0579
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 242.814
b: 256, m: 2048, n: 164, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 81.757
b: 256, m: 2048, n: 2048, k: 164,
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 58.311
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 176.911
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0313
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 230.755
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0298
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 241.940

Attention duration (in seconds): 0.0429
Attention throughput (in TFLOP/s): 184.926
MLP duration (in seconds): 0.0611
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 239.580
b: 256, m: 2048, n: 166, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.4745
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 0.751
b: 256, m: 2048, n: 2048, k: 166,
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 45.775
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 196.422
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0313
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 236.248
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0300
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 246.746

Attention duration (in seconds): 0.5149
Attention throughput (in TFLOP/s): 15.752
MLP duration (in seconds): 0.0613
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5761
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0240
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 236.662
b: 256, m: 2048, n: 168, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 39.611
b: 256, m: 2048, n: 2048, k: 168,
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 62.424
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 130.541
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0301
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 251.292
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0312
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 242.685

Attention duration (in seconds): 0.0534
Attention throughput (in TFLOP/s): 155.371
MLP duration (in seconds): 0.0614
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0240
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 242.318
b: 256, m: 2048, n: 170, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 68.563
b: 256, m: 2048, n: 2048, k: 170,
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 52.330
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 139.211
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0318
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 243.816
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0317
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 244.413

Attention duration (in seconds): 0.0502
Attention throughput (in TFLOP/s): 168.935
MLP duration (in seconds): 0.0636
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 231.726
b: 256, m: 2048, n: 172, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 69.460
b: 256, m: 2048, n: 2048, k: 172,
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 61.779
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 194.151
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 235.922
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0324
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 244.772

Attention duration (in seconds): 0.0472
Attention throughput (in TFLOP/s): 183.803
MLP duration (in seconds): 0.0661
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 247.969
b: 256, m: 2048, n: 174, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 91.970
b: 256, m: 2048, n: 2048, k: 174,
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 56.313
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 166.897
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0340
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 238.917
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0338
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 240.410

Attention duration (in seconds): 0.0475
Attention throughput (in TFLOP/s): 187.015
MLP duration (in seconds): 0.0678
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0269
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 231.803
b: 256, m: 2048, n: 176, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 80.028
b: 256, m: 2048, n: 2048, k: 176,
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 83.400
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 188.792
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0339
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 245.525
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0333
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 249.757

Attention duration (in seconds): 0.0472
Attention throughput (in TFLOP/s): 192.308
MLP duration (in seconds): 0.0672
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 235.374
b: 256, m: 2048, n: 178, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 77.420
b: 256, m: 2048, n: 2048, k: 178,
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.5746
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 0.665
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 196.767
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0353
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 240.748
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 252.415

Attention duration (in seconds): 0.6175
Attention throughput (in TFLOP/s): 15.013
MLP duration (in seconds): 0.0690
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6865
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0261
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 250.392
b: 256, m: 2048, n: 180, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 63.560
b: 256, m: 2048, n: 2048, k: 180,
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 56.458
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 165.589
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 242.780
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0376
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 231.214

Attention duration (in seconds): 0.0521
Attention throughput (in TFLOP/s): 181.738
MLP duration (in seconds): 0.0734
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 247.043
b: 256, m: 2048, n: 182, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 82.814
b: 256, m: 2048, n: 2048, k: 182,
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 28.907
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 228.760
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0379
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 234.315
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0368
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 241.609

Attention duration (in seconds): 0.0550
Attention throughput (in TFLOP/s): 176.034
MLP duration (in seconds): 0.0747
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 243.968
b: 256, m: 2048, n: 184, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 76.820
b: 256, m: 2048, n: 2048, k: 184,
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 70.190
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 204.810
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0374
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 243.143
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0374
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 243.023

Attention duration (in seconds): 0.0498
Attention throughput (in TFLOP/s): 198.341
MLP duration (in seconds): 0.0748
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 249.871
b: 256, m: 2048, n: 186, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 105.024
b: 256, m: 2048, n: 2048, k: 186,
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 53.780
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0106
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 219.678
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0388
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 239.314
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0380
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 244.611

Attention duration (in seconds): 0.0497
Attention throughput (in TFLOP/s): 203.038
MLP duration (in seconds): 0.0768
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 243.743
b: 256, m: 2048, n: 188, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 86.732
b: 256, m: 2048, n: 2048, k: 188,
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 65.864
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 154.794
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 212.790
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0389
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 244.207

Attention duration (in seconds): 0.0553
Attention throughput (in TFLOP/s): 186.164
MLP duration (in seconds): 0.0834
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1387
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0308
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 235.720
b: 256, m: 2048, n: 190, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 104.269
b: 256, m: 2048, n: 2048, k: 190,
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 55.838
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0107
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 226.147
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 205.768
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0388
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 249.837

Attention duration (in seconds): 0.0528
Attention throughput (in TFLOP/s): 199.117
MLP duration (in seconds): 0.0859
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1386
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0300
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 247.276
b: 256, m: 2048, n: 192, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 78.601
b: 256, m: 2048, n: 2048, k: 192,
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 76.868
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 211.878
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0476
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 207.931
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0395
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 250.674

Attention duration (in seconds): 0.0523
Attention throughput (in TFLOP/s): 204.978
MLP duration (in seconds): 0.0871
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1394
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0310
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 244.752
b: 256, m: 2048, n: 194, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.7155
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 0.582
b: 256, m: 2048, n: 2048, k: 194,
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 58.121
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 213.486
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0473
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 213.446
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0408
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 247.873

Attention duration (in seconds): 0.7655
Attention throughput (in TFLOP/s): 14.287
MLP duration (in seconds): 0.0881
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8535
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0316
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 245.089
b: 256, m: 2048, n: 196, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 111.446
b: 256, m: 2048, n: 2048, k: 196,
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 30.514
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0192
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 134.552
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0450
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 229.229
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0421
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 245.179

Attention duration (in seconds): 0.0683
Attention throughput (in TFLOP/s): 163.340
MLP duration (in seconds): 0.0870
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1553
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0352
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 224.093
b: 256, m: 2048, n: 198, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 82.330
b: 256, m: 2048, n: 2048, k: 198,
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 61.076
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 230.958
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 242.877
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0470
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 223.855

Attention duration (in seconds): 0.0587
Attention throughput (in TFLOP/s): 193.639
MLP duration (in seconds): 0.0903
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1491
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 248.856
b: 256, m: 2048, n: 200, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 104.571
b: 256, m: 2048, n: 2048, k: 200,
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 65.540
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 223.837
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 207.888
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0438
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 245.368

Attention duration (in seconds): 0.0550
Attention throughput (in TFLOP/s): 210.793
MLP duration (in seconds): 0.0954
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1504
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0338
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 243.219
b: 256, m: 2048, n: 202, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 90.014
b: 256, m: 2048, n: 2048, k: 202,
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 57.939
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 135.136
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0505
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 216.967
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0448
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 244.436

Attention duration (in seconds): 0.0663
Attention throughput (in TFLOP/s): 178.171
MLP duration (in seconds): 0.0953
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1616
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 230.824
b: 256, m: 2048, n: 204, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 113.768
b: 256, m: 2048, n: 2048, k: 204,
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 43.199
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 147.152
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0500
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 223.412
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0450
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 248.342

Attention duration (in seconds): 0.0693
Attention throughput (in TFLOP/s): 173.922
MLP duration (in seconds): 0.0950
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1643
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0347
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 246.497
b: 256, m: 2048, n: 206, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 104.411
b: 256, m: 2048, n: 2048, k: 206,
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 55.500
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 225.146
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0522
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 218.401
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0454
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 250.975

Attention duration (in seconds): 0.0595
Attention throughput (in TFLOP/s): 206.265
MLP duration (in seconds): 0.0975
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1571
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0428
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 203.519
b: 256, m: 2048, n: 208, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 87.249
b: 256, m: 2048, n: 2048, k: 208,
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 70.917
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 233.926
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0545
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 213.171
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0468
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 248.341

Attention duration (in seconds): 0.0666
Attention throughput (in TFLOP/s): 187.714
MLP duration (in seconds): 0.1012
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1679
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0391
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 226.917
b: 256, m: 2048, n: 210, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 87.834
b: 256, m: 2048, n: 2048, k: 210,
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 65.376
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 215.153
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0574
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 206.091
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0469
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 252.589

Attention duration (in seconds): 0.0649
Attention throughput (in TFLOP/s): 196.257
MLP duration (in seconds): 0.1043
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1692
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0372
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 243.010
b: 256, m: 2048, n: 212, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 91.404
b: 256, m: 2048, n: 2048, k: 212,
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 60.656
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0128
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 235.443
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0608
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 198.327
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0478
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 252.327

Attention duration (in seconds): 0.0625
Attention throughput (in TFLOP/s): 207.496
MLP duration (in seconds): 0.1086
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1712
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 217.521
b: 256, m: 2048, n: 214, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 95.409
b: 256, m: 2048, n: 2048, k: 214,
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 66.933
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 202.929
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0678
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 181.438
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0491
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 250.164

Attention duration (in seconds): 0.0692
Attention throughput (in TFLOP/s): 190.892
MLP duration (in seconds): 0.1169
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1861
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0415
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 226.387
b: 256, m: 2048, n: 216, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 89.964
b: 256, m: 2048, n: 2048, k: 216,
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 81.891
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 154.813
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0676
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 185.246
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0495
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 253.157

Attention duration (in seconds): 0.0725
Attention throughput (in TFLOP/s): 185.450
MLP duration (in seconds): 0.1171
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1896
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 210.025
b: 256, m: 2048, n: 218, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 92.941
b: 256, m: 2048, n: 2048, k: 218,
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 63.823
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 203.369
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0722
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 176.718
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0503
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 253.524

Attention duration (in seconds): 0.0736
Attention throughput (in TFLOP/s): 186.026
MLP duration (in seconds): 0.1225
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1961
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0470
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 207.272
b: 256, m: 2048, n: 220, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 121.205
b: 256, m: 2048, n: 2048, k: 220,
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 72.479
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 236.099
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0726
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 178.997
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0515
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 252.488

Attention duration (in seconds): 0.0712
Attention throughput (in TFLOP/s): 195.788
MLP duration (in seconds): 0.1240
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1952
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0434
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 228.643
b: 256, m: 2048, n: 222, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 93.922
b: 256, m: 2048, n: 2048, k: 222,
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 65.770
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 221.973
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0701
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 188.684
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0520
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 254.586

Attention duration (in seconds): 0.0706
Attention throughput (in TFLOP/s): 200.835
MLP duration (in seconds): 0.1221
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1927
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0412
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 245.161
b: 256, m: 2048, n: 224, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 64.719
b: 256, m: 2048, n: 2048, k: 224,
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 88.690
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 205.681
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0758
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 177.779
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 252.683

Attention duration (in seconds): 0.0704
Attention throughput (in TFLOP/s): 204.893
MLP duration (in seconds): 0.1291
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1995
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0502
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 204.893
b: 256, m: 2048, n: 226, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 117.354
b: 256, m: 2048, n: 2048, k: 226,
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 56.960
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 221.485
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0763
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 179.768
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0541
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 253.209

Attention duration (in seconds): 0.0783
Attention throughput (in TFLOP/s): 187.455
MLP duration (in seconds): 0.1304
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0509
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 205.482
b: 256, m: 2048, n: 228, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 126.230
b: 256, m: 2048, n: 2048, k: 228,
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 71.848
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 211.506
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0789
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 176.944
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0551
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 253.044

Attention duration (in seconds): 0.0781
Attention throughput (in TFLOP/s): 191.162
MLP duration (in seconds): 0.1340
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0497
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 214.492
b: 256, m: 2048, n: 230, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 112.066
b: 256, m: 2048, n: 2048, k: 230,
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 65.263
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 219.084
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0813
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 174.596
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0562
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 252.896

Attention duration (in seconds): 0.0778
Attention throughput (in TFLOP/s): 195.137
MLP duration (in seconds): 0.1375
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0538
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 201.484
b: 256, m: 2048, n: 232, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 74.257
b: 256, m: 2048, n: 2048, k: 232,
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 66.863
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 221.273
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0749
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 192.824
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0574
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 251.895

Attention duration (in seconds): 0.0843
Attention throughput (in TFLOP/s): 183.284
MLP duration (in seconds): 0.1323
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0548
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 201.312
b: 256, m: 2048, n: 234, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 115.155
b: 256, m: 2048, n: 2048, k: 234,
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 67.891
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 225.711
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0777
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 189.129
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0576
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 255.154

Attention duration (in seconds): 0.0828
Attention throughput (in TFLOP/s): 189.643
MLP duration (in seconds): 0.1353
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 254.926
b: 256, m: 2048, n: 236, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 94.371
b: 256, m: 2048, n: 2048, k: 236,
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 72.716
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 242.889
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0851
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 175.739
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0591
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 252.878

Attention duration (in seconds): 0.0717
Attention throughput (in TFLOP/s): 222.611
MLP duration (in seconds): 0.1442
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 194.362
b: 256, m: 2048, n: 238, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 115.278
b: 256, m: 2048, n: 2048, k: 238,
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 50.710
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 221.163
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0737
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 206.359
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0602
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 252.744

Attention duration (in seconds): 0.0904
Attention throughput (in TFLOP/s): 179.559
MLP duration (in seconds): 0.1338
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0557
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 208.142
b: 256, m: 2048, n: 240, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 66.792
b: 256, m: 2048, n: 2048, k: 240,
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 97.358
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 184.322
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0828
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 186.707
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0618
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 250.167

Attention duration (in seconds): 0.0897
Attention throughput (in TFLOP/s): 183.874
MLP duration (in seconds): 0.1446
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2343
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 189.040
b: 256, m: 2048, n: 242, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 102.129
b: 256, m: 2048, n: 2048, k: 242,
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 78.467
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 225.140
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0790
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 199.090
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0617
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 254.703

Attention duration (in seconds): 0.0915
Attention throughput (in TFLOP/s): 183.093
MLP duration (in seconds): 0.1407
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0631
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 189.949
b: 256, m: 2048, n: 244, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 128.524
b: 256, m: 2048, n: 2048, k: 244,
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 73.423
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 235.499
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.0873
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 183.132
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0632
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 252.744

Attention duration (in seconds): 0.0913
Attention throughput (in TFLOP/s): 186.561
MLP duration (in seconds): 0.1505
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2418
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0660
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 184.465
b: 256, m: 2048, n: 246, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 126.746
b: 256, m: 2048, n: 2048, k: 246,
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 80.012
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 219.402
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0834
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 194.704
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0644
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 252.084

Attention duration (in seconds): 0.0953
Attention throughput (in TFLOP/s): 181.491
MLP duration (in seconds): 0.1479
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2432
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 202.191
b: 256, m: 2048, n: 248, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 98.426
b: 256, m: 2048, n: 2048, k: 248,
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 106.012
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 235.969
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0897
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 184.138
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0652
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 253.222

Attention duration (in seconds): 0.0892
Attention throughput (in TFLOP/s): 197.101
MLP duration (in seconds): 0.1549
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2440
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0708
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 177.702
b: 256, m: 2048, n: 250, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 131.018
b: 256, m: 2048, n: 2048, k: 250,
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 68.613
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 237.363
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0995
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 168.555
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0830
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 202.090

Attention duration (in seconds): 0.1004
Attention throughput (in TFLOP/s): 177.795
MLP duration (in seconds): 0.1826
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2830
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0763
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 167.547
b: 256, m: 2048, n: 252, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 111.572
b: 256, m: 2048, n: 2048, k: 252,
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 85.692
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 221.242
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0967
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 176.312
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0669
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 254.844

Attention duration (in seconds): 0.1067
Attention throughput (in TFLOP/s): 169.851
MLP duration (in seconds): 0.1636
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2703
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0670
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 193.967
b: 256, m: 2048, n: 254, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 95.100
b: 256, m: 2048, n: 2048, k: 254,
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 79.629
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 224.218
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0954
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 181.474
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0682
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 253.809

Attention duration (in seconds): 0.0989
Attention throughput (in TFLOP/s): 186.217
MLP duration (in seconds): 0.1637
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2625
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0693
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 190.483
b: 256, m: 2048, n: 256, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 120.649
b: 256, m: 2048, n: 2048, k: 256,
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 96.451
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0364
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 120.774
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.1000
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 175.950
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0694
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 253.374

Attention duration (in seconds): 0.1159
Attention throughput (in TFLOP/s): 161.220
MLP duration (in seconds): 0.1694
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2854
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0697
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 192.397
b: 256, m: 2048, n: 258, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 95.561
b: 256, m: 2048, n: 2048, k: 258,
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 65.457
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 240.821
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.1008
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 177.219
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0695
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 257.003

Attention duration (in seconds): 0.1025
Attention throughput (in TFLOP/s): 185.198
MLP duration (in seconds): 0.1704
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2728
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0747
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 182.084
b: 256, m: 2048, n: 260, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 97.778
b: 256, m: 2048, n: 2048, k: 260,
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 83.510
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 238.377
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.1059
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 171.304
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0708
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 256.403

Attention duration (in seconds): 0.1062
Attention throughput (in TFLOP/s): 181.432
MLP duration (in seconds): 0.1767
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2829
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 225.640
b: 256, m: 2048, n: 262, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 116.016
b: 256, m: 2048, n: 2048, k: 262,
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 68.526
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 223.914
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.1079
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 170.784
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0723
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 254.783

Attention duration (in seconds): 0.0949
Attention throughput (in TFLOP/s): 206.067
MLP duration (in seconds): 0.1802
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2751
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0763
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 183.999
b: 256, m: 2048, n: 264, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 98.268
b: 256, m: 2048, n: 2048, k: 264,
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 84.602
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 225.558
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.1064
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 175.896
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0735
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 254.442

Attention duration (in seconds): 0.1095
Attention throughput (in TFLOP/s): 181.268
MLP duration (in seconds): 0.1799
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2894
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0762
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 186.979
b: 256, m: 2048, n: 266, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 97.411
b: 256, m: 2048, n: 2048, k: 266,
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 73.830
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 234.942
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.1031
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 184.249
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0744
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 255.223

Attention duration (in seconds): 0.1100
Attention throughput (in TFLOP/s): 183.058
MLP duration (in seconds): 0.1775
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2875
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0807
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 179.286
b: 256, m: 2048, n: 268, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 105.712
b: 256, m: 2048, n: 2048, k: 268,
Elapsed time for attention_prob_times_values (256x2048x2048x268): 1.4479
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 0.397
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0209
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 230.628
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.1061
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 181.769
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0759
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 253.929

Attention duration (in seconds): 1.5549
Attention throughput (in TFLOP/s): 13.140
MLP duration (in seconds): 0.1820
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7369
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0755
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 194.376
b: 256, m: 2048, n: 270, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 97.833
b: 256, m: 2048, n: 2048, k: 270,
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 64.583
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 226.575
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.1142
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 171.303
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0767
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 255.120

Attention duration (in seconds): 0.1120
Attention throughput (in TFLOP/s): 185.071
MLP duration (in seconds): 0.1909
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0790
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 188.491
b: 256, m: 2048, n: 272, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 122.934
b: 256, m: 2048, n: 2048, k: 272,
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 100.709
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 231.570
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.1149
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 172.791
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0787
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 252.501

Attention duration (in seconds): 0.1110
Attention throughput (in TFLOP/s): 189.419
MLP duration (in seconds): 0.1936
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0841
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 179.737
b: 256, m: 2048, n: 274, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 121.175
b: 256, m: 2048, n: 2048, k: 274,
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 68.557
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 229.255
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.1159
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 173.958
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0794
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 253.664

Attention duration (in seconds): 0.1195
Attention throughput (in TFLOP/s): 178.479
MLP duration (in seconds): 0.1953
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0839
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 182.789
b: 256, m: 2048, n: 276, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 122.156
b: 256, m: 2048, n: 2048, k: 276,
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 99.651
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 242.073
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.1206
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 169.580
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 256.493

Attention duration (in seconds): 0.1158
Attention throughput (in TFLOP/s): 186.789
MLP duration (in seconds): 0.2003
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0846
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 184.014
b: 256, m: 2048, n: 278, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 90.462
b: 256, m: 2048, n: 2048, k: 278,
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 73.693
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 243.505
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.1235
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 167.955
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0809
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 256.436

Attention duration (in seconds): 0.1206
Attention throughput (in TFLOP/s): 181.990
MLP duration (in seconds): 0.2044
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0890
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 177.256
b: 256, m: 2048, n: 280, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 94.756
b: 256, m: 2048, n: 2048, k: 280,
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 110.214
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0217
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 242.230
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.1268
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 165.923
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0836
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 251.713

Attention duration (in seconds): 0.1226
Attention throughput (in TFLOP/s): 181.515
MLP duration (in seconds): 0.2104
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0862
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 185.830
b: 256, m: 2048, n: 282, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 115.629
b: 256, m: 2048, n: 2048, k: 282,
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 80.185
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 241.864
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.1273
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 167.748
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0862
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 247.692

Attention duration (in seconds): 0.1210
Attention throughput (in TFLOP/s): 186.416
MLP duration (in seconds): 0.2134
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3345
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0973
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 166.873
b: 256, m: 2048, n: 284, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 95.385
b: 256, m: 2048, n: 2048, k: 284,
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 84.025
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0222
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 243.834
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.1391
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 155.624
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0857
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 252.733

Attention duration (in seconds): 0.1332
Attention throughput (in TFLOP/s): 171.754
MLP duration (in seconds): 0.2248
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3579
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0904
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 182.166
b: 256, m: 2048, n: 286, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 47.999
b: 256, m: 2048, n: 2048, k: 286,
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 74.940
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 238.620
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.1320
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 166.297
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0863
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 254.296

Attention duration (in seconds): 0.1344
Attention throughput (in TFLOP/s): 172.517
MLP duration (in seconds): 0.2184
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3528
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0965
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 172.971
b: 256, m: 2048, n: 288, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 129.645
b: 256, m: 2048, n: 2048, k: 288,
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 108.661
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 241.721
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.1381
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 161.260
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0869
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 256.192

Attention duration (in seconds): 0.1300
Attention throughput (in TFLOP/s): 180.742
MLP duration (in seconds): 0.2250
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3550
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.1016
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 166.640
b: 256, m: 2048, n: 290, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 111.604
b: 256, m: 2048, n: 2048, k: 290,
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 75.902
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 247.717
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.1369
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 164.935
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.0886
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 254.901

Attention duration (in seconds): 0.1382
Attention throughput (in TFLOP/s): 172.398
MLP duration (in seconds): 0.2254
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3636
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.1006
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 170.715
b: 256, m: 2048, n: 292, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 111.639
b: 256, m: 2048, n: 2048, k: 292,
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 85.696
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 243.634
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.1333
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 171.694
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0895
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 255.663

Attention duration (in seconds): 0.1370
Attention throughput (in TFLOP/s): 176.254
MLP duration (in seconds): 0.2228
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3598
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.1026
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 169.573
b: 256, m: 2048, n: 294, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 114.276
b: 256, m: 2048, n: 2048, k: 294,
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 74.744
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 246.593
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.1380
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 168.175
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0916
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 253.355

Attention duration (in seconds): 0.1401
Attention throughput (in TFLOP/s): 174.606
MLP duration (in seconds): 0.2295
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3697
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.1017
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 173.455
b: 256, m: 2048, n: 296, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 105.452
b: 256, m: 2048, n: 2048, k: 296,
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 114.157
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 230.945
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.1418
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 165.850
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.1040
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 226.177

Attention duration (in seconds): 0.1388
Attention throughput (in TFLOP/s): 178.670
MLP duration (in seconds): 0.2458
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3845
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.1073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 166.590
b: 256, m: 2048, n: 298, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 96.358
b: 256, m: 2048, n: 2048, k: 298,
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 77.062
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0262
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 227.546
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.1446
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 164.809
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0930
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 256.426

Attention duration (in seconds): 0.1485
Attention throughput (in TFLOP/s): 169.194
MLP duration (in seconds): 0.2376
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3861
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0946
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 191.452
b: 256, m: 2048, n: 300, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 116.845
b: 256, m: 2048, n: 2048, k: 300,
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 86.064
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0252
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 239.483
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.1530
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 157.952
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0945
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 255.746

Attention duration (in seconds): 0.1329
Attention throughput (in TFLOP/s): 191.536
MLP duration (in seconds): 0.2474
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3803
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.1078
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 170.400
b: 256, m: 2048, n: 302, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 120.314
b: 256, m: 2048, n: 2048, k: 302,
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 87.099
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 250.891
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.1497
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 163.523
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.0957
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 255.817

Attention duration (in seconds): 0.1450
Attention throughput (in TFLOP/s): 177.803
MLP duration (in seconds): 0.2454
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3904
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.1100
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 169.092
b: 256, m: 2048, n: 304, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 114.746
b: 256, m: 2048, n: 2048, k: 304,
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 71.874
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 238.304
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.1541
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 161.009
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.0966
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 256.826

Attention duration (in seconds): 0.1508
Attention throughput (in TFLOP/s): 173.130
MLP duration (in seconds): 0.2507
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.1103
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 170.880
b: 256, m: 2048, n: 306, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 67.238
b: 256, m: 2048, n: 2048, k: 306,
Elapsed time for attention_prob_times_values (256x2048x2048x306): 1.7448
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 0.377
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 246.053
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.1601
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 156.997
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.0980
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 256.355

Attention duration (in seconds): 1.8904
Attention throughput (in TFLOP/s): 13.991
MLP duration (in seconds): 0.2581
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1486
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.1064
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 179.419
b: 256, m: 2048, n: 308, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 115.232
b: 256, m: 2048, n: 2048, k: 308,
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 74.292
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 244.850
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.1608
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 158.408
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.0993
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 256.447

Attention duration (in seconds): 0.1471
Attention throughput (in TFLOP/s): 182.117
MLP duration (in seconds): 0.2601
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.1081
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 178.923
b: 256, m: 2048, n: 310, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 133.052
b: 256, m: 2048, n: 2048, k: 310,
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 88.222
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 250.542
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.1660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 155.437
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.1005
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 256.781

Attention duration (in seconds): 0.1464
Attention throughput (in TFLOP/s): 185.272
MLP duration (in seconds): 0.2664
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.1128
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 173.752
b: 256, m: 2048, n: 312, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 101.913
b: 256, m: 2048, n: 2048, k: 312,
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 96.370
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0273
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 239.273
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.1624
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 160.856
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.1021
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 255.958

Attention duration (in seconds): 0.1536
Attention throughput (in TFLOP/s): 178.820
MLP duration (in seconds): 0.2645
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.1201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 165.237
b: 256, m: 2048, n: 314, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 102.850
b: 256, m: 2048, n: 2048, k: 314,
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 80.255
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 248.784
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1620
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 163.381
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.1032
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 256.435

Attention duration (in seconds): 0.1617
Attention throughput (in TFLOP/s): 172.034
MLP duration (in seconds): 0.2652
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.1192
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 168.698
b: 256, m: 2048, n: 316, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 125.029
b: 256, m: 2048, n: 2048, k: 316,
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 108.636
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 251.602
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1637
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 163.736
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.1062
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 252.308

Attention duration (in seconds): 0.1575
Attention throughput (in TFLOP/s): 178.832
MLP duration (in seconds): 0.2699
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.1209
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 168.438
b: 256, m: 2048, n: 318, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 76.939
b: 256, m: 2048, n: 2048, k: 318,
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 89.559
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0278
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 244.229
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1701
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 159.602
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1056
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 257.152

Attention duration (in seconds): 0.1652
Attention throughput (in TFLOP/s): 172.630
MLP duration (in seconds): 0.2756
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4408
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.1234
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 167.114
b: 256, m: 2048, n: 320, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 123.376
b: 256, m: 2048, n: 2048, k: 320,
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 127.030
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0283
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 242.557
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1831
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 150.137
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1066
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 257.904

Attention duration (in seconds): 0.1627
Attention throughput (in TFLOP/s): 177.422
MLP duration (in seconds): 0.2897
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4523
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.1255
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 166.332
b: 256, m: 2048, n: 322, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 135.706
b: 256, m: 2048, n: 2048, k: 322,
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 84.437
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0284
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 245.256
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1760
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 158.172
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1088
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 255.696

Attention duration (in seconds): 0.1672
Attention throughput (in TFLOP/s): 174.782
MLP duration (in seconds): 0.2848
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4520
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.1273
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 165.963
b: 256, m: 2048, n: 324, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 142.964
b: 256, m: 2048, n: 2048, k: 324,
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 106.641
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 251.339
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1752
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 160.885
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1098
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 256.747

Attention duration (in seconds): 0.1668
Attention throughput (in TFLOP/s): 177.321
MLP duration (in seconds): 0.2849
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4517
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.1276
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 167.738
b: 256, m: 2048, n: 326, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 138.703
b: 256, m: 2048, n: 2048, k: 326,
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 73.541
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0295
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 242.152
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 161.038
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1126
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 253.462

Attention duration (in seconds): 0.1716
Attention throughput (in TFLOP/s): 174.432
MLP duration (in seconds): 0.2897
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4613
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.1268
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 170.778
b: 256, m: 2048, n: 328, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 119.171
b: 256, m: 2048, n: 2048, k: 328,
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 81.915
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 247.485
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1860
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 155.277
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1143
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 252.573

Attention duration (in seconds): 0.1705
Attention throughput (in TFLOP/s): 177.631
MLP duration (in seconds): 0.3003
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4708
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.1263
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 173.596
b: 256, m: 2048, n: 330, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 99.945
b: 256, m: 2048, n: 2048, k: 330,
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 76.379
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 245.522
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1886
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 155.021
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1467
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 199.203

Attention duration (in seconds): 0.1724
Attention throughput (in TFLOP/s): 177.753
MLP duration (in seconds): 0.3353
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.1369
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 162.060
b: 256, m: 2048, n: 332, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 140.006
b: 256, m: 2048, n: 2048, k: 332,
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 89.116
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0552
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 133.912
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1984
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 149.155
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1559
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 189.733

Attention duration (in seconds): 0.2053
Attention throughput (in TFLOP/s): 151.095
MLP duration (in seconds): 0.3543
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5596
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.1404
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 160.012
b: 256, m: 2048, n: 334, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 124.742
b: 256, m: 2048, n: 2048, k: 334,
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 81.786
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0392
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 190.746
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1881
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 159.216
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1168
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 256.395

Attention duration (in seconds): 0.1941
Attention throughput (in TFLOP/s): 161.647
MLP duration (in seconds): 0.3049
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4990
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.1333
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 170.450
b: 256, m: 2048, n: 336, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 128.113
b: 256, m: 2048, n: 2048, k: 336,
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 123.261
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 237.455
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1889
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 160.460
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1175
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 257.817

Attention duration (in seconds): 0.1767
Attention throughput (in TFLOP/s): 179.634
MLP duration (in seconds): 0.3064
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4832
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.1363
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 168.778
b: 256, m: 2048, n: 338, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 98.230
b: 256, m: 2048, n: 2048, k: 338,
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 85.785
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0318
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 241.023
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1934
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 158.604
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 256.309

Attention duration (in seconds): 0.1839
Attention throughput (in TFLOP/s): 174.620
MLP duration (in seconds): 0.3130
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4969
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.1384
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 168.140
b: 256, m: 2048, n: 340, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 116.914
b: 256, m: 2048, n: 2048, k: 340,
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 100.855
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 241.739
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1941
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 159.860
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1228
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 252.714

Attention duration (in seconds): 0.1840
Attention throughput (in TFLOP/s): 176.591
MLP duration (in seconds): 0.3169
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5009
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.1433
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 164.272
b: 256, m: 2048, n: 342, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 112.049
b: 256, m: 2048, n: 2048, k: 342,
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 83.481
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 252.022
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.2024
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 155.098
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.1329
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 236.247

Attention duration (in seconds): 0.1898
Attention throughput (in TFLOP/s): 173.121
MLP duration (in seconds): 0.3353
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.1459
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 163.336
b: 256, m: 2048, n: 344, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 139.151
b: 256, m: 2048, n: 2048, k: 344,
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 110.739
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 249.312
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.2016
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 157.565
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.1239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 256.420

Attention duration (in seconds): 0.1897
Attention throughput (in TFLOP/s): 175.247
MLP duration (in seconds): 0.3255
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.1456
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 165.505
b: 256, m: 2048, n: 346, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 60.748
b: 256, m: 2048, n: 2048, k: 346,
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 88.771
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0338
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 237.631
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.2048
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 156.906
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.1262
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 254.578

Attention duration (in seconds): 0.2000
Attention throughput (in TFLOP/s): 168.079
MLP duration (in seconds): 0.3310
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.1431
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 170.399
b: 256, m: 2048, n: 348, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 123.644
b: 256, m: 2048, n: 2048, k: 348,
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 90.222
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0338
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 240.192
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.2069
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 157.085
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.1274
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 255.072

Attention duration (in seconds): 0.1912
Attention throughput (in TFLOP/s): 177.797
MLP duration (in seconds): 0.3344
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.1489
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 165.581
b: 256, m: 2048, n: 350, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 131.213
b: 256, m: 2048, n: 2048, k: 350,
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0194
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 38.649
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0330
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 248.936
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.2234
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 147.163
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1327
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 247.761

Attention duration (in seconds): 0.2071
Attention throughput (in TFLOP/s): 166.003
MLP duration (in seconds): 0.3562
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5633
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.1562
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 159.723
b: 256, m: 2048, n: 352, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 44.982
b: 256, m: 2048, n: 2048, k: 352,
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 106.141
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0340
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 244.673
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.2187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 152.109
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1793
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 185.529

Attention duration (in seconds): 0.2141
Attention throughput (in TFLOP/s): 162.419
MLP duration (in seconds): 0.3979
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.1531
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 164.777
b: 256, m: 2048, n: 354, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 77.851
b: 256, m: 2048, n: 2048, k: 354,
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 94.005
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0339
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 247.972
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.2145
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 156.800
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1730
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 194.496

Attention duration (in seconds): 0.2049
Attention throughput (in TFLOP/s): 171.612
MLP duration (in seconds): 0.3875
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5924
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.1582
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 161.283
b: 256, m: 2048, n: 356, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 133.992
b: 256, m: 2048, n: 2048, k: 356,
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 111.575
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 244.020
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.2200
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 154.671
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1884
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 180.543

Attention duration (in seconds): 0.2056
Attention throughput (in TFLOP/s): 172.894
MLP duration (in seconds): 0.4084
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.1612
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 160.095
b: 256, m: 2048, n: 358, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 150.597
b: 256, m: 2048, n: 2048, k: 358,
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 92.876
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 241.797
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.2271
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 151.503
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.2004
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 171.683

Attention duration (in seconds): 0.2101
Attention throughput (in TFLOP/s): 171.047
MLP duration (in seconds): 0.4275
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6376
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.1633
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 159.776
b: 256, m: 2048, n: 360, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 125.255
b: 256, m: 2048, n: 2048, k: 360,
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 126.412
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0360
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 241.545
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.2267
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 153.446
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1350
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 257.697

Attention duration (in seconds): 0.2116
Attention throughput (in TFLOP/s): 171.719
MLP duration (in seconds): 0.3617
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5733
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.1648
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 160.120
b: 256, m: 2048, n: 362, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 128.168
b: 256, m: 2048, n: 2048, k: 362,
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 83.485
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0346
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 254.045
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.2316
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 151.908
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1378
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 255.221

Attention duration (in seconds): 0.2148
Attention throughput (in TFLOP/s): 171.035
MLP duration (in seconds): 0.3694
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5842
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1690
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 157.854
b: 256, m: 2048, n: 364, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 118.533
b: 256, m: 2048, n: 2048, k: 364,
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 92.837
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 249.885
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.2341
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 151.920
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.2132
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 166.852

Attention duration (in seconds): 0.2196
Attention throughput (in TFLOP/s): 169.093
MLP duration (in seconds): 0.4473
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6669
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1726
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 156.247
b: 256, m: 2048, n: 366, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 140.876
b: 256, m: 2048, n: 2048, k: 366,
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 81.084
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 242.534
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.2446
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 146.996
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1422
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 252.904

Attention duration (in seconds): 0.2249
Attention throughput (in TFLOP/s): 166.845
MLP duration (in seconds): 0.3868
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1709
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 159.491
b: 256, m: 2048, n: 368, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 94.488
b: 256, m: 2048, n: 2048, k: 368,
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 101.630
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0377
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 240.785
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.2366
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 153.617
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.2670
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 136.163

Attention duration (in seconds): 0.2248
Attention throughput (in TFLOP/s): 168.719
MLP duration (in seconds): 0.5036
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1710
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 161.161
b: 256, m: 2048, n: 370, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 139.163
b: 256, m: 2048, n: 2048, k: 370,
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 98.673
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 253.701
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.2392
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 153.609
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.2202
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 166.852

Attention duration (in seconds): 0.2210
Attention throughput (in TFLOP/s): 173.479
MLP duration (in seconds): 0.4595
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6805
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1764
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 157.962
b: 256, m: 2048, n: 372, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 130.225
b: 256, m: 2048, n: 2048, k: 372,
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 119.148
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 248.627
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.2425
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 153.200
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1450
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 256.171

Attention duration (in seconds): 0.2266
Attention throughput (in TFLOP/s): 171.009
MLP duration (in seconds): 0.3875
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1860
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 151.383
b: 256, m: 2048, n: 374, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 119.077
b: 256, m: 2048, n: 2048, k: 374,
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 89.495
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0384
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 244.288
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.2482
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 151.294
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.2129
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 176.370

Attention duration (in seconds): 0.2402
Attention throughput (in TFLOP/s): 163.028
MLP duration (in seconds): 0.4611
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1822
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 156.214
b: 256, m: 2048, n: 376, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 128.890
b: 256, m: 2048, n: 2048, k: 376,
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 126.087
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0486
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 195.352
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.2494
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 152.145
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.2073
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 183.092

Attention duration (in seconds): 0.2434
Attention throughput (in TFLOP/s): 162.526
MLP duration (in seconds): 0.4567
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7001
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1814
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 158.550
b: 256, m: 2048, n: 378, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 148.821
b: 256, m: 2048, n: 2048, k: 378,
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 73.127
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0387
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 247.680
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.2546
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 150.676
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1507
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 254.435

Attention duration (in seconds): 0.2367
Attention throughput (in TFLOP/s): 168.898
MLP duration (in seconds): 0.4053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6420
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1845
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 157.561
b: 256, m: 2048, n: 380, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 83.897
b: 256, m: 2048, n: 2048, k: 380,
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 105.127
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0386
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 250.853
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.2570
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 150.821
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1893
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 204.788

Attention duration (in seconds): 0.2406
Attention throughput (in TFLOP/s): 167.869
MLP duration (in seconds): 0.4463
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6869
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1877
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 156.547
b: 256, m: 2048, n: 382, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 136.641
b: 256, m: 2048, n: 2048, k: 382,
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 101.608
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0387
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 252.823
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.2578
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 151.968
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.2261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 173.211

Attention duration (in seconds): 0.2405
Attention throughput (in TFLOP/s): 169.713
MLP duration (in seconds): 0.4839
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.2024
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 146.647
b: 256, m: 2048, n: 384, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x384x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x384x2048): 127.979
b: 256, m: 2048, n: 2048, k: 384,
Elapsed time for attention_prob_times_values (256x2048x2048x384): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x384): 110.022
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0404
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 245.217
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.2638
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 150.027
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.2292
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 172.686

Attention duration (in seconds): 0.2567
Attention throughput (in TFLOP/s): 160.603
MLP duration (in seconds): 0.4931
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7498
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1926
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 155.760
b: 256, m: 2048, n: 386, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x386x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x386x2048): 109.532
b: 256, m: 2048, n: 2048, k: 386,
Elapsed time for attention_prob_times_values (256x2048x2048x386): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x386): 97.171
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0408
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 245.175
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.2660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 150.371
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.2410
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 165.937

Attention duration (in seconds): 0.2495
Attention throughput (in TFLOP/s): 166.972
MLP duration (in seconds): 0.5070
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7565
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1948
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 155.589
b: 256, m: 2048, n: 388, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x388x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x388x2048): 128.287
b: 256, m: 2048, n: 2048, k: 388,
Elapsed time for attention_prob_times_values (256x2048x2048x388): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x388): 100.454
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0411
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 245.889
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.2690
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 150.249
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.1594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 253.480

Attention duration (in seconds): 0.2507
Attention throughput (in TFLOP/s): 167.858
MLP duration (in seconds): 0.4284
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6791
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1943
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 157.637
b: 256, m: 2048, n: 390, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x390x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x390x2048): 137.536
b: 256, m: 2048, n: 2048, k: 390,
Elapsed time for attention_prob_times_values (256x2048x2048x390): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x390): 89.241
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0433
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 235.756
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.2723
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 149.933
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.2461
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 165.935

Attention duration (in seconds): 0.2530
Attention throughput (in TFLOP/s): 167.984
MLP duration (in seconds): 0.5184
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7714
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1986
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 155.761
b: 256, m: 2048, n: 392, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x392x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x392x2048): 118.655
b: 256, m: 2048, n: 2048, k: 392,
Elapsed time for attention_prob_times_values (256x2048x2048x392): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x392): 130.385
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 244.405
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.2730
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 151.108
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.1618
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 254.991

Attention duration (in seconds): 0.2544
Attention throughput (in TFLOP/s): 168.787
MLP duration (in seconds): 0.4347
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6891
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.2189
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 142.769
b: 256, m: 2048, n: 394, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x394x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x394x2048): 141.388
b: 256, m: 2048, n: 2048, k: 394,
Elapsed time for attention_prob_times_values (256x2048x2048x394): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x394): 84.943
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 253.817
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.2822
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 147.654
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.2580
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 161.487

Attention duration (in seconds): 0.2759
Attention throughput (in TFLOP/s): 157.172
MLP duration (in seconds): 0.5403
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.2022
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 156.110
b: 256, m: 2048, n: 396, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x396x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x396x2048): 122.640
b: 256, m: 2048, n: 2048, k: 396,
Elapsed time for attention_prob_times_values (256x2048x2048x396): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x396): 96.720
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0428
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 245.811
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.2885
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 145.890
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.2473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 170.186

Attention duration (in seconds): 0.2608
Attention throughput (in TFLOP/s): 167.944
MLP duration (in seconds): 0.5359
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7967
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.2044
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 155.999
b: 256, m: 2048, n: 398, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x398x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x398x2048): 120.044
b: 256, m: 2048, n: 2048, k: 398,
Elapsed time for attention_prob_times_values (256x2048x2048x398): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x398): 86.666
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 252.618
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.2887
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 147.300
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.2638
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 161.172

Attention duration (in seconds): 0.2635
Attention throughput (in TFLOP/s): 167.863
MLP duration (in seconds): 0.5525
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.2070
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 155.614
b: 256, m: 2048, n: 400, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x400x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x400x2048): 114.348
b: 256, m: 2048, n: 2048, k: 400,
Elapsed time for attention_prob_times_values (256x2048x2048x400): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x400): 124.417
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 246.042
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.2893
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 148.480
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.2636
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 162.958

Attention duration (in seconds): 0.2651
Attention throughput (in TFLOP/s): 168.520
MLP duration (in seconds): 0.5528
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.2158
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 150.755
b: 256, m: 2048, n: 402, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x402x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x402x2048): 116.341
b: 256, m: 2048, n: 2048, k: 402,
Elapsed time for attention_prob_times_values (256x2048x2048x402): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x402): 86.189
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0427
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 253.894
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.2901
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 149.538
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.2694
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 161.027

Attention duration (in seconds): 0.2760
Attention throughput (in TFLOP/s): 163.451
MLP duration (in seconds): 0.5595
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8355
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.2136
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 153.846
b: 256, m: 2048, n: 404, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 113.698
b: 256, m: 2048, n: 2048, k: 404,
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 120.978
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0430
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 254.817
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.2941
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 148.970
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.2647
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 165.522

Attention duration (in seconds): 0.2714
Attention throughput (in TFLOP/s): 167.842
MLP duration (in seconds): 0.5588
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.2369
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 140.060
b: 256, m: 2048, n: 406, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 124.292
b: 256, m: 2048, n: 2048, k: 406,
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 88.189
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 253.895
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.2974
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 148.762
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.2783
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 158.971

Attention duration (in seconds): 0.2974
Attention throughput (in TFLOP/s): 154.640
MLP duration (in seconds): 0.5758
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8732
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.2344
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 142.959
b: 256, m: 2048, n: 408, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 125.828
b: 256, m: 2048, n: 2048, k: 408,
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 102.994
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0454
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 245.987
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.2952
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 151.355
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.2719
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 164.351

Attention duration (in seconds): 0.2953
Attention throughput (in TFLOP/s): 157.248
MLP duration (in seconds): 0.5671
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8624
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.2220
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 152.421
b: 256, m: 2048, n: 410, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 112.077
b: 256, m: 2048, n: 2048, k: 410,
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 86.274
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0451
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 249.939
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.3015
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 149.661
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.2835
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 159.143

Attention duration (in seconds): 0.2852
Attention throughput (in TFLOP/s): 164.374
MLP duration (in seconds): 0.5851
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8703
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.2200
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 155.337
b: 256, m: 2048, n: 412, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 137.780
b: 256, m: 2048, n: 2048, k: 412,
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 108.460
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 2.7837
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 4.092
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.3093
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 147.296
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.2819
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 161.644

Attention duration (in seconds): 3.0183
Attention throughput (in TFLOP/s): 15.683
MLP duration (in seconds): 0.5912
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.6095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.2321
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 148.690
b: 256, m: 2048, n: 414, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 115.202
b: 256, m: 2048, n: 2048, k: 414,
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 87.091
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0481
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 238.973
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.3072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 149.779
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.2881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 159.695

Attention duration (in seconds): 0.2981
Attention throughput (in TFLOP/s): 160.290
MLP duration (in seconds): 0.5953
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8934
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.2254
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 154.599
b: 256, m: 2048, n: 416, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 139.865
b: 256, m: 2048, n: 2048, k: 416,
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 65.326
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0462
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 251.642
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.3245
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 143.144
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.2868
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 161.958

Attention duration (in seconds): 0.2916
Attention throughput (in TFLOP/s): 165.449
MLP duration (in seconds): 0.6114
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.2345
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 149.977
b: 256, m: 2048, n: 418, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 135.725
b: 256, m: 2048, n: 2048, k: 418,
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 103.622
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0482
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 243.478
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.3240
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 144.782
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.3010
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 155.810

Attention duration (in seconds): 0.2980
Attention throughput (in TFLOP/s): 163.425
MLP duration (in seconds): 0.6250
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.2378
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 149.356
b: 256, m: 2048, n: 420, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 68.692
b: 256, m: 2048, n: 2048, k: 420,
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 117.263
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0498
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 237.625
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.3192
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 148.352
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.2974
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 159.217

Attention duration (in seconds): 0.3084
Attention throughput (in TFLOP/s): 159.379
MLP duration (in seconds): 0.6166
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.2352
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 152.434
b: 256, m: 2048, n: 422, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 128.871
b: 256, m: 2048, n: 2048, k: 422,
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 90.910
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0482
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 247.911
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.3823
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 125.045
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.3050
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 156.744

Attention duration (in seconds): 0.3004
Attention throughput (in TFLOP/s): 165.162
MLP duration (in seconds): 0.6873
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9877
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.2502
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 144.649
b: 256, m: 2048, n: 424, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 128.666
b: 256, m: 2048, n: 2048, k: 424,
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 135.938
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0482
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 250.366
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.3263
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 147.878
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.3063
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 157.561

Attention duration (in seconds): 0.3122
Attention throughput (in TFLOP/s): 160.418
MLP duration (in seconds): 0.6326
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9448
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.2436
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 149.964
b: 256, m: 2048, n: 426, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 75.456
b: 256, m: 2048, n: 2048, k: 426,
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 87.722
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0495
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 246.179
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.3414
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 142.677
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.3097
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 157.300

Attention duration (in seconds): 0.3157
Attention throughput (in TFLOP/s): 160.125
MLP duration (in seconds): 0.6511
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9668
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.2425
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 152.051
b: 256, m: 2048, n: 428, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 151.049
b: 256, m: 2048, n: 2048, k: 428,
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 105.781
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0488
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 251.849
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.3372
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 145.807
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.3107
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 158.265

Attention duration (in seconds): 0.3061
Attention throughput (in TFLOP/s): 166.630
MLP duration (in seconds): 0.6479
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9541
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.2469
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 150.754
b: 256, m: 2048, n: 430, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 139.345
b: 256, m: 2048, n: 2048, k: 430,
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 93.773
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0498
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 249.160
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.3367
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 147.401
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.3151
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 157.526

Attention duration (in seconds): 0.3132
Attention throughput (in TFLOP/s): 164.368
MLP duration (in seconds): 0.6518
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9650
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.2473
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 151.914
b: 256, m: 2048, n: 432, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 158.246
b: 256, m: 2048, n: 2048, k: 432,
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 127.640
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0497
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 251.912
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.3468
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 144.461
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.3210
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 156.067

Attention duration (in seconds): 0.3102
Attention throughput (in TFLOP/s): 167.493
MLP duration (in seconds): 0.6678
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9780
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.2586
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 146.621
b: 256, m: 2048, n: 434, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 153.824
b: 256, m: 2048, n: 2048, k: 434,
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 107.752
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0656
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 192.832
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.3454
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 146.372
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.3212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 157.398

Attention duration (in seconds): 0.3389
Attention throughput (in TFLOP/s): 154.696
MLP duration (in seconds): 0.6667
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.2549
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 150.166
b: 256, m: 2048, n: 436, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 140.466
b: 256, m: 2048, n: 2048, k: 436,
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 102.365
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0628
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 202.984
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.3514
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 145.217
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.3289
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 155.127

Attention duration (in seconds): 0.3335
Attention throughput (in TFLOP/s): 158.614
MLP duration (in seconds): 0.6803
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.2576
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 149.943
b: 256, m: 2048, n: 438, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 143.659
b: 256, m: 2048, n: 2048, k: 438,
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 98.501
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0501
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 256.720
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.3527
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 145.993
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.3298
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 156.134

Attention duration (in seconds): 0.3238
Attention throughput (in TFLOP/s): 164.835
MLP duration (in seconds): 0.6826
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.2625
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 148.478
b: 256, m: 2048, n: 440, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 117.539
b: 256, m: 2048, n: 2048, k: 440,
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 143.614
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 250.535
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.3704
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 140.302
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.3333
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 155.941

Attention duration (in seconds): 0.3290
Attention throughput (in TFLOP/s): 163.712
MLP duration (in seconds): 0.7037
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0327
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.2615
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 150.423
b: 256, m: 2048, n: 442, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 78.898
b: 256, m: 2048, n: 2048, k: 442,
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 96.619
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0634
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 206.782
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.3581
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 146.456
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.3340
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 157.029

Attention duration (in seconds): 0.3467
Attention throughput (in TFLOP/s): 156.722
MLP duration (in seconds): 0.6920
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0388
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.2625
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 151.176
b: 256, m: 2048, n: 444, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 162.358
b: 256, m: 2048, n: 2048, k: 444,
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 129.705
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0728
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 181.737
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.3681
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 143.772
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.3470
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 152.504

Attention duration (in seconds): 0.3486
Attention throughput (in TFLOP/s): 157.294
MLP duration (in seconds): 0.7151
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0636
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.2657
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 150.712
b: 256, m: 2048, n: 446, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 129.835
b: 256, m: 2048, n: 2048, k: 446,
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 95.970
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0535
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 249.629
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.3827
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 139.512
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.3442
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 155.136

Attention duration (in seconds): 0.3366
Attention throughput (in TFLOP/s): 164.348
MLP duration (in seconds): 0.7269
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0635
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.2623
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 154.041
b: 256, m: 2048, n: 448, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 72.988
b: 256, m: 2048, n: 2048, k: 448,
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 131.423
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0540
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 249.237
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.3715
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 145.039
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.3278
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 164.336

Attention duration (in seconds): 0.3369
Attention throughput (in TFLOP/s): 165.650
MLP duration (in seconds): 0.6993
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0362
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.2742
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 148.685
b: 256, m: 2048, n: 450, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 102.081
b: 256, m: 2048, n: 2048, k: 450,
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 91.914
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 255.744
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.3712
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 146.450
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.3496
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 155.504

Attention duration (in seconds): 0.3473
Attention throughput (in TFLOP/s): 162.075
MLP duration (in seconds): 0.7207
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0680
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.2981
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 137.966
b: 256, m: 2048, n: 452, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 123.540
b: 256, m: 2048, n: 2048, k: 452,
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 90.732
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 218.082
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.3825
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 143.397
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.3498
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 156.771

Attention duration (in seconds): 0.3796
Attention throughput (in TFLOP/s): 149.606
MLP duration (in seconds): 0.7323
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.2780
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 149.283
b: 256, m: 2048, n: 454, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 103.536
b: 256, m: 2048, n: 2048, k: 454,
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 106.049
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0775
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 178.406
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.3863
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 143.227
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.3557
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 155.567

Attention duration (in seconds): 0.3741
Attention throughput (in TFLOP/s): 153.105
MLP duration (in seconds): 0.7420
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.2807
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 149.151
b: 256, m: 2048, n: 456, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 77.045
b: 256, m: 2048, n: 2048, k: 456,
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 113.233
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0734
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 190.234
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.3916
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 142.553
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.3600
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 155.066

Attention duration (in seconds): 0.3754
Attention throughput (in TFLOP/s): 153.910
MLP duration (in seconds): 0.7515
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.2813
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 150.134
b: 256, m: 2048, n: 458, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 141.205
b: 256, m: 2048, n: 2048, k: 458,
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 96.786
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0556
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 253.185
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.4109
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 137.048
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.3607
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 156.102

Attention duration (in seconds): 0.3540
Attention throughput (in TFLOP/s): 164.612
MLP duration (in seconds): 0.7716
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.2834
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 150.300
b: 256, m: 2048, n: 460, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 80.708
b: 256, m: 2048, n: 2048, k: 460,
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 126.263
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0569
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 249.768
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.3940
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 144.171
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.3667
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 154.879

Attention duration (in seconds): 0.3604
Attention throughput (in TFLOP/s): 163.107
MLP duration (in seconds): 0.7607
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.2884
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 149.018
b: 256, m: 2048, n: 462, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 90.113
b: 256, m: 2048, n: 2048, k: 462,
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 97.107
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0564
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 253.971
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.4004
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 143.111
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.3714
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 154.263

Attention duration (in seconds): 0.3660
Attention throughput (in TFLOP/s): 161.970
MLP duration (in seconds): 0.7718
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1378
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.2868
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 151.152
b: 256, m: 2048, n: 464, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 132.770
b: 256, m: 2048, n: 2048, k: 464,
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 117.904
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0828
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 174.594
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.3973
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 145.481
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.3681
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 156.984

Attention duration (in seconds): 0.3855
Attention throughput (in TFLOP/s): 155.098
MLP duration (in seconds): 0.7654
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1509
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.2952
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 148.103
b: 256, m: 2048, n: 466, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 136.980
b: 256, m: 2048, n: 2048, k: 466,
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 86.038
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0594
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 245.542
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.4184
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 139.317
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.3802
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 153.323

Attention duration (in seconds): 0.3735
Attention throughput (in TFLOP/s): 161.436
MLP duration (in seconds): 0.7986
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1721
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.2949
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 149.515
b: 256, m: 2048, n: 468, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 139.939
b: 256, m: 2048, n: 2048, k: 468,
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 59.049
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0577
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 254.633
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.4026
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 146.031
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.3746
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 156.954

Attention duration (in seconds): 0.3768
Attention throughput (in TFLOP/s): 161.348
MLP duration (in seconds): 0.7772
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1541
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.2971
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 149.696
b: 256, m: 2048, n: 470, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 81.177
b: 256, m: 2048, n: 2048, k: 470,
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 108.448
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0598
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 247.761
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.4136
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 143.365
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.4233
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 140.086

Attention duration (in seconds): 0.3787
Attention throughput (in TFLOP/s): 161.928
MLP duration (in seconds): 0.8369
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.2974
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 150.833
b: 256, m: 2048, n: 472, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 131.334
b: 256, m: 2048, n: 2048, k: 472,
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 90.677
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0779
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 191.981
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.4222
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 141.655
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.3858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 155.016

Attention duration (in seconds): 0.3941
Attention throughput (in TFLOP/s): 156.875
MLP duration (in seconds): 0.8080
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.3063
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 147.675
b: 256, m: 2048, n: 474, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 69.917
b: 256, m: 2048, n: 2048, k: 474,
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 74.392
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0819
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 184.127
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.4245
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 142.062
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.3898
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 154.712

Attention duration (in seconds): 0.4164
Attention throughput (in TFLOP/s): 149.716
MLP duration (in seconds): 0.8144
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.3075
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 148.325
b: 256, m: 2048, n: 476, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 107.751
b: 256, m: 2048, n: 2048, k: 476,
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 94.155
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0605
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 251.348
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.4355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 139.662
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.3956
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 153.727

Attention duration (in seconds): 0.3884
Attention throughput (in TFLOP/s): 161.867
MLP duration (in seconds): 0.8311
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.3082
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 149.236
b: 256, m: 2048, n: 478, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 77.927
b: 256, m: 2048, n: 2048, k: 478,
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 95.513
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0607
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 252.685
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.4242
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 144.601
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.4022
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 152.483

Attention duration (in seconds): 0.3928
Attention throughput (in TFLOP/s): 161.355
MLP duration (in seconds): 0.8264
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.3117
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 148.826
b: 256, m: 2048, n: 480, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 101.865
b: 256, m: 2048, n: 2048, k: 480,
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 150.601
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0615
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 251.276
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.4354
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 142.046
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.4353
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 142.077

Attention duration (in seconds): 0.3902
Attention throughput (in TFLOP/s): 163.796
MLP duration (in seconds): 0.8707
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2609
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.3156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 148.214
b: 256, m: 2048, n: 482, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 118.788
b: 256, m: 2048, n: 2048, k: 482,
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 102.764
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0668
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 233.409
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.4286
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 145.496
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.4043
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 154.266

Attention duration (in seconds): 0.4012
Attention throughput (in TFLOP/s): 160.619
MLP duration (in seconds): 0.8329
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2341
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.3178
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 148.392
b: 256, m: 2048, n: 484, k: 2048,
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 139.258
b: 256, m: 2048, n: 2048, k: 484,
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 114.212
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0615
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 255.500
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.4449
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 141.351
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.4130
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 152.273

Attention duration (in seconds): 0.3959
Attention throughput (in TFLOP/s): 164.080
MLP duration (in seconds): 0.8578
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2537
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
