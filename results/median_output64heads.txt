[2023-06-27 14:08:51,322] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-27 14:08:52,188] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.146.165, master_port=6000
[2023-06-27 14:08:52,188] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-27 14:08:54,963] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[2023-06-27 15:32:43,596] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-27 15:32:44,336] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.156.211, master_port=6000
[2023-06-27 15:32:44,336] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-27 15:32:46,653] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25792x77376, b=2048): 0.1302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25792x77376, b=2048): 251.157
Elapsed time for attention_key_query_prob (256x2048x403x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x403x2048): 73.653
Elapsed time for attention_prob_times_values (256x2048x2048x403): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x403): 73.343
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24765857792, 42481549312)
Elapsed time for attention_linear_projection (4x25792x25792, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_linear_projection (4x25792x25792, b=2048): 260.390
(24765857792, 42481549312)
Elapsed time for mlp_h_to_4h (4x25792x103168, b=2048): 0.1733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25792x103168, b=2048): 251.545
Elapsed time for mlp_fused_gelu (2048x4x103168): 0.0029
Elapsed time for mlp_4h_to_h (4x103168x25792, b=2048): 0.1812
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103168x25792, b=2048): 240.599
Elapsed time for transformer_add_bias_dropout (2048x4x25792): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25792): 0.0011

Attention duration (in seconds): 0.2311
Attention throughput (in TFLOP/s): 196.175
MLP duration (in seconds): 0.3574
MLP throughput (in TFLOP/s): 243.933
Transformer duration (in seconds): 0.5944
Transformer throughput (in TFLOP/s): 222.946
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.1302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 252.381
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 106.007
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 121.678
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(19441188864, 42481549312)
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0420
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 260.993
(19441188864, 42481549312)
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.1740
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 251.727
Elapsed time for mlp_fused_gelu (2048x4x103424): 0.0029
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.1809
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 242.210
Elapsed time for transformer_add_bias_dropout (2048x4x25856): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25856): 0.0010

Attention duration (in seconds): 0.2229
Attention throughput (in TFLOP/s): 204.299
MLP duration (in seconds): 0.3579
MLP throughput (in TFLOP/s): 244.848
Transformer duration (in seconds): 0.5867
Transformer throughput (in TFLOP/s): 226.995
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25920x77760, b=2048): 0.1305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25920x77760, b=2048): 253.106
Elapsed time for attention_key_query_prob (256x2048x405x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x405x2048): 74.035
Elapsed time for attention_prob_times_values (256x2048x2048x405): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x405): 73.562
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(14091354112, 42481549312)
Elapsed time for attention_linear_projection (4x25920x25920, b=2048): 0.0427
Throughput (in TFLOP/s) for attention_linear_projection (4x25920x25920, b=2048): 258.047
(14091354112, 42481549312)
Elapsed time for mlp_h_to_4h (4x25920x103680, b=2048): 0.1749
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25920x103680, b=2048): 251.806
Elapsed time for mlp_fused_gelu (2048x4x103680): 0.0029
Elapsed time for mlp_4h_to_h (4x103680x25920, b=2048): 0.1824
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103680x25920, b=2048): 241.397
Elapsed time for transformer_add_bias_dropout (2048x4x25920): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25920): 0.0011

Attention duration (in seconds): 0.2321
Attention throughput (in TFLOP/s): 197.164
MLP duration (in seconds): 0.3602
MLP throughput (in TFLOP/s): 244.475
Transformer duration (in seconds): 0.5983
Transformer throughput (in TFLOP/s): 223.699
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.1313
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 252.718
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 106.207
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 122.129
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(8716353536, 42481549312)
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0428
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 258.476
(8716353536, 42481549312)
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.1760
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 251.461
Elapsed time for mlp_fused_gelu (2048x4x103936): 0.0029
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.1832
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 241.543
Elapsed time for transformer_add_bias_dropout (2048x4x25984): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25984): 0.0011

Attention duration (in seconds): 0.2249
Attention throughput (in TFLOP/s): 204.499
MLP duration (in seconds): 0.3621
MLP throughput (in TFLOP/s): 244.398
Transformer duration (in seconds): 0.5929
Transformer throughput (in TFLOP/s): 226.830
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26048x78144, b=2048): 0.1320
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26048x78144, b=2048): 252.603
Elapsed time for attention_key_query_prob (256x2048x407x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x407x2048): 74.470
Elapsed time for attention_prob_times_values (256x2048x2048x407): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x407): 72.600
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(3314089984, 42481549312)
Elapsed time for attention_linear_projection (4x26048x26048, b=2048): 0.0429
Throughput (in TFLOP/s) for attention_linear_projection (4x26048x26048, b=2048): 258.969
(3314089984, 42481549312)
Elapsed time for mlp_h_to_4h (4x26048x104192, b=2048): 0.1755
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26048x104192, b=2048): 253.424
Elapsed time for mlp_fused_gelu (2048x4x104192): 0.0030
Elapsed time for mlp_4h_to_h (4x104192x26048, b=2048): 0.1841
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104192x26048, b=2048): 241.474
Elapsed time for transformer_add_bias_dropout (2048x4x26048): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26048): 0.0011

Attention duration (in seconds): 0.2342
Attention throughput (in TFLOP/s): 197.358
MLP duration (in seconds): 0.3626
MLP throughput (in TFLOP/s): 245.291
Transformer duration (in seconds): 0.6027
Transformer throughput (in TFLOP/s): 224.237
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1321
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 253.615
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 189.283
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 194.123
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25032196096, 42481549312)
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0430
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 259.501
(25032196096, 42481549312)
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.1784
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 250.530
Elapsed time for mlp_fused_gelu (2048x4x104448): 0.0030
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1845
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 242.224
Elapsed time for transformer_add_bias_dropout (2048x4x26112): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26112): 0.0011

Attention duration (in seconds): 0.2198
Attention throughput (in TFLOP/s): 211.294
MLP duration (in seconds): 0.3658
MLP throughput (in TFLOP/s): 244.316
Transformer duration (in seconds): 0.5915
Transformer throughput (in TFLOP/s): 229.598
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26176x78528, b=2048): 0.1328
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26176x78528, b=2048): 253.637
Elapsed time for attention_key_query_prob (256x2048x409x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x409x2048): 74.695
Elapsed time for attention_prob_times_values (256x2048x2048x409): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x409): 72.506
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19577503744, 42481549312)
Elapsed time for attention_linear_projection (4x26176x26176, b=2048): 0.0432
Throughput (in TFLOP/s) for attention_linear_projection (4x26176x26176, b=2048): 259.978
(19577503744, 42481549312)
Elapsed time for mlp_h_to_4h (4x26176x104704, b=2048): 0.1779
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26176x104704, b=2048): 252.383
Elapsed time for mlp_fused_gelu (2048x4x104704): 0.0030
Elapsed time for mlp_4h_to_h (4x104704x26176, b=2048): 0.1858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104704x26176, b=2048): 241.670
Elapsed time for transformer_add_bias_dropout (2048x4x26176): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26176): 0.0011

Attention duration (in seconds): 0.2352
Attention throughput (in TFLOP/s): 198.353
MLP duration (in seconds): 0.3667
MLP throughput (in TFLOP/s): 244.913
Transformer duration (in seconds): 0.6079
Transformer throughput (in TFLOP/s): 224.478
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1343
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 251.956
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 107.123
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 122.636
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(14095548416, 42481549312)
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0433
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 260.674
(14095548416, 42481549312)
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.1799
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 250.816
Elapsed time for mlp_fused_gelu (2048x4x104960): 0.0030
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1873
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 240.954
Elapsed time for transformer_add_bias_dropout (2048x4x26240): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26240): 0.0011

Attention duration (in seconds): 0.2284
Attention throughput (in TFLOP/s): 205.273
MLP duration (in seconds): 0.3702
MLP throughput (in TFLOP/s): 243.812
Transformer duration (in seconds): 0.6045
Transformer throughput (in TFLOP/s): 226.850
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26304x78912, b=2048): 0.1351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26304x78912, b=2048): 251.646
Elapsed time for attention_key_query_prob (256x2048x411x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x411x2048): 74.621
Elapsed time for attention_prob_times_values (256x2048x2048x411): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x411): 74.917
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8586330112, 42481549312)
Elapsed time for attention_linear_projection (4x26304x26304, b=2048): 0.0440
Throughput (in TFLOP/s) for attention_linear_projection (4x26304x26304, b=2048): 257.518
(8586330112, 42481549312)
Elapsed time for mlp_h_to_4h (4x26304x105216, b=2048): 0.1810
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26304x105216, b=2048): 250.484
Elapsed time for mlp_fused_gelu (2048x4x105216): 0.0030
Elapsed time for mlp_4h_to_h (4x105216x26304, b=2048): 0.1881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105216x26304, b=2048): 241.072
Elapsed time for transformer_add_bias_dropout (2048x4x26304): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26304): 0.0011

Attention duration (in seconds): 0.2382
Attention throughput (in TFLOP/s): 197.790
MLP duration (in seconds): 0.3721
MLP throughput (in TFLOP/s): 243.720
Transformer duration (in seconds): 0.6163
Transformer throughput (in TFLOP/s): 223.591
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.1357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 251.753
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 107.627
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 123.034
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3049848832, 42481549312)
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0441
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 258.117
(3049848832, 42481549312)
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.1817
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 250.760
Elapsed time for mlp_fused_gelu (2048x4x105472): 0.0030
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.1880
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 242.424
Elapsed time for transformer_add_bias_dropout (2048x4x26368): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26368): 0.0011

Attention duration (in seconds): 0.2307
Attention throughput (in TFLOP/s): 205.185
MLP duration (in seconds): 0.3727
MLP throughput (in TFLOP/s): 244.545
Transformer duration (in seconds): 0.6093
Transformer throughput (in TFLOP/s): 227.244
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26432x79296, b=2048): 0.1357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26432x79296, b=2048): 253.100
Elapsed time for attention_key_query_prob (256x2048x413x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x413x2048): 74.951
Elapsed time for attention_prob_times_values (256x2048x2048x413): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x413): 75.422
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24872812544, 42481549312)
Elapsed time for attention_linear_projection (4x26432x26432, b=2048): 0.0442
Throughput (in TFLOP/s) for attention_linear_projection (4x26432x26432, b=2048): 258.713
(24872812544, 42481549312)
Elapsed time for mlp_h_to_4h (4x26432x105728, b=2048): 0.1823
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26432x105728, b=2048): 251.158
Elapsed time for mlp_fused_gelu (2048x4x105728): 0.0030
Elapsed time for mlp_4h_to_h (4x105728x26432, b=2048): 0.1894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105728x26432, b=2048): 241.742
Elapsed time for transformer_add_bias_dropout (2048x4x26432): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26432): 0.0011

Attention duration (in seconds): 0.2390
Attention throughput (in TFLOP/s): 199.035
MLP duration (in seconds): 0.3747
MLP throughput (in TFLOP/s): 244.390
Transformer duration (in seconds): 0.6197
Transformer throughput (in TFLOP/s): 224.512
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1372
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 251.591
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 107.942
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 124.287
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(19281805312, 42481549312)
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 259.344
(19281805312, 42481549312)
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.1837
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 250.468
Elapsed time for mlp_fused_gelu (2048x4x105984): 0.0030
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1907
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 241.239
Elapsed time for transformer_add_bias_dropout (2048x4x26496): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26496): 0.0011

Attention duration (in seconds): 0.2323
Attention throughput (in TFLOP/s): 205.689
MLP duration (in seconds): 0.3774
MLP throughput (in TFLOP/s): 243.812
Transformer duration (in seconds): 0.6157
Transformer throughput (in TFLOP/s): 227.051
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26560x79680, b=2048): 0.1379
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26560x79680, b=2048): 251.517
Elapsed time for attention_key_query_prob (256x2048x415x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x415x2048): 75.761
Elapsed time for attention_prob_times_values (256x2048x2048x415): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x415): 74.919
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(13663535104, 42481549312)
Elapsed time for attention_linear_projection (4x26560x26560, b=2048): 0.0445
Throughput (in TFLOP/s) for attention_linear_projection (4x26560x26560, b=2048): 259.544
(13663535104, 42481549312)
Elapsed time for mlp_h_to_4h (4x26560x106240, b=2048): 0.1848
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26560x106240, b=2048): 250.215
Elapsed time for mlp_fused_gelu (2048x4x106240): 0.0030
Elapsed time for mlp_4h_to_h (4x106240x26560, b=2048): 0.1913
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106240x26560, b=2048): 241.695
Elapsed time for transformer_add_bias_dropout (2048x4x26560): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26560): 0.0011

Attention duration (in seconds): 0.2415
Attention throughput (in TFLOP/s): 198.833
MLP duration (in seconds): 0.3791
MLP throughput (in TFLOP/s): 243.929
Transformer duration (in seconds): 0.6266
Transformer throughput (in TFLOP/s): 224.192
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.1376
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 253.231
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 203.807
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 201.777
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(8018001920, 42481549312)
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 260.280
(8018001920, 42481549312)
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.1842
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 252.229
Elapsed time for mlp_fused_gelu (2048x4x106496): 0.0030
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1916
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 242.460
Elapsed time for transformer_add_bias_dropout (2048x4x26624): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26624): 0.0011

Attention duration (in seconds): 0.2264
Attention throughput (in TFLOP/s): 213.035
MLP duration (in seconds): 0.3788
MLP throughput (in TFLOP/s): 245.280
Transformer duration (in seconds): 0.6113
Transformer throughput (in TFLOP/s): 230.913
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26688x80064, b=2048): 0.1385
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26688x80064, b=2048): 252.805
Elapsed time for attention_key_query_prob (256x2048x417x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x417x2048): 73.982
Elapsed time for attention_prob_times_values (256x2048x2048x417): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x417): 74.628
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(2347302912, 42481549312)
Elapsed time for attention_linear_projection (4x26688x26688, b=2048): 0.0449
Throughput (in TFLOP/s) for attention_linear_projection (4x26688x26688, b=2048): 260.009
(2347302912, 42481549312)
Elapsed time for mlp_h_to_4h (4x26688x106752, b=2048): 0.1864
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26688x106752, b=2048): 250.371
Elapsed time for mlp_fused_gelu (2048x4x106752): 0.0030
Elapsed time for mlp_4h_to_h (4x106752x26688, b=2048): 0.1937
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106752x26688, b=2048): 240.998
Elapsed time for transformer_add_bias_dropout (2048x4x26688): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26688): 0.0011

Attention duration (in seconds): 0.2429
Attention throughput (in TFLOP/s): 199.547
MLP duration (in seconds): 0.3831
MLP throughput (in TFLOP/s): 243.658
Transformer duration (in seconds): 0.6322
Transformer throughput (in TFLOP/s): 224.350
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.1390
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 253.062
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 109.074
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 125.333
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24709234688, 42481549312)
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0450
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 260.847
(24709234688, 42481549312)
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.1870
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 250.834
Elapsed time for mlp_fused_gelu (2048x4x107008): 0.0030
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.1937
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 242.151
Elapsed time for transformer_add_bias_dropout (2048x4x26752): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26752): 0.0011

Attention duration (in seconds): 0.2348
Attention throughput (in TFLOP/s): 207.410
MLP duration (in seconds): 0.3837
MLP throughput (in TFLOP/s): 244.467
Transformer duration (in seconds): 0.6246
Transformer throughput (in TFLOP/s): 228.157
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26816x80448, b=2048): 0.1400
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26816x80448, b=2048): 252.389
Elapsed time for attention_key_query_prob (256x2048x419x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x419x2048): 72.966
Elapsed time for attention_prob_times_values (256x2048x2048x419): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x419): 76.244
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18981912576, 42481549312)
Elapsed time for attention_linear_projection (4x26816x26816, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_linear_projection (4x26816x26816, b=2048): 258.332
(18981912576, 42481549312)
Elapsed time for mlp_h_to_4h (4x26816x107264, b=2048): 0.1879
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26816x107264, b=2048): 250.830
Elapsed time for mlp_fused_gelu (2048x4x107264): 0.0030
Elapsed time for mlp_4h_to_h (4x107264x26816, b=2048): 0.1957
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107264x26816, b=2048): 240.807
Elapsed time for transformer_add_bias_dropout (2048x4x26816): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26816): 0.0011

Attention duration (in seconds): 0.2452
Attention throughput (in TFLOP/s): 199.550
MLP duration (in seconds): 0.3866
MLP throughput (in TFLOP/s): 243.785
Transformer duration (in seconds): 0.6379
Transformer throughput (in TFLOP/s): 224.441
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.1406
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 252.593
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 109.523
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 124.832
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(13227327488, 42481549312)
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 259.040
(13227327488, 42481549312)
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.1886
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 251.021
Elapsed time for mlp_fused_gelu (2048x4x107520): 0.0030
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.1967
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 240.746
Elapsed time for transformer_add_bias_dropout (2048x4x26880): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26880): 0.0011

Attention duration (in seconds): 0.2372
Attention throughput (in TFLOP/s): 207.278
MLP duration (in seconds): 0.3884
MLP throughput (in TFLOP/s): 243.850
Transformer duration (in seconds): 0.6316
Transformer throughput (in TFLOP/s): 227.765
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26944x80832, b=2048): 0.1417
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26944x80832, b=2048): 251.835
Elapsed time for attention_key_query_prob (256x2048x421x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x421x2048): 72.960
Elapsed time for attention_prob_times_values (256x2048x2048x421): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x421): 76.122
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7445479424, 42481549312)
Elapsed time for attention_linear_projection (4x26944x26944, b=2048): 0.0459
Throughput (in TFLOP/s) for attention_linear_projection (4x26944x26944, b=2048): 259.323
(7445479424, 42481549312)
Elapsed time for mlp_h_to_4h (4x26944x107776, b=2048): 0.1906
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26944x107776, b=2048): 249.627
Elapsed time for mlp_fused_gelu (2048x4x107776): 0.0031
Elapsed time for mlp_4h_to_h (4x107776x26944, b=2048): 0.1975
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107776x26944, b=2048): 240.919
Elapsed time for transformer_add_bias_dropout (2048x4x26944): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26944): 0.0011

Attention duration (in seconds): 0.2472
Attention throughput (in TFLOP/s): 199.769
MLP duration (in seconds): 0.3911
MLP throughput (in TFLOP/s): 243.283
Transformer duration (in seconds): 0.6445
Transformer throughput (in TFLOP/s): 224.269
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.1419
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 252.741
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 109.780
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 125.608
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1636368384, 42481549312)
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0460
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 259.849
(1636368384, 42481549312)
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.1909
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 250.467
Elapsed time for mlp_fused_gelu (2048x4x108032): 0.0031
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.1976
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 241.932
Elapsed time for transformer_add_bias_dropout (2048x4x27008): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27008): 0.0011

Attention duration (in seconds): 0.2387
Attention throughput (in TFLOP/s): 207.833
MLP duration (in seconds): 0.3915
MLP throughput (in TFLOP/s): 244.202
Transformer duration (in seconds): 0.6364
Transformer throughput (in TFLOP/s): 228.208
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27072x81216, b=2048): 0.1434
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27072x81216, b=2048): 251.195
Elapsed time for attention_key_query_prob (256x2048x423x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x423x2048): 72.575
Elapsed time for attention_prob_times_values (256x2048x2048x423): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x423): 74.827
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24547753984, 42481549312)
Elapsed time for attention_linear_projection (4x27072x27072, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_linear_projection (4x27072x27072, b=2048): 260.209
(24547753984, 42481549312)
Elapsed time for mlp_h_to_4h (4x27072x108288, b=2048): 0.1916
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27072x108288, b=2048): 250.738
Elapsed time for mlp_fused_gelu (2048x4x108288): 0.0031
Elapsed time for mlp_4h_to_h (4x108288x27072, b=2048): 0.1988
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108288x27072, b=2048): 241.647
Elapsed time for transformer_add_bias_dropout (2048x4x27072): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27072): 0.0011

Attention duration (in seconds): 0.2496
Attention throughput (in TFLOP/s): 199.673
MLP duration (in seconds): 0.3934
MLP throughput (in TFLOP/s): 244.191
Transformer duration (in seconds): 0.6492
Transformer throughput (in TFLOP/s): 224.747
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.1431
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 252.879
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 185.148
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 199.013
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18684116992, 42481549312)
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0463
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 260.792
(18684116992, 42481549312)
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.1928
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 250.360
Elapsed time for mlp_fused_gelu (2048x4x108544): 0.0031
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.2001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 241.211
Elapsed time for transformer_add_bias_dropout (2048x4x27136): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27136): 0.0011

Attention duration (in seconds): 0.2343
Attention throughput (in TFLOP/s): 213.753
MLP duration (in seconds): 0.3959
MLP throughput (in TFLOP/s): 243.793
Transformer duration (in seconds): 0.6363
Transformer throughput (in TFLOP/s): 230.374
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27200x81600, b=2048): 0.1442
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27200x81600, b=2048): 252.235
Elapsed time for attention_key_query_prob (256x2048x425x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x425x2048): 72.578
Elapsed time for attention_prob_times_values (256x2048x2048x425): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x425): 74.915
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12793217024, 42481549312)
Elapsed time for attention_linear_projection (4x27200x27200, b=2048): 0.0470
Throughput (in TFLOP/s) for attention_linear_projection (4x27200x27200, b=2048): 258.017
(12793217024, 42481549312)
Elapsed time for mlp_h_to_4h (4x27200x108800, b=2048): 0.1940
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27200x108800, b=2048): 249.945
Elapsed time for mlp_fused_gelu (2048x4x108800): 0.0031
Elapsed time for mlp_4h_to_h (4x108800x27200, b=2048): 0.2018
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108800x27200, b=2048): 240.266
Elapsed time for transformer_add_bias_dropout (2048x4x27200): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27200): 0.0011

Attention duration (in seconds): 0.2513
Attention throughput (in TFLOP/s): 200.195
MLP duration (in seconds): 0.3989
MLP throughput (in TFLOP/s): 243.118
Transformer duration (in seconds): 0.6564
Transformer throughput (in TFLOP/s): 224.379
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.1452
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 251.628
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 110.448
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 126.925
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6872956928, 42481549312)
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 258.680
(6872956928, 42481549312)
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.1944
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 250.557
Elapsed time for mlp_fused_gelu (2048x4x109056): 0.0031
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.2015
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 241.728
Elapsed time for transformer_add_bias_dropout (2048x4x27264): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27264): 0.0011

Attention duration (in seconds): 0.2432
Attention throughput (in TFLOP/s): 207.851
MLP duration (in seconds): 0.3990
MLP throughput (in TFLOP/s): 244.159
Transformer duration (in seconds): 0.6484
Transformer throughput (in TFLOP/s): 228.214
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27328x81984, b=2048): 0.1461
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27328x81984, b=2048): 251.200
Elapsed time for attention_key_query_prob (256x2048x427x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x427x2048): 72.822
Elapsed time for attention_prob_times_values (256x2048x2048x427): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x427): 77.447
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(925433856, 42481549312)
Elapsed time for attention_linear_projection (4x27328x27328, b=2048): 0.0477
Throughput (in TFLOP/s) for attention_linear_projection (4x27328x27328, b=2048): 256.337
(925433856, 42481549312)
Elapsed time for mlp_h_to_4h (4x27328x109312, b=2048): 0.1957
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27328x109312, b=2048): 250.156
Elapsed time for mlp_fused_gelu (2048x4x109312): 0.0031
Elapsed time for mlp_4h_to_h (4x109312x27328, b=2048): 0.2038
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109312x27328, b=2048): 240.199
Elapsed time for transformer_add_bias_dropout (2048x4x27328): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27328): 0.0011

Attention duration (in seconds): 0.2537
Attention throughput (in TFLOP/s): 200.153
MLP duration (in seconds): 0.4025
MLP throughput (in TFLOP/s): 243.187
Transformer duration (in seconds): 0.6625
Transformer throughput (in TFLOP/s): 224.407
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.1466
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 251.564
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 111.522
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 127.441
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24382078976, 42481549312)
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0473
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 259.723
(24382078976, 42481549312)
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.1966
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 250.095
Elapsed time for mlp_fused_gelu (2048x4x109568): 0.0031
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.2030
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 242.190
Elapsed time for transformer_add_bias_dropout (2048x4x27392): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27392): 0.0011

Attention duration (in seconds): 0.2448
Attention throughput (in TFLOP/s): 208.350
MLP duration (in seconds): 0.4028
MLP throughput (in TFLOP/s): 244.179
Transformer duration (in seconds): 0.6539
Transformer throughput (in TFLOP/s): 228.421
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27456x82368, b=2048): 0.1462
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27456x82368, b=2048): 253.362
Elapsed time for attention_key_query_prob (256x2048x429x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x429x2048): 73.293
Elapsed time for attention_prob_times_values (256x2048x2048x429): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x429): 77.623
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18377932800, 42481549312)
Elapsed time for attention_linear_projection (4x27456x27456, b=2048): 0.0475
Throughput (in TFLOP/s) for attention_linear_projection (4x27456x27456, b=2048): 260.027
(18377932800, 42481549312)
Elapsed time for mlp_h_to_4h (4x27456x109824, b=2048): 0.1980
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27456x109824, b=2048): 249.556
Elapsed time for mlp_fused_gelu (2048x4x109824): 0.0031
Elapsed time for mlp_4h_to_h (4x109824x27456, b=2048): 0.2055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109824x27456, b=2048): 240.439
Elapsed time for transformer_add_bias_dropout (2048x4x27456): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27456): 0.0011

Attention duration (in seconds): 0.2536
Attention throughput (in TFLOP/s): 202.087
MLP duration (in seconds): 0.4065
MLP throughput (in TFLOP/s): 243.037
Transformer duration (in seconds): 0.6664
Transformer throughput (in TFLOP/s): 225.159
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.1476
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 252.166
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 111.991
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 128.053
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12346523648, 42481549312)
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 260.706
(12346523648, 42481549312)
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.1979
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 250.828
Elapsed time for mlp_fused_gelu (2048x4x110080): 0.0031
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.2064
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 240.520
Elapsed time for transformer_add_bias_dropout (2048x4x27520): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27520): 0.0011

Attention duration (in seconds): 0.2461
Attention throughput (in TFLOP/s): 209.204
MLP duration (in seconds): 0.4074
MLP throughput (in TFLOP/s): 243.686
Transformer duration (in seconds): 0.6597
Transformer throughput (in TFLOP/s): 228.515
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27584x82752, b=2048): 0.1486
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27584x82752, b=2048): 251.656
Elapsed time for attention_key_query_prob (256x2048x431x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x431x2048): 73.509
Elapsed time for attention_prob_times_values (256x2048x2048x431): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x431): 76.561
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6285754368, 42481549312)
Elapsed time for attention_linear_projection (4x27584x27584, b=2048): 0.0485
Throughput (in TFLOP/s) for attention_linear_projection (4x27584x27584, b=2048): 257.241
(6285754368, 42481549312)
Elapsed time for mlp_h_to_4h (4x27584x110336, b=2048): 0.1995
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27584x110336, b=2048): 249.907
Elapsed time for mlp_fused_gelu (2048x4x110336): 0.0031
Elapsed time for mlp_4h_to_h (4x110336x27584, b=2048): 0.2075
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110336x27584, b=2048): 240.299
Elapsed time for transformer_add_bias_dropout (2048x4x27584): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27584): 0.0011

Attention duration (in seconds): 0.2571
Attention throughput (in TFLOP/s): 201.116
MLP duration (in seconds): 0.4102
MLP throughput (in TFLOP/s): 243.142
Transformer duration (in seconds): 0.6736
Transformer throughput (in TFLOP/s): 224.822
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.1491
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 252.037
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 195.151
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 206.820
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(197722112, 42481549312)
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0480
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 261.052
(197722112, 42481549312)
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.1996
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 250.947
Elapsed time for mlp_fused_gelu (2048x4x110592): 0.0031
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.2069
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 242.074
Elapsed time for transformer_add_bias_dropout (2048x4x27648): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27648): 0.0011

Attention duration (in seconds): 0.2417
Attention throughput (in TFLOP/s): 214.951
MLP duration (in seconds): 0.4097
MLP throughput (in TFLOP/s): 244.543
Transformer duration (in seconds): 0.6577
Transformer throughput (in TFLOP/s): 231.328
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27712x83136, b=2048): 0.1503
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27712x83136, b=2048): 251.171
Elapsed time for attention_key_query_prob (256x2048x433x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x433x2048): 73.939
Elapsed time for attention_prob_times_values (256x2048x2048x433): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x433): 76.738
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(24216403968, 42481549312)
Elapsed time for attention_linear_projection (4x27712x27712, b=2048): 0.0486
Throughput (in TFLOP/s) for attention_linear_projection (4x27712x27712, b=2048): 258.898
(24216403968, 42481549312)
Elapsed time for mlp_h_to_4h (4x27712x110848, b=2048): 0.2015
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27712x110848, b=2048): 249.806
Elapsed time for mlp_fused_gelu (2048x4x110848): 0.0031
Elapsed time for mlp_4h_to_h (4x110848x27712, b=2048): 0.2098
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110848x27712, b=2048): 239.861
Elapsed time for transformer_add_bias_dropout (2048x4x27712): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27712): 0.0012

Attention duration (in seconds): 0.2590
Attention throughput (in TFLOP/s): 201.484
MLP duration (in seconds): 0.4144
MLP throughput (in TFLOP/s): 242.876
Transformer duration (in seconds): 0.6798
Transformer throughput (in TFLOP/s): 224.836
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.1508
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 251.540
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 112.997
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 129.176
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18071748608, 42481549312)
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0487
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 259.440
(18071748608, 42481549312)
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.2021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 250.196
Elapsed time for mlp_fused_gelu (2048x4x111104): 0.0032
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.2098
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 241.001
Elapsed time for transformer_add_bias_dropout (2048x4x27776): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27776): 0.0011

Attention duration (in seconds): 0.2503
Attention throughput (in TFLOP/s): 209.412
MLP duration (in seconds): 0.4150
MLP throughput (in TFLOP/s): 243.648
Transformer duration (in seconds): 0.6717
Transformer throughput (in TFLOP/s): 228.592
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27840x83520, b=2048): 0.1510
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27840x83520, b=2048): 252.248
Elapsed time for attention_key_query_prob (256x2048x435x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x435x2048): 74.287
Elapsed time for attention_prob_times_values (256x2048x2048x435): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x435): 78.869
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11897733120, 42481549312)
Elapsed time for attention_linear_projection (4x27840x27840, b=2048): 0.0489
Throughput (in TFLOP/s) for attention_linear_projection (4x27840x27840, b=2048): 259.581
(11897733120, 42481549312)
Elapsed time for mlp_h_to_4h (4x27840x111360, b=2048): 0.2025
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27840x111360, b=2048): 250.825
Elapsed time for mlp_fused_gelu (2048x4x111360): 0.0032
Elapsed time for mlp_4h_to_h (4x111360x27840, b=2048): 0.2112
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111360x27840, b=2048): 240.541
Elapsed time for transformer_add_bias_dropout (2048x4x27840): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27840): 0.0012

Attention duration (in seconds): 0.2598
Attention throughput (in TFLOP/s): 202.730
MLP duration (in seconds): 0.4168
MLP throughput (in TFLOP/s): 243.715
Transformer duration (in seconds): 0.6830
Transformer throughput (in TFLOP/s): 225.841
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.1520
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 251.799
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 113.314
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 129.489
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5696454656, 42481549312)
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 257.097
(5696454656, 42481549312)
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.2037
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 250.565
Elapsed time for mlp_fused_gelu (2048x4x111616): 0.0032
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.2116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 241.203
Elapsed time for transformer_add_bias_dropout (2048x4x27904): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27904): 0.0011

Attention duration (in seconds): 0.2525
Attention throughput (in TFLOP/s): 209.497
MLP duration (in seconds): 0.4184
MLP throughput (in TFLOP/s): 243.938
Transformer duration (in seconds): 0.6773
Transformer throughput (in TFLOP/s): 228.804
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27968x83904, b=2048): 0.1528
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27968x83904, b=2048): 251.688
Elapsed time for attention_key_query_prob (256x2048x437x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x437x2048): 75.142
Elapsed time for attention_prob_times_values (256x2048x2048x437): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x437): 79.085
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24080089088, 42481549312)
Elapsed time for attention_linear_projection (4x27968x27968, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_linear_projection (4x27968x27968, b=2048): 256.178
(24080089088, 42481549312)
Elapsed time for mlp_h_to_4h (4x27968x111872, b=2048): 0.2052
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27968x111872, b=2048): 249.846
Elapsed time for mlp_fused_gelu (2048x4x111872): 0.0032
Elapsed time for mlp_4h_to_h (4x111872x27968, b=2048): 0.2142
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111872x27968, b=2048): 239.346
Elapsed time for transformer_add_bias_dropout (2048x4x27968): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27968): 0.0012

Attention duration (in seconds): 0.2626
Attention throughput (in TFLOP/s): 202.395
MLP duration (in seconds): 0.4225
MLP throughput (in TFLOP/s): 242.650
Transformer duration (in seconds): 0.6915
Transformer throughput (in TFLOP/s): 225.119
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.1535
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 251.645
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 113.677
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 130.186
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17822187520, 42481549312)
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 257.499
(17822187520, 42481549312)
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.2060
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 249.994
Elapsed time for mlp_fused_gelu (2048x4x112128): 0.0032
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.2140
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 240.635
Elapsed time for transformer_add_bias_dropout (2048x4x28032): 0.0020
Elapsed time for transformer_layer_norm (2048x4x28032): 0.0011

Attention duration (in seconds): 0.2544
Attention throughput (in TFLOP/s): 209.850
MLP duration (in seconds): 0.4232
MLP throughput (in TFLOP/s): 243.385
Transformer duration (in seconds): 0.6839
Transformer throughput (in TFLOP/s): 228.644
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28096x84288, b=2048): 0.1543
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28096x84288, b=2048): 251.451
Elapsed time for attention_key_query_prob (256x2048x439x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x439x2048): 74.659
Elapsed time for attention_prob_times_values (256x2048x2048x439): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x439): 77.571
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11534925824, 42481549312)
Elapsed time for attention_linear_projection (4x28096x28096, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x28096x28096, b=2048): 255.336
(11534925824, 42481549312)
Elapsed time for mlp_h_to_4h (4x28096x112384, b=2048): 0.2072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28096x112384, b=2048): 249.624
Elapsed time for mlp_fused_gelu (2048x4x112384): 0.0032
Elapsed time for mlp_4h_to_h (4x112384x28096, b=2048): 0.2148
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112384x28096, b=2048): 240.810
Elapsed time for transformer_add_bias_dropout (2048x4x28096): 0.0020
Elapsed time for transformer_layer_norm (2048x4x28096): 0.0012

Attention duration (in seconds): 0.2651
Attention throughput (in TFLOP/s): 202.240
MLP duration (in seconds): 0.4253
MLP throughput (in TFLOP/s): 243.303
Transformer duration (in seconds): 0.6968
Transformer throughput (in TFLOP/s): 225.425
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.1547
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 251.914
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 187.207
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 206.544
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5218304000, 42481549312)
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 256.210
(5218304000, 42481549312)
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.2074
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 250.519
Elapsed time for mlp_fused_gelu (2048x4x112640): 0.0032
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.2155
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 241.201
Elapsed time for transformer_add_bias_dropout (2048x4x28160): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28160): 0.0012

Attention duration (in seconds): 0.2504
Attention throughput (in TFLOP/s): 215.050
MLP duration (in seconds): 0.4261
MLP throughput (in TFLOP/s): 243.934
Transformer duration (in seconds): 0.6830
Transformer throughput (in TFLOP/s): 231.052
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28224x84672, b=2048): 0.1554
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28224x84672, b=2048): 251.923
Elapsed time for attention_key_query_prob (256x2048x441x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x441x2048): 75.151
Elapsed time for attention_prob_times_values (256x2048x2048x441): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x441): 77.683
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23945871360, 42481549312)
Elapsed time for attention_linear_projection (4x28224x28224, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_linear_projection (4x28224x28224, b=2048): 259.484
(23945871360, 42481549312)
Elapsed time for mlp_h_to_4h (4x28224x112896, b=2048): 0.2087
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28224x112896, b=2048): 250.173
Elapsed time for mlp_fused_gelu (2048x4x112896): 0.0032
Elapsed time for mlp_4h_to_h (4x112896x28224, b=2048): 0.2175
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112896x28224, b=2048): 240.065
Elapsed time for transformer_add_bias_dropout (2048x4x28224): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28224): 0.0012

Attention duration (in seconds): 0.2659
Attention throughput (in TFLOP/s): 203.445
MLP duration (in seconds): 0.4293
MLP throughput (in TFLOP/s): 243.191
Transformer duration (in seconds): 0.7017
Transformer throughput (in TFLOP/s): 225.894
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.1564
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 251.457
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 115.166
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 131.579
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17572626432, 42481549312)
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0511
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 256.775
(17572626432, 42481549312)
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.2104
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 249.244
Elapsed time for mlp_fused_gelu (2048x4x113152): 0.0032
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.2182
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 240.319
Elapsed time for transformer_add_bias_dropout (2048x4x28288): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28288): 0.0011

Attention duration (in seconds): 0.2583
Attention throughput (in TFLOP/s): 210.361
MLP duration (in seconds): 0.4318
MLP throughput (in TFLOP/s): 242.885
Transformer duration (in seconds): 0.6966
Transformer throughput (in TFLOP/s): 228.588
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28352x85056, b=2048): 0.1576
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28352x85056, b=2048): 250.688
Elapsed time for attention_key_query_prob (256x2048x443x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x443x2048): 76.227
Elapsed time for attention_prob_times_values (256x2048x2048x443): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x443): 80.709
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11170021376, 42481549312)
Elapsed time for attention_linear_projection (4x28352x28352, b=2048): 0.0514
Throughput (in TFLOP/s) for attention_linear_projection (4x28352x28352, b=2048): 256.378
(11170021376, 42481549312)
Elapsed time for mlp_h_to_4h (4x28352x113408, b=2048): 0.2107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28352x113408, b=2048): 250.036
Elapsed time for mlp_fused_gelu (2048x4x113408): 0.0032
Elapsed time for mlp_4h_to_h (4x113408x28352, b=2048): 0.2194
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113408x28352, b=2048): 240.126
Elapsed time for transformer_add_bias_dropout (2048x4x28352): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28352): 0.0012

Attention duration (in seconds): 0.2686
Attention throughput (in TFLOP/s): 203.186
MLP duration (in seconds): 0.4333
MLP throughput (in TFLOP/s): 243.166
Transformer duration (in seconds): 0.7084
Transformer throughput (in TFLOP/s): 225.781
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.1578
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 251.538
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 115.711
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 132.402
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4738056192, 42481549312)
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 257.679
(4738056192, 42481549312)
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.2115
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 250.157
Elapsed time for mlp_fused_gelu (2048x4x113664): 0.0032
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.2205
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 239.998
Elapsed time for transformer_add_bias_dropout (2048x4x28416): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28416): 0.0011

Attention duration (in seconds): 0.2600
Attention throughput (in TFLOP/s): 210.892
MLP duration (in seconds): 0.4353
MLP throughput (in TFLOP/s): 243.159
Transformer duration (in seconds): 0.7017
Transformer throughput (in TFLOP/s): 228.970
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28480x85440, b=2048): 0.1585
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28480x85440, b=2048): 251.532
Elapsed time for attention_key_query_prob (256x2048x445x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x445x2048): 76.928
Elapsed time for attention_prob_times_values (256x2048x2048x445): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x445): 81.136
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25954942976, 42481549312)
Elapsed time for attention_linear_projection (4x28480x28480, b=2048): 0.0521
Throughput (in TFLOP/s) for attention_linear_projection (4x28480x28480, b=2048): 255.127
(25954942976, 42481549312)
Elapsed time for mlp_h_to_4h (4x28480x113920, b=2048): 0.2126
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28480x113920, b=2048): 250.086
Elapsed time for mlp_fused_gelu (2048x4x113920): 0.0032
Elapsed time for mlp_4h_to_h (4x113920x28480, b=2048): 0.2218
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113920x28480, b=2048): 239.621
Elapsed time for transformer_add_bias_dropout (2048x4x28480): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28480): 0.0012

Attention duration (in seconds): 0.2702
Attention throughput (in TFLOP/s): 203.813
MLP duration (in seconds): 0.4376
MLP throughput (in TFLOP/s): 242.938
Transformer duration (in seconds): 0.7143
Transformer throughput (in TFLOP/s): 225.927
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.1592
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 251.473
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 116.081
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 133.418
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(19464257536, 42481549312)
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 258.634
(19464257536, 42481549312)
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.2133
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 250.331
Elapsed time for mlp_fused_gelu (2048x4x114176): 0.0032
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.2218
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 240.737
Elapsed time for transformer_add_bias_dropout (2048x4x28544): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28544): 0.0012

Attention duration (in seconds): 0.2617
Attention throughput (in TFLOP/s): 211.355
MLP duration (in seconds): 0.4383
MLP throughput (in TFLOP/s): 243.631
Transformer duration (in seconds): 0.7065
Transformer throughput (in TFLOP/s): 229.444
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28608x85824, b=2048): 0.1598
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28608x85824, b=2048): 251.743
Elapsed time for attention_key_query_prob (256x2048x447x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x447x2048): 77.108
Elapsed time for attention_prob_times_values (256x2048x2048x447): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x447): 80.186
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12944211968, 42481549312)
Elapsed time for attention_linear_projection (4x28608x28608, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_linear_projection (4x28608x28608, b=2048): 255.820
(12944211968, 42481549312)
Elapsed time for mlp_h_to_4h (4x28608x114432, b=2048): 0.2144
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28608x114432, b=2048): 250.169
Elapsed time for mlp_fused_gelu (2048x4x114432): 0.0032
Elapsed time for mlp_4h_to_h (4x114432x28608, b=2048): 0.2239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114432x28608, b=2048): 239.522
Elapsed time for transformer_add_bias_dropout (2048x4x28608): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28608): 0.0012

Attention duration (in seconds): 0.2720
Attention throughput (in TFLOP/s): 204.229
MLP duration (in seconds): 0.4416
MLP throughput (in TFLOP/s): 242.934
Transformer duration (in seconds): 0.7202
Transformer throughput (in TFLOP/s): 226.099
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.1607
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 251.441
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 202.908
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 214.537
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6394806272, 42481549312)
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0526
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 256.098
(6394806272, 42481549312)
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.2153
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 250.270
Elapsed time for mlp_fused_gelu (2048x4x114688): 0.0032
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2244
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 240.122
Elapsed time for transformer_add_bias_dropout (2048x4x28672): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28672): 0.0012

Attention duration (in seconds): 0.2579
Attention throughput (in TFLOP/s): 216.335
MLP duration (in seconds): 0.4429
MLP throughput (in TFLOP/s): 243.292
Transformer duration (in seconds): 0.7073
Transformer throughput (in TFLOP/s): 231.225
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28736x86208, b=2048): 0.1615
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28736x86208, b=2048): 251.270
Elapsed time for attention_key_query_prob (256x2048x449x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x449x2048): 75.706
Elapsed time for attention_prob_times_values (256x2048x2048x449): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x449): 79.620
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25818628096, 42481549312)
Elapsed time for attention_linear_projection (4x28736x28736, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_linear_projection (4x28736x28736, b=2048): 256.099
(25818628096, 42481549312)
Elapsed time for mlp_h_to_4h (4x28736x114944, b=2048): 0.2179
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28736x114944, b=2048): 248.340
Elapsed time for mlp_fused_gelu (2048x4x114944): 0.0033
Elapsed time for mlp_4h_to_h (4x114944x28736, b=2048): 0.2267
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114944x28736, b=2048): 238.690
Elapsed time for transformer_add_bias_dropout (2048x4x28736): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28736): 0.0012

Attention duration (in seconds): 0.2746
Attention throughput (in TFLOP/s): 204.100
MLP duration (in seconds): 0.4479
MLP throughput (in TFLOP/s): 241.653
Transformer duration (in seconds): 0.7291
Transformer throughput (in TFLOP/s): 225.331
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.1621
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 251.500
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 108.823
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 133.713
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(19210502144, 42481549312)
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0527
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 257.628
(19210502144, 42481549312)
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.2168
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 250.705
Elapsed time for mlp_fused_gelu (2048x4x115200): 0.0033
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.2263
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 240.195
Elapsed time for transformer_add_bias_dropout (2048x4x28800): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28800): 0.0012

Attention duration (in seconds): 0.2664
Attention throughput (in TFLOP/s): 211.317
MLP duration (in seconds): 0.4464
MLP throughput (in TFLOP/s): 243.547
Transformer duration (in seconds): 0.7193
Transformer throughput (in TFLOP/s): 229.397
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28864x86592, b=2048): 0.1632
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28864x86592, b=2048): 250.929
Elapsed time for attention_key_query_prob (256x2048x451x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x451x2048): 75.839
Elapsed time for attention_prob_times_values (256x2048x2048x451): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x451): 81.486
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(12573016064, 42481549312)
Elapsed time for attention_linear_projection (4x28864x28864, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_linear_projection (4x28864x28864, b=2048): 256.838
(12573016064, 42481549312)
Elapsed time for mlp_h_to_4h (4x28864x115456, b=2048): 0.2185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28864x115456, b=2048): 249.841
Elapsed time for mlp_fused_gelu (2048x4x115456): 0.0033
Elapsed time for mlp_4h_to_h (4x115456x28864, b=2048): 0.2281
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115456x28864, b=2048): 239.406
Elapsed time for transformer_add_bias_dropout (2048x4x28864): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28864): 0.0012

Attention duration (in seconds): 0.2764
Attention throughput (in TFLOP/s): 204.530
MLP duration (in seconds): 0.4499
MLP throughput (in TFLOP/s): 242.736
Transformer duration (in seconds): 0.7329
Transformer throughput (in TFLOP/s): 226.141
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.1634
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 251.783
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 109.284
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 132.740
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(5906169856, 42481549312)
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 257.076
(5906169856, 42481549312)
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.2192
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 250.240
Elapsed time for mlp_fused_gelu (2048x4x115712): 0.0033
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.2286
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 239.858
Elapsed time for transformer_add_bias_dropout (2048x4x28928): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28928): 0.0012

Attention duration (in seconds): 0.2683
Attention throughput (in TFLOP/s): 211.622
MLP duration (in seconds): 0.4511
MLP throughput (in TFLOP/s): 243.156
Transformer duration (in seconds): 0.7260
Transformer throughput (in TFLOP/s): 229.305
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28992x86976, b=2048): 0.1644
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28992x86976, b=2048): 251.362
Elapsed time for attention_key_query_prob (256x2048x453x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x453x2048): 76.001
Elapsed time for attention_prob_times_values (256x2048x2048x453): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x453): 81.538
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25678118912, 42481549312)
Elapsed time for attention_linear_projection (4x28992x28992, b=2048): 0.0539
Throughput (in TFLOP/s) for attention_linear_projection (4x28992x28992, b=2048): 255.652
(25678118912, 42481549312)
Elapsed time for mlp_h_to_4h (4x28992x115968, b=2048): 0.2203
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28992x115968, b=2048): 250.054
Elapsed time for mlp_fused_gelu (2048x4x115968): 0.0033
Elapsed time for mlp_4h_to_h (4x115968x28992, b=2048): 0.2306
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115968x28992, b=2048): 238.888
Elapsed time for transformer_add_bias_dropout (2048x4x28992): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28992): 0.0012

Attention duration (in seconds): 0.2784
Attention throughput (in TFLOP/s): 204.878
MLP duration (in seconds): 0.4542
MLP throughput (in TFLOP/s): 242.579
Transformer duration (in seconds): 0.7392
Transformer throughput (in TFLOP/s): 226.208
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.1652
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 251.173
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 109.714
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 131.976
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18952552448, 42481549312)
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 259.592
(18952552448, 42481549312)
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.2212
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 250.143
Elapsed time for mlp_fused_gelu (2048x4x116224): 0.0033
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.2309
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 239.611
Elapsed time for transformer_add_bias_dropout (2048x4x29056): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29056): 0.0012

Attention duration (in seconds): 0.2702
Attention throughput (in TFLOP/s): 212.010
MLP duration (in seconds): 0.4554
MLP throughput (in TFLOP/s): 242.996
Transformer duration (in seconds): 0.7321
Transformer throughput (in TFLOP/s): 229.379
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29120x87360, b=2048): 0.1657
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29120x87360, b=2048): 251.510
Elapsed time for attention_key_query_prob (256x2048x455x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x455x2048): 75.300
Elapsed time for attention_prob_times_values (256x2048x2048x455): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x455): 79.143
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(12197625856, 42481549312)
Elapsed time for attention_linear_projection (4x29120x29120, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_linear_projection (4x29120x29120, b=2048): 256.093
(12197625856, 42481549312)
Elapsed time for mlp_h_to_4h (4x29120x116480, b=2048): 0.2223
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29120x116480, b=2048): 250.026
Elapsed time for mlp_fused_gelu (2048x4x116480): 0.0033
Elapsed time for mlp_4h_to_h (4x116480x29120, b=2048): 0.2324
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116480x29120, b=2048): 239.141
Elapsed time for transformer_add_bias_dropout (2048x4x29120): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29120): 0.0012

Attention duration (in seconds): 0.2807
Attention throughput (in TFLOP/s): 204.952
MLP duration (in seconds): 0.4579
MLP throughput (in TFLOP/s): 242.703
Transformer duration (in seconds): 0.7453
Transformer throughput (in TFLOP/s): 226.318
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.1665
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 251.457
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 187.830
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 211.160
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(5413339136, 42481549312)
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0544
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 256.384
(5413339136, 42481549312)
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.2235
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 249.788
Elapsed time for mlp_fused_gelu (2048x4x116736): 0.0033
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.2323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 240.302
Elapsed time for transformer_add_bias_dropout (2048x4x29184): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29184): 0.0012

Attention duration (in seconds): 0.2662
Attention throughput (in TFLOP/s): 217.072
MLP duration (in seconds): 0.4590
MLP throughput (in TFLOP/s): 243.188
Transformer duration (in seconds): 0.7318
Transformer throughput (in TFLOP/s): 231.485
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29248x87744, b=2048): 0.1679
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29248x87744, b=2048): 250.373
Elapsed time for attention_key_query_prob (256x2048x457x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x457x2048): 75.333
Elapsed time for attention_prob_times_values (256x2048x2048x457): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x457): 78.705
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25539706880, 42481549312)
Elapsed time for attention_linear_projection (4x29248x29248, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_linear_projection (4x29248x29248, b=2048): 256.465
(25539706880, 42481549312)
Elapsed time for mlp_h_to_4h (4x29248x116992, b=2048): 0.2250
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29248x116992, b=2048): 249.134
Elapsed time for mlp_fused_gelu (2048x4x116992): 0.0033
Elapsed time for mlp_4h_to_h (4x116992x29248, b=2048): 0.2341
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116992x29248, b=2048): 239.455
Elapsed time for transformer_add_bias_dropout (2048x4x29248): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29248): 0.0012

Attention duration (in seconds): 0.2835
Attention throughput (in TFLOP/s): 204.659
MLP duration (in seconds): 0.4625
MLP throughput (in TFLOP/s): 242.447
Transformer duration (in seconds): 0.7527
Transformer throughput (in TFLOP/s): 226.062
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.1680
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 251.338
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 110.349
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 131.660
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(18694602752, 42481549312)
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 257.683
(18694602752, 42481549312)
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.2265
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 248.616
Elapsed time for mlp_fused_gelu (2048x4x117248): 0.0033
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2349
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 239.733
Elapsed time for transformer_add_bias_dropout (2048x4x29312): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29312): 0.0012

Attention duration (in seconds): 0.2745
Attention throughput (in TFLOP/s): 212.317
MLP duration (in seconds): 0.4647
MLP throughput (in TFLOP/s): 242.350
Transformer duration (in seconds): 0.7458
Transformer throughput (in TFLOP/s): 229.138
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29376x88128, b=2048): 0.1683
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29376x88128, b=2048): 252.035
Elapsed time for attention_key_query_prob (256x2048x459x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x459x2048): 76.327
Elapsed time for attention_prob_times_values (256x2048x2048x459): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x459): 82.133
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(11820138496, 42481549312)
Elapsed time for attention_linear_projection (4x29376x29376, b=2048): 0.0552
Throughput (in TFLOP/s) for attention_linear_projection (4x29376x29376, b=2048): 256.272
(11820138496, 42481549312)
Elapsed time for mlp_h_to_4h (4x29376x117504, b=2048): 0.2258
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29376x117504, b=2048): 250.494
Elapsed time for mlp_fused_gelu (2048x4x117504): 0.0033
Elapsed time for mlp_4h_to_h (4x117504x29376, b=2048): 0.2367
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117504x29376, b=2048): 238.894
Elapsed time for transformer_add_bias_dropout (2048x4x29376): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29376): 0.0012

Attention duration (in seconds): 0.2838
Attention throughput (in TFLOP/s): 206.205
MLP duration (in seconds): 0.4658
MLP throughput (in TFLOP/s): 242.811
Transformer duration (in seconds): 0.7564
Transformer throughput (in TFLOP/s): 226.922
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.1697
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 251.062
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 110.997
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 132.603
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(4916314112, 42481549312)
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 256.150
(4916314112, 42481549312)
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.2269
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 250.379
Elapsed time for mlp_fused_gelu (2048x4x117760): 0.0033
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2370
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 239.707
Elapsed time for transformer_add_bias_dropout (2048x4x29440): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29440): 0.0012

Attention duration (in seconds): 0.2769
Attention throughput (in TFLOP/s): 212.260
MLP duration (in seconds): 0.4672
MLP throughput (in TFLOP/s): 243.175
Transformer duration (in seconds): 0.7507
Transformer throughput (in TFLOP/s): 229.610
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29504x88512, b=2048): 0.1702
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29504x88512, b=2048): 251.389
Elapsed time for attention_key_query_prob (256x2048x461x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x461x2048): 76.016
Elapsed time for attention_prob_times_values (256x2048x2048x461): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x461): 82.340
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25397100544, 42481549312)
Elapsed time for attention_linear_projection (4x29504x29504, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_linear_projection (4x29504x29504, b=2048): 255.349
(25397100544, 42481549312)
Elapsed time for mlp_h_to_4h (4x29504x118016, b=2048): 0.2281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29504x118016, b=2048): 250.066
Elapsed time for mlp_fused_gelu (2048x4x118016): 0.0033
Elapsed time for mlp_4h_to_h (4x118016x29504, b=2048): 0.2386
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118016x29504, b=2048): 239.089
Elapsed time for transformer_add_bias_dropout (2048x4x29504): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29504): 0.0012

Attention duration (in seconds): 0.2865
Attention throughput (in TFLOP/s): 206.044
MLP duration (in seconds): 0.4701
MLP throughput (in TFLOP/s): 242.713
Transformer duration (in seconds): 0.7633
Transformer throughput (in TFLOP/s): 226.805
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.1710
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 251.284
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 111.221
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 132.873
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18432458752, 42481549312)
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 256.099
(18432458752, 42481549312)
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.2293
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 249.873
Elapsed time for mlp_fused_gelu (2048x4x118272): 0.0033
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2394
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 239.283
Elapsed time for transformer_add_bias_dropout (2048x4x29568): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29568): 0.0012

Attention duration (in seconds): 0.2787
Attention throughput (in TFLOP/s): 212.679
MLP duration (in seconds): 0.4721
MLP throughput (in TFLOP/s): 242.730
Transformer duration (in seconds): 0.7576
Transformer throughput (in TFLOP/s): 229.514
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29632x88896, b=2048): 0.1724
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29632x88896, b=2048): 250.276
Elapsed time for attention_key_query_prob (256x2048x463x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x463x2048): 75.830
Elapsed time for attention_prob_times_values (256x2048x2048x463): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x463): 80.650
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(11436359680, 42481549312)
Elapsed time for attention_linear_projection (4x29632x29632, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_linear_projection (4x29632x29632, b=2048): 256.017
(11436359680, 42481549312)
Elapsed time for mlp_h_to_4h (4x29632x118528, b=2048): 0.2302
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29632x118528, b=2048): 250.008
Elapsed time for mlp_fused_gelu (2048x4x118528): 0.0034
Elapsed time for mlp_4h_to_h (4x118528x29632, b=2048): 0.2414
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118528x29632, b=2048): 238.409
Elapsed time for transformer_add_bias_dropout (2048x4x29632): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29632): 0.0012

Attention duration (in seconds): 0.2895
Attention throughput (in TFLOP/s): 205.661
MLP duration (in seconds): 0.4749
MLP throughput (in TFLOP/s): 242.344
Transformer duration (in seconds): 0.7711
Transformer throughput (in TFLOP/s): 226.446
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.1714
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 252.874
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 197.316
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 216.613
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(4410900480, 42481549312)
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 256.888
(4410900480, 42481549312)
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.2323
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 248.803
Elapsed time for mlp_fused_gelu (2048x4x118784): 0.0034
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2412
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 239.569
Elapsed time for transformer_add_bias_dropout (2048x4x29696): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29696): 0.0012

Attention duration (in seconds): 0.2727
Attention throughput (in TFLOP/s): 219.241
MLP duration (in seconds): 0.4769
MLP throughput (in TFLOP/s): 242.379
Transformer duration (in seconds): 0.7563
Transformer throughput (in TFLOP/s): 231.872
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29760x89280, b=2048): 0.1733
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29760x89280, b=2048): 251.234
Elapsed time for attention_key_query_prob (256x2048x465x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x465x2048): 76.177
Elapsed time for attention_prob_times_values (256x2048x2048x465): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x465): 80.702
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(25256591360, 42481549312)
Elapsed time for attention_linear_projection (4x29760x29760, b=2048): 0.0569
Throughput (in TFLOP/s) for attention_linear_projection (4x29760x29760, b=2048): 255.099
(25256591360, 42481549312)
Elapsed time for mlp_h_to_4h (4x29760x119040, b=2048): 0.2330
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29760x119040, b=2048): 249.138
Elapsed time for mlp_fused_gelu (2048x4x119040): 0.0034
Elapsed time for mlp_4h_to_h (4x119040x29760, b=2048): 0.2431
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119040x29760, b=2048): 238.723
Elapsed time for transformer_add_bias_dropout (2048x4x29760): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29760): 0.0012

Attention duration (in seconds): 0.2911
Attention throughput (in TFLOP/s): 206.276
MLP duration (in seconds): 0.4795
MLP throughput (in TFLOP/s): 242.101
Transformer duration (in seconds): 0.7774
Transformer throughput (in TFLOP/s): 226.561
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.1735
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 251.918
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 111.941
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 134.088
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(18170314752, 42481549312)
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0565
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 257.901
(18170314752, 42481549312)
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.2331
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 250.102
Elapsed time for mlp_fused_gelu (2048x4x119296): 0.0034
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.2439
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 238.997
Elapsed time for transformer_add_bias_dropout (2048x4x29824): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29824): 0.0012

Attention duration (in seconds): 0.2819
Attention throughput (in TFLOP/s): 213.915
MLP duration (in seconds): 0.4804
MLP throughput (in TFLOP/s): 242.700
Transformer duration (in seconds): 0.7690
Transformer throughput (in TFLOP/s): 230.004
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29888x89664, b=2048): 0.1743
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29888x89664, b=2048): 251.951
Elapsed time for attention_key_query_prob (256x2048x467x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x467x2048): 76.314
Elapsed time for attention_prob_times_values (256x2048x2048x467): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x467): 83.125
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(11052580864, 42481549312)
Elapsed time for attention_linear_projection (4x29888x29888, b=2048): 0.0573
Throughput (in TFLOP/s) for attention_linear_projection (4x29888x29888, b=2048): 255.229
(11052580864, 42481549312)
Elapsed time for mlp_h_to_4h (4x29888x119552, b=2048): 0.2354
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29888x119552, b=2048): 248.726
Elapsed time for mlp_fused_gelu (2048x4x119552): 0.0034
Elapsed time for mlp_4h_to_h (4x119552x29888, b=2048): 0.2451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119552x29888, b=2048): 238.813
Elapsed time for transformer_add_bias_dropout (2048x4x29888): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29888): 0.0012

Attention duration (in seconds): 0.2922
Attention throughput (in TFLOP/s): 207.188
MLP duration (in seconds): 0.4839
MLP throughput (in TFLOP/s): 241.964
Transformer duration (in seconds): 0.7830
Transformer throughput (in TFLOP/s): 226.870
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.1758
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 250.788
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 112.768
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 134.672
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3905486848, 42481549312)
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0565
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 259.960
(3905486848, 42481549312)
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.2368
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 248.254
Elapsed time for mlp_fused_gelu (2048x4x119808): 0.0034
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.2450
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 239.977
Elapsed time for transformer_add_bias_dropout (2048x4x29952): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29952): 0.0012

Attention duration (in seconds): 0.2841
Attention throughput (in TFLOP/s): 213.995
MLP duration (in seconds): 0.4852
MLP throughput (in TFLOP/s): 242.337
Transformer duration (in seconds): 0.7761
Transformer throughput (in TFLOP/s): 229.844
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30016x90048, b=2048): 0.1763
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30016x90048, b=2048): 251.171
Elapsed time for attention_key_query_prob (256x2048x469x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x469x2048): 76.563
Elapsed time for attention_prob_times_values (256x2048x2048x469): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x469): 83.264
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(25111887872, 42481549312)
Elapsed time for attention_linear_projection (4x30016x30016, b=2048): 0.0577
Throughput (in TFLOP/s) for attention_linear_projection (4x30016x30016, b=2048): 255.856
(25111887872, 42481549312)
Elapsed time for mlp_h_to_4h (4x30016x120064, b=2048): 0.2366
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30016x120064, b=2048): 249.540
Elapsed time for mlp_fused_gelu (2048x4x120064): 0.0034
Elapsed time for mlp_4h_to_h (4x120064x30016, b=2048): 0.2471
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120064x30016, b=2048): 238.928
Elapsed time for transformer_add_bias_dropout (2048x4x30016): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30016): 0.0012

Attention duration (in seconds): 0.2947
Attention throughput (in TFLOP/s): 207.215
MLP duration (in seconds): 0.4871
MLP throughput (in TFLOP/s): 242.416
Transformer duration (in seconds): 0.7887
Transformer throughput (in TFLOP/s): 227.160
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.1769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 251.379
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 112.938
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 135.468
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17903976448, 42481549312)
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 256.083
(17903976448, 42481549312)
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.2386
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 248.499
Elapsed time for mlp_fused_gelu (2048x4x120320): 0.0034
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2480
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 239.137
Elapsed time for transformer_add_bias_dropout (2048x4x30080): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30080): 0.0012

Attention duration (in seconds): 0.2866
Attention throughput (in TFLOP/s): 213.939
MLP duration (in seconds): 0.4900
MLP throughput (in TFLOP/s): 242.031
Transformer duration (in seconds): 0.7834
Transformer throughput (in TFLOP/s): 229.652
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30144x90432, b=2048): 0.1780
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30144x90432, b=2048): 250.884
Elapsed time for attention_key_query_prob (256x2048x471x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x471x2048): 76.423
Elapsed time for attention_prob_times_values (256x2048x2048x471): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x471): 81.605
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(10664607744, 42481549312)
Elapsed time for attention_linear_projection (4x30144x30144, b=2048): 0.0580
Throughput (in TFLOP/s) for attention_linear_projection (4x30144x30144, b=2048): 256.547
(10664607744, 42481549312)
Elapsed time for mlp_h_to_4h (4x30144x120576, b=2048): 0.2384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30144x120576, b=2048): 249.840
Elapsed time for mlp_fused_gelu (2048x4x120576): 0.0034
Elapsed time for mlp_4h_to_h (4x120576x30144, b=2048): 0.2484
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120576x30144, b=2048): 239.712
Elapsed time for transformer_add_bias_dropout (2048x4x30144): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30144): 0.0012

Attention duration (in seconds): 0.2971
Attention throughput (in TFLOP/s): 207.254
MLP duration (in seconds): 0.4902
MLP throughput (in TFLOP/s): 242.968
Transformer duration (in seconds): 0.7942
Transformer throughput (in TFLOP/s): 227.500
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.1777
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 252.390
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 191.160
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 218.255
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(3393781760, 42481549312)
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0583
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 256.652
(3393781760, 42481549312)
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.2394
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 249.851
Elapsed time for mlp_fused_gelu (2048x4x120832): 0.0034
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2492
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 239.937
Elapsed time for transformer_add_bias_dropout (2048x4x30208): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30208): 0.0012

Attention duration (in seconds): 0.2813
Attention throughput (in TFLOP/s): 219.789
MLP duration (in seconds): 0.4920
MLP throughput (in TFLOP/s): 243.093
Transformer duration (in seconds): 0.7802
Transformer throughput (in TFLOP/s): 232.552
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30272x90816, b=2048): 0.1798
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30272x90816, b=2048): 250.559
Elapsed time for attention_key_query_prob (256x2048x473x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x473x2048): 76.443
Elapsed time for attention_prob_times_values (256x2048x2048x473): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x473): 81.534
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24969281536, 42481549312)
Elapsed time for attention_linear_projection (4x30272x30272, b=2048): 0.0590
Throughput (in TFLOP/s) for attention_linear_projection (4x30272x30272, b=2048): 254.679
(24969281536, 42481549312)
Elapsed time for mlp_h_to_4h (4x30272x121088, b=2048): 0.2420
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30272x121088, b=2048): 248.162
Elapsed time for mlp_fused_gelu (2048x4x121088): 0.0034
Elapsed time for mlp_4h_to_h (4x121088x30272, b=2048): 0.2508
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121088x30272, b=2048): 239.474
Elapsed time for transformer_add_bias_dropout (2048x4x30272): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30272): 0.0013

Attention duration (in seconds): 0.2999
Attention throughput (in TFLOP/s): 207.048
MLP duration (in seconds): 0.4962
MLP throughput (in TFLOP/s): 242.058
Transformer duration (in seconds): 0.8030
Transformer throughput (in TFLOP/s): 226.889
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.1801
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 251.189
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 113.699
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 136.351
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17637638144, 42481549312)
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0591
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 255.262
(17637638144, 42481549312)
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.2419
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 249.338
Elapsed time for mlp_fused_gelu (2048x4x121344): 0.0034
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2515
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 239.801
Elapsed time for transformer_add_bias_dropout (2048x4x30336): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30336): 0.0012

Attention duration (in seconds): 0.2910
Attention throughput (in TFLOP/s): 214.273
MLP duration (in seconds): 0.4968
MLP throughput (in TFLOP/s): 242.788
Transformer duration (in seconds): 0.7947
Transformer throughput (in TFLOP/s): 230.242
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30400x91200, b=2048): 0.1813
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30400x91200, b=2048): 250.616
Elapsed time for attention_key_query_prob (256x2048x475x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x475x2048): 76.736
Elapsed time for attention_prob_times_values (256x2048x2048x475): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x475): 84.602
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(10274537472, 42481549312)
Elapsed time for attention_linear_projection (4x30400x30400, b=2048): 0.0594
Throughput (in TFLOP/s) for attention_linear_projection (4x30400x30400, b=2048): 254.974
(10274537472, 42481549312)
Elapsed time for mlp_h_to_4h (4x30400x121600, b=2048): 0.2436
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30400x121600, b=2048): 248.585
Elapsed time for mlp_fused_gelu (2048x4x121600): 0.0034
Elapsed time for mlp_4h_to_h (4x121600x30400, b=2048): 0.2525
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121600x30400, b=2048): 239.870
Elapsed time for transformer_add_bias_dropout (2048x4x30400): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30400): 0.0013

Attention duration (in seconds): 0.3014
Attention throughput (in TFLOP/s): 207.722
MLP duration (in seconds): 0.4996
MLP throughput (in TFLOP/s): 242.469
Transformer duration (in seconds): 0.8079
Transformer throughput (in TFLOP/s): 227.425
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.1819
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 250.717
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 114.380
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 137.334
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2879979520, 42481549312)
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0595
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 255.520
(2879979520, 42481549312)
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.2432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 250.039
Elapsed time for mlp_fused_gelu (2048x4x121856): 0.0034
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 242.190
Elapsed time for transformer_add_bias_dropout (2048x4x30464): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30464): 0.0012

Attention duration (in seconds): 0.2932
Attention throughput (in TFLOP/s): 214.387
MLP duration (in seconds): 0.4978
MLP throughput (in TFLOP/s): 244.347
Transformer duration (in seconds): 0.7980
Transformer throughput (in TFLOP/s): 231.224
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30528x91584, b=2048): 0.1825
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30528x91584, b=2048): 250.986
Elapsed time for attention_key_query_prob (256x2048x477x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x477x2048): 77.008
Elapsed time for attention_prob_times_values (256x2048x2048x477): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x477): 85.054
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24822480896, 42481549312)
Elapsed time for attention_linear_projection (4x30528x30528, b=2048): 0.0597
Throughput (in TFLOP/s) for attention_linear_projection (4x30528x30528, b=2048): 255.615
(24822480896, 42481549312)
Elapsed time for mlp_h_to_4h (4x30528x122112, b=2048): 0.2446
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30528x122112, b=2048): 249.708
Elapsed time for mlp_fused_gelu (2048x4x122112): 0.0035
Elapsed time for mlp_4h_to_h (4x122112x30528, b=2048): 0.2548
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122112x30528, b=2048): 239.698
Elapsed time for transformer_add_bias_dropout (2048x4x30528): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30528): 0.0013

Attention duration (in seconds): 0.3030
Attention throughput (in TFLOP/s): 208.337
MLP duration (in seconds): 0.5029
MLP throughput (in TFLOP/s): 242.915
Transformer duration (in seconds): 0.8128
Transformer throughput (in TFLOP/s): 227.938
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.1832
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 251.034
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 114.842
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 138.479
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17365008384, 42481549312)
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0597
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 256.721
(17365008384, 42481549312)
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.2457
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 249.599
Elapsed time for mlp_fused_gelu (2048x4x122368): 0.0035
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2546
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 240.943
Elapsed time for transformer_add_bias_dropout (2048x4x30592): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30592): 0.0013

Attention duration (in seconds): 0.2947
Attention throughput (in TFLOP/s): 215.052
MLP duration (in seconds): 0.5038
MLP throughput (in TFLOP/s): 243.506
Transformer duration (in seconds): 0.8055
Transformer throughput (in TFLOP/s): 230.983
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30656x91968, b=2048): 0.1835
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30656x91968, b=2048): 251.758
Elapsed time for attention_key_query_prob (256x2048x479x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x479x2048): 77.822
Elapsed time for attention_prob_times_values (256x2048x2048x479): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x479): 83.983
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(9876078592, 42481549312)
Elapsed time for attention_linear_projection (4x30656x30656, b=2048): 0.0603
Throughput (in TFLOP/s) for attention_linear_projection (4x30656x30656, b=2048): 255.430
(9876078592, 42481549312)
Elapsed time for mlp_h_to_4h (4x30656x122624, b=2048): 0.2479
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30656x122624, b=2048): 248.488
Elapsed time for mlp_fused_gelu (2048x4x122624): 0.0035
Elapsed time for mlp_4h_to_h (4x122624x30656, b=2048): 0.2565
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122624x30656, b=2048): 240.149
Elapsed time for transformer_add_bias_dropout (2048x4x30656): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30656): 0.0013

Attention duration (in seconds): 0.3047
Attention throughput (in TFLOP/s): 208.912
MLP duration (in seconds): 0.5078
MLP throughput (in TFLOP/s): 242.579
Transformer duration (in seconds): 0.8195
Transformer throughput (in TFLOP/s): 227.985
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.1844
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 251.602
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 206.735
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 224.025
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(2355691520, 42481549312)
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0603
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 256.506
(2355691520, 42481549312)
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.2471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 250.281
Elapsed time for mlp_fused_gelu (2048x4x122880): 0.0035
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2570
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 240.679
Elapsed time for transformer_add_bias_dropout (2048x4x30720): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30720): 0.0013

Attention duration (in seconds): 0.2897
Attention throughput (in TFLOP/s): 220.639
MLP duration (in seconds): 0.5076
MLP throughput (in TFLOP/s): 243.706
Transformer duration (in seconds): 0.8042
Transformer throughput (in TFLOP/s): 233.282
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30784x92352, b=2048): 0.1857
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30784x92352, b=2048): 250.764
Elapsed time for attention_key_query_prob (256x2048x481x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x481x2048): 76.421
Elapsed time for attention_prob_times_values (256x2048x2048x481): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x481): 83.466
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24677777408, 42481549312)
Elapsed time for attention_linear_projection (4x30784x30784, b=2048): 0.0607
Throughput (in TFLOP/s) for attention_linear_projection (4x30784x30784, b=2048): 255.889
(24677777408, 42481549312)
Elapsed time for mlp_h_to_4h (4x30784x123136, b=2048): 0.2489
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30784x123136, b=2048): 249.559
Elapsed time for mlp_fused_gelu (2048x4x123136): 0.0035
Elapsed time for mlp_4h_to_h (4x123136x30784, b=2048): 0.2584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123136x30784, b=2048): 240.324
Elapsed time for transformer_add_bias_dropout (2048x4x30784): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30784): 0.0013

Attention duration (in seconds): 0.3077
Attention throughput (in TFLOP/s): 208.543
MLP duration (in seconds): 0.5108
MLP throughput (in TFLOP/s): 243.183
Transformer duration (in seconds): 0.8255
Transformer throughput (in TFLOP/s): 228.202
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.1867
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 250.479
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 116.014
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 139.966
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(17094475776, 42481549312)
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0610
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 255.705
(17094475776, 42481549312)
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.2498
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 249.701
Elapsed time for mlp_fused_gelu (2048x4x123392): 0.0035
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.2584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 241.305
Elapsed time for transformer_add_bias_dropout (2048x4x30848): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30848): 0.0012

Attention duration (in seconds): 0.2994
Attention throughput (in TFLOP/s): 215.189
MLP duration (in seconds): 0.5117
MLP throughput (in TFLOP/s): 243.755
Transformer duration (in seconds): 0.8181
Transformer throughput (in TFLOP/s): 231.222
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30912x92736, b=2048): 0.1878
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30912x92736, b=2048): 250.135
Elapsed time for attention_key_query_prob (256x2048x483x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x483x2048): 75.755
Elapsed time for attention_prob_times_values (256x2048x2048x483): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x483): 85.820
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(9479716864, 42481549312)
Elapsed time for attention_linear_projection (4x30912x30912, b=2048): 0.0610
Throughput (in TFLOP/s) for attention_linear_projection (4x30912x30912, b=2048): 256.718
(9479716864, 42481549312)
Elapsed time for mlp_h_to_4h (4x30912x123648, b=2048): 0.2515
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30912x123648, b=2048): 249.010
Elapsed time for mlp_fused_gelu (2048x4x123648): 0.0035
Elapsed time for mlp_4h_to_h (4x123648x30912, b=2048): 0.2608
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123648x30912, b=2048): 240.157
Elapsed time for transformer_add_bias_dropout (2048x4x30912): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30912): 0.0013

Attention duration (in seconds): 0.3099
Attention throughput (in TFLOP/s): 208.745
MLP duration (in seconds): 0.5157
MLP throughput (in TFLOP/s): 242.844
Transformer duration (in seconds): 0.8327
Transformer throughput (in TFLOP/s): 228.098
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.1880
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 250.826
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 116.473
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 140.028
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1833500672, 42481549312)
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 256.923
(1833500672, 42481549312)
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.2518
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 249.743
Elapsed time for mlp_fused_gelu (2048x4x123904): 0.0035
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.2588
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 243.015
Elapsed time for transformer_add_bias_dropout (2048x4x30976): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30976): 0.0012

Attention duration (in seconds): 0.3010
Attention throughput (in TFLOP/s): 215.842
MLP duration (in seconds): 0.5141
MLP throughput (in TFLOP/s): 244.653
Transformer duration (in seconds): 0.8220
Transformer throughput (in TFLOP/s): 232.019
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31040x93120, b=2048): 0.1882
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31040x93120, b=2048): 251.648
Elapsed time for attention_key_query_prob (256x2048x485x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x485x2048): 76.114
Elapsed time for attention_prob_times_values (256x2048x2048x485): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x485): 86.006
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24528879616, 42481549312)
Elapsed time for attention_linear_projection (4x31040x31040, b=2048): 0.0615
Throughput (in TFLOP/s) for attention_linear_projection (4x31040x31040, b=2048): 256.849
(24528879616, 42481549312)
Elapsed time for mlp_h_to_4h (4x31040x124160, b=2048): 0.2530
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31040x124160, b=2048): 249.569
Elapsed time for mlp_fused_gelu (2048x4x124160): 0.0035
Elapsed time for mlp_4h_to_h (4x124160x31040, b=2048): 0.2627
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124160x31040, b=2048): 240.374
Elapsed time for transformer_add_bias_dropout (2048x4x31040): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31040): 0.0013

Attention duration (in seconds): 0.3108
Attention throughput (in TFLOP/s): 209.831
MLP duration (in seconds): 0.5192
MLP throughput (in TFLOP/s): 243.229
Transformer duration (in seconds): 0.8372
Transformer throughput (in TFLOP/s): 228.765
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x93312, b=2048): 0.1888
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x93312, b=2048): 251.851
Elapsed time for attention_key_query_prob (256x2048x486x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x486x2048): 116.839
Elapsed time for attention_prob_times_values (256x2048x2048x486): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x486): 140.448
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(16819748864, 42481549312)
Elapsed time for attention_linear_projection (4x31104x31104, b=2048): 0.0616
Throughput (in TFLOP/s) for attention_linear_projection (4x31104x31104, b=2048): 257.349
(16819748864, 42481549312)
Elapsed time for mlp_h_to_4h (4x31104x124416, b=2048): 0.2546
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x124416, b=2048): 249.039
Elapsed time for mlp_fused_gelu (2048x4x124416): 0.0035
Elapsed time for mlp_4h_to_h (4x124416x31104, b=2048): 0.2627
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124416x31104, b=2048): 241.364
Elapsed time for transformer_add_bias_dropout (2048x4x31104): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31104): 0.0013

Attention duration (in seconds): 0.3022
Attention throughput (in TFLOP/s): 216.723
MLP duration (in seconds): 0.5208
MLP throughput (in TFLOP/s): 243.478
Transformer duration (in seconds): 0.8301
Transformer throughput (in TFLOP/s): 231.659
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31168x93504, b=2048): 0.1905
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31168x93504, b=2048): 250.696
Elapsed time for attention_key_query_prob (256x2048x487x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x487x2048): 75.819
Elapsed time for attention_prob_times_values (256x2048x2048x487): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x487): 84.049
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(9079160832, 42481549312)
Elapsed time for attention_linear_projection (4x31168x31168, b=2048): 0.0623
Throughput (in TFLOP/s) for attention_linear_projection (4x31168x31168, b=2048): 255.434
(9079160832, 42481549312)
Elapsed time for mlp_h_to_4h (4x31168x124672, b=2048): 0.2549
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31168x124672, b=2048): 249.786
Elapsed time for mlp_fused_gelu (2048x4x124672): 0.0035
Elapsed time for mlp_4h_to_h (4x124672x31168, b=2048): 0.2655
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124672x31168, b=2048): 239.788
Elapsed time for transformer_add_bias_dropout (2048x4x31168): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31168): 0.0013

Attention duration (in seconds): 0.3144
Attention throughput (in TFLOP/s): 209.124
MLP duration (in seconds): 0.5239
MLP throughput (in TFLOP/s): 243.035
Transformer duration (in seconds): 0.8455
Transformer throughput (in TFLOP/s): 228.370
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x93696, b=2048): 0.1914
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x93696, b=2048): 250.523
Elapsed time for attention_key_query_prob (256x2048x488x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x488x2048): 191.517
Elapsed time for attention_prob_times_values (256x2048x2048x488): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x488): 222.344
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(1307115520, 42481549312)
Elapsed time for attention_linear_projection (4x31232x31232, b=2048): 0.0625
Throughput (in TFLOP/s) for attention_linear_projection (4x31232x31232, b=2048): 255.865
(1307115520, 42481549312)
Elapsed time for mlp_h_to_4h (4x31232x124928, b=2048): 0.2576
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x124928, b=2048): 248.136
Elapsed time for mlp_fused_gelu (2048x4x124928): 0.0035
Elapsed time for mlp_4h_to_h (4x124928x31232, b=2048): 0.2654
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124928x31232, b=2048): 240.872
Elapsed time for transformer_add_bias_dropout (2048x4x31232): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31232): 0.0013

Attention duration (in seconds): 0.2994
Attention throughput (in TFLOP/s): 220.479
MLP duration (in seconds): 0.5266
MLP throughput (in TFLOP/s): 242.811
Transformer duration (in seconds): 0.8331
Transformer throughput (in TFLOP/s): 232.720
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31296x93888, b=2048): 0.1923
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31296x93888, b=2048): 250.374
Elapsed time for attention_key_query_prob (256x2048x489x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x489x2048): 76.198
Elapsed time for attention_prob_times_values (256x2048x2048x489): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x489): 84.091
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24382078976, 42481549312)
Elapsed time for attention_linear_projection (4x31296x31296, b=2048): 0.0627
Throughput (in TFLOP/s) for attention_linear_projection (4x31296x31296, b=2048): 256.027
(24382078976, 42481549312)
Elapsed time for mlp_h_to_4h (4x31296x125184, b=2048): 0.2568
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31296x125184, b=2048): 249.968
Elapsed time for mlp_fused_gelu (2048x4x125184): 0.0035
Elapsed time for mlp_4h_to_h (4x125184x31296, b=2048): 0.2663
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125184x31296, b=2048): 241.016
Elapsed time for transformer_add_bias_dropout (2048x4x31296): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31296): 0.0013

Attention duration (in seconds): 0.3166
Attention throughput (in TFLOP/s): 209.351
MLP duration (in seconds): 0.5267
MLP throughput (in TFLOP/s): 243.761
Transformer duration (in seconds): 0.8504
Transformer throughput (in TFLOP/s): 228.899
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x94080, b=2048): 0.1933
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x94080, b=2048): 250.110
Elapsed time for attention_key_query_prob (256x2048x490x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x490x2048): 117.616
Elapsed time for attention_prob_times_values (256x2048x2048x490): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x490): 141.773
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(16545021952, 42481549312)
Elapsed time for attention_linear_projection (4x31360x31360, b=2048): 0.0630
Throughput (in TFLOP/s) for attention_linear_projection (4x31360x31360, b=2048): 255.630
(16545021952, 42481549312)
Elapsed time for mlp_h_to_4h (4x31360x125440, b=2048): 0.2589
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x125440, b=2048): 248.938
Elapsed time for mlp_fused_gelu (2048x4x125440): 0.0036
Elapsed time for mlp_4h_to_h (4x125440x31360, b=2048): 0.2679
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125440x31360, b=2048): 240.577
Elapsed time for transformer_add_bias_dropout (2048x4x31360): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31360): 0.0013

Attention duration (in seconds): 0.3081
Attention throughput (in TFLOP/s): 216.020
MLP duration (in seconds): 0.5304
MLP throughput (in TFLOP/s): 243.047
Transformer duration (in seconds): 0.8456
Transformer throughput (in TFLOP/s): 231.156
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31424x94272, b=2048): 0.1944
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31424x94272, b=2048): 249.619
Elapsed time for attention_key_query_prob (256x2048x491x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x491x2048): 77.173
Elapsed time for attention_prob_times_values (256x2048x2048x491): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x491): 87.242
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8676507648, 42481549312)
Elapsed time for attention_linear_projection (4x31424x31424, b=2048): 0.0635
Throughput (in TFLOP/s) for attention_linear_projection (4x31424x31424, b=2048): 254.708
(8676507648, 42481549312)
Elapsed time for mlp_h_to_4h (4x31424x125696, b=2048): 0.2610
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31424x125696, b=2048): 247.920
Elapsed time for mlp_fused_gelu (2048x4x125696): 0.0036
Elapsed time for mlp_4h_to_h (4x125696x31424, b=2048): 0.2691
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125696x31424, b=2048): 240.463
Elapsed time for transformer_add_bias_dropout (2048x4x31424): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31424): 0.0013

Attention duration (in seconds): 0.3191
Attention throughput (in TFLOP/s): 209.392
MLP duration (in seconds): 0.5337
MLP throughput (in TFLOP/s): 242.509
Transformer duration (in seconds): 0.8600
Transformer throughput (in TFLOP/s): 228.201
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x94464, b=2048): 0.1946
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x94464, b=2048): 250.426
Elapsed time for attention_key_query_prob (256x2048x492x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x492x2048): 118.744
Elapsed time for attention_prob_times_values (256x2048x2048x492): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x492): 142.911
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(776536064, 42481549312)
Elapsed time for attention_linear_projection (4x31488x31488, b=2048): 0.0634
Throughput (in TFLOP/s) for attention_linear_projection (4x31488x31488, b=2048): 256.407
(776536064, 42481549312)
Elapsed time for mlp_h_to_4h (4x31488x125952, b=2048): 0.2610
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x125952, b=2048): 248.955
Elapsed time for mlp_fused_gelu (2048x4x125952): 0.0036
Elapsed time for mlp_4h_to_h (4x125952x31488, b=2048): 0.2691
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125952x31488, b=2048): 241.500
Elapsed time for transformer_add_bias_dropout (2048x4x31488): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31488): 0.0013

Attention duration (in seconds): 0.3097
Attention throughput (in TFLOP/s): 216.656
MLP duration (in seconds): 0.5336
MLP throughput (in TFLOP/s): 243.535
Transformer duration (in seconds): 0.8504
Transformer throughput (in TFLOP/s): 231.710
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31552x94656, b=2048): 0.1956
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31552x94656, b=2048): 250.117
Elapsed time for attention_key_query_prob (256x2048x493x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x493x2048): 77.934
Elapsed time for attention_prob_times_values (256x2048x2048x493): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x493): 87.313
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24231084032, 42481549312)
Elapsed time for attention_linear_projection (4x31552x31552, b=2048): 0.0640
Throughput (in TFLOP/s) for attention_linear_projection (4x31552x31552, b=2048): 254.661
(24231084032, 42481549312)
Elapsed time for mlp_h_to_4h (4x31552x126208, b=2048): 0.2622
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31552x126208, b=2048): 248.852
Elapsed time for mlp_fused_gelu (2048x4x126208): 0.0036
Elapsed time for mlp_4h_to_h (4x126208x31552, b=2048): 0.2714
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126208x31552, b=2048): 240.411
Elapsed time for transformer_add_bias_dropout (2048x4x31552): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31552): 0.0013

Attention duration (in seconds): 0.3208
Attention throughput (in TFLOP/s): 209.976
MLP duration (in seconds): 0.5371
MLP throughput (in TFLOP/s): 242.933
Transformer duration (in seconds): 0.8651
Transformer throughput (in TFLOP/s): 228.691
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x94848, b=2048): 0.1965
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x94848, b=2048): 249.967
Elapsed time for attention_key_query_prob (256x2048x494x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x494x2048): 119.186
Elapsed time for attention_prob_times_values (256x2048x2048x494): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x494): 143.432
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(16266100736, 42481549312)
Elapsed time for attention_linear_projection (4x31616x31616, b=2048): 0.0641
Throughput (in TFLOP/s) for attention_linear_projection (4x31616x31616, b=2048): 255.511
(16266100736, 42481549312)
Elapsed time for mlp_h_to_4h (4x31616x126464, b=2048): 0.2629
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x126464, b=2048): 249.221
Elapsed time for mlp_fused_gelu (2048x4x126464): 0.0036
Elapsed time for mlp_4h_to_h (4x126464x31616, b=2048): 0.2724
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126464x31616, b=2048): 240.497
Elapsed time for transformer_add_bias_dropout (2048x4x31616): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31616): 0.0013

Attention duration (in seconds): 0.3124
Attention throughput (in TFLOP/s): 216.515
MLP duration (in seconds): 0.5388
MLP throughput (in TFLOP/s): 243.151
Transformer duration (in seconds): 0.8583
Transformer throughput (in TFLOP/s): 231.434
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31680x95040, b=2048): 0.1971
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31680x95040, b=2048): 250.244
Elapsed time for attention_key_query_prob (256x2048x495x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x495x2048): 77.765
Elapsed time for attention_prob_times_values (256x2048x2048x495): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x495): 85.965
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(8267563008, 42481549312)
Elapsed time for attention_linear_projection (4x31680x31680, b=2048): 0.0644
Throughput (in TFLOP/s) for attention_linear_projection (4x31680x31680, b=2048): 255.142
(8267563008, 42481549312)
Elapsed time for mlp_h_to_4h (4x31680x126720, b=2048): 0.2635
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31680x126720, b=2048): 249.638
Elapsed time for mlp_fused_gelu (2048x4x126720): 0.0036
Elapsed time for mlp_4h_to_h (4x126720x31680, b=2048): 0.2733
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126720x31680, b=2048): 240.680
Elapsed time for transformer_add_bias_dropout (2048x4x31680): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31680): 0.0013

Attention duration (in seconds): 0.3230
Attention throughput (in TFLOP/s): 210.203
MLP duration (in seconds): 0.5403
MLP throughput (in TFLOP/s): 243.452
Transformer duration (in seconds): 0.8706
Transformer throughput (in TFLOP/s): 229.097
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x95232, b=2048): 0.1977
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x95232, b=2048): 250.476
Elapsed time for attention_key_query_prob (256x2048x496x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x496x2048): 202.620
Elapsed time for attention_prob_times_values (256x2048x2048x496): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x496): 231.217
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(237568000, 42481549312)
Elapsed time for attention_linear_projection (4x31744x31744, b=2048): 0.0646
Throughput (in TFLOP/s) for attention_linear_projection (4x31744x31744, b=2048): 255.713
(237568000, 42481549312)
Elapsed time for mlp_h_to_4h (4x31744x126976, b=2048): 0.2663
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x126976, b=2048): 247.958
Elapsed time for mlp_fused_gelu (2048x4x126976): 0.0036
Elapsed time for mlp_4h_to_h (4x126976x31744, b=2048): 0.2745
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126976x31744, b=2048): 240.562
Elapsed time for transformer_add_bias_dropout (2048x4x31744): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31744): 0.0013

Attention duration (in seconds): 0.3076
Attention throughput (in TFLOP/s): 221.634
MLP duration (in seconds): 0.5444
MLP throughput (in TFLOP/s): 242.594
Transformer duration (in seconds): 0.8592
Transformer throughput (in TFLOP/s): 233.062
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31808x95424, b=2048): 0.1990
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31808x95424, b=2048): 249.847
Elapsed time for attention_key_query_prob (256x2048x497x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x497x2048): 78.262
Elapsed time for attention_prob_times_values (256x2048x2048x497): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x497): 86.380
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(24082186240, 42481549312)
Elapsed time for attention_linear_projection (4x31808x31808, b=2048): 0.0650
Throughput (in TFLOP/s) for attention_linear_projection (4x31808x31808, b=2048): 255.131
(24082186240, 42481549312)
Elapsed time for mlp_h_to_4h (4x31808x127232, b=2048): 0.2663
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31808x127232, b=2048): 249.008
Elapsed time for mlp_fused_gelu (2048x4x127232): 0.0036
Elapsed time for mlp_4h_to_h (4x127232x31808, b=2048): 0.2760
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127232x31808, b=2048): 240.278
Elapsed time for transformer_add_bias_dropout (2048x4x31808): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31808): 0.0013

Attention duration (in seconds): 0.3254
Attention throughput (in TFLOP/s): 210.324
MLP duration (in seconds): 0.5458
MLP throughput (in TFLOP/s): 242.953
Transformer duration (in seconds): 0.8785
Transformer throughput (in TFLOP/s): 228.862
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x95616, b=2048): 0.2001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x95616, b=2048): 249.550
Elapsed time for attention_key_query_prob (256x2048x498x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x498x2048): 120.587
Elapsed time for attention_prob_times_values (256x2048x2048x498): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x498): 145.721
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15987179520, 42481549312)
Elapsed time for attention_linear_projection (4x31872x31872, b=2048): 0.0647
Throughput (in TFLOP/s) for attention_linear_projection (4x31872x31872, b=2048): 257.094
(15987179520, 42481549312)
Elapsed time for mlp_h_to_4h (4x31872x127488, b=2048): 0.2678
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x127488, b=2048): 248.597
Elapsed time for mlp_fused_gelu (2048x4x127488): 0.0036
Elapsed time for mlp_4h_to_h (4x127488x31872, b=2048): 0.2764
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127488x31872, b=2048): 240.820
Elapsed time for transformer_add_bias_dropout (2048x4x31872): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31872): 0.0013

Attention duration (in seconds): 0.3164
Attention throughput (in TFLOP/s): 217.142
MLP duration (in seconds): 0.5478
MLP throughput (in TFLOP/s): 243.037
Transformer duration (in seconds): 0.8715
Transformer throughput (in TFLOP/s): 231.609
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31936x95808, b=2048): 0.2009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31936x95808, b=2048): 249.539
Elapsed time for attention_key_query_prob (256x2048x499x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x499x2048): 79.477
Elapsed time for attention_prob_times_values (256x2048x2048x499): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x499): 88.900
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7858618368, 42481549312)
Elapsed time for attention_linear_projection (4x31936x31936, b=2048): 0.0657
Throughput (in TFLOP/s) for attention_linear_projection (4x31936x31936, b=2048): 254.411
(7858618368, 42481549312)
Elapsed time for mlp_h_to_4h (4x31936x127744, b=2048): 0.2682
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31936x127744, b=2048): 249.250
Elapsed time for mlp_fused_gelu (2048x4x127744): 0.0036
Elapsed time for mlp_4h_to_h (4x127744x31936, b=2048): 0.2782
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127744x31936, b=2048): 240.223
Elapsed time for transformer_add_bias_dropout (2048x4x31936): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31936): 0.0013

Attention duration (in seconds): 0.3275
Attention throughput (in TFLOP/s): 210.626
MLP duration (in seconds): 0.5500
MLP throughput (in TFLOP/s): 243.042
Transformer duration (in seconds): 0.8848
Transformer throughput (in TFLOP/s): 229.041
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x96000, b=2048): 0.2012
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x96000, b=2048): 250.144
Elapsed time for attention_key_query_prob (256x2048x500x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x500x2048): 121.420
Elapsed time for attention_prob_times_values (256x2048x2048x500): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x500): 146.740
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(23966842880, 42481549312)
Elapsed time for attention_linear_projection (4x32000x32000, b=2048): 0.0655
Throughput (in TFLOP/s) for attention_linear_projection (4x32000x32000, b=2048): 256.008
(23966842880, 42481549312)
Elapsed time for mlp_h_to_4h (4x32000x128000, b=2048): 0.2698
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x128000, b=2048): 248.706
Elapsed time for mlp_fused_gelu (2048x4x128000): 0.0036
Elapsed time for mlp_4h_to_h (4x128000x32000, b=2048): 0.2779
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128000x32000, b=2048): 241.447
Elapsed time for transformer_add_bias_dropout (2048x4x32000): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32000): 0.0013

Attention duration (in seconds): 0.3183
Attention throughput (in TFLOP/s): 217.552
MLP duration (in seconds): 0.5514
MLP throughput (in TFLOP/s): 243.414
Transformer duration (in seconds): 0.8770
Transformer throughput (in TFLOP/s): 232.014
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32064x96192, b=2048): 0.2022
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32064x96192, b=2048): 249.864
Elapsed time for attention_key_query_prob (256x2048x501x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x501x2048): 80.293
Elapsed time for attention_prob_times_values (256x2048x2048x501): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x501): 89.117
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(15773270016, 42481549312)
Elapsed time for attention_linear_projection (4x32064x32064, b=2048): 0.0659
Throughput (in TFLOP/s) for attention_linear_projection (4x32064x32064, b=2048): 255.617
(15773270016, 42481549312)
Elapsed time for mlp_h_to_4h (4x32064x128256, b=2048): 0.2700
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32064x128256, b=2048): 249.538
Elapsed time for mlp_fused_gelu (2048x4x128256): 0.0036
Elapsed time for mlp_4h_to_h (4x128256x32064, b=2048): 0.2797
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128256x32064, b=2048): 240.863
Elapsed time for transformer_add_bias_dropout (2048x4x32064): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32064): 0.0013

Attention duration (in seconds): 0.3291
Attention throughput (in TFLOP/s): 211.302
MLP duration (in seconds): 0.5534
MLP throughput (in TFLOP/s): 243.517
Transformer duration (in seconds): 0.8898
Transformer throughput (in TFLOP/s): 229.592
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x96384, b=2048): 0.2028
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x96384, b=2048): 250.116
Elapsed time for attention_key_query_prob (256x2048x502x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x502x2048): 121.814
Elapsed time for attention_prob_times_values (256x2048x2048x502): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x502): 148.085
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(7548239872, 42481549312)
Elapsed time for attention_linear_projection (4x32128x32128, b=2048): 0.0660
Throughput (in TFLOP/s) for attention_linear_projection (4x32128x32128, b=2048): 256.296
(7548239872, 42481549312)
Elapsed time for mlp_h_to_4h (4x32128x128512, b=2048): 0.2724
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x128512, b=2048): 248.294
Elapsed time for mlp_fused_gelu (2048x4x128512): 0.0036
Elapsed time for mlp_4h_to_h (4x128512x32128, b=2048): 0.2807
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128512x32128, b=2048): 241.018
Elapsed time for transformer_add_bias_dropout (2048x4x32128): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32128): 0.0013

Attention duration (in seconds): 0.3204
Attention throughput (in TFLOP/s): 217.861
MLP duration (in seconds): 0.5568
MLP throughput (in TFLOP/s): 243.005
Transformer duration (in seconds): 0.8844
Transformer throughput (in TFLOP/s): 231.899
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32192x96576, b=2048): 0.2035
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32192x96576, b=2048): 250.318
Elapsed time for attention_key_query_prob (256x2048x503x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x503x2048): 78.993
Elapsed time for attention_prob_times_values (256x2048x2048x503): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x503): 87.977
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23853596672, 42481549312)
Elapsed time for attention_linear_projection (4x32192x32192, b=2048): 0.0663
Throughput (in TFLOP/s) for attention_linear_projection (4x32192x32192, b=2048): 256.263
(23853596672, 42481549312)
Elapsed time for mlp_h_to_4h (4x32192x128768, b=2048): 0.2730
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32192x128768, b=2048): 248.743
Elapsed time for mlp_fused_gelu (2048x4x128768): 0.0036
Elapsed time for mlp_4h_to_h (4x128768x32192, b=2048): 0.2822
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128768x32192, b=2048): 240.659
Elapsed time for transformer_add_bias_dropout (2048x4x32192): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32192): 0.0013

Attention duration (in seconds): 0.3311
Attention throughput (in TFLOP/s): 211.647
MLP duration (in seconds): 0.5589
MLP throughput (in TFLOP/s): 243.040
Transformer duration (in seconds): 0.8973
Transformer throughput (in TFLOP/s): 229.467
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x96768, b=2048): 0.2041
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x96768, b=2048): 250.588
Elapsed time for attention_key_query_prob (256x2048x504x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x504x2048): 193.621
Elapsed time for attention_prob_times_values (256x2048x2048x504): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x504): 229.506
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15561457664, 42481549312)
Elapsed time for attention_linear_projection (4x32256x32256, b=2048): 0.0664
Throughput (in TFLOP/s) for attention_linear_projection (4x32256x32256, b=2048): 256.829
(15561457664, 42481549312)
Elapsed time for mlp_h_to_4h (4x32256x129024, b=2048): 0.2730
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x129024, b=2048): 249.751
Elapsed time for mlp_fused_gelu (2048x4x129024): 0.0037
Elapsed time for mlp_4h_to_h (4x129024x32256, b=2048): 0.2835
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129024x32256, b=2048): 240.494
Elapsed time for transformer_add_bias_dropout (2048x4x32256): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32256): 0.0013

Attention duration (in seconds): 0.3162
Attention throughput (in TFLOP/s): 222.519
MLP duration (in seconds): 0.5602
MLP throughput (in TFLOP/s): 243.435
Transformer duration (in seconds): 0.8837
Transformer throughput (in TFLOP/s): 233.942
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32320x96960, b=2048): 0.2059
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32320x96960, b=2048): 249.391
Elapsed time for attention_key_query_prob (256x2048x505x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x505x2048): 79.354
Elapsed time for attention_prob_times_values (256x2048x2048x505): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x505): 88.350
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(7237861376, 42481549312)
Elapsed time for attention_linear_projection (4x32320x32320, b=2048): 0.0670
Throughput (in TFLOP/s) for attention_linear_projection (4x32320x32320, b=2048): 255.392
(7237861376, 42481549312)
Elapsed time for mlp_h_to_4h (4x32320x129280, b=2048): 0.2748
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32320x129280, b=2048): 249.140
Elapsed time for mlp_fused_gelu (2048x4x129280): 0.0037
Elapsed time for mlp_4h_to_h (4x129280x32320, b=2048): 0.2848
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129280x32320, b=2048): 240.359
Elapsed time for transformer_add_bias_dropout (2048x4x32320): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32320): 0.0013

Attention duration (in seconds): 0.3342
Attention throughput (in TFLOP/s): 211.315
MLP duration (in seconds): 0.5632
MLP throughput (in TFLOP/s): 243.081
Transformer duration (in seconds): 0.9048
Transformer throughput (in TFLOP/s): 229.368
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x97152, b=2048): 0.2064
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x97152, b=2048): 249.735
Elapsed time for attention_key_query_prob (256x2048x506x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x506x2048): 123.150
Elapsed time for attention_prob_times_values (256x2048x2048x506): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x506): 150.606
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23738253312, 42481549312)
Elapsed time for attention_linear_projection (4x32384x32384, b=2048): 0.0675
Throughput (in TFLOP/s) for attention_linear_projection (4x32384x32384, b=2048): 254.704
(23738253312, 42481549312)
Elapsed time for mlp_h_to_4h (4x32384x129536, b=2048): 0.2762
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x129536, b=2048): 248.809
Elapsed time for mlp_fused_gelu (2048x4x129536): 0.0037
Elapsed time for mlp_4h_to_h (4x129536x32384, b=2048): 0.2859
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129536x32384, b=2048): 240.425
Elapsed time for transformer_add_bias_dropout (2048x4x32384): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32384): 0.0013

Attention duration (in seconds): 0.3253
Attention throughput (in TFLOP/s): 217.944
MLP duration (in seconds): 0.5658
MLP throughput (in TFLOP/s): 242.961
Transformer duration (in seconds): 0.8984
Transformer throughput (in TFLOP/s): 231.919
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32448x97344, b=2048): 0.2075
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32448x97344, b=2048): 249.370
Elapsed time for attention_key_query_prob (256x2048x507x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x507x2048): 80.960
Elapsed time for attention_prob_times_values (256x2048x2048x507): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x507): 91.202
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(15347548160, 42481549312)
Elapsed time for attention_linear_projection (4x32448x32448, b=2048): 0.0676
Throughput (in TFLOP/s) for attention_linear_projection (4x32448x32448, b=2048): 255.127
(15347548160, 42481549312)
Elapsed time for mlp_h_to_4h (4x32448x129792, b=2048): 0.2772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32448x129792, b=2048): 248.959
Elapsed time for mlp_fused_gelu (2048x4x129792): 0.0037
Elapsed time for mlp_4h_to_h (4x129792x32448, b=2048): 0.2874
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129792x32448, b=2048): 240.091
Elapsed time for transformer_add_bias_dropout (2048x4x32448): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32448): 0.0013

Attention duration (in seconds): 0.3360
Attention throughput (in TFLOP/s): 211.865
MLP duration (in seconds): 0.5682
MLP throughput (in TFLOP/s): 242.865
Transformer duration (in seconds): 0.9116
Transformer throughput (in TFLOP/s): 229.471
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x97536, b=2048): 0.2074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x97536, b=2048): 250.502
Elapsed time for attention_key_query_prob (256x2048x508x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x508x2048): 124.059
Elapsed time for attention_prob_times_values (256x2048x2048x508): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x508): 152.136
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0309
(6923288576, 42481549312)
Elapsed time for attention_linear_projection (4x32512x32512, b=2048): 0.0678
Throughput (in TFLOP/s) for attention_linear_projection (4x32512x32512, b=2048): 255.385
(6923288576, 42481549312)
Elapsed time for mlp_h_to_4h (4x32512x130048, b=2048): 0.2774
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x130048, b=2048): 249.749
Elapsed time for mlp_fused_gelu (2048x4x130048): 0.0037
Elapsed time for mlp_4h_to_h (4x130048x32512, b=2048): 0.2870
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130048x32512, b=2048): 241.394
Elapsed time for transformer_add_bias_dropout (2048x4x32512): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32512): 0.0013

Attention duration (in seconds): 0.3266
Attention throughput (in TFLOP/s): 218.768
MLP duration (in seconds): 0.5680
MLP throughput (in TFLOP/s): 243.906
Transformer duration (in seconds): 0.9020
Transformer throughput (in TFLOP/s): 232.816
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32576x97728, b=2048): 0.2091
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32576x97728, b=2048): 249.439
Elapsed time for attention_key_query_prob (256x2048x509x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x509x2048): 81.793
Elapsed time for attention_prob_times_values (256x2048x2048x509): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x509): 91.701
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23622909952, 42481549312)
Elapsed time for attention_linear_projection (4x32576x32576, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_linear_projection (4x32576x32576, b=2048): 254.979
(23622909952, 42481549312)
Elapsed time for mlp_h_to_4h (4x32576x130304, b=2048): 0.2788
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32576x130304, b=2048): 249.482
Elapsed time for mlp_fused_gelu (2048x4x130304): 0.0037
Elapsed time for mlp_4h_to_h (4x130304x32576, b=2048): 0.2904
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130304x32576, b=2048): 239.479
Elapsed time for transformer_add_bias_dropout (2048x4x32576): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32576): 0.0013

Attention duration (in seconds): 0.3380
Attention throughput (in TFLOP/s): 212.236
MLP duration (in seconds): 0.5729
MLP throughput (in TFLOP/s): 242.805
Transformer duration (in seconds): 0.9183
Transformer throughput (in TFLOP/s): 229.589
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x97920, b=2048): 0.2085
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x97920, b=2048): 251.101
Elapsed time for attention_key_query_prob (256x2048x510x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x510x2048): 124.416
Elapsed time for attention_prob_times_values (256x2048x2048x510): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x510): 153.306
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(15131541504, 42481549312)
Elapsed time for attention_linear_projection (4x32640x32640, b=2048): 0.0683
Throughput (in TFLOP/s) for attention_linear_projection (4x32640x32640, b=2048): 255.399
(15131541504, 42481549312)
Elapsed time for mlp_h_to_4h (4x32640x130560, b=2048): 0.2796
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x130560, b=2048): 249.694
Elapsed time for mlp_fused_gelu (2048x4x130560): 0.0037
Elapsed time for mlp_4h_to_h (4x130560x32640, b=2048): 0.2919
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130560x32640, b=2048): 239.182
Elapsed time for transformer_add_bias_dropout (2048x4x32640): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32640): 0.0013

Attention duration (in seconds): 0.3282
Attention throughput (in TFLOP/s): 219.382
MLP duration (in seconds): 0.5752
MLP throughput (in TFLOP/s): 242.757
Transformer duration (in seconds): 0.9108
Transformer throughput (in TFLOP/s): 232.368
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32704x98112, b=2048): 0.2097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32704x98112, b=2048): 250.680
Elapsed time for attention_key_query_prob (256x2048x511x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x511x2048): 81.223
Elapsed time for attention_prob_times_values (256x2048x2048x511): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x511): 91.319
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(6606618624, 42481549312)
Elapsed time for attention_linear_projection (4x32704x32704, b=2048): 0.0689
Throughput (in TFLOP/s) for attention_linear_projection (4x32704x32704, b=2048): 254.376
(6606618624, 42481549312)
Elapsed time for mlp_h_to_4h (4x32704x130816, b=2048): 0.2809
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32704x130816, b=2048): 249.492
Elapsed time for mlp_fused_gelu (2048x4x130816): 0.0037
Elapsed time for mlp_4h_to_h (4x130816x32704, b=2048): 0.2928
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130816x32704, b=2048): 239.394
Elapsed time for transformer_add_bias_dropout (2048x4x32704): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32704): 0.0013

Attention duration (in seconds): 0.3395
Attention throughput (in TFLOP/s): 212.906
MLP duration (in seconds): 0.5774
MLP throughput (in TFLOP/s): 242.773
Transformer duration (in seconds): 0.9244
Transformer throughput (in TFLOP/s): 229.847
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32768x98304, b=2048): 0.2106
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32768x98304, b=2048): 250.589
Elapsed time for attention_key_query_prob (256x2048x512x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x512x2048): 211.129
Elapsed time for attention_prob_times_values (256x2048x2048x512): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x512): 238.010
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
(23505469440, 42481549312)
Elapsed time for attention_linear_projection (4x32768x32768, b=2048): 0.0691
Throughput (in TFLOP/s) for attention_linear_projection (4x32768x32768, b=2048): 254.462
(23505469440, 42481549312)
Elapsed time for mlp_h_to_4h (4x32768x131072, b=2048): 0.2839
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32768x131072, b=2048): 247.863
Elapsed time for mlp_fused_gelu (2048x4x131072): 0.0037
Elapsed time for mlp_4h_to_h (4x131072x32768, b=2048): 0.2922
Throughput (in TFLOP/s) for mlp_4h_to_h (4x131072x32768, b=2048): 240.858
Elapsed time for transformer_add_bias_dropout (2048x4x32768): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32768): 0.0013

Attention duration (in seconds): 0.3250
Attention throughput (in TFLOP/s): 223.306
MLP duration (in seconds): 0.5798
MLP throughput (in TFLOP/s): 242.748
Transformer duration (in seconds): 0.9121
Transformer throughput (in TFLOP/s): 233.850
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
