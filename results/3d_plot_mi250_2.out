[2023-11-22 13:02:12,308] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 20, hidden_size: 20, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1x2048): 0.991
Elapsed time for attention_prob_times_values (80x2048x2048x1): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1): 0.206

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 0.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 40, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x2x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x2x2048): 1.825
Elapsed time for attention_prob_times_values (80x2048x2048x2): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x2): 1.726

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 1.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 60, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x3x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x3x2048): 2.537
Elapsed time for attention_prob_times_values (80x2048x2048x3): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x3): 2.376

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 2.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x4x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x4x2048): 3.309
Elapsed time for attention_prob_times_values (80x2048x2048x4): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x4): 3.004

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 3.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x5x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x5x2048): 4.040
Elapsed time for attention_prob_times_values (80x2048x2048x5): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x5): 3.158

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 3.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x6x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x6x2048): 4.870
Elapsed time for attention_prob_times_values (80x2048x2048x6): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x6): 3.662

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 4.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x7x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x7x2048): 5.617
Elapsed time for attention_prob_times_values (80x2048x2048x7): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x7): 5.120

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 6.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x8x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x8x2048): 6.518
Elapsed time for attention_prob_times_values (80x2048x2048x8): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x8): 6.926

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 7.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x9x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x9x2048): 7.006
Elapsed time for attention_prob_times_values (80x2048x2048x9): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x9): 7.399

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 8.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x10x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x10x2048): 7.591
Elapsed time for attention_prob_times_values (80x2048x2048x10): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x10): 8.496

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 9.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x11x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x11x2048): 8.355
Elapsed time for attention_prob_times_values (80x2048x2048x11): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x11): 9.043

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 10.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x12x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x12x2048): 8.523
Elapsed time for attention_prob_times_values (80x2048x2048x12): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x12): 10.014

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 11.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x13x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x13x2048): 9.973
Elapsed time for attention_prob_times_values (80x2048x2048x13): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x13): 10.548

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 12.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x14x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x14x2048): 10.737
Elapsed time for attention_prob_times_values (80x2048x2048x14): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x14): 11.617

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 14.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x15x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x15x2048): 11.371
Elapsed time for attention_prob_times_values (80x2048x2048x15): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x15): 12.087

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 15.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x16x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x16x2048): 12.295
Elapsed time for attention_prob_times_values (80x2048x2048x16): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x16): 13.492

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 16.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x17x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x17x2048): 12.530
Elapsed time for attention_prob_times_values (80x2048x2048x17): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x17): 13.422

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 17.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x18x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x18x2048): 13.428
Elapsed time for attention_prob_times_values (80x2048x2048x18): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x18): 14.784

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 19.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x19x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x19x2048): 14.103
Elapsed time for attention_prob_times_values (80x2048x2048x19): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x19): 15.202

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 20.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x20x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x20x2048): 14.637
Elapsed time for attention_prob_times_values (80x2048x2048x20): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x20): 16.181

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 21.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x21x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x21x2048): 15.543
Elapsed time for attention_prob_times_values (80x2048x2048x21): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x21): 16.609

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 22.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x22x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x22x2048): 16.287
Elapsed time for attention_prob_times_values (80x2048x2048x22): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x22): 14.141

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 21.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x23x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x23x2048): 17.005
Elapsed time for attention_prob_times_values (80x2048x2048x23): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x23): 18.165

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 25.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x24x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x24x2048): 17.735
Elapsed time for attention_prob_times_values (80x2048x2048x24): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x24): 19.758

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 27.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x25x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x25x2048): 16.315
Elapsed time for attention_prob_times_values (80x2048x2048x25): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x25): 18.387

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 25.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x26x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x26x2048): 18.746
Elapsed time for attention_prob_times_values (80x2048x2048x26): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x26): 20.738

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 29.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x27x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x27x2048): 19.407
Elapsed time for attention_prob_times_values (80x2048x2048x27): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x27): 21.017

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 30.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x28x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x28x2048): 20.039
Elapsed time for attention_prob_times_values (80x2048x2048x28): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x28): 19.877

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 30.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x29x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x29x2048): 20.733
Elapsed time for attention_prob_times_values (80x2048x2048x29): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x29): 22.438

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 33.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x30x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x30x2048): 21.476
Elapsed time for attention_prob_times_values (80x2048x2048x30): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x30): 23.780

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 35.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x31x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x31x2048): 22.090
Elapsed time for attention_prob_times_values (80x2048x2048x31): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x31): 23.906

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 36.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x32x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x32x2048): 25.640
Elapsed time for attention_prob_times_values (80x2048x2048x32): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x32): 25.647

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 41.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x33x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x33x2048): 25.924
Elapsed time for attention_prob_times_values (80x2048x2048x33): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x33): 25.490

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 42.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x34x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x34x2048): 26.020
Elapsed time for attention_prob_times_values (80x2048x2048x34): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x34): 26.451

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 43.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x35x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x35x2048): 25.339
Elapsed time for attention_prob_times_values (80x2048x2048x35): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x35): 26.932

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 43.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x36x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x36x2048): 23.343
Elapsed time for attention_prob_times_values (80x2048x2048x36): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x36): 21.186

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 37.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x37x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x37x2048): 25.614
Elapsed time for attention_prob_times_values (80x2048x2048x37): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x37): 21.986

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 40.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x38x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x38x2048): 26.462
Elapsed time for attention_prob_times_values (80x2048x2048x38): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x38): 29.137

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 48.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x39x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x39x2048): 26.651
Elapsed time for attention_prob_times_values (80x2048x2048x39): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x39): 29.653

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 49.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x40x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x40x2048): 27.562
Elapsed time for attention_prob_times_values (80x2048x2048x40): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x40): 31.538

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 52.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x41x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x41x2048): 26.764
Elapsed time for attention_prob_times_values (80x2048x2048x41): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x41): 27.464

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 48.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x42x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x42x2048): 27.712
Elapsed time for attention_prob_times_values (80x2048x2048x42): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x42): 30.243

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 52.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x43x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x43x2048): 28.499
Elapsed time for attention_prob_times_values (80x2048x2048x43): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x43): 32.011

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 55.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x44x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x44x2048): 29.432
Elapsed time for attention_prob_times_values (80x2048x2048x44): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x44): 33.509

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 58.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x45x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x45x2048): 29.838
Elapsed time for attention_prob_times_values (80x2048x2048x45): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x45): 33.228

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 59.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x46x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x46x2048): 30.777
Elapsed time for attention_prob_times_values (80x2048x2048x46): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x46): 34.797

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 62.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x47x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x47x2048): 31.157
Elapsed time for attention_prob_times_values (80x2048x2048x47): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x47): 29.438

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 58.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x48x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x48x2048): 31.993
Elapsed time for attention_prob_times_values (80x2048x2048x48): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x48): 37.364

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 66.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x49x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x49x2048): 32.652
Elapsed time for attention_prob_times_values (80x2048x2048x49): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x49): 35.964

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 66.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x50x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x50x2048): 33.421
Elapsed time for attention_prob_times_values (80x2048x2048x50): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x50): 37.183

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 69.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x51x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x51x2048): 34.009
Elapsed time for attention_prob_times_values (80x2048x2048x51): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x51): 28.353

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 61.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x52x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x52x2048): 34.198
Elapsed time for attention_prob_times_values (80x2048x2048x52): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x52): 38.958

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 73.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x53x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x53x2048): 35.385
Elapsed time for attention_prob_times_values (80x2048x2048x53): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x53): 32.403

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 68.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x54x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x54x2048): 36.172
Elapsed time for attention_prob_times_values (80x2048x2048x54): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x54): 39.894

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 77.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x55x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x55x2048): 36.790
Elapsed time for attention_prob_times_values (80x2048x2048x55): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x55): 39.941

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 79.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x56x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x56x2048): 37.450
Elapsed time for attention_prob_times_values (80x2048x2048x56): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x56): 43.138

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 83.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x57x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x57x2048): 36.900
Elapsed time for attention_prob_times_values (80x2048x2048x57): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x57): 41.619

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 82.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x58x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x58x2048): 34.471
Elapsed time for attention_prob_times_values (80x2048x2048x58): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x58): 43.098

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 81.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x59x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x59x2048): 38.107
Elapsed time for attention_prob_times_values (80x2048x2048x59): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x59): 42.462

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 86.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x60x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x60x2048): 38.532
Elapsed time for attention_prob_times_values (80x2048x2048x60): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x60): 43.775

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 89.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x61x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x61x2048): 37.399
Elapsed time for attention_prob_times_values (80x2048x2048x61): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x61): 43.756

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 88.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x62x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x62x2048): 38.448
Elapsed time for attention_prob_times_values (80x2048x2048x62): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x62): 42.513

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 89.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x63x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x63x2048): 37.923
Elapsed time for attention_prob_times_values (80x2048x2048x63): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x63): 44.893

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 91.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x64x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x64x2048): 55.600
Elapsed time for attention_prob_times_values (80x2048x2048x64): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x64): 48.886

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 117.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x65x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x65x2048): 39.448
Elapsed time for attention_prob_times_values (80x2048x2048x65): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x65): 29.557

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 76.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x66x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x66x2048): 43.324
Elapsed time for attention_prob_times_values (80x2048x2048x66): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x66): 34.212

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 87.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x67x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x67x2048): 42.998
Elapsed time for attention_prob_times_values (80x2048x2048x67): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x67): 33.955

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 87.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x68x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x68x2048): 43.910
Elapsed time for attention_prob_times_values (80x2048x2048x68): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x68): 35.295

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 91.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x69x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x69x2048): 41.966
Elapsed time for attention_prob_times_values (80x2048x2048x69): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x69): 34.819

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 89.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x70x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x70x2048): 44.658
Elapsed time for attention_prob_times_values (80x2048x2048x70): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x70): 35.463

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 93.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x71x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x71x2048): 44.440
Elapsed time for attention_prob_times_values (80x2048x2048x71): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x71): 35.547

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 94.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x72x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x72x2048): 45.645
Elapsed time for attention_prob_times_values (80x2048x2048x72): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x72): 35.674

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 96.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x73x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x73x2048): 44.351
Elapsed time for attention_prob_times_values (80x2048x2048x73): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x73): 36.458

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 97.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x74x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x74x2048): 45.163
Elapsed time for attention_prob_times_values (80x2048x2048x74): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x74): 38.198

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 101.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x75x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x75x2048): 37.285
Elapsed time for attention_prob_times_values (80x2048x2048x75): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x75): 37.437

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 92.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x76x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x76x2048): 40.654
Elapsed time for attention_prob_times_values (80x2048x2048x76): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x76): 39.144

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 99.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x77x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x77x2048): 46.045
Elapsed time for attention_prob_times_values (80x2048x2048x77): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x77): 38.051

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 104.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x78x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x78x2048): 43.227
Elapsed time for attention_prob_times_values (80x2048x2048x78): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x78): 39.759

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 104.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x79x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x79x2048): 46.765
Elapsed time for attention_prob_times_values (80x2048x2048x79): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x79): 39.011

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 108.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x80x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x80x2048): 48.825
Elapsed time for attention_prob_times_values (80x2048x2048x80): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x80): 39.340

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 111.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x81x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x81x2048): 45.819
Elapsed time for attention_prob_times_values (80x2048x2048x81): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x81): 39.908

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 110.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x82x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x82x2048): 46.544
Elapsed time for attention_prob_times_values (80x2048x2048x82): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x82): 41.587

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 114.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x83x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x83x2048): 46.446
Elapsed time for attention_prob_times_values (80x2048x2048x83): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x83): 40.616

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 113.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x84x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x84x2048): 47.373
Elapsed time for attention_prob_times_values (80x2048x2048x84): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x84): 42.494

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 118.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x85x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x85x2048): 47.014
Elapsed time for attention_prob_times_values (80x2048x2048x85): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x85): 41.336

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 117.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x86x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x86x2048): 48.068
Elapsed time for attention_prob_times_values (80x2048x2048x86): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x86): 36.850

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 111.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x87x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x87x2048): 48.339
Elapsed time for attention_prob_times_values (80x2048x2048x87): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x87): 42.250

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 121.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x88x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x88x2048): 49.264
Elapsed time for attention_prob_times_values (80x2048x2048x88): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x88): 42.378

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 123.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x89x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x89x2048): 47.945
Elapsed time for attention_prob_times_values (80x2048x2048x89): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x89): 43.147

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 124.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x90x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x90x2048): 48.679
Elapsed time for attention_prob_times_values (80x2048x2048x90): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x90): 41.475

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 123.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x91x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x91x2048): 48.826
Elapsed time for attention_prob_times_values (80x2048x2048x91): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x91): 43.837

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 128.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x92x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x92x2048): 49.675
Elapsed time for attention_prob_times_values (80x2048x2048x92): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x92): 42.355

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 127.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x93x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x93x2048): 49.298
Elapsed time for attention_prob_times_values (80x2048x2048x93): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x93): 41.703

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 127.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x94x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x94x2048): 50.121
Elapsed time for attention_prob_times_values (80x2048x2048x94): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x94): 41.264

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 128.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x95x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x95x2048): 48.751
Elapsed time for attention_prob_times_values (80x2048x2048x95): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x95): 41.477

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 127.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x96x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x96x2048): 59.769
Elapsed time for attention_prob_times_values (80x2048x2048x96): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x96): 42.688

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 143.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x97x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x97x2048): 51.015
Elapsed time for attention_prob_times_values (80x2048x2048x97): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x97): 41.268

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 132.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x98x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x98x2048): 49.302
Elapsed time for attention_prob_times_values (80x2048x2048x98): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x98): 43.709

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 135.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 1980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x99x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x99x2048): 51.480
Elapsed time for attention_prob_times_values (80x2048x2048x99): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x99): 6.720

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 34.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x100x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x100x2048): 53.058
Elapsed time for attention_prob_times_values (80x2048x2048x100): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x100): 44.299

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 142.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x101x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x101x2048): 51.944
Elapsed time for attention_prob_times_values (80x2048x2048x101): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x101): 44.399

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 142.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x102x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x102x2048): 52.372
Elapsed time for attention_prob_times_values (80x2048x2048x102): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x102): 49.775

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 152.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x103x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x103x2048): 50.168
Elapsed time for attention_prob_times_values (80x2048x2048x103): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x103): 48.907

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 149.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x104x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x104x2048): 53.051
Elapsed time for attention_prob_times_values (80x2048x2048x104): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x104): 49.323

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 154.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x105x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x105x2048): 52.116
Elapsed time for attention_prob_times_values (80x2048x2048x105): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x105): 49.456

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 154.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x106x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x106x2048): 53.470
Elapsed time for attention_prob_times_values (80x2048x2048x106): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x106): 51.407

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 160.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x107x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x107x2048): 53.278
Elapsed time for attention_prob_times_values (80x2048x2048x107): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x107): 50.088

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 159.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x108x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x108x2048): 54.596
Elapsed time for attention_prob_times_values (80x2048x2048x108): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x108): 52.152

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 165.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x109x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x109x2048): 53.892
Elapsed time for attention_prob_times_values (80x2048x2048x109): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x109): 50.872

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 163.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x110x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x110x2048): 54.937
Elapsed time for attention_prob_times_values (80x2048x2048x110): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x110): 52.846

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 169.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x111x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x111x2048): 52.396
Elapsed time for attention_prob_times_values (80x2048x2048x111): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x111): 51.354

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 164.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x112x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x112x2048): 54.093
Elapsed time for attention_prob_times_values (80x2048x2048x112): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x112): 53.396

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 171.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x113x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x113x2048): 54.548
Elapsed time for attention_prob_times_values (80x2048x2048x113): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x113): 51.972

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 170.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x114x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x114x2048): 55.757
Elapsed time for attention_prob_times_values (80x2048x2048x114): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x114): 53.984

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 176.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x115x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x115x2048): 55.219
Elapsed time for attention_prob_times_values (80x2048x2048x115): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x115): 48.429

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 167.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x116x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x116x2048): 56.402
Elapsed time for attention_prob_times_values (80x2048x2048x116): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x116): 54.848

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 181.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x117x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x117x2048): 55.970
Elapsed time for attention_prob_times_values (80x2048x2048x117): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x117): 53.384

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 179.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x118x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x118x2048): 56.873
Elapsed time for attention_prob_times_values (80x2048x2048x118): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x118): 55.682

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 185.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x119x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x119x2048): 56.504
Elapsed time for attention_prob_times_values (80x2048x2048x119): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x119): 52.263

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 180.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x120x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x120x2048): 58.176
Elapsed time for attention_prob_times_values (80x2048x2048x120): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x120): 50.196

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 180.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x121x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x121x2048): 53.509
Elapsed time for attention_prob_times_values (80x2048x2048x121): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x121): 42.916

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 160.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x122x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x122x2048): 55.147
Elapsed time for attention_prob_times_values (80x2048x2048x122): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x122): 57.005

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 189.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x123x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x123x2048): 55.940
Elapsed time for attention_prob_times_values (80x2048x2048x123): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x123): 45.405

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 170.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x124x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x124x2048): 57.406
Elapsed time for attention_prob_times_values (80x2048x2048x124): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x124): 57.302

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 196.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x125x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x125x2048): 56.512
Elapsed time for attention_prob_times_values (80x2048x2048x125): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x125): 43.909

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 170.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x126x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x126x2048): 57.425
Elapsed time for attention_prob_times_values (80x2048x2048x126): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x126): 56.718

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 197.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x127x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x127x2048): 56.995
Elapsed time for attention_prob_times_values (80x2048x2048x127): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x127): 43.121

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 170.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x128x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x128x2048): 68.198
Elapsed time for attention_prob_times_values (80x2048x2048x128): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x128): 61.346

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 226.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x129x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x129x2048): 56.744
Elapsed time for attention_prob_times_values (80x2048x2048x129): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x129): 42.613

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 171.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x130x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x130x2048): 57.853
Elapsed time for attention_prob_times_values (80x2048x2048x130): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x130): 46.900

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 183.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x131x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x131x2048): 57.462
Elapsed time for attention_prob_times_values (80x2048x2048x131): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x131): 45.277

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 180.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x132x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x132x2048): 48.147
Elapsed time for attention_prob_times_values (80x2048x2048x132): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x132): 47.769

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 171.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x133x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x133x2048): 57.523
Elapsed time for attention_prob_times_values (80x2048x2048x133): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x133): 45.940

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 183.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x134x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x134x2048): 59.258
Elapsed time for attention_prob_times_values (80x2048x2048x134): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x134): 48.060

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 191.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x135x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x135x2048): 58.559
Elapsed time for attention_prob_times_values (80x2048x2048x135): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x135): 45.242

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 185.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x136x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x136x2048): 60.322
Elapsed time for attention_prob_times_values (80x2048x2048x136): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x136): 45.654

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 190.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x137x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x137x2048): 58.489
Elapsed time for attention_prob_times_values (80x2048x2048x137): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x137): 46.838

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 191.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x138x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x138x2048): 59.658
Elapsed time for attention_prob_times_values (80x2048x2048x138): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x138): 47.860

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 196.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x139x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x139x2048): 57.050
Elapsed time for attention_prob_times_values (80x2048x2048x139): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x139): 45.404

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 187.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x140x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x140x2048): 60.071
Elapsed time for attention_prob_times_values (80x2048x2048x140): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x140): 49.388

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 202.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x141x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x141x2048): 59.645
Elapsed time for attention_prob_times_values (80x2048x2048x141): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x141): 48.212

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 200.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x142x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x142x2048): 60.799
Elapsed time for attention_prob_times_values (80x2048x2048x142): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x142): 50.710

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 208.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x143x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x143x2048): 60.283
Elapsed time for attention_prob_times_values (80x2048x2048x143): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x143): 48.801

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 204.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x144x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x144x2048): 62.656
Elapsed time for attention_prob_times_values (80x2048x2048x144): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x144): 49.023

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 209.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x145x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x145x2048): 60.003
Elapsed time for attention_prob_times_values (80x2048x2048x145): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x145): 46.527

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 200.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x146x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x146x2048): 61.254
Elapsed time for attention_prob_times_values (80x2048x2048x146): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x146): 51.967

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 216.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x147x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x147x2048): 60.599
Elapsed time for attention_prob_times_values (80x2048x2048x147): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x147): 49.996

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 212.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x148x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x148x2048): 61.992
Elapsed time for attention_prob_times_values (80x2048x2048x148): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x148): 52.566

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 221.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 2980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x149x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x149x2048): 61.039
Elapsed time for attention_prob_times_values (80x2048x2048x149): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x149): 48.379

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 211.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x150x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x150x2048): 61.655
Elapsed time for attention_prob_times_values (80x2048x2048x150): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x150): 52.949

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 223.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x151x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x151x2048): 57.819
Elapsed time for attention_prob_times_values (80x2048x2048x151): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x151): 51.295

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 214.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x152x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x152x2048): 63.290
Elapsed time for attention_prob_times_values (80x2048x2048x152): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x152): 51.013

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 224.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x153x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x153x2048): 58.849
Elapsed time for attention_prob_times_values (80x2048x2048x153): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x153): 51.712

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 219.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x154x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x154x2048): 62.541
Elapsed time for attention_prob_times_values (80x2048x2048x154): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x154): 54.029

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 232.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x155x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x155x2048): 59.238
Elapsed time for attention_prob_times_values (80x2048x2048x155): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x155): 52.331

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 223.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x156x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x156x2048): 63.362
Elapsed time for attention_prob_times_values (80x2048x2048x156): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x156): 52.367

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 232.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x157x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x157x2048): 62.250
Elapsed time for attention_prob_times_values (80x2048x2048x157): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x157): 53.000

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 232.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x158x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x158x2048): 63.935
Elapsed time for attention_prob_times_values (80x2048x2048x158): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x158): 55.020

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 241.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x159x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x159x2048): 63.123
Elapsed time for attention_prob_times_values (80x2048x2048x159): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x159): 53.419

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 237.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x160x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x160x2048): 68.820
Elapsed time for attention_prob_times_values (80x2048x2048x160): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x160): 55.259

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 252.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x161x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x161x2048): 62.232
Elapsed time for attention_prob_times_values (80x2048x2048x161): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x161): 51.620

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 233.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x162x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x162x2048): 62.464
Elapsed time for attention_prob_times_values (80x2048x2048x162): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x162): 56.420

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 246.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x163x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x163x2048): 62.704
Elapsed time for attention_prob_times_values (80x2048x2048x163): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x163): 55.052

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 245.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x164x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x164x2048): 64.562
Elapsed time for attention_prob_times_values (80x2048x2048x164): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x164): 57.401

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 255.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x165x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x165x2048): 63.223
Elapsed time for attention_prob_times_values (80x2048x2048x165): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x165): 55.257

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 249.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x166x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x166x2048): 64.466
Elapsed time for attention_prob_times_values (80x2048x2048x166): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x166): 57.685

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 258.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x167x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x167x2048): 63.772
Elapsed time for attention_prob_times_values (80x2048x2048x167): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x167): 56.047

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 254.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x168x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x168x2048): 65.559
Elapsed time for attention_prob_times_values (80x2048x2048x168): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x168): 55.567

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 257.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x169x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x169x2048): 63.324
Elapsed time for attention_prob_times_values (80x2048x2048x169): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x169): 56.435

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 256.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x170x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x170x2048): 64.023
Elapsed time for attention_prob_times_values (80x2048x2048x170): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x170): 58.763

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 264.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x171x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x171x2048): 63.895
Elapsed time for attention_prob_times_values (80x2048x2048x171): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x171): 56.813

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 261.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x172x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x172x2048): 65.628
Elapsed time for attention_prob_times_values (80x2048x2048x172): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x172): 59.542

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 272.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x173x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x173x2048): 64.574
Elapsed time for attention_prob_times_values (80x2048x2048x173): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x173): 53.074

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 255.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x174x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x174x2048): 65.953
Elapsed time for attention_prob_times_values (80x2048x2048x174): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x174): 60.124

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 276.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x175x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x175x2048): 64.491
Elapsed time for attention_prob_times_values (80x2048x2048x175): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x175): 56.083

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 265.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x176x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x176x2048): 68.133
Elapsed time for attention_prob_times_values (80x2048x2048x176): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x176): 59.244

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 281.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x177x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x177x2048): 57.918
Elapsed time for attention_prob_times_values (80x2048x2048x177): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x177): 58.779

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 260.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x178x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x178x2048): 66.194
Elapsed time for attention_prob_times_values (80x2048x2048x178): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x178): 61.168

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 284.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x179x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x179x2048): 65.324
Elapsed time for attention_prob_times_values (80x2048x2048x179): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x179): 56.484

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 272.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x180x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x180x2048): 59.705
Elapsed time for attention_prob_times_values (80x2048x2048x180): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x180): 55.579

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 259.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x181x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x181x2048): 62.032
Elapsed time for attention_prob_times_values (80x2048x2048x181): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x181): 59.790

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 276.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x182x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x182x2048): 67.273
Elapsed time for attention_prob_times_values (80x2048x2048x182): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x182): 62.005

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 293.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x183x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x183x2048): 66.396
Elapsed time for attention_prob_times_values (80x2048x2048x183): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x183): 56.264

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 278.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x184x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x184x2048): 66.323
Elapsed time for attention_prob_times_values (80x2048x2048x184): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x184): 60.688

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 291.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x185x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x185x2048): 65.566
Elapsed time for attention_prob_times_values (80x2048x2048x185): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x185): 61.075

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 291.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x186x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x186x2048): 57.780
Elapsed time for attention_prob_times_values (80x2048x2048x186): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x186): 56.728

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 265.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x187x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x187x2048): 66.661
Elapsed time for attention_prob_times_values (80x2048x2048x187): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x187): 60.162

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 294.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x188x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x188x2048): 65.274
Elapsed time for attention_prob_times_values (80x2048x2048x188): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x188): 59.800

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 291.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x189x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x189x2048): 66.824
Elapsed time for attention_prob_times_values (80x2048x2048x189): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x189): 62.003

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 301.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x190x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x190x2048): 68.293
Elapsed time for attention_prob_times_values (80x2048x2048x190): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x190): 64.310

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 312.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x191x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x191x2048): 67.676
Elapsed time for attention_prob_times_values (80x2048x2048x191): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x191): 22.297

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 158.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x192x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x192x2048): 77.782
Elapsed time for attention_prob_times_values (80x2048x2048x192): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x192): 61.100

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 325.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x193x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x193x2048): 64.123
Elapsed time for attention_prob_times_values (80x2048x2048x193): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x193): 51.052

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 271.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x194x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x194x2048): 67.699
Elapsed time for attention_prob_times_values (80x2048x2048x194): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x194): 53.441

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 286.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x195x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x195x2048): 66.627
Elapsed time for attention_prob_times_values (80x2048x2048x195): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x195): 51.077

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 278.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x196x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x196x2048): 68.279
Elapsed time for attention_prob_times_values (80x2048x2048x196): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x196): 47.605

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 270.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x197x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x197x2048): 66.503
Elapsed time for attention_prob_times_values (80x2048x2048x197): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x197): 52.130

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 283.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x198x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x198x2048): 68.576
Elapsed time for attention_prob_times_values (80x2048x2048x198): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x198): 53.894

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 293.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 3980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x199x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x199x2048): 67.657
Elapsed time for attention_prob_times_values (80x2048x2048x199): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x199): 51.874

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 286.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x200x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x200x2048): 64.176
Elapsed time for attention_prob_times_values (80x2048x2048x200): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x200): 51.373

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 279.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x201x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x201x2048): 67.337
Elapsed time for attention_prob_times_values (80x2048x2048x201): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x201): 40.891

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 250.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x202x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x202x2048): 68.702
Elapsed time for attention_prob_times_values (80x2048x2048x202): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x202): 48.982

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 282.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x203x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x203x2048): 62.694
Elapsed time for attention_prob_times_values (80x2048x2048x203): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x203): 51.361

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 280.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x204x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x204x2048): 69.240
Elapsed time for attention_prob_times_values (80x2048x2048x204): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x204): 55.250

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 306.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x205x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x205x2048): 67.949
Elapsed time for attention_prob_times_values (80x2048x2048x205): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x205): 52.658

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 296.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x206x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x206x2048): 69.400
Elapsed time for attention_prob_times_values (80x2048x2048x206): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x206): 51.624

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 297.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x207x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x207x2048): 68.709
Elapsed time for attention_prob_times_values (80x2048x2048x207): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x207): 52.978

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 301.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x208x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x208x2048): 71.669
Elapsed time for attention_prob_times_values (80x2048x2048x208): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x208): 52.457

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 306.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x209x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x209x2048): 68.365
Elapsed time for attention_prob_times_values (80x2048x2048x209): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x209): 53.622

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 305.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x210x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x210x2048): 69.582
Elapsed time for attention_prob_times_values (80x2048x2048x210): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x210): 56.140

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 317.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x211x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x211x2048): 68.756
Elapsed time for attention_prob_times_values (80x2048x2048x211): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x211): 53.972

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 309.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x212x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x212x2048): 70.063
Elapsed time for attention_prob_times_values (80x2048x2048x212): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x212): 56.793

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 322.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x213x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x213x2048): 68.692
Elapsed time for attention_prob_times_values (80x2048x2048x213): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x213): 50.675

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 300.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x214x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x214x2048): 70.410
Elapsed time for attention_prob_times_values (80x2048x2048x214): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x214): 57.009

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 326.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x215x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x215x2048): 69.488
Elapsed time for attention_prob_times_values (80x2048x2048x215): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x215): 54.762

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 318.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x216x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x216x2048): 70.198
Elapsed time for attention_prob_times_values (80x2048x2048x216): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x216): 53.071

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 315.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x217x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x217x2048): 67.447
Elapsed time for attention_prob_times_values (80x2048x2048x217): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x217): 55.111

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 317.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x218x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x218x2048): 70.514
Elapsed time for attention_prob_times_values (80x2048x2048x218): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x218): 54.288

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 322.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x219x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x219x2048): 69.582
Elapsed time for attention_prob_times_values (80x2048x2048x219): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x219): 55.589

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 326.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x220x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x220x2048): 70.844
Elapsed time for attention_prob_times_values (80x2048x2048x220): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x220): 58.265

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 338.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x221x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x221x2048): 69.871
Elapsed time for attention_prob_times_values (80x2048x2048x221): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x221): 54.295

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 324.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x222x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x222x2048): 71.284
Elapsed time for attention_prob_times_values (80x2048x2048x222): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x222): 58.734

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 343.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x223x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x223x2048): 70.519
Elapsed time for attention_prob_times_values (80x2048x2048x223): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x223): 54.088

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 327.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x224x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x224x2048): 80.452
Elapsed time for attention_prob_times_values (80x2048x2048x224): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x224): 55.066

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 351.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x225x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x225x2048): 63.843
Elapsed time for attention_prob_times_values (80x2048x2048x225): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x225): 57.324

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 325.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x226x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x226x2048): 70.674
Elapsed time for attention_prob_times_values (80x2048x2048x226): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x226): 57.088

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 341.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x227x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x227x2048): 63.888
Elapsed time for attention_prob_times_values (80x2048x2048x227): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x227): 57.542

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 329.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x228x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x228x2048): 71.117
Elapsed time for attention_prob_times_values (80x2048x2048x228): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x228): 57.398

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 346.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x229x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x229x2048): 69.797
Elapsed time for attention_prob_times_values (80x2048x2048x229): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x229): 58.081

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 346.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x230x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x230x2048): 69.344
Elapsed time for attention_prob_times_values (80x2048x2048x230): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x230): 60.245

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 354.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x231x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x231x2048): 70.179
Elapsed time for attention_prob_times_values (80x2048x2048x231): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x231): 55.201

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 340.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x232x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x232x2048): 71.942
Elapsed time for attention_prob_times_values (80x2048x2048x232): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x232): 58.688

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 357.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x233x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x233x2048): 69.651
Elapsed time for attention_prob_times_values (80x2048x2048x233): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x233): 54.130

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 338.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x234x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x234x2048): 70.467
Elapsed time for attention_prob_times_values (80x2048x2048x234): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x234): 61.031

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 364.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x235x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x235x2048): 68.228
Elapsed time for attention_prob_times_values (80x2048x2048x235): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x235): 58.766

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 352.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x236x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x236x2048): 69.660
Elapsed time for attention_prob_times_values (80x2048x2048x236): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x236): 60.241

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 362.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x237x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x237x2048): 70.191
Elapsed time for attention_prob_times_values (80x2048x2048x237): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x237): 59.061

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 361.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x238x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x238x2048): 71.779
Elapsed time for attention_prob_times_values (80x2048x2048x238): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x238): 59.426

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 367.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x239x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x239x2048): 70.627
Elapsed time for attention_prob_times_values (80x2048x2048x239): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x239): 59.538

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 366.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x240x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x240x2048): 55.955
Elapsed time for attention_prob_times_values (80x2048x2048x240): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x240): 61.823

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 334.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x241x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x241x2048): 53.511
Elapsed time for attention_prob_times_values (80x2048x2048x241): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x241): 58.494

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 318.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x242x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x242x2048): 54.268
Elapsed time for attention_prob_times_values (80x2048x2048x242): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x242): 60.737

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 328.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x243x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x243x2048): 53.989
Elapsed time for attention_prob_times_values (80x2048x2048x243): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x243): 59.508

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 325.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x244x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x244x2048): 55.032
Elapsed time for attention_prob_times_values (80x2048x2048x244): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x244): 62.266

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 336.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x245x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x245x2048): 52.743
Elapsed time for attention_prob_times_values (80x2048x2048x245): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x245): 60.834

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 326.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x246x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x246x2048): 54.937
Elapsed time for attention_prob_times_values (80x2048x2048x246): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x246): 63.409

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 341.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x247x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x247x2048): 54.576
Elapsed time for attention_prob_times_values (80x2048x2048x247): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x247): 61.024

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 335.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x248x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x248x2048): 54.345
Elapsed time for attention_prob_times_values (80x2048x2048x248): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x248): 62.121

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 338.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 4980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x249x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x249x2048): 53.888
Elapsed time for attention_prob_times_values (80x2048x2048x249): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x249): 63.034

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 340.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x250x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x250x2048): 54.364
Elapsed time for attention_prob_times_values (80x2048x2048x250): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x250): 60.878

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 337.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x251x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x251x2048): 51.800
Elapsed time for attention_prob_times_values (80x2048x2048x251): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x251): 62.478

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 334.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x252x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x252x2048): 55.088
Elapsed time for attention_prob_times_values (80x2048x2048x252): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x252): 64.654

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 352.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x253x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x253x2048): 54.085
Elapsed time for attention_prob_times_values (80x2048x2048x253): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x253): 61.731

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 342.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x254x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x254x2048): 52.908
Elapsed time for attention_prob_times_values (80x2048x2048x254): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x254): 65.042

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 347.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x255x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x255x2048): 53.931
Elapsed time for attention_prob_times_values (80x2048x2048x255): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x255): 65.675

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 354.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x256x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x256x2048): 77.125
Elapsed time for attention_prob_times_values (80x2048x2048x256): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x256): 68.646

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 435.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x257x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x257x2048): 55.568
Elapsed time for attention_prob_times_values (80x2048x2048x257): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x257): 52.129

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 323.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x258x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x258x2048): 54.365
Elapsed time for attention_prob_times_values (80x2048x2048x258): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x258): 57.056

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 336.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x259x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x259x2048): 56.990
Elapsed time for attention_prob_times_values (80x2048x2048x259): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x259): 54.478

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 337.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x260x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x260x2048): 58.101
Elapsed time for attention_prob_times_values (80x2048x2048x260): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x260): 58.031

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 352.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x261x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x261x2048): 56.668
Elapsed time for attention_prob_times_values (80x2048x2048x261): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x261): 55.117

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 340.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x262x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x262x2048): 57.556
Elapsed time for attention_prob_times_values (80x2048x2048x262): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x262): 56.218

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 347.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x263x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x263x2048): 56.376
Elapsed time for attention_prob_times_values (80x2048x2048x263): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x263): 51.022

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 328.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x264x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x264x2048): 58.113
Elapsed time for attention_prob_times_values (80x2048x2048x264): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x264): 53.843

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 344.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x265x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x265x2048): 55.769
Elapsed time for attention_prob_times_values (80x2048x2048x265): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x265): 55.339

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 343.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x266x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x266x2048): 54.920
Elapsed time for attention_prob_times_values (80x2048x2048x266): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x266): 58.558

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 351.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x267x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x267x2048): 53.007
Elapsed time for attention_prob_times_values (80x2048x2048x267): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x267): 55.649

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 337.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x268x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x268x2048): 56.485
Elapsed time for attention_prob_times_values (80x2048x2048x268): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x268): 59.221

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 360.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x269x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x269x2048): 56.547
Elapsed time for attention_prob_times_values (80x2048x2048x269): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x269): 56.295

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 352.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x270x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x270x2048): 57.167
Elapsed time for attention_prob_times_values (80x2048x2048x270): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x270): 58.968

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 364.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x271x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x271x2048): 54.677
Elapsed time for attention_prob_times_values (80x2048x2048x271): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x271): 56.709

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 350.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x272x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x272x2048): 58.301
Elapsed time for attention_prob_times_values (80x2048x2048x272): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x272): 73.751

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 411.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x273x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x273x2048): 55.659
Elapsed time for attention_prob_times_values (80x2048x2048x273): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x273): 56.681

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 355.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x274x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x274x2048): 56.755
Elapsed time for attention_prob_times_values (80x2048x2048x274): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x274): 59.809

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 369.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x275x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x275x2048): 56.361
Elapsed time for attention_prob_times_values (80x2048x2048x275): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x275): 57.312

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 362.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x276x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x276x2048): 57.406
Elapsed time for attention_prob_times_values (80x2048x2048x276): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x276): 60.546

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 376.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x277x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x277x2048): 53.127
Elapsed time for attention_prob_times_values (80x2048x2048x277): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x277): 57.900

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 355.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x278x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x278x2048): 56.864
Elapsed time for attention_prob_times_values (80x2048x2048x278): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x278): 59.797

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 374.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x279x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x279x2048): 55.211
Elapsed time for attention_prob_times_values (80x2048x2048x279): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x279): 58.210

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 365.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x280x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x280x2048): 58.497
Elapsed time for attention_prob_times_values (80x2048x2048x280): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x280): 74.853

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 424.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x281x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x281x2048): 56.171
Elapsed time for attention_prob_times_values (80x2048x2048x281): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x281): 58.636

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 372.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x282x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x282x2048): 56.339
Elapsed time for attention_prob_times_values (80x2048x2048x282): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x282): 61.484

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 382.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x283x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x283x2048): 56.615
Elapsed time for attention_prob_times_values (80x2048x2048x283): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x283): 57.752

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 373.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x284x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x284x2048): 55.142
Elapsed time for attention_prob_times_values (80x2048x2048x284): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x284): 62.233

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 382.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x285x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x285x2048): 56.685
Elapsed time for attention_prob_times_values (80x2048x2048x285): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x285): 58.619

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 378.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x286x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x286x2048): 57.403
Elapsed time for attention_prob_times_values (80x2048x2048x286): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x286): 62.173

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 393.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x287x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x287x2048): 55.701
Elapsed time for attention_prob_times_values (80x2048x2048x287): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x287): 59.795

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 380.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x288x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x288x2048): 77.708
Elapsed time for attention_prob_times_values (80x2048x2048x288): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x288): 76.368

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 510.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x289x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x289x2048): 60.779
Elapsed time for attention_prob_times_values (80x2048x2048x289): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x289): 56.790

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 390.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x290x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x290x2048): 59.474
Elapsed time for attention_prob_times_values (80x2048x2048x290): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x290): 62.627

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 406.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x291x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x291x2048): 60.223
Elapsed time for attention_prob_times_values (80x2048x2048x291): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x291): 60.463

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 403.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x292x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x292x2048): 61.199
Elapsed time for attention_prob_times_values (80x2048x2048x292): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x292): 63.581

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 418.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x293x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x293x2048): 60.655
Elapsed time for attention_prob_times_values (80x2048x2048x293): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x293): 60.517

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 407.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x294x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x294x2048): 61.344
Elapsed time for attention_prob_times_values (80x2048x2048x294): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x294): 63.637

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 421.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x295x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x295x2048): 59.066
Elapsed time for attention_prob_times_values (80x2048x2048x295): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x295): 61.576

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 407.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x296x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x296x2048): 62.232
Elapsed time for attention_prob_times_values (80x2048x2048x296): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x296): 79.440

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 473.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x297x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x297x2048): 59.424
Elapsed time for attention_prob_times_values (80x2048x2048x297): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x297): 61.325

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 410.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x298x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x298x2048): 60.030
Elapsed time for attention_prob_times_values (80x2048x2048x298): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x298): 64.657

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 424.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 5980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x299x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x299x2048): 59.676
Elapsed time for attention_prob_times_values (80x2048x2048x299): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x299): 59.537

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 407.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x300x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x300x2048): 61.283
Elapsed time for attention_prob_times_values (80x2048x2048x300): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x300): 65.816

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 435.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x301x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x301x2048): 60.126
Elapsed time for attention_prob_times_values (80x2048x2048x301): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x301): 62.712

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 422.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x302x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x302x2048): 60.857
Elapsed time for attention_prob_times_values (80x2048x2048x302): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x302): 65.937

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 436.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x303x2048): 0.0181
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x303x2048): 11.239
Elapsed time for attention_prob_times_values (80x2048x2048x303): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x303): 63.340

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 132.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x304x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x304x2048): 60.545
Elapsed time for attention_prob_times_values (80x2048x2048x304): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x304): 79.799

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 477.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x305x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x305x2048): 59.552
Elapsed time for attention_prob_times_values (80x2048x2048x305): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x305): 63.659

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 428.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x306x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x306x2048): 59.201
Elapsed time for attention_prob_times_values (80x2048x2048x306): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x306): 66.619

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 437.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x307x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x307x2048): 58.202
Elapsed time for attention_prob_times_values (80x2048x2048x307): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x307): 63.986

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 426.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x308x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x308x2048): 61.329
Elapsed time for attention_prob_times_values (80x2048x2048x308): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x308): 67.852

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 451.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x309x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x309x2048): 60.404
Elapsed time for attention_prob_times_values (80x2048x2048x309): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x309): 64.484

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 438.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x310x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x310x2048): 60.866
Elapsed time for attention_prob_times_values (80x2048x2048x310): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x310): 67.883

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 452.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x311x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x311x2048): 60.635
Elapsed time for attention_prob_times_values (80x2048x2048x311): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x311): 64.772

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 443.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x312x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x312x2048): 62.125
Elapsed time for attention_prob_times_values (80x2048x2048x312): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x312): 77.327

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 488.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x313x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x313x2048): 57.943
Elapsed time for attention_prob_times_values (80x2048x2048x313): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x313): 65.198

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 436.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x314x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x314x2048): 60.449
Elapsed time for attention_prob_times_values (80x2048x2048x314): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x314): 64.645

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 445.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x315x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x315x2048): 57.092
Elapsed time for attention_prob_times_values (80x2048x2048x315): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x315): 65.445

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 436.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x316x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x316x2048): 61.730
Elapsed time for attention_prob_times_values (80x2048x2048x316): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x316): 64.702

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 453.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x317x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x317x2048): 60.469
Elapsed time for attention_prob_times_values (80x2048x2048x317): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x317): 65.904

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 453.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x318x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x318x2048): 58.675
Elapsed time for attention_prob_times_values (80x2048x2048x318): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x318): 68.753

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 456.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x319x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x319x2048): 60.242
Elapsed time for attention_prob_times_values (80x2048x2048x319): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x319): 66.168

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 455.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x320x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x320x2048): 74.249
Elapsed time for attention_prob_times_values (80x2048x2048x320): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x320): 82.152

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 565.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x321x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x321x2048): 63.286
Elapsed time for attention_prob_times_values (80x2048x2048x321): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x321): 58.759

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 442.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x322x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x322x2048): 62.823
Elapsed time for attention_prob_times_values (80x2048x2048x322): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x322): 57.733

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 438.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x323x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x323x2048): 60.309
Elapsed time for attention_prob_times_values (80x2048x2048x323): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x323): 59.148

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 436.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x324x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x324x2048): 65.267
Elapsed time for attention_prob_times_values (80x2048x2048x324): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x324): 59.335

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 455.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x325x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x325x2048): 62.687
Elapsed time for attention_prob_times_values (80x2048x2048x325): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x325): 59.333

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 447.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x326x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x326x2048): 63.153
Elapsed time for attention_prob_times_values (80x2048x2048x326): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x326): 61.296

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 458.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x327x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x327x2048): 62.452
Elapsed time for attention_prob_times_values (80x2048x2048x327): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x327): 59.787

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 451.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x328x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x328x2048): 65.003
Elapsed time for attention_prob_times_values (80x2048x2048x328): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x328): 73.395

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 510.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x329x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x329x2048): 59.071
Elapsed time for attention_prob_times_values (80x2048x2048x329): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x329): 57.994

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 434.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x330x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x330x2048): 61.506
Elapsed time for attention_prob_times_values (80x2048x2048x330): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x330): 62.685

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 462.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x331x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x331x2048): 62.299
Elapsed time for attention_prob_times_values (80x2048x2048x331): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x331): 58.299

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 449.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x332x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x332x2048): 59.477
Elapsed time for attention_prob_times_values (80x2048x2048x332): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x332): 61.748

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 453.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x333x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x333x2048): 62.525
Elapsed time for attention_prob_times_values (80x2048x2048x333): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x333): 58.465

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 453.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x334x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x334x2048): 60.126
Elapsed time for attention_prob_times_values (80x2048x2048x334): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x334): 63.270

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 463.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x335x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x335x2048): 62.306
Elapsed time for attention_prob_times_values (80x2048x2048x335): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x335): 58.871

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 456.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x336x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x336x2048): 64.394
Elapsed time for attention_prob_times_values (80x2048x2048x336): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x336): 75.275

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 524.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x337x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x337x2048): 60.188
Elapsed time for attention_prob_times_values (80x2048x2048x337): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x337): 59.300

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 452.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x338x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x338x2048): 62.464
Elapsed time for attention_prob_times_values (80x2048x2048x338): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x338): 62.040

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 473.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x339x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x339x2048): 62.220
Elapsed time for attention_prob_times_values (80x2048x2048x339): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x339): 58.963

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 461.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x340x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x340x2048): 63.475
Elapsed time for attention_prob_times_values (80x2048x2048x340): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x340): 63.015

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 483.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x341x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x341x2048): 60.238
Elapsed time for attention_prob_times_values (80x2048x2048x341): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x341): 58.441

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 454.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x342x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x342x2048): 63.099
Elapsed time for attention_prob_times_values (80x2048x2048x342): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x342): 62.701

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 483.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x343x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x343x2048): 62.461
Elapsed time for attention_prob_times_values (80x2048x2048x343): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x343): 60.078

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 471.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x344x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x344x2048): 64.388
Elapsed time for attention_prob_times_values (80x2048x2048x344): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x344): 77.141

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 541.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x345x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x345x2048): 61.888
Elapsed time for attention_prob_times_values (80x2048x2048x345): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x345): 60.309

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 472.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x346x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x346x2048): 58.203
Elapsed time for attention_prob_times_values (80x2048x2048x346): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x346): 63.003

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 469.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x347x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x347x2048): 62.073
Elapsed time for attention_prob_times_values (80x2048x2048x347): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x347): 58.877

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 470.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x348x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x348x2048): 60.758
Elapsed time for attention_prob_times_values (80x2048x2048x348): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x348): 63.755

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 485.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 6980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x349x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x349x2048): 60.635
Elapsed time for attention_prob_times_values (80x2048x2048x349): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x349): 60.962

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 475.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x350x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x350x2048): 63.034
Elapsed time for attention_prob_times_values (80x2048x2048x350): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x350): 60.588

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 484.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x351x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x351x2048): 60.338
Elapsed time for attention_prob_times_values (80x2048x2048x351): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x351): 61.300

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 477.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x352x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x352x2048): 81.406
Elapsed time for attention_prob_times_values (80x2048x2048x352): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x352): 75.969

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 618.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x353x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x353x2048): 62.452
Elapsed time for attention_prob_times_values (80x2048x2048x353): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x353): 62.311

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 492.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x354x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x354x2048): 66.036
Elapsed time for attention_prob_times_values (80x2048x2048x354): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x354): 61.311

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 503.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x355x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x355x2048): 59.468
Elapsed time for attention_prob_times_values (80x2048x2048x355): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x355): 62.169

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 482.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x356x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x356x2048): 66.670
Elapsed time for attention_prob_times_values (80x2048x2048x356): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x356): 64.845

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 522.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x357x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x357x2048): 61.732
Elapsed time for attention_prob_times_values (80x2048x2048x357): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x357): 62.589

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 495.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x358x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x358x2048): 65.553
Elapsed time for attention_prob_times_values (80x2048x2048x358): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x358): 62.302

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 510.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x359x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x359x2048): 62.047
Elapsed time for attention_prob_times_values (80x2048x2048x359): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x359): 62.869

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 500.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x360x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x360x2048): 62.304
Elapsed time for attention_prob_times_values (80x2048x2048x360): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x360): 80.300

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 563.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x361x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x361x2048): 63.579
Elapsed time for attention_prob_times_values (80x2048x2048x361): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x361): 62.770

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 508.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x362x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x362x2048): 64.209
Elapsed time for attention_prob_times_values (80x2048x2048x362): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x362): 65.071

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 521.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x363x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x363x2048): 60.288
Elapsed time for attention_prob_times_values (80x2048x2048x363): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x363): 62.993

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 498.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x364x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x364x2048): 65.168
Elapsed time for attention_prob_times_values (80x2048x2048x364): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x364): 66.071

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 532.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x365x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x365x2048): 62.315
Elapsed time for attention_prob_times_values (80x2048x2048x365): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x365): 63.080

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 509.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x366x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x366x2048): 61.252
Elapsed time for attention_prob_times_values (80x2048x2048x366): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x366): 65.966

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 517.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x367x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x367x2048): 61.216
Elapsed time for attention_prob_times_values (80x2048x2048x367): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x367): 63.116

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 507.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x368x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x368x2048): 66.006
Elapsed time for attention_prob_times_values (80x2048x2048x368): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x368): 80.855

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 595.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x369x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x369x2048): 62.083
Elapsed time for attention_prob_times_values (80x2048x2048x369): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x369): 64.007

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 517.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x370x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x370x2048): 63.111
Elapsed time for attention_prob_times_values (80x2048x2048x370): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x370): 66.250

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 531.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x371x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x371x2048): 63.301
Elapsed time for attention_prob_times_values (80x2048x2048x371): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x371): 63.393

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 522.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x372x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x372x2048): 62.483
Elapsed time for attention_prob_times_values (80x2048x2048x372): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x372): 67.168

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 535.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x373x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x373x2048): 62.989
Elapsed time for attention_prob_times_values (80x2048x2048x373): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x373): 64.829

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 529.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x374x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x374x2048): 64.272
Elapsed time for attention_prob_times_values (80x2048x2048x374): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x374): 65.415

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 538.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x375x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x375x2048): 64.228
Elapsed time for attention_prob_times_values (80x2048x2048x375): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x375): 64.856

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 537.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x376x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x376x2048): 62.769
Elapsed time for attention_prob_times_values (80x2048x2048x376): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x376): 83.949

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 599.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x377x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x377x2048): 63.672
Elapsed time for attention_prob_times_values (80x2048x2048x377): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x377): 64.567

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 536.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x378x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x378x2048): 62.221
Elapsed time for attention_prob_times_values (80x2048x2048x378): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x378): 66.565

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 539.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x379x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x379x2048): 60.352
Elapsed time for attention_prob_times_values (80x2048x2048x379): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x379): 65.568

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 528.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x380x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x380x2048): 64.586
Elapsed time for attention_prob_times_values (80x2048x2048x380): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x380): 66.250

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 550.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x381x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x381x2048): 63.483
Elapsed time for attention_prob_times_values (80x2048x2048x381): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x381): 65.954

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 546.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x382x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x382x2048): 64.536
Elapsed time for attention_prob_times_values (80x2048x2048x382): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x382): 68.456

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 562.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x383x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x383x2048): 63.594
Elapsed time for attention_prob_times_values (80x2048x2048x383): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x383): 65.147

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 545.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x384x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x384x2048): 78.551
Elapsed time for attention_prob_times_values (80x2048x2048x384): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x384): 83.131

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 686.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x385x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x385x2048): 65.705
Elapsed time for attention_prob_times_values (80x2048x2048x385): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x385): 58.530

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 527.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x386x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x386x2048): 66.935
Elapsed time for attention_prob_times_values (80x2048x2048x386): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x386): 61.610

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 547.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x387x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x387x2048): 65.823
Elapsed time for attention_prob_times_values (80x2048x2048x387): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x387): 59.202

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 533.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x388x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x388x2048): 67.195
Elapsed time for attention_prob_times_values (80x2048x2048x388): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x388): 62.536

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 555.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x389x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x389x2048): 66.048
Elapsed time for attention_prob_times_values (80x2048x2048x389): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x389): 59.199

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 536.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x390x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x390x2048): 64.631
Elapsed time for attention_prob_times_values (80x2048x2048x390): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x390): 61.915

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 544.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x391x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x391x2048): 64.957
Elapsed time for attention_prob_times_values (80x2048x2048x391): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x391): 58.616

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 532.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x392x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x392x2048): 65.854
Elapsed time for attention_prob_times_values (80x2048x2048x392): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x392): 73.076

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 599.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x393x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x393x2048): 64.785
Elapsed time for attention_prob_times_values (80x2048x2048x393): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x393): 58.868

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 535.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x394x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x394x2048): 65.785
Elapsed time for attention_prob_times_values (80x2048x2048x394): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x394): 60.204

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 546.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x395x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x395x2048): 65.168
Elapsed time for attention_prob_times_values (80x2048x2048x395): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x395): 59.686

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 542.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x396x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x396x2048): 66.265
Elapsed time for attention_prob_times_values (80x2048x2048x396): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x396): 61.599

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 557.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x397x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x397x2048): 63.427
Elapsed time for attention_prob_times_values (80x2048x2048x397): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x397): 59.531

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 537.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x398x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x398x2048): 66.355
Elapsed time for attention_prob_times_values (80x2048x2048x398): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x398): 63.750

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 570.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 7980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x399x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x399x2048): 65.482
Elapsed time for attention_prob_times_values (80x2048x2048x399): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x399): 58.576

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 543.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x400x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x400x2048): 67.711
Elapsed time for attention_prob_times_values (80x2048x2048x400): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x400): 75.941

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 630.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x401x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x401x2048): 63.131
Elapsed time for attention_prob_times_values (80x2048x2048x401): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x401): 59.132

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 539.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x402x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x402x2048): 63.763
Elapsed time for attention_prob_times_values (80x2048x2048x402): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x402): 61.540

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 554.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x403x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x403x2048): 65.312
Elapsed time for attention_prob_times_values (80x2048x2048x403): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x403): 61.108

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 560.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x404x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x404x2048): 66.380
Elapsed time for attention_prob_times_values (80x2048x2048x404): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x404): 60.652

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 563.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x405x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x405x2048): 64.079
Elapsed time for attention_prob_times_values (80x2048x2048x405): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x405): 61.388

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 558.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x406x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x406x2048): 66.368
Elapsed time for attention_prob_times_values (80x2048x2048x406): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x406): 62.260

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 573.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x407x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x407x2048): 62.393
Elapsed time for attention_prob_times_values (80x2048x2048x407): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x407): 59.904

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 547.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x408x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x408x2048): 66.948
Elapsed time for attention_prob_times_values (80x2048x2048x408): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x408): 77.774

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 645.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x409x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x409x2048): 63.408
Elapsed time for attention_prob_times_values (80x2048x2048x409): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x409): 61.366

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 560.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x410x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x410x2048): 64.174
Elapsed time for attention_prob_times_values (80x2048x2048x410): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x410): 65.149

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 582.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x411x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x411x2048): 65.444
Elapsed time for attention_prob_times_values (80x2048x2048x411): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x411): 61.960

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 574.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x412x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x412x2048): 66.617
Elapsed time for attention_prob_times_values (80x2048x2048x412): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x412): 65.760

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 598.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x413x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x413x2048): 65.478
Elapsed time for attention_prob_times_values (80x2048x2048x413): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x413): 59.341

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 564.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x414x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x414x2048): 65.937
Elapsed time for attention_prob_times_values (80x2048x2048x414): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x414): 59.276

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 567.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x415x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x415x2048): 65.423
Elapsed time for attention_prob_times_values (80x2048x2048x415): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x415): 62.667

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 582.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x416x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x416x2048): 80.665
Elapsed time for attention_prob_times_values (80x2048x2048x416): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x416): 77.015

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 719.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x417x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x417x2048): 66.486
Elapsed time for attention_prob_times_values (80x2048x2048x417): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x417): 61.692

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 585.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x418x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x418x2048): 69.508
Elapsed time for attention_prob_times_values (80x2048x2048x418): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x418): 66.312

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 621.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x419x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x419x2048): 67.513
Elapsed time for attention_prob_times_values (80x2048x2048x419): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x419): 63.356

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 600.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x420x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x420x2048): 67.786
Elapsed time for attention_prob_times_values (80x2048x2048x420): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x420): 66.768

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 619.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x421x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x421x2048): 68.605
Elapsed time for attention_prob_times_values (80x2048x2048x421): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x421): 60.935

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 595.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x422x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x422x2048): 64.342
Elapsed time for attention_prob_times_values (80x2048x2048x422): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x422): 63.381

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 590.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x423x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x423x2048): 68.239
Elapsed time for attention_prob_times_values (80x2048x2048x423): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x423): 63.047

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 607.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x424x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x424x2048): 67.691
Elapsed time for attention_prob_times_values (80x2048x2048x424): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x424): 77.522

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 670.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x425x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x425x2048): 61.121
Elapsed time for attention_prob_times_values (80x2048x2048x425): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x425): 63.794

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 580.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x426x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x426x2048): 64.648
Elapsed time for attention_prob_times_values (80x2048x2048x426): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x426): 67.078

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 613.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x427x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x427x2048): 67.416
Elapsed time for attention_prob_times_values (80x2048x2048x427): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x427): 64.187

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 614.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x428x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x428x2048): 67.164
Elapsed time for attention_prob_times_values (80x2048x2048x428): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x428): 66.208

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 624.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x429x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x429x2048): 66.818
Elapsed time for attention_prob_times_values (80x2048x2048x429): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x429): 61.700

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 601.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x430x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x430x2048): 65.587
Elapsed time for attention_prob_times_values (80x2048x2048x430): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x430): 63.921

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 608.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x431x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x431x2048): 64.885
Elapsed time for attention_prob_times_values (80x2048x2048x431): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x431): 63.460

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 604.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x432x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x432x2048): 69.611
Elapsed time for attention_prob_times_values (80x2048x2048x432): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x432): 81.418

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 708.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x433x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x433x2048): 67.147
Elapsed time for attention_prob_times_values (80x2048x2048x433): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x433): 65.235

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 625.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x434x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x434x2048): 65.250
Elapsed time for attention_prob_times_values (80x2048x2048x434): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x434): 68.491

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 633.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x435x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x435x2048): 67.298
Elapsed time for attention_prob_times_values (80x2048x2048x435): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x435): 65.417

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 630.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x436x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x436x2048): 69.066
Elapsed time for attention_prob_times_values (80x2048x2048x436): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x436): 67.154

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 647.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x437x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x437x2048): 63.988
Elapsed time for attention_prob_times_values (80x2048x2048x437): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x437): 63.287

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 606.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x438x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x438x2048): 68.431
Elapsed time for attention_prob_times_values (80x2048x2048x438): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x438): 66.031

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 642.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x439x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x439x2048): 66.146
Elapsed time for attention_prob_times_values (80x2048x2048x439): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x439): 66.242

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 633.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x440x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x440x2048): 66.137
Elapsed time for attention_prob_times_values (80x2048x2048x440): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x440): 81.207

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 699.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x441x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x441x2048): 64.864
Elapsed time for attention_prob_times_values (80x2048x2048x441): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x441): 63.742

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 618.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x442x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x442x2048): 67.799
Elapsed time for attention_prob_times_values (80x2048x2048x442): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x442): 69.535

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 661.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x443x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x443x2048): 65.535
Elapsed time for attention_prob_times_values (80x2048x2048x443): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x443): 65.360

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 631.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x444x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x444x2048): 68.724
Elapsed time for attention_prob_times_values (80x2048x2048x444): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x444): 68.588

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 664.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x445x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x445x2048): 65.126
Elapsed time for attention_prob_times_values (80x2048x2048x445): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x445): 67.007

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 640.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x446x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x446x2048): 65.360
Elapsed time for attention_prob_times_values (80x2048x2048x446): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x446): 70.098

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 656.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x447x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x447x2048): 65.144
Elapsed time for attention_prob_times_values (80x2048x2048x447): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x447): 67.100

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 643.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x448x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x448x2048): 82.457
Elapsed time for attention_prob_times_values (80x2048x2048x448): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x448): 17.960

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 287.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 8980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x449x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x449x2048): 70.134
Elapsed time for attention_prob_times_values (80x2048x2048x449): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x449): 60.612

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 635.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x450x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x450x2048): 71.372
Elapsed time for attention_prob_times_values (80x2048x2048x450): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x450): 61.199

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 645.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x451x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x451x2048): 70.279
Elapsed time for attention_prob_times_values (80x2048x2048x451): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x451): 61.528

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 643.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x452x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x452x2048): 71.916
Elapsed time for attention_prob_times_values (80x2048x2048x452): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x452): 63.287

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 661.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x453x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x453x2048): 68.457
Elapsed time for attention_prob_times_values (80x2048x2048x453): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x453): 60.471

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 632.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x454x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x454x2048): 68.923
Elapsed time for attention_prob_times_values (80x2048x2048x454): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x454): 62.951

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 649.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x455x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x455x2048): 69.843
Elapsed time for attention_prob_times_values (80x2048x2048x455): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x455): 61.715

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 647.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x456x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x456x2048): 71.514
Elapsed time for attention_prob_times_values (80x2048x2048x456): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x456): 82.521

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 759.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x457x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x457x2048): 68.583
Elapsed time for attention_prob_times_values (80x2048x2048x457): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x457): 59.852

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 634.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x458x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x458x2048): 66.585
Elapsed time for attention_prob_times_values (80x2048x2048x458): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x458): 63.883

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 648.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x459x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x459x2048): 69.160
Elapsed time for attention_prob_times_values (80x2048x2048x459): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x459): 60.243

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 641.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x460x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x460x2048): 70.784
Elapsed time for attention_prob_times_values (80x2048x2048x460): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x460): 65.552

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 679.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x461x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x461x2048): 66.807
Elapsed time for attention_prob_times_values (80x2048x2048x461): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x461): 62.469

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 645.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x462x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x462x2048): 70.062
Elapsed time for attention_prob_times_values (80x2048x2048x462): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x462): 65.522

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 678.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x463x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x463x2048): 69.383
Elapsed time for attention_prob_times_values (80x2048x2048x463): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x463): 60.508

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 649.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x464x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x464x2048): 71.151
Elapsed time for attention_prob_times_values (80x2048x2048x464): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x464): 81.409

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 764.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x465x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x465x2048): 67.659
Elapsed time for attention_prob_times_values (80x2048x2048x465): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x465): 62.673

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 656.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x466x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x466x2048): 69.430
Elapsed time for attention_prob_times_values (80x2048x2048x466): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x466): 65.831

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 682.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x467x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x467x2048): 68.699
Elapsed time for attention_prob_times_values (80x2048x2048x467): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x467): 63.096

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 665.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x468x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x468x2048): 70.418
Elapsed time for attention_prob_times_values (80x2048x2048x468): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x468): 66.527

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 693.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x469x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x469x2048): 68.635
Elapsed time for attention_prob_times_values (80x2048x2048x469): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x469): 63.010

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 667.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x470x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x470x2048): 69.676
Elapsed time for attention_prob_times_values (80x2048x2048x470): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x470): 64.311

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 680.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x471x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x471x2048): 66.987
Elapsed time for attention_prob_times_values (80x2048x2048x471): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x471): 61.207

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 652.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x472x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x472x2048): 71.153
Elapsed time for attention_prob_times_values (80x2048x2048x472): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x472): 85.908

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 795.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x473x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x473x2048): 68.578
Elapsed time for attention_prob_times_values (80x2048x2048x473): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x473): 63.696

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 676.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x474x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x474x2048): 69.058
Elapsed time for attention_prob_times_values (80x2048x2048x474): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x474): 62.931

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 675.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x475x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x475x2048): 68.571
Elapsed time for attention_prob_times_values (80x2048x2048x475): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x475): 63.816

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 679.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x476x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x476x2048): 70.126
Elapsed time for attention_prob_times_values (80x2048x2048x476): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x476): 67.423

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 707.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x477x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x477x2048): 68.800
Elapsed time for attention_prob_times_values (80x2048x2048x477): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x477): 63.671

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 682.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x478x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x478x2048): 69.075
Elapsed time for attention_prob_times_values (80x2048x2048x478): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x478): 67.390

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 705.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x479x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x479x2048): 68.894
Elapsed time for attention_prob_times_values (80x2048x2048x479): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x479): 61.321

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 671.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x480x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x480x2048): 86.089
Elapsed time for attention_prob_times_values (80x2048x2048x480): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x480): 88.688

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 906.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x481x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x481x2048): 67.752
Elapsed time for attention_prob_times_values (80x2048x2048x481): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x481): 62.325

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 674.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x482x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x482x2048): 72.227
Elapsed time for attention_prob_times_values (80x2048x2048x482): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x482): 64.372

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 708.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x483x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x483x2048): 71.131
Elapsed time for attention_prob_times_values (80x2048x2048x483): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x483): 64.962

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 708.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x484x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x484x2048): 69.567
Elapsed time for attention_prob_times_values (80x2048x2048x484): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x484): 66.088

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 708.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x485x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x485x2048): 69.936
Elapsed time for attention_prob_times_values (80x2048x2048x485): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x485): 64.955

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 705.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x486x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x486x2048): 70.536
Elapsed time for attention_prob_times_values (80x2048x2048x486): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x486): 66.284

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 717.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x487x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x487x2048): 69.141
Elapsed time for attention_prob_times_values (80x2048x2048x487): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x487): 65.269

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 705.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x488x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x488x2048): 72.359
Elapsed time for attention_prob_times_values (80x2048x2048x488): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x488): 88.734

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 839.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x489x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x489x2048): 69.299
Elapsed time for attention_prob_times_values (80x2048x2048x489): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x489): 64.369

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 704.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x490x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x490x2048): 69.630
Elapsed time for attention_prob_times_values (80x2048x2048x490): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x490): 66.325

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 718.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x491x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x491x2048): 69.948
Elapsed time for attention_prob_times_values (80x2048x2048x491): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x491): 63.539

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 705.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x492x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x492x2048): 67.832
Elapsed time for attention_prob_times_values (80x2048x2048x492): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x492): 66.535

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 712.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x493x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x493x2048): 70.127
Elapsed time for attention_prob_times_values (80x2048x2048x493): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x493): 63.463

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 708.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x494x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x494x2048): 67.720
Elapsed time for attention_prob_times_values (80x2048x2048x494): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x494): 66.372

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 713.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x495x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x495x2048): 66.934
Elapsed time for attention_prob_times_values (80x2048x2048x495): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x495): 63.549

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 695.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x496x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x496x2048): 70.761
Elapsed time for attention_prob_times_values (80x2048x2048x496): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x496): 89.089

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 842.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x497x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x497x2048): 68.568
Elapsed time for attention_prob_times_values (80x2048x2048x497): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x497): 62.584

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 700.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x498x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x498x2048): 68.092
Elapsed time for attention_prob_times_values (80x2048x2048x498): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x498): 67.082

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 724.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 9980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x499x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x499x2048): 69.698
Elapsed time for attention_prob_times_values (80x2048x2048x499): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x499): 64.229

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 718.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x500x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x500x2048): 69.579
Elapsed time for attention_prob_times_values (80x2048x2048x500): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x500): 66.326

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 731.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x501x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x501x2048): 69.794
Elapsed time for attention_prob_times_values (80x2048x2048x501): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x501): 64.827

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 724.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x502x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x502x2048): 70.502
Elapsed time for attention_prob_times_values (80x2048x2048x502): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x502): 68.018

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 748.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x503x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x503x2048): 70.052
Elapsed time for attention_prob_times_values (80x2048x2048x503): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x503): 64.364

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 726.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x504x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x504x2048): 69.503
Elapsed time for attention_prob_times_values (80x2048x2048x504): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x504): 90.083

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 850.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x505x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x505x2048): 67.454
Elapsed time for attention_prob_times_values (80x2048x2048x505): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x505): 60.811

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 694.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x506x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x506x2048): 67.712
Elapsed time for attention_prob_times_values (80x2048x2048x506): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x506): 65.556

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 724.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x507x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x507x2048): 69.215
Elapsed time for attention_prob_times_values (80x2048x2048x507): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x507): 61.493

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 710.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x508x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x508x2048): 69.551
Elapsed time for attention_prob_times_values (80x2048x2048x508): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x508): 67.483

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 748.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x509x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x509x2048): 66.990
Elapsed time for attention_prob_times_values (80x2048x2048x509): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x509): 61.052

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 698.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x510x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x510x2048): 69.642
Elapsed time for attention_prob_times_values (80x2048x2048x510): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x510): 68.063

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 754.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x511x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x511x2048): 69.411
Elapsed time for attention_prob_times_values (80x2048x2048x511): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x511): 64.581

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 734.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x512x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x512x2048): 79.604
Elapsed time for attention_prob_times_values (80x2048x2048x512): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x512): 89.803

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 928.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x513x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x513x2048): 71.597
Elapsed time for attention_prob_times_values (80x2048x2048x513): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x513): 58.420

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 709.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x514x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x514x2048): 74.404
Elapsed time for attention_prob_times_values (80x2048x2048x514): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x514): 65.241

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 767.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x515x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x515x2048): 71.366
Elapsed time for attention_prob_times_values (80x2048x2048x515): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x515): 59.965

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 720.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x516x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x516x2048): 71.284
Elapsed time for attention_prob_times_values (80x2048x2048x516): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x516): 65.195

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 754.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x517x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x517x2048): 71.538
Elapsed time for attention_prob_times_values (80x2048x2048x517): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x517): 61.459

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 733.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x518x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x518x2048): 72.062
Elapsed time for attention_prob_times_values (80x2048x2048x518): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x518): 64.961

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 759.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x519x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x519x2048): 71.306
Elapsed time for attention_prob_times_values (80x2048x2048x519): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x519): 61.603

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 736.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x520x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x520x2048): 72.776
Elapsed time for attention_prob_times_values (80x2048x2048x520): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x520): 75.740

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 828.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x521x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x521x2048): 70.504
Elapsed time for attention_prob_times_values (80x2048x2048x521): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x521): 61.323

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 733.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x522x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x522x2048): 71.177
Elapsed time for attention_prob_times_values (80x2048x2048x522): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x522): 65.997

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 766.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x523x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x523x2048): 70.751
Elapsed time for attention_prob_times_values (80x2048x2048x523): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x523): 61.433

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 737.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x524x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x524x2048): 71.738
Elapsed time for attention_prob_times_values (80x2048x2048x524): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x524): 65.622

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 770.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x525x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x525x2048): 70.960
Elapsed time for attention_prob_times_values (80x2048x2048x525): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x525): 61.712

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 742.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x526x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x526x2048): 70.145
Elapsed time for attention_prob_times_values (80x2048x2048x526): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x526): 66.551

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 769.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x527x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x527x2048): 71.046
Elapsed time for attention_prob_times_values (80x2048x2048x527): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x527): 61.467

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 744.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x528x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x528x2048): 73.003
Elapsed time for attention_prob_times_values (80x2048x2048x528): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x528): 77.285

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 849.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x529x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x529x2048): 70.170
Elapsed time for attention_prob_times_values (80x2048x2048x529): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x529): 62.016

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 746.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x530x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x530x2048): 69.213
Elapsed time for attention_prob_times_values (80x2048x2048x530): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x530): 66.498

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 769.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x531x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x531x2048): 66.128
Elapsed time for attention_prob_times_values (80x2048x2048x531): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x531): 62.168

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 728.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x532x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x532x2048): 69.039
Elapsed time for attention_prob_times_values (80x2048x2048x532): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x532): 65.955

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 768.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x533x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x533x2048): 70.807
Elapsed time for attention_prob_times_values (80x2048x2048x533): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x533): 62.371

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 756.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x534x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x534x2048): 70.810
Elapsed time for attention_prob_times_values (80x2048x2048x534): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x534): 66.821

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 785.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x535x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x535x2048): 68.797
Elapsed time for attention_prob_times_values (80x2048x2048x535): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x535): 61.307

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 742.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x536x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x536x2048): 72.628
Elapsed time for attention_prob_times_values (80x2048x2048x536): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x536): 78.062

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 862.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x537x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x537x2048): 68.687
Elapsed time for attention_prob_times_values (80x2048x2048x537): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x537): 62.664

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 752.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x538x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x538x2048): 70.952
Elapsed time for attention_prob_times_values (80x2048x2048x538): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x538): 67.367

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 795.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x539x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x539x2048): 68.842
Elapsed time for attention_prob_times_values (80x2048x2048x539): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x539): 62.219

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 753.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x540x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x540x2048): 71.846
Elapsed time for attention_prob_times_values (80x2048x2048x540): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x540): 68.087

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 807.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x541x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x541x2048): 68.659
Elapsed time for attention_prob_times_values (80x2048x2048x541): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x541): 63.186

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 761.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x542x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x542x2048): 71.376
Elapsed time for attention_prob_times_values (80x2048x2048x542): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x542): 66.449

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 797.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x543x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x543x2048): 70.766
Elapsed time for attention_prob_times_values (80x2048x2048x543): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x543): 61.886

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 766.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x544x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x544x2048): 87.394
Elapsed time for attention_prob_times_values (80x2048x2048x544): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x544): 80.152

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 972.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x545x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x545x2048): 71.238
Elapsed time for attention_prob_times_values (80x2048x2048x545): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x545): 63.764

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 783.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x546x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x546x2048): 74.090
Elapsed time for attention_prob_times_values (80x2048x2048x546): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x546): 66.519

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 817.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x547x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x547x2048): 73.408
Elapsed time for attention_prob_times_values (80x2048x2048x547): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x547): 63.929

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 798.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x548x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x548x2048): 74.538
Elapsed time for attention_prob_times_values (80x2048x2048x548): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x548): 67.178

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 827.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 10980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x549x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x549x2048): 72.427
Elapsed time for attention_prob_times_values (80x2048x2048x549): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x549): 62.968

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 789.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x550x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x550x2048): 72.518
Elapsed time for attention_prob_times_values (80x2048x2048x550): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x550): 67.394

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 820.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x551x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x551x2048): 72.575
Elapsed time for attention_prob_times_values (80x2048x2048x551): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x551): 64.485

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 803.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x552x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x552x2048): 74.800
Elapsed time for attention_prob_times_values (80x2048x2048x552): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x552): 80.264

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 912.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x553x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x553x2048): 72.391
Elapsed time for attention_prob_times_values (80x2048x2048x553): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x553): 60.967

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 781.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x554x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x554x2048): 70.043
Elapsed time for attention_prob_times_values (80x2048x2048x554): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x554): 67.631

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 813.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x555x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x555x2048): 72.424
Elapsed time for attention_prob_times_values (80x2048x2048x555): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x555): 64.612

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 808.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x556x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x556x2048): 72.757
Elapsed time for attention_prob_times_values (80x2048x2048x556): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x556): 68.698

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 838.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x557x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x557x2048): 71.216
Elapsed time for attention_prob_times_values (80x2048x2048x557): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x557): 65.119

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 808.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x558x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x558x2048): 73.725
Elapsed time for attention_prob_times_values (80x2048x2048x558): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x558): 69.534

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 851.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x559x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x559x2048): 70.621
Elapsed time for attention_prob_times_values (80x2048x2048x559): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x559): 65.609

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 810.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x560x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x560x2048): 72.891
Elapsed time for attention_prob_times_values (80x2048x2048x560): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x560): 81.956

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 921.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x561x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x561x2048): 71.917
Elapsed time for attention_prob_times_values (80x2048x2048x561): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x561): 64.029

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 810.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x562x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x562x2048): 71.099
Elapsed time for attention_prob_times_values (80x2048x2048x562): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x562): 67.383

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 828.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x563x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x563x2048): 70.528
Elapsed time for attention_prob_times_values (80x2048x2048x563): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x563): 64.682

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 809.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x564x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x564x2048): 73.859
Elapsed time for attention_prob_times_values (80x2048x2048x564): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x564): 70.827

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 868.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x565x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x565x2048): 72.492
Elapsed time for attention_prob_times_values (80x2048x2048x565): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x565): 66.300

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 833.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x566x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x566x2048): 72.926
Elapsed time for attention_prob_times_values (80x2048x2048x566): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x566): 70.715

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 865.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x567x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x567x2048): 72.517
Elapsed time for attention_prob_times_values (80x2048x2048x567): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x567): 66.201

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 835.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x568x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x568x2048): 73.819
Elapsed time for attention_prob_times_values (80x2048x2048x568): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x568): 82.740

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 943.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x569x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x569x2048): 71.496
Elapsed time for attention_prob_times_values (80x2048x2048x569): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x569): 66.486

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 834.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x570x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x570x2048): 72.552
Elapsed time for attention_prob_times_values (80x2048x2048x570): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x570): 70.166

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 865.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x571x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x571x2048): 69.779
Elapsed time for attention_prob_times_values (80x2048x2048x571): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x571): 66.340

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 826.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x572x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x572x2048): 73.703
Elapsed time for attention_prob_times_values (80x2048x2048x572): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x572): 69.530

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 870.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x573x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x573x2048): 69.463
Elapsed time for attention_prob_times_values (80x2048x2048x573): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x573): 66.540

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 828.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x574x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x574x2048): 69.737
Elapsed time for attention_prob_times_values (80x2048x2048x574): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x574): 67.023

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 834.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x575x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x575x2048): 72.138
Elapsed time for attention_prob_times_values (80x2048x2048x575): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x575): 67.132

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 850.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x576x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x576x2048): 88.047
Elapsed time for attention_prob_times_values (80x2048x2048x576): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x576): 85.098

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1060.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x577x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x577x2048): 74.357
Elapsed time for attention_prob_times_values (80x2048x2048x577): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x577): 62.741

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 835.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x578x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x578x2048): 75.523
Elapsed time for attention_prob_times_values (80x2048x2048x578): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x578): 64.364

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 854.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x579x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x579x2048): 70.945
Elapsed time for attention_prob_times_values (80x2048x2048x579): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x579): 62.457

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 817.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x580x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x580x2048): 76.334
Elapsed time for attention_prob_times_values (80x2048x2048x580): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x580): 62.586

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 847.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x581x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x581x2048): 74.355
Elapsed time for attention_prob_times_values (80x2048x2048x581): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x581): 61.142

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 828.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x582x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x582x2048): 75.288
Elapsed time for attention_prob_times_values (80x2048x2048x582): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x582): 65.696

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 867.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x583x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x583x2048): 75.737
Elapsed time for attention_prob_times_values (80x2048x2048x583): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x583): 63.327

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 854.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x584x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x584x2048): 76.603
Elapsed time for attention_prob_times_values (80x2048x2048x584): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x584): 84.763

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 998.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x585x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x585x2048): 73.258
Elapsed time for attention_prob_times_values (80x2048x2048x585): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x585): 62.748

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 839.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x586x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x586x2048): 71.830
Elapsed time for attention_prob_times_values (80x2048x2048x586): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x586): 62.614

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 832.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x587x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x587x2048): 73.587
Elapsed time for attention_prob_times_values (80x2048x2048x587): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x587): 62.907

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 845.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x588x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x588x2048): 73.821
Elapsed time for attention_prob_times_values (80x2048x2048x588): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x588): 66.453

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 873.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x589x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x589x2048): 72.778
Elapsed time for attention_prob_times_values (80x2048x2048x589): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x589): 63.718

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 849.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x590x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x590x2048): 72.129
Elapsed time for attention_prob_times_values (80x2048x2048x590): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x590): 66.190

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 864.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x591x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x591x2048): 73.568
Elapsed time for attention_prob_times_values (80x2048x2048x591): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x591): 63.761

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 856.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x592x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x592x2048): 75.653
Elapsed time for attention_prob_times_values (80x2048x2048x592): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x592): 86.536

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1014.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x593x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x593x2048): 70.928
Elapsed time for attention_prob_times_values (80x2048x2048x593): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x593): 61.848

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 831.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x594x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x594x2048): 73.746
Elapsed time for attention_prob_times_values (80x2048x2048x594): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x594): 66.506

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 881.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x595x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x595x2048): 73.077
Elapsed time for attention_prob_times_values (80x2048x2048x595): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x595): 64.212

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 862.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x596x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x596x2048): 74.417
Elapsed time for attention_prob_times_values (80x2048x2048x596): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x596): 66.946

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 890.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x597x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x597x2048): 73.055
Elapsed time for attention_prob_times_values (80x2048x2048x597): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x597): 62.639

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 853.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x598x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x598x2048): 71.148
Elapsed time for attention_prob_times_values (80x2048x2048x598): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x598): 66.673

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 872.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 11980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x599x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x599x2048): 71.433
Elapsed time for attention_prob_times_values (80x2048x2048x599): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x599): 64.572

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 861.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x600x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x600x2048): 75.382
Elapsed time for attention_prob_times_values (80x2048x2048x600): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x600): 86.996

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1027.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x601x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x601x2048): 72.557
Elapsed time for attention_prob_times_values (80x2048x2048x601): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x601): 63.150

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 860.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x602x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x602x2048): 73.357
Elapsed time for attention_prob_times_values (80x2048x2048x602): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x602): 67.193

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 894.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x603x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x603x2048): 73.005
Elapsed time for attention_prob_times_values (80x2048x2048x603): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x603): 62.886

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 863.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x604x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x604x2048): 72.869
Elapsed time for attention_prob_times_values (80x2048x2048x604): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x604): 66.468

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 889.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x605x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x605x2048): 72.987
Elapsed time for attention_prob_times_values (80x2048x2048x605): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x605): 63.713

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 871.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x606x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x606x2048): 72.671
Elapsed time for attention_prob_times_values (80x2048x2048x606): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x606): 66.623

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 892.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x607x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x607x2048): 73.184
Elapsed time for attention_prob_times_values (80x2048x2048x607): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x607): 65.157

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 886.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x608x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x608x2048): 86.638
Elapsed time for attention_prob_times_values (80x2048x2048x608): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x608): 89.329

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1132.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x609x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x609x2048): 75.029
Elapsed time for attention_prob_times_values (80x2048x2048x609): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x609): 62.903

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 882.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x610x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x610x2048): 73.935
Elapsed time for attention_prob_times_values (80x2048x2048x610): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x610): 64.758

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 891.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x611x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x611x2048): 74.931
Elapsed time for attention_prob_times_values (80x2048x2048x611): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x611): 65.755

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 905.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x612x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x612x2048): 76.412
Elapsed time for attention_prob_times_values (80x2048x2048x612): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x612): 65.669

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 914.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x613x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x613x2048): 74.491
Elapsed time for attention_prob_times_values (80x2048x2048x613): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x613): 64.045

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 893.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x614x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x614x2048): 75.522
Elapsed time for attention_prob_times_values (80x2048x2048x614): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x614): 67.526

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 926.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x615x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x615x2048): 73.603
Elapsed time for attention_prob_times_values (80x2048x2048x615): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x615): 64.474

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 894.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x616x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x616x2048): 76.238
Elapsed time for attention_prob_times_values (80x2048x2048x616): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x616): 89.616

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1073.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x617x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x617x2048): 73.567
Elapsed time for attention_prob_times_values (80x2048x2048x617): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x617): 66.168

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 909.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x618x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x618x2048): 74.373
Elapsed time for attention_prob_times_values (80x2048x2048x618): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x618): 68.337

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 930.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x619x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x619x2048): 73.774
Elapsed time for attention_prob_times_values (80x2048x2048x619): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x619): 66.471

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 915.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x620x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x620x2048): 70.528
Elapsed time for attention_prob_times_values (80x2048x2048x620): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x620): 68.103

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 908.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x621x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x621x2048): 73.651
Elapsed time for attention_prob_times_values (80x2048x2048x621): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x621): 65.961

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 913.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x622x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x622x2048): 68.720
Elapsed time for attention_prob_times_values (80x2048x2048x622): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x622): 65.393

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 881.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x623x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x623x2048): 72.218
Elapsed time for attention_prob_times_values (80x2048x2048x623): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x623): 66.722

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 913.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x624x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x624x2048): 76.048
Elapsed time for attention_prob_times_values (80x2048x2048x624): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x624): 91.161

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1093.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x625x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x625x2048): 71.339
Elapsed time for attention_prob_times_values (80x2048x2048x625): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x625): 65.287

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 900.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x626x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x626x2048): 72.709
Elapsed time for attention_prob_times_values (80x2048x2048x626): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x626): 68.808

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 935.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x627x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x627x2048): 73.303
Elapsed time for attention_prob_times_values (80x2048x2048x627): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x627): 67.258

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 929.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x628x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x628x2048): 70.834
Elapsed time for attention_prob_times_values (80x2048x2048x628): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x628): 69.141

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 928.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x629x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x629x2048): 73.708
Elapsed time for attention_prob_times_values (80x2048x2048x629): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x629): 66.848

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 931.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x630x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x630x2048): 71.371
Elapsed time for attention_prob_times_values (80x2048x2048x630): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x630): 66.234

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 914.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x631x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x631x2048): 72.672
Elapsed time for attention_prob_times_values (80x2048x2048x631): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x631): 66.831

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 927.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x632x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x632x2048): 74.719
Elapsed time for attention_prob_times_values (80x2048x2048x632): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x632): 91.996

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1100.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x633x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x633x2048): 72.659
Elapsed time for attention_prob_times_values (80x2048x2048x633): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x633): 65.406

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 919.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x634x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x634x2048): 72.240
Elapsed time for attention_prob_times_values (80x2048x2048x634): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x634): 69.281

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 946.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x635x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x635x2048): 71.868
Elapsed time for attention_prob_times_values (80x2048x2048x635): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x635): 67.660

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 934.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x636x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x636x2048): 74.318
Elapsed time for attention_prob_times_values (80x2048x2048x636): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x636): 68.800

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 959.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x637x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x637x2048): 68.750
Elapsed time for attention_prob_times_values (80x2048x2048x637): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x637): 67.809

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 917.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x638x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x638x2048): 71.045
Elapsed time for attention_prob_times_values (80x2048x2048x638): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x638): 68.175

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 936.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x639x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x639x2048): 69.760
Elapsed time for attention_prob_times_values (80x2048x2048x639): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x639): 65.512

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 910.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x640x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x640x2048): 81.966
Elapsed time for attention_prob_times_values (80x2048x2048x640): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x640): 94.983

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1187.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x641x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x641x2048): 72.393
Elapsed time for attention_prob_times_values (80x2048x2048x641): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x641): 59.535

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 883.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x642x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x642x2048): 75.867
Elapsed time for attention_prob_times_values (80x2048x2048x642): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x642): 64.879

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 946.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x643x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x643x2048): 74.696
Elapsed time for attention_prob_times_values (80x2048x2048x643): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x643): 62.593

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 923.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x644x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x644x2048): 76.476
Elapsed time for attention_prob_times_values (80x2048x2048x644): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x644): 64.904

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 953.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x645x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x645x2048): 74.922
Elapsed time for attention_prob_times_values (80x2048x2048x645): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x645): 62.524

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 926.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x646x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x646x2048): 75.324
Elapsed time for attention_prob_times_values (80x2048x2048x646): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x646): 65.311

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 952.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x647x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x647x2048): 70.230
Elapsed time for attention_prob_times_values (80x2048x2048x647): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x647): 63.114

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 906.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x648x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x648x2048): 76.460
Elapsed time for attention_prob_times_values (80x2048x2048x648): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x648): 77.655

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1052.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 12980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x649x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x649x2048): 74.096
Elapsed time for attention_prob_times_values (80x2048x2048x649): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x649): 62.780

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 929.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x650x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x650x2048): 74.756
Elapsed time for attention_prob_times_values (80x2048x2048x650): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x650): 65.319

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 954.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x651x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x651x2048): 74.147
Elapsed time for attention_prob_times_values (80x2048x2048x651): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x651): 63.128

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 935.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x652x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x652x2048): 75.378
Elapsed time for attention_prob_times_values (80x2048x2048x652): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x652): 63.415

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 946.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x653x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x653x2048): 71.885
Elapsed time for attention_prob_times_values (80x2048x2048x653): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x653): 62.546

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 920.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x654x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x654x2048): 73.610
Elapsed time for attention_prob_times_values (80x2048x2048x654): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x654): 65.394

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 953.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x655x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x655x2048): 71.206
Elapsed time for attention_prob_times_values (80x2048x2048x655): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x655): 63.632

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 926.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x656x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x656x2048): 76.595
Elapsed time for attention_prob_times_values (80x2048x2048x656): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x656): 79.463

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1077.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x657x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x657x2048): 72.286
Elapsed time for attention_prob_times_values (80x2048x2048x657): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x657): 63.777

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 937.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x658x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x658x2048): 74.787
Elapsed time for attention_prob_times_values (80x2048x2048x658): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x658): 66.455

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 974.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x659x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x659x2048): 73.881
Elapsed time for attention_prob_times_values (80x2048x2048x659): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x659): 63.892

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 950.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x660x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x660x2048): 75.346
Elapsed time for attention_prob_times_values (80x2048x2048x660): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x660): 66.490

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 981.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x661x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x661x2048): 72.570
Elapsed time for attention_prob_times_values (80x2048x2048x661): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x661): 64.141

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 947.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x662x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x662x2048): 74.924
Elapsed time for attention_prob_times_values (80x2048x2048x662): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x662): 66.175

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 978.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x663x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x663x2048): 73.885
Elapsed time for attention_prob_times_values (80x2048x2048x663): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x663): 64.203

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 958.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x664x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x664x2048): 75.628
Elapsed time for attention_prob_times_values (80x2048x2048x664): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x664): 79.481

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1082.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x665x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x665x2048): 73.775
Elapsed time for attention_prob_times_values (80x2048x2048x665): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x665): 64.412

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 962.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x666x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x666x2048): 74.490
Elapsed time for attention_prob_times_values (80x2048x2048x666): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x666): 66.478

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 984.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x667x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x667x2048): 73.770
Elapsed time for attention_prob_times_values (80x2048x2048x667): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x667): 64.599

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 966.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x668x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x668x2048): 71.235
Elapsed time for attention_prob_times_values (80x2048x2048x668): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x668): 65.503

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 958.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x669x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x669x2048): 72.242
Elapsed time for attention_prob_times_values (80x2048x2048x669): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x669): 63.673

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 952.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x670x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x670x2048): 74.511
Elapsed time for attention_prob_times_values (80x2048x2048x670): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x670): 67.116

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 994.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x671x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x671x2048): 73.636
Elapsed time for attention_prob_times_values (80x2048x2048x671): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x671): 63.668

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 963.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x672x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x672x2048): 87.086
Elapsed time for attention_prob_times_values (80x2048x2048x672): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x672): 82.065

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1193.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x673x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x673x2048): 76.437
Elapsed time for attention_prob_times_values (80x2048x2048x673): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x673): 65.131

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 994.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x674x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x674x2048): 76.239
Elapsed time for attention_prob_times_values (80x2048x2048x674): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x674): 65.509

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 998.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x675x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x675x2048): 76.007
Elapsed time for attention_prob_times_values (80x2048x2048x675): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x675): 65.141

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 995.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x676x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x676x2048): 78.005
Elapsed time for attention_prob_times_values (80x2048x2048x676): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x676): 65.329

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1009.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x677x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x677x2048): 76.263
Elapsed time for attention_prob_times_values (80x2048x2048x677): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x677): 63.611

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 986.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x678x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x678x2048): 77.138
Elapsed time for attention_prob_times_values (80x2048x2048x678): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x678): 67.342

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1024.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x679x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x679x2048): 76.336
Elapsed time for attention_prob_times_values (80x2048x2048x679): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x679): 63.031

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 984.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x680x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x680x2048): 81.803
Elapsed time for attention_prob_times_values (80x2048x2048x680): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x680): 81.845

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1168.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x681x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x681x2048): 75.681
Elapsed time for attention_prob_times_values (80x2048x2048x681): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x681): 64.993

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1000.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x682x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x682x2048): 71.792
Elapsed time for attention_prob_times_values (80x2048x2048x682): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x682): 65.668

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 982.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x683x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x683x2048): 75.056
Elapsed time for attention_prob_times_values (80x2048x2048x683): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x683): 63.003

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 982.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x684x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x684x2048): 77.445
Elapsed time for attention_prob_times_values (80x2048x2048x684): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x684): 66.642

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1028.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x685x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x685x2048): 75.790
Elapsed time for attention_prob_times_values (80x2048x2048x685): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x685): 63.099

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 990.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x686x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x686x2048): 76.825
Elapsed time for attention_prob_times_values (80x2048x2048x686): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x686): 65.548

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1018.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x687x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x687x2048): 76.559
Elapsed time for attention_prob_times_values (80x2048x2048x687): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x687): 65.684

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1019.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x688x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x688x2048): 77.499
Elapsed time for attention_prob_times_values (80x2048x2048x688): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x688): 84.022

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1164.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x689x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x689x2048): 75.254
Elapsed time for attention_prob_times_values (80x2048x2048x689): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x689): 63.732

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 997.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x690x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x690x2048): 75.692
Elapsed time for attention_prob_times_values (80x2048x2048x690): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x690): 68.507

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1041.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x691x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x691x2048): 75.469
Elapsed time for attention_prob_times_values (80x2048x2048x691): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x691): 63.904

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1003.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x692x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x692x2048): 76.964
Elapsed time for attention_prob_times_values (80x2048x2048x692): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x692): 66.198

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1033.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x693x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x693x2048): 75.723
Elapsed time for attention_prob_times_values (80x2048x2048x693): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x693): 66.170

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1026.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x694x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x694x2048): 76.258
Elapsed time for attention_prob_times_values (80x2048x2048x694): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x694): 66.697

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1035.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x695x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x695x2048): 75.665
Elapsed time for attention_prob_times_values (80x2048x2048x695): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x695): 65.591

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1024.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x696x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x696x2048): 77.485
Elapsed time for attention_prob_times_values (80x2048x2048x696): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x696): 84.724

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1181.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x697x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x697x2048): 75.067
Elapsed time for attention_prob_times_values (80x2048x2048x697): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x697): 65.218

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1019.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x698x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x698x2048): 74.417
Elapsed time for attention_prob_times_values (80x2048x2048x698): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x698): 69.226

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1049.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 13980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x699x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x699x2048): 75.251
Elapsed time for attention_prob_times_values (80x2048x2048x699): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x699): 66.797

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1036.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x700x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x700x2048): 74.011
Elapsed time for attention_prob_times_values (80x2048x2048x700): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x700): 65.734

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1021.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x701x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x701x2048): 73.052
Elapsed time for attention_prob_times_values (80x2048x2048x701): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x701): 65.734

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1016.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x702x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x702x2048): 76.277
Elapsed time for attention_prob_times_values (80x2048x2048x702): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x702): 69.603

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1070.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x703x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x703x2048): 73.112
Elapsed time for attention_prob_times_values (80x2048x2048x703): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x703): 65.946

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1021.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x704x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x704x2048): 89.446
Elapsed time for attention_prob_times_values (80x2048x2048x704): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x704): 84.531

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1282.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x705x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x705x2048): 75.178
Elapsed time for attention_prob_times_values (80x2048x2048x705): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x705): 64.098

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1022.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x706x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x706x2048): 78.416
Elapsed time for attention_prob_times_values (80x2048x2048x706): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x706): 66.822

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1067.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x707x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x707x2048): 75.101
Elapsed time for attention_prob_times_values (80x2048x2048x707): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x707): 61.305

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 999.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x708x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x708x2048): 79.160
Elapsed time for attention_prob_times_values (80x2048x2048x708): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x708): 65.640

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1064.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x709x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x709x2048): 75.121
Elapsed time for attention_prob_times_values (80x2048x2048x709): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x709): 62.279

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1011.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x710x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x710x2048): 78.099
Elapsed time for attention_prob_times_values (80x2048x2048x710): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x710): 64.494

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1050.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x711x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x711x2048): 77.088
Elapsed time for attention_prob_times_values (80x2048x2048x711): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x711): 62.666

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1029.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x712x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x712x2048): 76.176
Elapsed time for attention_prob_times_values (80x2048x2048x712): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x712): 86.544

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1207.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x713x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x713x2048): 76.294
Elapsed time for attention_prob_times_values (80x2048x2048x713): 0.0195
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x713): 24.530

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 554.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x714x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x714x2048): 72.627
Elapsed time for attention_prob_times_values (80x2048x2048x714): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x714): 67.568

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1046.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x715x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x715x2048): 73.709
Elapsed time for attention_prob_times_values (80x2048x2048x715): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x715): 62.967

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1016.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x716x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x716x2048): 78.012
Elapsed time for attention_prob_times_values (80x2048x2048x716): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x716): 66.595

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1076.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x717x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x717x2048): 73.877
Elapsed time for attention_prob_times_values (80x2048x2048x717): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x717): 62.950

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1019.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x718x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x718x2048): 75.843
Elapsed time for attention_prob_times_values (80x2048x2048x718): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x718): 67.689

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1074.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x719x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x719x2048): 76.648
Elapsed time for attention_prob_times_values (80x2048x2048x719): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x719): 63.262

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1042.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x720x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x720x2048): 76.100
Elapsed time for attention_prob_times_values (80x2048x2048x720): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x720): 85.819

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1215.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x721x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x721x2048): 73.384
Elapsed time for attention_prob_times_values (80x2048x2048x721): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x721): 63.242

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1024.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x722x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x722x2048): 75.146
Elapsed time for attention_prob_times_values (80x2048x2048x722): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x722): 65.914

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1060.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x723x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x723x2048): 76.217
Elapsed time for attention_prob_times_values (80x2048x2048x723): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x723): 63.321

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1045.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x724x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x724x2048): 75.223
Elapsed time for attention_prob_times_values (80x2048x2048x724): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x724): 65.087

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1056.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x725x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x725x2048): 75.070
Elapsed time for attention_prob_times_values (80x2048x2048x725): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x725): 63.456

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1042.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x726x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x726x2048): 76.151
Elapsed time for attention_prob_times_values (80x2048x2048x726): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x726): 66.536

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1078.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x727x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x727x2048): 72.490
Elapsed time for attention_prob_times_values (80x2048x2048x727): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x727): 61.925

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1015.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x728x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x728x2048): 78.301
Elapsed time for attention_prob_times_values (80x2048x2048x728): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x728): 86.611

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1251.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x729x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x729x2048): 75.459
Elapsed time for attention_prob_times_values (80x2048x2048x729): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x729): 64.156

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1056.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x730x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x730x2048): 73.897
Elapsed time for attention_prob_times_values (80x2048x2048x730): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x730): 65.998

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1063.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x731x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x731x2048): 75.867
Elapsed time for attention_prob_times_values (80x2048x2048x731): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x731): 64.251

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1062.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x732x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x732x2048): 74.182
Elapsed time for attention_prob_times_values (80x2048x2048x732): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x732): 66.550

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1073.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x733x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x733x2048): 72.325
Elapsed time for attention_prob_times_values (80x2048x2048x733): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x733): 64.459

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1044.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x734x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x734x2048): 76.636
Elapsed time for attention_prob_times_values (80x2048x2048x734): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x734): 66.929

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1095.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x735x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x735x2048): 72.808
Elapsed time for attention_prob_times_values (80x2048x2048x735): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x735): 65.109

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1055.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x736x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x736x2048): 90.092
Elapsed time for attention_prob_times_values (80x2048x2048x736): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x736): 90.371

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1387.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x737x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x737x2048): 76.125
Elapsed time for attention_prob_times_values (80x2048x2048x737): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x737): 65.708

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1085.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x738x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x738x2048): 78.308
Elapsed time for attention_prob_times_values (80x2048x2048x738): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x738): 65.055

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1095.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x739x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x739x2048): 77.711
Elapsed time for attention_prob_times_values (80x2048x2048x739): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x739): 65.795

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1099.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x740x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x740x2048): 79.343
Elapsed time for attention_prob_times_values (80x2048x2048x740): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x740): 67.245

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1124.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x741x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x741x2048): 76.135
Elapsed time for attention_prob_times_values (80x2048x2048x741): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x741): 63.972

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1075.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x742x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x742x2048): 76.790
Elapsed time for attention_prob_times_values (80x2048x2048x742): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x742): 66.518

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1104.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x743x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x743x2048): 74.935
Elapsed time for attention_prob_times_values (80x2048x2048x743): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x743): 64.524

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1075.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x744x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x744x2048): 76.822
Elapsed time for attention_prob_times_values (80x2048x2048x744): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x744): 89.991

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1287.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x745x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x745x2048): 76.132
Elapsed time for attention_prob_times_values (80x2048x2048x745): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x745): 65.586

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1095.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x746x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x746x2048): 77.199
Elapsed time for attention_prob_times_values (80x2048x2048x746): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x746): 64.389

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1093.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x747x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x747x2048): 76.867
Elapsed time for attention_prob_times_values (80x2048x2048x747): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x747): 64.541

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1093.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x748x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x748x2048): 78.170
Elapsed time for attention_prob_times_values (80x2048x2048x748): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x748): 65.380

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1111.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 14980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x749x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x749x2048): 76.758
Elapsed time for attention_prob_times_values (80x2048x2048x749): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x749): 63.559

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1086.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x750x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x750x2048): 75.277
Elapsed time for attention_prob_times_values (80x2048x2048x750): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x750): 68.072

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1118.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x751x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x751x2048): 76.877
Elapsed time for attention_prob_times_values (80x2048x2048x751): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x751): 65.488

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1108.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x752x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x752x2048): 78.644
Elapsed time for attention_prob_times_values (80x2048x2048x752): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x752): 84.022

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1274.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x753x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x753x2048): 76.181
Elapsed time for attention_prob_times_values (80x2048x2048x753): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x753): 65.858

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1109.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x754x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x754x2048): 76.546
Elapsed time for attention_prob_times_values (80x2048x2048x754): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x754): 68.365

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1135.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x755x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x755x2048): 76.192
Elapsed time for attention_prob_times_values (80x2048x2048x755): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x755): 66.150

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1115.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x756x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x756x2048): 77.425
Elapsed time for attention_prob_times_values (80x2048x2048x756): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x756): 68.683

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1147.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x757x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x757x2048): 74.414
Elapsed time for attention_prob_times_values (80x2048x2048x757): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x757): 66.031

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1104.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x758x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x758x2048): 75.253
Elapsed time for attention_prob_times_values (80x2048x2048x758): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x758): 68.388

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1132.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x759x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x759x2048): 76.404
Elapsed time for attention_prob_times_values (80x2048x2048x759): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x759): 65.863

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1119.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x760x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x760x2048): 74.999
Elapsed time for attention_prob_times_values (80x2048x2048x760): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x760): 91.466

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1305.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x761x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x761x2048): 76.061
Elapsed time for attention_prob_times_values (80x2048x2048x761): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x761): 62.693

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1090.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x762x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x762x2048): 76.736
Elapsed time for attention_prob_times_values (80x2048x2048x762): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x762): 67.580

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1141.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x763x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x763x2048): 76.085
Elapsed time for attention_prob_times_values (80x2048x2048x763): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x763): 66.271

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1126.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x764x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x764x2048): 77.413
Elapsed time for attention_prob_times_values (80x2048x2048x764): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x764): 68.925

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1161.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x765x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x765x2048): 75.996
Elapsed time for attention_prob_times_values (80x2048x2048x765): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x765): 65.461

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1121.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x766x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x766x2048): 76.842
Elapsed time for attention_prob_times_values (80x2048x2048x766): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x766): 69.130

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1161.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x767x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x767x2048): 75.911
Elapsed time for attention_prob_times_values (80x2048x2048x767): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x767): 65.451

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1123.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x768x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x768x2048): 86.893
Elapsed time for attention_prob_times_values (80x2048x2048x768): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x768): 96.743

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1464.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x769x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x769x2048): 75.917
Elapsed time for attention_prob_times_values (80x2048x2048x769): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x769): 61.606

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1089.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x770x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x770x2048): 78.196
Elapsed time for attention_prob_times_values (80x2048x2048x770): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x770): 64.537

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1134.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x771x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x771x2048): 76.965
Elapsed time for attention_prob_times_values (80x2048x2048x771): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x771): 62.438

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1107.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x772x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x772x2048): 79.297
Elapsed time for attention_prob_times_values (80x2048x2048x772): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x772): 65.003

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1148.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x773x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x773x2048): 77.181
Elapsed time for attention_prob_times_values (80x2048x2048x773): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x773): 61.950

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1106.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x774x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x774x2048): 75.578
Elapsed time for attention_prob_times_values (80x2048x2048x774): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x774): 65.544

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1131.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x775x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x775x2048): 77.573
Elapsed time for attention_prob_times_values (80x2048x2048x775): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x775): 60.192

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1093.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x776x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x776x2048): 79.197
Elapsed time for attention_prob_times_values (80x2048x2048x776): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x776): 77.928

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1269.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x777x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x777x2048): 74.907
Elapsed time for attention_prob_times_values (80x2048x2048x777): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x777): 61.137

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1089.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x778x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x778x2048): 74.973
Elapsed time for attention_prob_times_values (80x2048x2048x778): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x778): 64.236

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1120.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x779x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x779x2048): 75.342
Elapsed time for attention_prob_times_values (80x2048x2048x779): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x779): 61.418

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1097.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x780x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x780x2048): 77.065
Elapsed time for attention_prob_times_values (80x2048x2048x780): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x780): 65.923

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1153.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x781x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x781x2048): 77.251
Elapsed time for attention_prob_times_values (80x2048x2048x781): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x781): 62.080

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1118.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x782x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x782x2048): 76.665
Elapsed time for attention_prob_times_values (80x2048x2048x782): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x782): 63.808

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1133.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x783x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x783x2048): 75.646
Elapsed time for attention_prob_times_values (80x2048x2048x783): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x783): 63.020

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1120.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x784x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x784x2048): 76.557
Elapsed time for attention_prob_times_values (80x2048x2048x784): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x784): 82.759

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1297.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x785x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x785x2048): 75.761
Elapsed time for attention_prob_times_values (80x2048x2048x785): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x785): 62.912

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1122.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x786x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x786x2048): 76.715
Elapsed time for attention_prob_times_values (80x2048x2048x786): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x786): 64.851

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1149.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x787x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x787x2048): 76.898
Elapsed time for attention_prob_times_values (80x2048x2048x787): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x787): 61.960

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1123.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x788x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x788x2048): 78.287
Elapsed time for attention_prob_times_values (80x2048x2048x788): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x788): 66.655

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1180.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x789x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x789x2048): 75.219
Elapsed time for attention_prob_times_values (80x2048x2048x789): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x789): 63.511

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1130.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x790x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x790x2048): 76.994
Elapsed time for attention_prob_times_values (80x2048x2048x790): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x790): 66.660

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1173.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x791x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x791x2048): 74.594
Elapsed time for attention_prob_times_values (80x2048x2048x791): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x791): 63.646

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1129.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x792x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x792x2048): 78.924
Elapsed time for attention_prob_times_values (80x2048x2048x792): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x792): 84.436

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1343.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x793x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x793x2048): 75.064
Elapsed time for attention_prob_times_values (80x2048x2048x793): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x793): 61.984

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1119.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x794x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x794x2048): 77.367
Elapsed time for attention_prob_times_values (80x2048x2048x794): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x794): 66.819

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1183.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x795x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x795x2048): 76.750
Elapsed time for attention_prob_times_values (80x2048x2048x795): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x795): 61.649

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1130.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x796x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x796x2048): 76.378
Elapsed time for attention_prob_times_values (80x2048x2048x796): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x796): 65.509

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1167.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x797x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x797x2048): 73.875
Elapsed time for attention_prob_times_values (80x2048x2048x797): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x797): 62.746

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1124.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x798x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x798x2048): 75.171
Elapsed time for attention_prob_times_values (80x2048x2048x798): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x798): 65.377

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1159.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 15980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x799x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x799x2048): 74.725
Elapsed time for attention_prob_times_values (80x2048x2048x799): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x799): 62.479

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1130.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x800x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x800x2048): 91.507
Elapsed time for attention_prob_times_values (80x2048x2048x800): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x800): 83.872

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1455.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x801x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x801x2048): 79.123
Elapsed time for attention_prob_times_values (80x2048x2048x801): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x801): 62.785

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1165.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x802x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x802x2048): 77.304
Elapsed time for attention_prob_times_values (80x2048x2048x802): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x802): 67.416

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1200.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x803x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x803x2048): 76.401
Elapsed time for attention_prob_times_values (80x2048x2048x803): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x803): 64.453

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1166.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x804x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x804x2048): 80.128
Elapsed time for attention_prob_times_values (80x2048x2048x804): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x804): 67.233

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1221.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x805x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x805x2048): 78.613
Elapsed time for attention_prob_times_values (80x2048x2048x805): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x805): 63.966

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1179.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x806x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x806x2048): 79.553
Elapsed time for attention_prob_times_values (80x2048x2048x806): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x806): 65.082

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1198.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x807x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x807x2048): 78.716
Elapsed time for attention_prob_times_values (80x2048x2048x807): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x807): 64.658

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1190.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x808x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x808x2048): 80.467
Elapsed time for attention_prob_times_values (80x2048x2048x808): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x808): 84.673

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1384.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x809x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x809x2048): 77.784
Elapsed time for attention_prob_times_values (80x2048x2048x809): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x809): 64.724

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1187.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x810x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x810x2048): 75.695
Elapsed time for attention_prob_times_values (80x2048x2048x810): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x810): 66.030

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1186.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x811x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x811x2048): 77.913
Elapsed time for attention_prob_times_values (80x2048x2048x811): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x811): 61.523

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1157.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x812x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x812x2048): 76.448
Elapsed time for attention_prob_times_values (80x2048x2048x812): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x812): 67.928

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1212.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x813x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x813x2048): 74.872
Elapsed time for attention_prob_times_values (80x2048x2048x813): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x813): 62.223

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1147.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x814x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x814x2048): 79.316
Elapsed time for attention_prob_times_values (80x2048x2048x814): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x814): 80.884

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1353.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x815x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x815x2048): 78.252
Elapsed time for attention_prob_times_values (80x2048x2048x815): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x815): 78.884

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1329.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x816x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x816x2048): 83.573
Elapsed time for attention_prob_times_values (80x2048x2048x816): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x816): 87.107

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1444.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x817x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x817x2048): 75.594
Elapsed time for attention_prob_times_values (80x2048x2048x817): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x817): 77.284

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1296.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x818x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x818x2048): 76.484
Elapsed time for attention_prob_times_values (80x2048x2048x818): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x818): 77.833

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1309.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x819x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x819x2048): 75.926
Elapsed time for attention_prob_times_values (80x2048x2048x819): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x819): 76.228

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1293.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x820x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x820x2048): 78.259
Elapsed time for attention_prob_times_values (80x2048x2048x820): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x820): 79.796

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1344.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x821x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x821x2048): 77.459
Elapsed time for attention_prob_times_values (80x2048x2048x821): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x821): 75.495

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1302.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x822x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x822x2048): 74.888
Elapsed time for attention_prob_times_values (80x2048x2048x822): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x822): 78.370

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1306.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x823x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x823x2048): 78.482
Elapsed time for attention_prob_times_values (80x2048x2048x823): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x823): 76.373

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1321.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x824x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x824x2048): 77.719
Elapsed time for attention_prob_times_values (80x2048x2048x824): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x824): 87.878

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1410.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x825x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x825x2048): 72.310
Elapsed time for attention_prob_times_values (80x2048x2048x825): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x825): 77.092

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1277.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x826x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x826x2048): 75.965
Elapsed time for attention_prob_times_values (80x2048x2048x826): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x826): 77.435

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1313.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x827x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x827x2048): 73.077
Elapsed time for attention_prob_times_values (80x2048x2048x827): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x827): 77.503

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1290.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x828x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x828x2048): 74.245
Elapsed time for attention_prob_times_values (80x2048x2048x828): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x828): 82.023

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1338.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x829x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x829x2048): 77.828
Elapsed time for attention_prob_times_values (80x2048x2048x829): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x829): 80.002

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1356.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x830x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x830x2048): 76.701
Elapsed time for attention_prob_times_values (80x2048x2048x830): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x830): 82.667

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1369.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x831x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x831x2048): 74.876
Elapsed time for attention_prob_times_values (80x2048x2048x831): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x831): 79.590

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1329.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x832x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x832x2048): 91.831
Elapsed time for attention_prob_times_values (80x2048x2048x832): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x832): 88.761

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1557.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x833x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x833x2048): 79.621
Elapsed time for attention_prob_times_values (80x2048x2048x833): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x833): 78.544

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1365.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x834x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x834x2048): 79.655
Elapsed time for attention_prob_times_values (80x2048x2048x834): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x834): 83.006

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1405.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x835x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x835x2048): 79.523
Elapsed time for attention_prob_times_values (80x2048x2048x835): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x835): 79.981

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1380.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x836x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x836x2048): 81.358
Elapsed time for attention_prob_times_values (80x2048x2048x836): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x836): 82.994

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1423.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x837x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x837x2048): 79.327
Elapsed time for attention_prob_times_values (80x2048x2048x837): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x837): 80.819

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1388.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x838x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x838x2048): 80.235
Elapsed time for attention_prob_times_values (80x2048x2048x838): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x838): 80.416

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1395.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x839x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x839x2048): 79.160
Elapsed time for attention_prob_times_values (80x2048x2048x839): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x839): 81.020

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1392.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x840x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x840x2048): 80.870
Elapsed time for attention_prob_times_values (80x2048x2048x840): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x840): 89.131

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1476.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x841x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x841x2048): 75.865
Elapsed time for attention_prob_times_values (80x2048x2048x841): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x841): 77.687

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1337.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x842x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x842x2048): 78.922
Elapsed time for attention_prob_times_values (80x2048x2048x842): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x842): 83.179

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1412.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x843x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x843x2048): 78.301
Elapsed time for attention_prob_times_values (80x2048x2048x843): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x843): 81.111

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1391.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x844x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x844x2048): 79.937
Elapsed time for attention_prob_times_values (80x2048x2048x844): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x844): 78.724

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1386.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x845x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x845x2048): 78.242
Elapsed time for attention_prob_times_values (80x2048x2048x845): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x845): 81.121

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1394.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x846x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x846x2048): 78.764
Elapsed time for attention_prob_times_values (80x2048x2048x846): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x846): 83.565

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1421.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x847x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x847x2048): 77.749
Elapsed time for attention_prob_times_values (80x2048x2048x847): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x847): 81.255

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1394.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x848x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x848x2048): 75.636
Elapsed time for attention_prob_times_values (80x2048x2048x848): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x848): 89.619

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1440.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 16980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x849x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x849x2048): 77.385
Elapsed time for attention_prob_times_values (80x2048x2048x849): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x849): 81.588

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1396.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x850x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x850x2048): 75.821
Elapsed time for attention_prob_times_values (80x2048x2048x850): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x850): 83.623

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1399.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x851x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x851x2048): 77.898
Elapsed time for attention_prob_times_values (80x2048x2048x851): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x851): 81.722

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1405.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x852x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x852x2048): 78.177
Elapsed time for attention_prob_times_values (80x2048x2048x852): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x852): 83.656

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1425.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x853x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x853x2048): 77.441
Elapsed time for attention_prob_times_values (80x2048x2048x853): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x853): 81.830

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1405.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x854x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x854x2048): 78.767
Elapsed time for attention_prob_times_values (80x2048x2048x854): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x854): 83.940

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1436.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x855x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x855x2048): 78.167
Elapsed time for attention_prob_times_values (80x2048x2048x855): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x855): 81.145

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1409.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x856x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x856x2048): 80.141
Elapsed time for attention_prob_times_values (80x2048x2048x856): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x856): 90.549

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1506.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x857x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x857x2048): 77.504
Elapsed time for attention_prob_times_values (80x2048x2048x857): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x857): 80.682

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1402.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x858x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x858x2048): 76.944
Elapsed time for attention_prob_times_values (80x2048x2048x858): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x858): 82.656

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1415.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x859x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x859x2048): 75.570
Elapsed time for attention_prob_times_values (80x2048x2048x859): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x859): 81.701

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1395.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x860x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x860x2048): 79.408
Elapsed time for attention_prob_times_values (80x2048x2048x860): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x860): 84.731

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1459.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x861x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x861x2048): 75.313
Elapsed time for attention_prob_times_values (80x2048x2048x861): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x861): 81.911

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1398.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x862x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x862x2048): 78.534
Elapsed time for attention_prob_times_values (80x2048x2048x862): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x862): 84.899

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1455.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x863x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x863x2048): 77.755
Elapsed time for attention_prob_times_values (80x2048x2048x863): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x863): 81.833

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1423.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x864x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x864x2048): 90.042
Elapsed time for attention_prob_times_values (80x2048x2048x864): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x864): 90.343

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1612.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x865x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x865x2048): 79.458
Elapsed time for attention_prob_times_values (80x2048x2048x865): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x865): 82.823

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1451.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x866x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x866x2048): 80.313
Elapsed time for attention_prob_times_values (80x2048x2048x866): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x866): 85.167

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1480.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x867x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x867x2048): 77.485
Elapsed time for attention_prob_times_values (80x2048x2048x867): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x867): 81.411

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1423.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x868x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x868x2048): 80.557
Elapsed time for attention_prob_times_values (80x2048x2048x868): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x868): 84.286

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1478.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x869x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x869x2048): 78.772
Elapsed time for attention_prob_times_values (80x2048x2048x869): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x869): 82.936

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1452.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x870x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x870x2048): 77.270
Elapsed time for attention_prob_times_values (80x2048x2048x870): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x870): 83.644

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1445.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x871x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x871x2048): 78.125
Elapsed time for attention_prob_times_values (80x2048x2048x871): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x871): 83.048

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1450.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x872x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x872x2048): 77.131
Elapsed time for attention_prob_times_values (80x2048x2048x872): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x872): 91.989

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1512.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x873x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x873x2048): 77.927
Elapsed time for attention_prob_times_values (80x2048x2048x873): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x873): 83.202

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1452.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x874x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x874x2048): 78.960
Elapsed time for attention_prob_times_values (80x2048x2048x874): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x874): 82.211

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1455.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x875x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x875x2048): 78.257
Elapsed time for attention_prob_times_values (80x2048x2048x875): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x875): 83.173

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1458.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x876x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x876x2048): 77.349
Elapsed time for attention_prob_times_values (80x2048x2048x876): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x876): 81.888

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1440.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x877x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x877x2048): 78.277
Elapsed time for attention_prob_times_values (80x2048x2048x877): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x877): 83.292

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1463.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x878x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x878x2048): 79.076
Elapsed time for attention_prob_times_values (80x2048x2048x878): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x878): 85.674

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1492.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x879x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x879x2048): 78.291
Elapsed time for attention_prob_times_values (80x2048x2048x879): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x879): 83.449

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1467.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x880x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x880x2048): 80.437
Elapsed time for attention_prob_times_values (80x2048x2048x880): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x880): 90.662

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1550.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x881x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x881x2048): 77.819
Elapsed time for attention_prob_times_values (80x2048x2048x881): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x881): 83.647

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1467.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x882x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x882x2048): 76.121
Elapsed time for attention_prob_times_values (80x2048x2048x882): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x882): 84.379

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1458.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x883x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x883x2048): 76.301
Elapsed time for attention_prob_times_values (80x2048x2048x883): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x883): 81.645

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1439.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x884x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x884x2048): 79.145
Elapsed time for attention_prob_times_values (80x2048x2048x884): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x884): 85.252

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1499.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x885x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x885x2048): 77.809
Elapsed time for attention_prob_times_values (80x2048x2048x885): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x885): 83.823

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1475.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x886x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x886x2048): 77.150
Elapsed time for attention_prob_times_values (80x2048x2048x886): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x886): 86.174

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1490.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x887x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x887x2048): 75.204
Elapsed time for attention_prob_times_values (80x2048x2048x887): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x887): 83.795

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1452.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x888x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x888x2048): 79.744
Elapsed time for attention_prob_times_values (80x2048x2048x888): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x888): 93.338

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1577.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x889x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x889x2048): 77.642
Elapsed time for attention_prob_times_values (80x2048x2048x889): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x889): 84.097

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1482.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x890x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x890x2048): 78.214
Elapsed time for attention_prob_times_values (80x2048x2048x890): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x890): 86.737

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1512.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x891x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x891x2048): 76.548
Elapsed time for attention_prob_times_values (80x2048x2048x891): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x891): 84.315

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1476.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x892x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x892x2048): 75.847
Elapsed time for attention_prob_times_values (80x2048x2048x892): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x892): 87.052

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1493.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x893x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x893x2048): 74.051
Elapsed time for attention_prob_times_values (80x2048x2048x893): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x893): 82.553

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1439.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x894x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x894x2048): 78.368
Elapsed time for attention_prob_times_values (80x2048x2048x894): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x894): 87.081

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1522.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x895x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x895x2048): 74.965
Elapsed time for attention_prob_times_values (80x2048x2048x895): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x895): 84.489

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1468.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x896x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x896x2048): 85.117
Elapsed time for attention_prob_times_values (80x2048x2048x896): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x896): 95.745

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1667.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x897x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x897x2048): 77.089
Elapsed time for attention_prob_times_values (80x2048x2048x897): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x897): 74.003

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1398.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x898x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x898x2048): 77.879
Elapsed time for attention_prob_times_values (80x2048x2048x898): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x898): 77.004

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1435.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 17980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x899x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x899x2048): 78.875
Elapsed time for attention_prob_times_values (80x2048x2048x899): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x899): 76.602

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1442.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x900x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x900x2048): 78.524
Elapsed time for attention_prob_times_values (80x2048x2048x900): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x900): 76.964

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1444.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x901x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x901x2048): 76.589
Elapsed time for attention_prob_times_values (80x2048x2048x901): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x901): 74.118

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1401.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x902x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x902x2048): 76.159
Elapsed time for attention_prob_times_values (80x2048x2048x902): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x902): 77.159

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1427.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x903x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x903x2048): 76.938
Elapsed time for attention_prob_times_values (80x2048x2048x903): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x903): 76.625

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1430.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x904x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x904x2048): 80.822
Elapsed time for attention_prob_times_values (80x2048x2048x904): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x904): 84.623

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1542.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x905x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x905x2048): 78.799
Elapsed time for attention_prob_times_values (80x2048x2048x905): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x905): 76.966

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1454.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x906x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x906x2048): 79.770
Elapsed time for attention_prob_times_values (80x2048x2048x906): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x906): 79.210

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1486.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x907x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x907x2048): 79.436
Elapsed time for attention_prob_times_values (80x2048x2048x907): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x907): 77.134

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1464.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x908x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x908x2048): 79.017
Elapsed time for attention_prob_times_values (80x2048x2048x908): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x908): 76.707

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1458.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x909x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x909x2048): 77.672
Elapsed time for attention_prob_times_values (80x2048x2048x909): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x909): 74.894

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1430.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x910x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x910x2048): 78.216
Elapsed time for attention_prob_times_values (80x2048x2048x910): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x910): 77.172

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1458.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x911x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x911x2048): 76.673
Elapsed time for attention_prob_times_values (80x2048x2048x911): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x911): 75.187

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1426.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x912x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x912x2048): 81.110
Elapsed time for attention_prob_times_values (80x2048x2048x912): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x912): 80.396

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1519.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x913x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x913x2048): 79.263
Elapsed time for attention_prob_times_values (80x2048x2048x913): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x913): 77.898

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1479.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x914x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x914x2048): 76.158
Elapsed time for attention_prob_times_values (80x2048x2048x914): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x914): 76.361

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1437.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x915x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x915x2048): 78.901
Elapsed time for attention_prob_times_values (80x2048x2048x915): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x915): 74.889

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1450.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x916x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x916x2048): 77.711
Elapsed time for attention_prob_times_values (80x2048x2048x916): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x916): 79.097

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1480.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x917x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x917x2048): 74.612
Elapsed time for attention_prob_times_values (80x2048x2048x917): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x917): 78.214

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1444.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x918x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x918x2048): 79.712
Elapsed time for attention_prob_times_values (80x2048x2048x918): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x918): 76.351

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1476.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x919x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x919x2048): 76.522
Elapsed time for attention_prob_times_values (80x2048x2048x919): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x919): 78.439

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1467.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x920x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x920x2048): 80.752
Elapsed time for attention_prob_times_values (80x2048x2048x920): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x920): 86.147

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1581.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x921x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x921x2048): 78.450
Elapsed time for attention_prob_times_values (80x2048x2048x921): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x921): 75.810

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1464.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x922x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x922x2048): 75.651
Elapsed time for attention_prob_times_values (80x2048x2048x922): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x922): 78.540

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1464.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x923x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x923x2048): 77.104
Elapsed time for attention_prob_times_values (80x2048x2048x923): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x923): 76.219

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1458.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x924x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x924x2048): 80.186
Elapsed time for attention_prob_times_values (80x2048x2048x924): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x924): 78.317

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1509.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x925x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x925x2048): 78.643
Elapsed time for attention_prob_times_values (80x2048x2048x925): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x925): 78.612

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1499.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x926x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x926x2048): 77.313
Elapsed time for attention_prob_times_values (80x2048x2048x926): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x926): 79.441

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1495.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x927x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x927x2048): 78.961
Elapsed time for attention_prob_times_values (80x2048x2048x927): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x927): 77.861

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1498.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x928x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x928x2048): 92.310
Elapsed time for attention_prob_times_values (80x2048x2048x928): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x928): 87.542

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1718.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x929x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x929x2048): 79.599
Elapsed time for attention_prob_times_values (80x2048x2048x929): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x929): 76.470

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1493.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x930x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x930x2048): 81.132
Elapsed time for attention_prob_times_values (80x2048x2048x930): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x930): 79.423

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1538.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x931x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x931x2048): 77.751
Elapsed time for attention_prob_times_values (80x2048x2048x931): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x931): 75.618

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1470.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x932x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x932x2048): 79.681
Elapsed time for attention_prob_times_values (80x2048x2048x932): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x932): 79.243

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1525.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x933x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x933x2048): 80.360
Elapsed time for attention_prob_times_values (80x2048x2048x933): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x933): 78.480

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1526.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x934x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x934x2048): 77.866
Elapsed time for attention_prob_times_values (80x2048x2048x934): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x934): 78.599

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1505.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x935x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x935x2048): 79.233
Elapsed time for attention_prob_times_values (80x2048x2048x935): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x935): 77.905

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1513.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x936x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x936x2048): 82.072
Elapsed time for attention_prob_times_values (80x2048x2048x936): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x936): 84.583

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1606.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x937x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x937x2048): 79.233
Elapsed time for attention_prob_times_values (80x2048x2048x937): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x937): 79.268

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1529.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x938x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x938x2048): 76.986
Elapsed time for attention_prob_times_values (80x2048x2048x938): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x938): 79.329

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1509.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x939x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x939x2048): 77.300
Elapsed time for attention_prob_times_values (80x2048x2048x939): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x939): 76.813

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1490.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x940x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x940x2048): 79.014
Elapsed time for attention_prob_times_values (80x2048x2048x940): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x940): 81.622

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1554.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x941x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x941x2048): 78.713
Elapsed time for attention_prob_times_values (80x2048x2048x941): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x941): 79.578

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1533.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x942x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x942x2048): 80.649
Elapsed time for attention_prob_times_values (80x2048x2048x942): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x942): 80.954

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1567.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x943x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x943x2048): 79.642
Elapsed time for attention_prob_times_values (80x2048x2048x943): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x943): 79.394

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1544.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x944x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x944x2048): 75.996
Elapsed time for attention_prob_times_values (80x2048x2048x944): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x944): 83.209

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1544.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x945x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x945x2048): 77.084
Elapsed time for attention_prob_times_values (80x2048x2048x945): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x945): 77.640

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1505.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x946x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x946x2048): 80.281
Elapsed time for attention_prob_times_values (80x2048x2048x946): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x946): 80.121

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1562.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x947x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x947x2048): 79.344
Elapsed time for attention_prob_times_values (80x2048x2048x947): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x947): 77.245

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1526.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x948x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x948x2048): 81.062
Elapsed time for attention_prob_times_values (80x2048x2048x948): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x948): 82.213

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1593.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 18980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x949x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x949x2048): 79.637
Elapsed time for attention_prob_times_values (80x2048x2048x949): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x949): 76.940

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1528.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x950x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x950x2048): 80.334
Elapsed time for attention_prob_times_values (80x2048x2048x950): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x950): 82.201

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1588.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x951x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x951x2048): 79.544
Elapsed time for attention_prob_times_values (80x2048x2048x951): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x951): 76.248

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1524.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x952x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x952x2048): 77.874
Elapsed time for attention_prob_times_values (80x2048x2048x952): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x952): 88.806

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1625.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x953x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x953x2048): 79.252
Elapsed time for attention_prob_times_values (80x2048x2048x953): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x953): 78.096

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1542.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x954x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x954x2048): 79.926
Elapsed time for attention_prob_times_values (80x2048x2048x954): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x954): 79.142

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1561.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x955x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x955x2048): 79.325
Elapsed time for attention_prob_times_values (80x2048x2048x955): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x955): 79.496

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1560.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x956x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x956x2048): 78.331
Elapsed time for attention_prob_times_values (80x2048x2048x956): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x956): 82.095

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1577.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x957x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x957x2048): 79.332
Elapsed time for attention_prob_times_values (80x2048x2048x957): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x957): 77.790

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1546.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x958x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x958x2048): 78.924
Elapsed time for attention_prob_times_values (80x2048x2048x958): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x958): 80.763

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1573.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x959x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x959x2048): 77.869
Elapsed time for attention_prob_times_values (80x2048x2048x959): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x959): 79.491

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1552.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x960x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x960x2048): 92.527
Elapsed time for attention_prob_times_values (80x2048x2048x960): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x960): 90.192

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1804.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x961x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x961x2048): 81.033
Elapsed time for attention_prob_times_values (80x2048x2048x961): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x961): 80.709

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1598.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x962x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x962x2048): 80.243
Elapsed time for attention_prob_times_values (80x2048x2048x962): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x962): 81.745

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1602.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x963x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x963x2048): 79.364
Elapsed time for attention_prob_times_values (80x2048x2048x963): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x963): 77.723

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1555.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x964x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x964x2048): 82.605
Elapsed time for attention_prob_times_values (80x2048x2048x964): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x964): 83.275

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1644.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x965x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x965x2048): 78.055
Elapsed time for attention_prob_times_values (80x2048x2048x965): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x965): 79.585

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1564.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x966x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x966x2048): 80.619
Elapsed time for attention_prob_times_values (80x2048x2048x966): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x966): 80.184

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1597.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x967x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x967x2048): 80.665
Elapsed time for attention_prob_times_values (80x2048x2048x967): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x967): 79.643

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1593.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x968x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x968x2048): 79.059
Elapsed time for attention_prob_times_values (80x2048x2048x968): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x968): 89.016

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1666.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x969x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x969x2048): 79.833
Elapsed time for attention_prob_times_values (80x2048x2048x969): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x969): 81.214

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1604.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x970x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x970x2048): 78.071
Elapsed time for attention_prob_times_values (80x2048x2048x970): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x970): 81.460

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1590.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x971x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x971x2048): 78.554
Elapsed time for attention_prob_times_values (80x2048x2048x971): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x971): 81.608

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1598.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x972x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x972x2048): 79.577
Elapsed time for attention_prob_times_values (80x2048x2048x972): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x972): 81.736

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1611.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x973x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x973x2048): 78.691
Elapsed time for attention_prob_times_values (80x2048x2048x973): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x973): 81.869

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1605.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x974x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x974x2048): 81.215
Elapsed time for attention_prob_times_values (80x2048x2048x974): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x974): 84.030

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1653.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x975x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x975x2048): 80.243
Elapsed time for attention_prob_times_values (80x2048x2048x975): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x975): 78.600

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1591.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x976x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x976x2048): 80.285
Elapsed time for attention_prob_times_values (80x2048x2048x976): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x976): 88.981

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1693.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x977x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x977x2048): 79.644
Elapsed time for attention_prob_times_values (80x2048x2048x977): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x977): 82.230

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1624.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x978x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x978x2048): 80.548
Elapsed time for attention_prob_times_values (80x2048x2048x978): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x978): 79.617

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1609.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x979x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x979x2048): 79.249
Elapsed time for attention_prob_times_values (80x2048x2048x979): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x979): 82.425

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1625.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x980x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x980x2048): 78.719
Elapsed time for attention_prob_times_values (80x2048x2048x980): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x980): 83.011

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1627.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x981x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x981x2048): 79.870
Elapsed time for attention_prob_times_values (80x2048x2048x981): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x981): 79.329

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1604.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x982x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x982x2048): 78.758
Elapsed time for attention_prob_times_values (80x2048x2048x982): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x982): 84.598

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1646.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x983x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x983x2048): 78.538
Elapsed time for attention_prob_times_values (80x2048x2048x983): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x983): 82.656

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1626.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x984x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x984x2048): 80.463
Elapsed time for attention_prob_times_values (80x2048x2048x984): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x984): 89.013

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1708.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x985x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x985x2048): 78.600
Elapsed time for attention_prob_times_values (80x2048x2048x985): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x985): 81.230

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1616.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x986x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x986x2048): 77.832
Elapsed time for attention_prob_times_values (80x2048x2048x986): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x986): 84.914

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1645.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x987x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x987x2048): 77.231
Elapsed time for attention_prob_times_values (80x2048x2048x987): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x987): 82.326

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1616.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x988x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x988x2048): 78.492
Elapsed time for attention_prob_times_values (80x2048x2048x988): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x988): 84.770

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1654.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x989x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x989x2048): 79.774
Elapsed time for attention_prob_times_values (80x2048x2048x989): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x989): 79.966

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1622.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x990x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x990x2048): 77.554
Elapsed time for attention_prob_times_values (80x2048x2048x990): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x990): 85.196

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1651.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x991x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x991x2048): 76.937
Elapsed time for attention_prob_times_values (80x2048x2048x991): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x991): 83.093

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1626.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x992x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x992x2048): 90.633
Elapsed time for attention_prob_times_values (80x2048x2048x992): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x992): 89.323

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1833.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x993x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x993x2048): 77.339
Elapsed time for attention_prob_times_values (80x2048x2048x993): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x993): 80.913

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1612.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x994x2048): 0.0199
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x994x2048): 33.460
Elapsed time for attention_prob_times_values (80x2048x2048x994): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x994): 80.791

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 966.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0282
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x995x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x995x2048): 80.980
Elapsed time for attention_prob_times_values (80x2048x2048x995): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x995): 82.875

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1673.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x996x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x996x2048): 82.699
Elapsed time for attention_prob_times_values (80x2048x2048x996): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x996): 85.340

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1718.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x997x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x997x2048): 78.035
Elapsed time for attention_prob_times_values (80x2048x2048x997): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x997): 79.995

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1617.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x998x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x998x2048): 81.450
Elapsed time for attention_prob_times_values (80x2048x2048x998): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x998): 83.238

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1687.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 19980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x999x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x999x2048): 77.682
Elapsed time for attention_prob_times_values (80x2048x2048x999): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x999): 82.141

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1637.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1000x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1000x2048): 79.019
Elapsed time for attention_prob_times_values (80x2048x2048x1000): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1000): 93.114

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1755.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1001x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1001x2048): 79.972
Elapsed time for attention_prob_times_values (80x2048x2048x1001): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1001): 78.787

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1631.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1002x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1002x2048): 78.867
Elapsed time for attention_prob_times_values (80x2048x2048x1002): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1002): 84.236

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1675.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1003x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1003x2048): 74.326
Elapsed time for attention_prob_times_values (80x2048x2048x1003): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1003): 81.652

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1602.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1004x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1004x2048): 81.906
Elapsed time for attention_prob_times_values (80x2048x2048x1004): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1004): 85.923

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1728.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1005x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1005x2048): 79.526
Elapsed time for attention_prob_times_values (80x2048x2048x1005): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1005): 83.909

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1684.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1006x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1006x2048): 80.992
Elapsed time for attention_prob_times_values (80x2048x2048x1006): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1006): 85.265

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1715.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1007x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1007x2048): 80.239
Elapsed time for attention_prob_times_values (80x2048x2048x1007): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1007): 83.479

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1691.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1008x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1008x2048): 82.428
Elapsed time for attention_prob_times_values (80x2048x2048x1008): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1008): 94.124

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1818.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1009x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1009x2048): 78.280
Elapsed time for attention_prob_times_values (80x2048x2048x1009): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1009): 83.417

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1672.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1010x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1010x2048): 80.563
Elapsed time for attention_prob_times_values (80x2048x2048x1010): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1010): 86.573

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1729.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1011x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1011x2048): 78.080
Elapsed time for attention_prob_times_values (80x2048x2048x1011): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1011): 82.833

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1667.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1012x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1012x2048): 80.572
Elapsed time for attention_prob_times_values (80x2048x2048x1012): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1012): 86.912

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1736.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1013x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1013x2048): 77.986
Elapsed time for attention_prob_times_values (80x2048x2048x1013): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1013): 82.951

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1670.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1014x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1014x2048): 79.443
Elapsed time for attention_prob_times_values (80x2048x2048x1014): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1014): 84.008

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1698.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1015x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1015x2048): 76.987
Elapsed time for attention_prob_times_values (80x2048x2048x1015): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1015): 82.986

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1663.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1016x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1016x2048): 82.025
Elapsed time for attention_prob_times_values (80x2048x2048x1016): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1016): 93.019

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1817.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1017x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1017x2048): 74.357
Elapsed time for attention_prob_times_values (80x2048x2048x1017): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1017): 84.497

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1650.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1018x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1018x2048): 76.825
Elapsed time for attention_prob_times_values (80x2048x2048x1018): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1018): 83.116

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1667.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1019x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1019x2048): 80.424
Elapsed time for attention_prob_times_values (80x2048x2048x1019): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1019): 82.349

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1700.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1020x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1020x2048): 85.275
Elapsed time for attention_prob_times_values (80x2048x2048x1020): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1020): 87.125

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1803.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1021x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1021x2048): 78.243
Elapsed time for attention_prob_times_values (80x2048x2048x1021): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1021): 80.210

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1658.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1022x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1022x2048): 78.811
Elapsed time for attention_prob_times_values (80x2048x2048x1022): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1022): 85.071

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1715.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1023x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1023x2048): 79.678
Elapsed time for attention_prob_times_values (80x2048x2048x1023): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1023): 83.410

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1709.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1024x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1024x2048): 91.183
Elapsed time for attention_prob_times_values (80x2048x2048x1024): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1024): 96.456

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1968.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1025x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1025x2048): 79.401
Elapsed time for attention_prob_times_values (80x2048x2048x1025): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1025): 76.930

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1642.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1026x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1026x2048): 80.671
Elapsed time for attention_prob_times_values (80x2048x2048x1026): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1026): 78.215

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1671.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1027x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1027x2048): 80.493
Elapsed time for attention_prob_times_values (80x2048x2048x1027): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1027): 75.940

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1645.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1028x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1028x2048): 87.437
Elapsed time for attention_prob_times_values (80x2048x2048x1028): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1028): 79.524

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1755.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1029x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1029x2048): 80.574
Elapsed time for attention_prob_times_values (80x2048x2048x1029): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1029): 75.375

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1643.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1030x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1030x2048): 80.064
Elapsed time for attention_prob_times_values (80x2048x2048x1030): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1030): 78.697

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1676.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1031x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1031x2048): 81.148
Elapsed time for attention_prob_times_values (80x2048x2048x1031): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1031): 77.631

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1677.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1032x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1032x2048): 80.683
Elapsed time for attention_prob_times_values (80x2048x2048x1032): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1032): 84.969

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1751.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1033x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1033x2048): 78.879
Elapsed time for attention_prob_times_values (80x2048x2048x1033): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1033): 75.945

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1638.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1034x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1034x2048): 81.143
Elapsed time for attention_prob_times_values (80x2048x2048x1034): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1034): 78.701

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1693.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1035x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1035x2048): 79.532
Elapsed time for attention_prob_times_values (80x2048x2048x1035): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1035): 78.248

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1673.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1036x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1036x2048): 79.855
Elapsed time for attention_prob_times_values (80x2048x2048x1036): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1036): 78.434

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1680.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1037x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1037x2048): 77.773
Elapsed time for attention_prob_times_values (80x2048x2048x1037): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1037): 76.666

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1641.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1038x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1038x2048): 80.737
Elapsed time for attention_prob_times_values (80x2048x2048x1038): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1038): 80.721

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1717.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1039x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1039x2048): 77.442
Elapsed time for attention_prob_times_values (80x2048x2048x1039): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1039): 76.233

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1635.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1040x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1040x2048): 79.890
Elapsed time for attention_prob_times_values (80x2048x2048x1040): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1040): 83.533

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1740.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1041x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1041x2048): 80.004
Elapsed time for attention_prob_times_values (80x2048x2048x1041): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1041): 78.765

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1693.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1042x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1042x2048): 79.274
Elapsed time for attention_prob_times_values (80x2048x2048x1042): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1042): 79.586

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1695.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1043x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1043x2048): 80.092
Elapsed time for attention_prob_times_values (80x2048x2048x1043): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1043): 78.932

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1699.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1044x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1044x2048): 81.917
Elapsed time for attention_prob_times_values (80x2048x2048x1044): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1044): 81.222

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1744.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1045x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1045x2048): 80.182
Elapsed time for attention_prob_times_values (80x2048x2048x1045): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1045): 78.987

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1703.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1046x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1046x2048): 81.001
Elapsed time for attention_prob_times_values (80x2048x2048x1046): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1046): 74.713

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1665.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1047x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1047x2048): 80.408
Elapsed time for attention_prob_times_values (80x2048x2048x1047): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1047): 74.084

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1654.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1048x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1048x2048): 79.618
Elapsed time for attention_prob_times_values (80x2048x2048x1048): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1048): 86.178

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1776.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 20980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1049x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1049x2048): 78.470
Elapsed time for attention_prob_times_values (80x2048x2048x1049): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1049): 76.759

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1667.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1050x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1050x2048): 80.694
Elapsed time for attention_prob_times_values (80x2048x2048x1050): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1050): 80.961

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1738.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1051x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1051x2048): 79.593
Elapsed time for attention_prob_times_values (80x2048x2048x1051): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1051): 79.078

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1707.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1052x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1052x2048): 81.564
Elapsed time for attention_prob_times_values (80x2048x2048x1052): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1052): 81.693

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1758.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1053x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1053x2048): 80.297
Elapsed time for attention_prob_times_values (80x2048x2048x1053): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1053): 79.646

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1724.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1054x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1054x2048): 80.428
Elapsed time for attention_prob_times_values (80x2048x2048x1054): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1054): 81.819

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1750.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1055x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1055x2048): 80.317
Elapsed time for attention_prob_times_values (80x2048x2048x1055): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1055): 79.273

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1723.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1056x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1056x2048): 93.500
Elapsed time for attention_prob_times_values (80x2048x2048x1056): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1056): 85.728

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1934.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1057x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1057x2048): 80.648
Elapsed time for attention_prob_times_values (80x2048x2048x1057): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1057): 78.607

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1723.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1058x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1058x2048): 81.116
Elapsed time for attention_prob_times_values (80x2048x2048x1058): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1058): 81.650

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1763.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1059x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1059x2048): 80.255
Elapsed time for attention_prob_times_values (80x2048x2048x1059): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1059): 78.697

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1723.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1060x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1060x2048): 83.458
Elapsed time for attention_prob_times_values (80x2048x2048x1060): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1060): 82.387

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1799.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1061x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1061x2048): 81.812
Elapsed time for attention_prob_times_values (80x2048x2048x1061): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1061): 79.935

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1756.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1062x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1062x2048): 79.837
Elapsed time for attention_prob_times_values (80x2048x2048x1062): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1062): 82.372

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1762.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1063x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1063x2048): 76.929
Elapsed time for attention_prob_times_values (80x2048x2048x1063): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1063): 74.240

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1644.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1064x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1064x2048): 81.683
Elapsed time for attention_prob_times_values (80x2048x2048x1064): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1064): 83.607

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1799.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1065x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1065x2048): 80.975
Elapsed time for attention_prob_times_values (80x2048x2048x1065): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1065): 78.078

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1733.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1066x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1066x2048): 78.787
Elapsed time for attention_prob_times_values (80x2048x2048x1066): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1066): 82.632

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1760.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1067x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1067x2048): 76.411
Elapsed time for attention_prob_times_values (80x2048x2048x1067): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1067): 76.065

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1665.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1068x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1068x2048): 78.463
Elapsed time for attention_prob_times_values (80x2048x2048x1068): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1068): 80.628

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1738.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1069x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1069x2048): 81.271
Elapsed time for attention_prob_times_values (80x2048x2048x1069): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1069): 76.304

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1722.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1070x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1070x2048): 78.369
Elapsed time for attention_prob_times_values (80x2048x2048x1070): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1070): 82.835

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1763.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1071x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1071x2048): 78.935
Elapsed time for attention_prob_times_values (80x2048x2048x1071): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1071): 78.685

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1727.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1072x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1072x2048): 83.435
Elapsed time for attention_prob_times_values (80x2048x2048x1072): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1072): 86.927

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1867.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1073x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1073x2048): 78.513
Elapsed time for attention_prob_times_values (80x2048x2048x1073): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1073): 79.265

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1732.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1074x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1074x2048): 79.656
Elapsed time for attention_prob_times_values (80x2048x2048x1074): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1074): 80.873

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1763.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1075x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1075x2048): 79.128
Elapsed time for attention_prob_times_values (80x2048x2048x1075): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1075): 79.513

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1744.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1076x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1076x2048): 82.672
Elapsed time for attention_prob_times_values (80x2048x2048x1076): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1076): 82.238

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1815.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1077x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1077x2048): 79.527
Elapsed time for attention_prob_times_values (80x2048x2048x1077): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1077): 79.602

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1753.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1078x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1078x2048): 80.381
Elapsed time for attention_prob_times_values (80x2048x2048x1078): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1078): 82.242

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1793.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1079x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1079x2048): 80.262
Elapsed time for attention_prob_times_values (80x2048x2048x1079): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1079): 81.074

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1780.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1080x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1080x2048): 83.179
Elapsed time for attention_prob_times_values (80x2048x2048x1080): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1080): 88.496

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1894.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1081x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1081x2048): 79.213
Elapsed time for attention_prob_times_values (80x2048x2048x1081): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1081): 80.121

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1761.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1082x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1082x2048): 81.419
Elapsed time for attention_prob_times_values (80x2048x2048x1082): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1082): 83.532

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1825.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1083x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1083x2048): 80.764
Elapsed time for attention_prob_times_values (80x2048x2048x1083): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1083): 80.930

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1790.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1084x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1084x2048): 81.500
Elapsed time for attention_prob_times_values (80x2048x2048x1084): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1084): 83.105

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1824.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1085x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1085x2048): 80.963
Elapsed time for attention_prob_times_values (80x2048x2048x1085): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1085): 81.457

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1802.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1086x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1086x2048): 80.815
Elapsed time for attention_prob_times_values (80x2048x2048x1086): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1086): 83.769

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1827.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1087x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1087x2048): 78.462
Elapsed time for attention_prob_times_values (80x2048x2048x1087): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1087): 77.465

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1733.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1088x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1088x2048): 93.544
Elapsed time for attention_prob_times_values (80x2048x2048x1088): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1088): 86.888

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 2004.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1089x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1089x2048): 82.411
Elapsed time for attention_prob_times_values (80x2048x2048x1089): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1089): 81.436

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1824.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1090x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1090x2048): 80.749
Elapsed time for attention_prob_times_values (80x2048x2048x1090): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1090): 81.102

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1803.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1091x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1091x2048): 79.286
Elapsed time for attention_prob_times_values (80x2048x2048x1091): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1091): 76.901

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1741.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1092x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1092x2048): 83.999
Elapsed time for attention_prob_times_values (80x2048x2048x1092): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1092): 80.806

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1839.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1093x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1093x2048): 77.172
Elapsed time for attention_prob_times_values (80x2048x2048x1093): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1093): 81.768

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1774.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1094x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1094x2048): 82.569
Elapsed time for attention_prob_times_values (80x2048x2048x1094): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1094): 84.128

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1864.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1095x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1095x2048): 81.889
Elapsed time for attention_prob_times_values (80x2048x2048x1095): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1095): 81.878

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1833.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1096x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1096x2048): 83.936
Elapsed time for attention_prob_times_values (80x2048x2048x1096): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1096): 89.878

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1944.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1097x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1097x2048): 77.477
Elapsed time for attention_prob_times_values (80x2048x2048x1097): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1097): 80.088

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1766.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1098x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1098x2048): 80.105
Elapsed time for attention_prob_times_values (80x2048x2048x1098): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1098): 82.462

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1824.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 21980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1099x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1099x2048): 81.513
Elapsed time for attention_prob_times_values (80x2048x2048x1099): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1099): 80.352

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1818.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1100x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1100x2048): 83.147
Elapsed time for attention_prob_times_values (80x2048x2048x1100): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1100): 82.504

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1862.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1101x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1101x2048): 79.573
Elapsed time for attention_prob_times_values (80x2048x2048x1101): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1101): 82.509

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1823.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1102x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1102x2048): 79.961
Elapsed time for attention_prob_times_values (80x2048x2048x1102): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1102): 82.926

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1833.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1103x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1103x2048): 80.263
Elapsed time for attention_prob_times_values (80x2048x2048x1103): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1103): 80.394

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1810.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1104x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1104x2048): 83.830
Elapsed time for attention_prob_times_values (80x2048x2048x1104): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1104): 89.096

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1949.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1105x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1105x2048): 81.018
Elapsed time for attention_prob_times_values (80x2048x2048x1105): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1105): 82.829

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1849.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1106x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1106x2048): 81.581
Elapsed time for attention_prob_times_values (80x2048x2048x1106): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1106): 85.048

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1882.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1107x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1107x2048): 81.028
Elapsed time for attention_prob_times_values (80x2048x2048x1107): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1107): 82.987

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1854.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1108x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1108x2048): 82.787
Elapsed time for attention_prob_times_values (80x2048x2048x1108): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1108): 85.209

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1901.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1109x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1109x2048): 81.380
Elapsed time for attention_prob_times_values (80x2048x2048x1109): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1109): 83.157

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1863.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1110x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1110x2048): 82.013
Elapsed time for attention_prob_times_values (80x2048x2048x1110): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1110): 85.172

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1895.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1111x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1111x2048): 81.278
Elapsed time for attention_prob_times_values (80x2048x2048x1111): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1111): 82.875

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1862.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1112x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1112x2048): 83.623
Elapsed time for attention_prob_times_values (80x2048x2048x1112): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1112): 91.142

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1981.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1113x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1113x2048): 80.914
Elapsed time for attention_prob_times_values (80x2048x2048x1113): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1113): 83.382

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1867.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1114x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1114x2048): 78.511
Elapsed time for attention_prob_times_values (80x2048x2048x1114): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1114): 81.823

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1823.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1115x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1115x2048): 81.109
Elapsed time for attention_prob_times_values (80x2048x2048x1115): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1115): 83.427

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1873.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1116x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1116x2048): 82.868
Elapsed time for attention_prob_times_values (80x2048x2048x1116): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1116): 85.458

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1918.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1117x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1117x2048): 78.557
Elapsed time for attention_prob_times_values (80x2048x2048x1117): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1117): 83.413

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1846.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1118x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1118x2048): 81.899
Elapsed time for attention_prob_times_values (80x2048x2048x1118): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1118): 83.102

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1883.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1119x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1119x2048): 80.998
Elapsed time for attention_prob_times_values (80x2048x2048x1119): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1119): 81.044

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1851.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1120x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1120x2048): 91.814
Elapsed time for attention_prob_times_values (80x2048x2048x1120): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1120): 92.748

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2110.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1121x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1121x2048): 82.605
Elapsed time for attention_prob_times_values (80x2048x2048x1121): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1121): 81.409

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1877.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1122x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1122x2048): 83.503
Elapsed time for attention_prob_times_values (80x2048x2048x1122): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1122): 85.733

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1938.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1123x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1123x2048): 80.049
Elapsed time for attention_prob_times_values (80x2048x2048x1123): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1123): 79.254

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1826.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1124x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1124x2048): 84.031
Elapsed time for attention_prob_times_values (80x2048x2048x1124): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1124): 82.429

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1910.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1125x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1125x2048): 82.204
Elapsed time for attention_prob_times_values (80x2048x2048x1125): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1125): 83.783

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1906.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1126x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1126x2048): 78.977
Elapsed time for attention_prob_times_values (80x2048x2048x1126): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1126): 85.957

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1892.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1127x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1127x2048): 81.906
Elapsed time for attention_prob_times_values (80x2048x2048x1127): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1127): 80.819

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1872.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1128x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1128x2048): 83.812
Elapsed time for attention_prob_times_values (80x2048x2048x1128): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1128): 92.324

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 2023.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1129x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1129x2048): 81.417
Elapsed time for attention_prob_times_values (80x2048x2048x1129): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1129): 84.117

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1907.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1130x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1130x2048): 82.052
Elapsed time for attention_prob_times_values (80x2048x2048x1130): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1130): 80.785

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1878.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1131x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1131x2048): 81.550
Elapsed time for attention_prob_times_values (80x2048x2048x1131): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1131): 84.183

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1912.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1132x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1132x2048): 82.929
Elapsed time for attention_prob_times_values (80x2048x2048x1132): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1132): 86.521

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1957.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1133x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1133x2048): 81.669
Elapsed time for attention_prob_times_values (80x2048x2048x1133): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1133): 84.270

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1918.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1134x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1134x2048): 82.423
Elapsed time for attention_prob_times_values (80x2048x2048x1134): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1134): 86.591

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1955.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1135x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1135x2048): 81.851
Elapsed time for attention_prob_times_values (80x2048x2048x1135): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1135): 84.403

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1925.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1136x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1136x2048): 82.085
Elapsed time for attention_prob_times_values (80x2048x2048x1136): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1136): 90.873

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 2000.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1137x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1137x2048): 81.259
Elapsed time for attention_prob_times_values (80x2048x2048x1137): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1137): 82.137

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1895.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1138x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1138x2048): 82.200
Elapsed time for attention_prob_times_values (80x2048x2048x1138): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1138): 86.741

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1960.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1139x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1139x2048): 81.412
Elapsed time for attention_prob_times_values (80x2048x2048x1139): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1139): 84.506

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1927.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1140x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1140x2048): 82.797
Elapsed time for attention_prob_times_values (80x2048x2048x1140): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1140): 87.071

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1974.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1141x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1141x2048): 81.474
Elapsed time for attention_prob_times_values (80x2048x2048x1141): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1141): 84.577

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1932.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1142x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1142x2048): 81.915
Elapsed time for attention_prob_times_values (80x2048x2048x1142): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1142): 87.058

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1967.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1143x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1143x2048): 81.388
Elapsed time for attention_prob_times_values (80x2048x2048x1143): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1143): 84.792

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1937.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1144x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1144x2048): 83.465
Elapsed time for attention_prob_times_values (80x2048x2048x1144): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1144): 93.645

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 2060.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1145x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1145x2048): 81.108
Elapsed time for attention_prob_times_values (80x2048x2048x1145): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1145): 84.849

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1937.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1146x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1146x2048): 81.794
Elapsed time for attention_prob_times_values (80x2048x2048x1146): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1146): 87.344

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1975.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1147x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1147x2048): 81.163
Elapsed time for attention_prob_times_values (80x2048x2048x1147): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1147): 85.127

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1944.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1148x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1148x2048): 79.069
Elapsed time for attention_prob_times_values (80x2048x2048x1148): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1148): 84.832

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1917.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 22980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1149x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1149x2048): 78.140
Elapsed time for attention_prob_times_values (80x2048x2048x1149): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1149): 85.190

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1910.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1150x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1150x2048): 81.863
Elapsed time for attention_prob_times_values (80x2048x2048x1150): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1150): 85.197

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1958.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1151x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1151x2048): 77.883
Elapsed time for attention_prob_times_values (80x2048x2048x1151): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1151): 85.253

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1911.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1152x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1152x2048): 90.816
Elapsed time for attention_prob_times_values (80x2048x2048x1152): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1152): 93.836

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 2169.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1153x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1153x2048): 79.503
Elapsed time for attention_prob_times_values (80x2048x2048x1153): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1153): 76.253

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1830.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1154x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1154x2048): 83.637
Elapsed time for attention_prob_times_values (80x2048x2048x1154): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1154): 79.019

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1912.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1155x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1155x2048): 82.095
Elapsed time for attention_prob_times_values (80x2048x2048x1155): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1155): 78.817

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1894.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1156x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1156x2048): 84.275
Elapsed time for attention_prob_times_values (80x2048x2048x1156): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1156): 76.103

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1885.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1157x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1157x2048): 82.600
Elapsed time for attention_prob_times_values (80x2048x2048x1157): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1157): 78.871

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1904.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1158x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1158x2048): 77.023
Elapsed time for attention_prob_times_values (80x2048x2048x1158): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1158): 77.976

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1830.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1159x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1159x2048): 81.727
Elapsed time for attention_prob_times_values (80x2048x2048x1159): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1159): 79.021

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1899.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1160x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1160x2048): 83.455
Elapsed time for attention_prob_times_values (80x2048x2048x1160): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1160): 85.660

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1999.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1161x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1161x2048): 76.887
Elapsed time for attention_prob_times_values (80x2048x2048x1161): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1161): 79.236

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1847.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1162x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1162x2048): 79.323
Elapsed time for attention_prob_times_values (80x2048x2048x1162): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1162): 81.453

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1904.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1163x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1163x2048): 82.229
Elapsed time for attention_prob_times_values (80x2048x2048x1163): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1163): 79.317

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1914.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1164x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1164x2048): 83.685
Elapsed time for attention_prob_times_values (80x2048x2048x1164): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1164): 81.589

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1961.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1165x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1165x2048): 78.523
Elapsed time for attention_prob_times_values (80x2048x2048x1165): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1165): 78.253

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1862.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1166x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1166x2048): 84.012
Elapsed time for attention_prob_times_values (80x2048x2048x1166): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1166): 81.455

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1966.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1167x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1167x2048): 82.643
Elapsed time for attention_prob_times_values (80x2048x2048x1167): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1167): 79.555

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1928.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1168x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1168x2048): 84.526
Elapsed time for attention_prob_times_values (80x2048x2048x1168): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1168): 87.087

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2042.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1169x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1169x2048): 81.848
Elapsed time for attention_prob_times_values (80x2048x2048x1169): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1169): 79.691

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1924.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1170x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1170x2048): 76.555
Elapsed time for attention_prob_times_values (80x2048x2048x1170): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1170): 81.452

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1882.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1171x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1171x2048): 80.502
Elapsed time for attention_prob_times_values (80x2048x2048x1171): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1171): 78.844

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1901.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1172x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1172x2048): 80.815
Elapsed time for attention_prob_times_values (80x2048x2048x1172): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1172): 81.984

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1944.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1173x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1173x2048): 80.726
Elapsed time for attention_prob_times_values (80x2048x2048x1173): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1173): 79.759

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1918.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1174x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1174x2048): 83.055
Elapsed time for attention_prob_times_values (80x2048x2048x1174): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1174): 78.440

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1930.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1175x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1175x2048): 81.421
Elapsed time for attention_prob_times_values (80x2048x2048x1175): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1175): 76.020

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1883.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1176x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1176x2048): 82.876
Elapsed time for attention_prob_times_values (80x2048x2048x1176): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1176): 83.176

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1990.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1177x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1177x2048): 81.682
Elapsed time for attention_prob_times_values (80x2048x2048x1177): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1177): 78.693

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1922.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1178x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1178x2048): 80.254
Elapsed time for attention_prob_times_values (80x2048x2048x1178): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1178): 82.425

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1952.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1179x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1179x2048): 79.007
Elapsed time for attention_prob_times_values (80x2048x2048x1179): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1179): 78.243

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1889.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1180x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1180x2048): 83.127
Elapsed time for attention_prob_times_values (80x2048x2048x1180): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1180): 79.988

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1960.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1181x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1181x2048): 81.666
Elapsed time for attention_prob_times_values (80x2048x2048x1181): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1181): 80.497

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1951.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1182x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1182x2048): 81.125
Elapsed time for attention_prob_times_values (80x2048x2048x1182): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1182): 82.698

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1972.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1183x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1183x2048): 81.686
Elapsed time for attention_prob_times_values (80x2048x2048x1183): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1183): 80.145

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1950.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1184x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1184x2048): 93.957
Elapsed time for attention_prob_times_values (80x2048x2048x1184): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1184): 87.940

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2191.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1185x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1185x2048): 81.939
Elapsed time for attention_prob_times_values (80x2048x2048x1185): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1185): 80.674

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1962.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1186x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1186x2048): 84.366
Elapsed time for attention_prob_times_values (80x2048x2048x1186): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1186): 81.695

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2005.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1187x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1187x2048): 83.369
Elapsed time for attention_prob_times_values (80x2048x2048x1187): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1187): 80.785

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1984.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1188x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1188x2048): 82.517
Elapsed time for attention_prob_times_values (80x2048x2048x1188): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1188): 82.986

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2002.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1189x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1189x2048): 82.652
Elapsed time for attention_prob_times_values (80x2048x2048x1189): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1189): 75.187

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1907.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1190x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1190x2048): 83.970
Elapsed time for attention_prob_times_values (80x2048x2048x1190): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1190): 82.075

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2012.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1191x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1191x2048): 81.638
Elapsed time for attention_prob_times_values (80x2048x2048x1191): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1191): 80.987

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1972.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1192x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1192x2048): 80.700
Elapsed time for attention_prob_times_values (80x2048x2048x1192): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1192): 83.282

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1990.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1193x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1193x2048): 82.468
Elapsed time for attention_prob_times_values (80x2048x2048x1193): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1193): 81.138

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1987.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1194x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1194x2048): 80.256
Elapsed time for attention_prob_times_values (80x2048x2048x1194): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1194): 83.474

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1990.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1195x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1195x2048): 76.862
Elapsed time for attention_prob_times_values (80x2048x2048x1195): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1195): 78.035

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1884.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1196x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1196x2048): 84.435
Elapsed time for attention_prob_times_values (80x2048x2048x1196): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1196): 83.595

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2046.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1197x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1197x2048): 82.736
Elapsed time for attention_prob_times_values (80x2048x2048x1197): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1197): 81.183

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1997.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1198x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1198x2048): 78.318
Elapsed time for attention_prob_times_values (80x2048x2048x1198): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1198): 80.177

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1933.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 23980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1199x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1199x2048): 81.954
Elapsed time for attention_prob_times_values (80x2048x2048x1199): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1199): 81.391

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1994.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1200x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1200x2048): 84.324
Elapsed time for attention_prob_times_values (80x2048x2048x1200): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1200): 89.086

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2117.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1201x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1201x2048): 80.278
Elapsed time for attention_prob_times_values (80x2048x2048x1201): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1201): 81.556

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1978.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1202x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1202x2048): 83.165
Elapsed time for attention_prob_times_values (80x2048x2048x1202): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1202): 78.851

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1981.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1203x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1203x2048): 76.466
Elapsed time for attention_prob_times_values (80x2048x2048x1203): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1203): 81.606

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1934.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1204x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1204x2048): 81.379
Elapsed time for attention_prob_times_values (80x2048x2048x1204): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1204): 84.106

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2027.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1205x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1205x2048): 82.579
Elapsed time for attention_prob_times_values (80x2048x2048x1205): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1205): 77.288

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1959.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1206x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1206x2048): 79.001
Elapsed time for attention_prob_times_values (80x2048x2048x1206): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1206): 83.985

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1999.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1207x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1207x2048): 80.580
Elapsed time for attention_prob_times_values (80x2048x2048x1207): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1207): 81.836

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1995.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1208x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1208x2048): 83.755
Elapsed time for attention_prob_times_values (80x2048x2048x1208): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1208): 89.454

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2127.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1209x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1209x2048): 77.930
Elapsed time for attention_prob_times_values (80x2048x2048x1209): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1209): 79.882

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1941.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1210x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1210x2048): 77.499
Elapsed time for attention_prob_times_values (80x2048x2048x1210): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1210): 83.591

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1981.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1211x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1211x2048): 79.194
Elapsed time for attention_prob_times_values (80x2048x2048x1211): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1211): 78.021

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1937.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1212x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1212x2048): 79.105
Elapsed time for attention_prob_times_values (80x2048x2048x1212): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1212): 81.158

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1976.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1213x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1213x2048): 80.031
Elapsed time for attention_prob_times_values (80x2048x2048x1213): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1213): 80.691

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1984.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1214x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1214x2048): 83.091
Elapsed time for attention_prob_times_values (80x2048x2048x1214): 0.0321
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1214): 25.400

Attention duration (in seconds): 0.0419
Attention throughput (in TFLOP/s): 961.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0419
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1215x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1215x2048): 77.625
Elapsed time for attention_prob_times_values (80x2048x2048x1215): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1215): 82.291

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1975.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1216x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1216x2048): 91.770
Elapsed time for attention_prob_times_values (80x2048x2048x1216): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1216): 89.810

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2246.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1217x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1217x2048): 83.771
Elapsed time for attention_prob_times_values (80x2048x2048x1217): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1217): 79.479

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2020.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1218x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1218x2048): 81.636
Elapsed time for attention_prob_times_values (80x2048x2048x1218): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1218): 84.804

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2062.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1219x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1219x2048): 79.350
Elapsed time for attention_prob_times_values (80x2048x2048x1219): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1219): 78.200

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1954.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1220x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1220x2048): 85.602
Elapsed time for attention_prob_times_values (80x2048x2048x1220): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1220): 84.994

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2117.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1221x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1221x2048): 81.609
Elapsed time for attention_prob_times_values (80x2048x2048x1221): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1221): 82.604

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2040.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1222x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1222x2048): 82.072
Elapsed time for attention_prob_times_values (80x2048x2048x1222): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1222): 84.954

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2076.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1223x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1223x2048): 83.241
Elapsed time for attention_prob_times_values (80x2048x2048x1223): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1223): 82.622

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2063.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1224x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1224x2048): 83.621
Elapsed time for attention_prob_times_values (80x2048x2048x1224): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1224): 90.607

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2166.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1225x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1225x2048): 79.281
Elapsed time for attention_prob_times_values (80x2048x2048x1225): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1225): 82.892

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2020.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1226x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1226x2048): 84.236
Elapsed time for attention_prob_times_values (80x2048x2048x1226): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1226): 82.743

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2082.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1227x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1227x2048): 79.727
Elapsed time for attention_prob_times_values (80x2048x2048x1227): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1227): 82.898

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2029.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1228x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1228x2048): 81.666
Elapsed time for attention_prob_times_values (80x2048x2048x1228): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1228): 85.418

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2086.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1229x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1229x2048): 82.906
Elapsed time for attention_prob_times_values (80x2048x2048x1229): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1229): 83.047

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2074.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1230x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1230x2048): 83.772
Elapsed time for attention_prob_times_values (80x2048x2048x1230): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1230): 85.458

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2117.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1231x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1231x2048): 82.969
Elapsed time for attention_prob_times_values (80x2048x2048x1231): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1231): 83.260

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2081.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1232x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1232x2048): 85.468
Elapsed time for attention_prob_times_values (80x2048x2048x1232): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1232): 91.716

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2217.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1233x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1233x2048): 82.600
Elapsed time for attention_prob_times_values (80x2048x2048x1233): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1233): 83.434

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2082.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1234x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1234x2048): 83.519
Elapsed time for attention_prob_times_values (80x2048x2048x1234): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1234): 85.719

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2123.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1235x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1235x2048): 82.631
Elapsed time for attention_prob_times_values (80x2048x2048x1235): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1235): 83.380

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2085.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1236x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1236x2048): 84.110
Elapsed time for attention_prob_times_values (80x2048x2048x1236): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1236): 85.513

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2132.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1237x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1237x2048): 82.722
Elapsed time for attention_prob_times_values (80x2048x2048x1237): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1237): 72.932

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1950.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1238x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1238x2048): 83.469
Elapsed time for attention_prob_times_values (80x2048x2048x1238): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1238): 78.819

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2041.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1239x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1239x2048): 82.724
Elapsed time for attention_prob_times_values (80x2048x2048x1239): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1239): 72.490

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1947.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1240x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1240x2048): 82.338
Elapsed time for attention_prob_times_values (80x2048x2048x1240): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1240): 88.487

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2151.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1241x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1241x2048): 81.938
Elapsed time for attention_prob_times_values (80x2048x2048x1241): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1241): 72.507

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1941.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1242x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1242x2048): 80.364
Elapsed time for attention_prob_times_values (80x2048x2048x1242): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1242): 79.179

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2014.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1243x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1243x2048): 78.735
Elapsed time for attention_prob_times_values (80x2048x2048x1243): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1243): 71.555

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1895.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1244x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1244x2048): 83.894
Elapsed time for attention_prob_times_values (80x2048x2048x1244): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1244): 79.521

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2065.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1245x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1245x2048): 81.998
Elapsed time for attention_prob_times_values (80x2048x2048x1245): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1245): 74.276

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1973.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1246x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1246x2048): 83.108
Elapsed time for attention_prob_times_values (80x2048x2048x1246): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1246): 79.423

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2057.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1247x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1247x2048): 82.213
Elapsed time for attention_prob_times_values (80x2048x2048x1247): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1247): 74.830

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1986.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1248x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1248x2048): 94.104
Elapsed time for attention_prob_times_values (80x2048x2048x1248): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1248): 90.487

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2341.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 24980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1249x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1249x2048): 81.561
Elapsed time for attention_prob_times_values (80x2048x2048x1249): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1249): 74.558

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1978.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1250x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1250x2048): 83.482
Elapsed time for attention_prob_times_values (80x2048x2048x1250): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1250): 77.943

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2048.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1251x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1251x2048): 82.900
Elapsed time for attention_prob_times_values (80x2048x2048x1251): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1251): 74.355

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1993.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1252x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1252x2048): 78.921
Elapsed time for attention_prob_times_values (80x2048x2048x1252): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1252): 74.910

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1956.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1253x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1253x2048): 82.771
Elapsed time for attention_prob_times_values (80x2048x2048x1253): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1253): 74.270

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1994.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1254x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1254x2048): 83.817
Elapsed time for attention_prob_times_values (80x2048x2048x1254): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1254): 79.843

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2084.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1255x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1255x2048): 78.728
Elapsed time for attention_prob_times_values (80x2048x2048x1255): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1255): 70.660

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1900.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1256x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1256x2048): 85.246
Elapsed time for attention_prob_times_values (80x2048x2048x1256): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1256): 92.764

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2268.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1257x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1257x2048): 79.934
Elapsed time for attention_prob_times_values (80x2048x2048x1257): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1257): 74.102

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1965.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1258x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1258x2048): 83.232
Elapsed time for attention_prob_times_values (80x2048x2048x1258): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1258): 73.751

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1999.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1259x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1259x2048): 82.373
Elapsed time for attention_prob_times_values (80x2048x2048x1259): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1259): 74.615

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2003.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1260x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1260x2048): 80.744
Elapsed time for attention_prob_times_values (80x2048x2048x1260): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1260): 80.367

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2062.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1261x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1261x2048): 82.592
Elapsed time for attention_prob_times_values (80x2048x2048x1261): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1261): 73.662

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1995.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1262x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1262x2048): 82.667
Elapsed time for attention_prob_times_values (80x2048x2048x1262): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1262): 78.172

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2061.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1263x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1263x2048): 82.610
Elapsed time for attention_prob_times_values (80x2048x2048x1263): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1263): 71.510

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1967.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1264x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1264x2048): 84.672
Elapsed time for attention_prob_times_values (80x2048x2048x1264): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1264): 93.768

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2285.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1265x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1265x2048): 81.149
Elapsed time for attention_prob_times_values (80x2048x2048x1265): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1265): 72.699

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1971.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1266x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1266x2048): 83.117
Elapsed time for attention_prob_times_values (80x2048x2048x1266): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1266): 80.389

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2102.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1267x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1267x2048): 82.398
Elapsed time for attention_prob_times_values (80x2048x2048x1267): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1267): 74.489

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2014.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1268x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1268x2048): 83.578
Elapsed time for attention_prob_times_values (80x2048x2048x1268): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1268): 80.785

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2116.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1269x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1269x2048): 82.449
Elapsed time for attention_prob_times_values (80x2048x2048x1269): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1269): 72.965

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1996.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1270x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1270x2048): 83.422
Elapsed time for attention_prob_times_values (80x2048x2048x1270): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1270): 76.561

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2060.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1271x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1271x2048): 82.768
Elapsed time for attention_prob_times_values (80x2048x2048x1271): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1271): 74.459

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2024.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1272x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1272x2048): 80.958
Elapsed time for attention_prob_times_values (80x2048x2048x1272): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1272): 93.712

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2245.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1273x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1273x2048): 80.757
Elapsed time for attention_prob_times_values (80x2048x2048x1273): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1273): 73.250

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1986.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1274x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1274x2048): 83.335
Elapsed time for attention_prob_times_values (80x2048x2048x1274): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1274): 79.857

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2110.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1275x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1275x2048): 81.325
Elapsed time for attention_prob_times_values (80x2048x2048x1275): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1275): 74.287

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2011.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1276x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1276x2048): 83.501
Elapsed time for attention_prob_times_values (80x2048x2048x1276): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1276): 80.769

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2128.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1277x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1277x2048): 81.923
Elapsed time for attention_prob_times_values (80x2048x2048x1277): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1277): 73.065

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2003.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1278x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1278x2048): 82.995
Elapsed time for attention_prob_times_values (80x2048x2048x1278): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1278): 79.350

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2106.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1279x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1279x2048): 79.973
Elapsed time for attention_prob_times_values (80x2048x2048x1279): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1279): 74.517

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2004.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1280x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1280x2048): 91.472
Elapsed time for attention_prob_times_values (80x2048x2048x1280): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1280): 96.300

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2439.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1281x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1281x2048): 82.730
Elapsed time for attention_prob_times_values (80x2048x2048x1281): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1281): 65.481

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1902.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1282x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1282x2048): 83.950
Elapsed time for attention_prob_times_values (80x2048x2048x1282): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1282): 75.426

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2069.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1283x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1283x2048): 77.033
Elapsed time for attention_prob_times_values (80x2048x2048x1283): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1283): 69.950

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1910.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1284x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1284x2048): 84.409
Elapsed time for attention_prob_times_values (80x2048x2048x1284): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1284): 75.657

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2080.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1285x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1285x2048): 83.093
Elapsed time for attention_prob_times_values (80x2048x2048x1285): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1285): 70.054

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1983.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1286x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1286x2048): 84.232
Elapsed time for attention_prob_times_values (80x2048x2048x1286): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1286): 74.898

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2070.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1287x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1287x2048): 83.354
Elapsed time for attention_prob_times_values (80x2048x2048x1287): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1287): 68.598

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1967.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1288x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1288x2048): 82.624
Elapsed time for attention_prob_times_values (80x2048x2048x1288): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1288): 83.722

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2175.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1289x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1289x2048): 82.306
Elapsed time for attention_prob_times_values (80x2048x2048x1289): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1289): 66.830

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1930.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1290x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1290x2048): 82.768
Elapsed time for attention_prob_times_values (80x2048x2048x1290): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1290): 75.691

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2071.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1291x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1291x2048): 83.090
Elapsed time for attention_prob_times_values (80x2048x2048x1291): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1291): 69.630

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1986.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1292x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1292x2048): 84.436
Elapsed time for attention_prob_times_values (80x2048x2048x1292): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1292): 76.105

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2100.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1293x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1293x2048): 82.429
Elapsed time for attention_prob_times_values (80x2048x2048x1293): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1293): 70.686

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1998.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1294x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1294x2048): 84.363
Elapsed time for attention_prob_times_values (80x2048x2048x1294): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1294): 75.997

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2100.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1295x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1295x2048): 83.233
Elapsed time for attention_prob_times_values (80x2048x2048x1295): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1295): 71.139

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2016.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1296x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1296x2048): 85.417
Elapsed time for attention_prob_times_values (80x2048x2048x1296): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1296): 87.713

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2277.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1297x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1297x2048): 82.517
Elapsed time for attention_prob_times_values (80x2048x2048x1297): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1297): 71.155

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2012.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1298x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1298x2048): 83.999
Elapsed time for attention_prob_times_values (80x2048x2048x1298): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1298): 75.877

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2101.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 25980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1299x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1299x2048): 82.843
Elapsed time for attention_prob_times_values (80x2048x2048x1299): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1299): 71.307

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2021.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1300x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1300x2048): 84.529
Elapsed time for attention_prob_times_values (80x2048x2048x1300): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1300): 75.995

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2112.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1301x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1301x2048): 82.772
Elapsed time for attention_prob_times_values (80x2048x2048x1301): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1301): 71.118

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2020.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1302x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1302x2048): 84.185
Elapsed time for attention_prob_times_values (80x2048x2048x1302): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1302): 76.318

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2115.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1303x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1303x2048): 82.965
Elapsed time for attention_prob_times_values (80x2048x2048x1303): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1303): 71.140

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2025.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1304x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1304x2048): 85.208
Elapsed time for attention_prob_times_values (80x2048x2048x1304): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1304): 87.591

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2286.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1305x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1305x2048): 82.445
Elapsed time for attention_prob_times_values (80x2048x2048x1305): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1305): 71.361

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2026.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1306x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1306x2048): 83.879
Elapsed time for attention_prob_times_values (80x2048x2048x1306): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1306): 76.540

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2121.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1307x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1307x2048): 82.766
Elapsed time for attention_prob_times_values (80x2048x2048x1307): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1307): 71.949

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2042.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1308x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1308x2048): 84.580
Elapsed time for attention_prob_times_values (80x2048x2048x1308): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1308): 76.836

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2137.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1309x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1309x2048): 82.869
Elapsed time for attention_prob_times_values (80x2048x2048x1309): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1309): 72.026

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2047.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1310x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1310x2048): 84.185
Elapsed time for attention_prob_times_values (80x2048x2048x1310): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1310): 76.781

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2135.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1311x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1311x2048): 83.020
Elapsed time for attention_prob_times_values (80x2048x2048x1311): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1311): 72.350

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2057.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1312x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1312x2048): 96.123
Elapsed time for attention_prob_times_values (80x2048x2048x1312): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1312): 89.243

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2464.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1313x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1313x2048): 84.403
Elapsed time for attention_prob_times_values (80x2048x2048x1313): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1313): 72.362

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2076.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1314x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1314x2048): 85.449
Elapsed time for attention_prob_times_values (80x2048x2048x1314): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1314): 76.908

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2158.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1315x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1315x2048): 84.216
Elapsed time for attention_prob_times_values (80x2048x2048x1315): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1315): 72.307

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2076.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1316x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1316x2048): 85.833
Elapsed time for attention_prob_times_values (80x2048x2048x1316): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1316): 77.168

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2170.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1317x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1317x2048): 83.272
Elapsed time for attention_prob_times_values (80x2048x2048x1317): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1317): 70.734

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2044.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1318x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1318x2048): 85.047
Elapsed time for attention_prob_times_values (80x2048x2048x1318): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1318): 75.411

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2137.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1319x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1319x2048): 81.703
Elapsed time for attention_prob_times_values (80x2048x2048x1319): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1319): 69.850

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2015.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1320x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1320x2048): 85.307
Elapsed time for attention_prob_times_values (80x2048x2048x1320): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1320): 86.446

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2299.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1321x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1321x2048): 83.443
Elapsed time for attention_prob_times_values (80x2048x2048x1321): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1321): 70.725

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2051.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1322x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1322x2048): 83.440
Elapsed time for attention_prob_times_values (80x2048x2048x1322): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1322): 75.450

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2125.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1323x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1323x2048): 82.079
Elapsed time for attention_prob_times_values (80x2048x2048x1323): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1323): 70.732

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2039.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1324x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1324x2048): 85.231
Elapsed time for attention_prob_times_values (80x2048x2048x1324): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1324): 75.702

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2153.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1325x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1325x2048): 81.245
Elapsed time for attention_prob_times_values (80x2048x2048x1325): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1325): 70.413

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2027.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1326x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1326x2048): 82.765
Elapsed time for attention_prob_times_values (80x2048x2048x1326): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1326): 75.374

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2122.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1327x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1327x2048): 83.662
Elapsed time for attention_prob_times_values (80x2048x2048x1327): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1327): 72.365

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2088.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1328x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1328x2048): 83.018
Elapsed time for attention_prob_times_values (80x2048x2048x1328): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1328): 87.459

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2294.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1329x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1329x2048): 81.398
Elapsed time for attention_prob_times_values (80x2048x2048x1329): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1329): 69.772

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2025.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1330x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1330x2048): 84.230
Elapsed time for attention_prob_times_values (80x2048x2048x1330): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1330): 77.572

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2178.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1331x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1331x2048): 80.962
Elapsed time for attention_prob_times_values (80x2048x2048x1331): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1331): 70.210

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2030.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1332x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1332x2048): 84.973
Elapsed time for attention_prob_times_values (80x2048x2048x1332): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1332): 75.730

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2163.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1333x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1333x2048): 82.584
Elapsed time for attention_prob_times_values (80x2048x2048x1333): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1333): 72.165

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2082.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1334x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1334x2048): 81.598
Elapsed time for attention_prob_times_values (80x2048x2048x1334): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1334): 75.901

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2127.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1335x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1335x2048): 83.378
Elapsed time for attention_prob_times_values (80x2048x2048x1335): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1335): 69.644

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2054.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1336x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1336x2048): 83.872
Elapsed time for attention_prob_times_values (80x2048x2048x1336): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1336): 89.745

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2349.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1337x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1337x2048): 80.692
Elapsed time for attention_prob_times_values (80x2048x2048x1337): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1337): 70.624

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2042.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1338x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1338x2048): 83.884
Elapsed time for attention_prob_times_values (80x2048x2048x1338): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1338): 76.296

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2168.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1339x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1339x2048): 81.524
Elapsed time for attention_prob_times_values (80x2048x2048x1339): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1339): 72.220

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2079.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1340x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1340x2048): 82.940
Elapsed time for attention_prob_times_values (80x2048x2048x1340): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1340): 76.494

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2162.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1341x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1341x2048): 81.251
Elapsed time for attention_prob_times_values (80x2048x2048x1341): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1341): 70.400

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2051.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1342x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1342x2048): 82.541
Elapsed time for attention_prob_times_values (80x2048x2048x1342): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1342): 78.037

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2183.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1343x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1343x2048): 80.857
Elapsed time for attention_prob_times_values (80x2048x2048x1343): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1343): 70.828

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2056.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1344x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1344x2048): 93.385
Elapsed time for attention_prob_times_values (80x2048x2048x1344): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1344): 89.788

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2494.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1345x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1345x2048): 84.390
Elapsed time for attention_prob_times_values (80x2048x2048x1345): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1345): 70.720

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2098.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1346x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1346x2048): 87.220
Elapsed time for attention_prob_times_values (80x2048x2048x1346): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1346): 78.268

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2251.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1347x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1347x2048): 82.669
Elapsed time for attention_prob_times_values (80x2048x2048x1347): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1347): 68.575

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2047.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1348x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1348x2048): 82.113
Elapsed time for attention_prob_times_values (80x2048x2048x1348): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1348): 77.360

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2177.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 26980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1349x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1349x2048): 84.109
Elapsed time for attention_prob_times_values (80x2048x2048x1349): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1349): 70.518

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2098.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1350x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1350x2048): 84.962
Elapsed time for attention_prob_times_values (80x2048x2048x1350): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1350): 78.228

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2229.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1351x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1351x2048): 83.762
Elapsed time for attention_prob_times_values (80x2048x2048x1351): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1351): 71.431

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2111.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1352x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1352x2048): 86.250
Elapsed time for attention_prob_times_values (80x2048x2048x1352): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1352): 89.315

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2405.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1353x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1353x2048): 83.317
Elapsed time for attention_prob_times_values (80x2048x2048x1353): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1353): 72.268

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2122.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1354x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1354x2048): 83.430
Elapsed time for attention_prob_times_values (80x2048x2048x1354): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1354): 78.430

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2219.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1355x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1355x2048): 81.963
Elapsed time for attention_prob_times_values (80x2048x2048x1355): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1355): 71.118

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2091.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1356x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1356x2048): 87.363
Elapsed time for attention_prob_times_values (80x2048x2048x1356): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1356): 78.537

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2273.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1357x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1357x2048): 87.345
Elapsed time for attention_prob_times_values (80x2048x2048x1357): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1357): 72.202

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2174.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1358x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1358x2048): 85.480
Elapsed time for attention_prob_times_values (80x2048x2048x1358): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1358): 78.488

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2252.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1359x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1359x2048): 86.115
Elapsed time for attention_prob_times_values (80x2048x2048x1359): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1359): 72.470

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2167.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1360x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1360x2048): 88.705
Elapsed time for attention_prob_times_values (80x2048x2048x1360): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1360): 91.712

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2485.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1361x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1361x2048): 88.542
Elapsed time for attention_prob_times_values (80x2048x2048x1361): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1361): 73.068

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2208.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1362x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1362x2048): 85.650
Elapsed time for attention_prob_times_values (80x2048x2048x1362): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1362): 78.675

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2263.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1363x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1363x2048): 84.306
Elapsed time for attention_prob_times_values (80x2048x2048x1363): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1363): 72.546

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2154.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1364x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1364x2048): 86.707
Elapsed time for attention_prob_times_values (80x2048x2048x1364): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1364): 78.950

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2284.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1365x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1365x2048): 83.284
Elapsed time for attention_prob_times_values (80x2048x2048x1365): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1365): 73.114

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2153.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1366x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1366x2048): 83.735
Elapsed time for attention_prob_times_values (80x2048x2048x1366): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1366): 78.872

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2248.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1367x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1367x2048): 84.930
Elapsed time for attention_prob_times_values (80x2048x2048x1367): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1367): 73.025

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2175.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1368x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1368x2048): 85.782
Elapsed time for attention_prob_times_values (80x2048x2048x1368): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1368): 91.987

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2460.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1369x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1369x2048): 84.964
Elapsed time for attention_prob_times_values (80x2048x2048x1369): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1369): 73.531

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2186.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1370x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1370x2048): 88.791
Elapsed time for attention_prob_times_values (80x2048x2048x1370): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1370): 79.118

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2322.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1371x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1371x2048): 86.921
Elapsed time for attention_prob_times_values (80x2048x2048x1371): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1371): 73.529

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2212.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1372x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1372x2048): 85.931
Elapsed time for attention_prob_times_values (80x2048x2048x1372): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1372): 75.005

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2226.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1373x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1373x2048): 83.896
Elapsed time for attention_prob_times_values (80x2048x2048x1373): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1373): 69.975

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2122.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1374x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1374x2048): 82.044
Elapsed time for attention_prob_times_values (80x2048x2048x1374): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1374): 75.432

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2187.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1375x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1375x2048): 87.428
Elapsed time for attention_prob_times_values (80x2048x2048x1375): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1375): 70.897

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2181.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1376x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1376x2048): 95.952
Elapsed time for attention_prob_times_values (80x2048x2048x1376): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1376): 93.339

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2637.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1377x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1377x2048): 84.655
Elapsed time for attention_prob_times_values (80x2048x2048x1377): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1377): 70.880

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2152.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1378x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1378x2048): 82.293
Elapsed time for attention_prob_times_values (80x2048x2048x1378): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1378): 79.760

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2261.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1379x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1379x2048): 77.505
Elapsed time for attention_prob_times_values (80x2048x2048x1379): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1379): 69.335

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2044.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1380x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1380x2048): 78.372
Elapsed time for attention_prob_times_values (80x2048x2048x1380): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1380): 76.761

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2167.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1381x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1381x2048): 83.974
Elapsed time for attention_prob_times_values (80x2048x2048x1381): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1381): 69.645

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2129.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1382x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1382x2048): 83.868
Elapsed time for attention_prob_times_values (80x2048x2048x1382): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1382): 80.041

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2292.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1383x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1383x2048): 80.799
Elapsed time for attention_prob_times_values (80x2048x2048x1383): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1383): 71.683

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2127.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1384x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1384x2048): 86.066
Elapsed time for attention_prob_times_values (80x2048x2048x1384): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1384): 87.856

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2437.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1385x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1385x2048): 83.882
Elapsed time for attention_prob_times_values (80x2048x2048x1385): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1385): 74.355

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2211.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1386x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1386x2048): 84.970
Elapsed time for attention_prob_times_values (80x2048x2048x1386): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1386): 80.249

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2316.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1387x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1387x2048): 82.503
Elapsed time for attention_prob_times_values (80x2048x2048x1387): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1387): 72.880

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2173.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1388x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1388x2048): 85.212
Elapsed time for attention_prob_times_values (80x2048x2048x1388): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1388): 78.877

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2302.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1389x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1389x2048): 79.822
Elapsed time for attention_prob_times_values (80x2048x2048x1389): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1389): 75.043

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2176.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1390x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1390x2048): 82.931
Elapsed time for attention_prob_times_values (80x2048x2048x1390): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1390): 78.916

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2276.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1391x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1391x2048): 78.228
Elapsed time for attention_prob_times_values (80x2048x2048x1391): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1391): 70.102

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2082.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1392x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1392x2048): 86.516
Elapsed time for attention_prob_times_values (80x2048x2048x1392): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1392): 88.060

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2460.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1393x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1393x2048): 80.014
Elapsed time for attention_prob_times_values (80x2048x2048x1393): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1393): 75.058

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2184.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1394x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1394x2048): 80.852
Elapsed time for attention_prob_times_values (80x2048x2048x1394): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1394): 75.551

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2204.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1395x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1395x2048): 83.471
Elapsed time for attention_prob_times_values (80x2048x2048x1395): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1395): 74.684

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2226.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1396x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1396x2048): 81.597
Elapsed time for attention_prob_times_values (80x2048x2048x1396): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1396): 81.079

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2299.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1397x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1397x2048): 78.492
Elapsed time for attention_prob_times_values (80x2048x2048x1397): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1397): 69.974

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2092.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1398x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1398x2048): 84.579
Elapsed time for attention_prob_times_values (80x2048x2048x1398): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1398): 80.019

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2327.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 27980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1399x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1399x2048): 83.661
Elapsed time for attention_prob_times_values (80x2048x2048x1399): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1399): 73.833

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2221.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1400x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1400x2048): 83.595
Elapsed time for attention_prob_times_values (80x2048x2048x1400): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1400): 93.890

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2506.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1401x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1401x2048): 80.774
Elapsed time for attention_prob_times_values (80x2048x2048x1401): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1401): 74.481

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2198.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1402x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1402x2048): 83.689
Elapsed time for attention_prob_times_values (80x2048x2048x1402): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1402): 80.320

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2326.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1403x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1403x2048): 83.293
Elapsed time for attention_prob_times_values (80x2048x2048x1403): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1403): 75.026

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2242.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1404x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1404x2048): 84.834
Elapsed time for attention_prob_times_values (80x2048x2048x1404): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1404): 81.560

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2363.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1405x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1405x2048): 83.217
Elapsed time for attention_prob_times_values (80x2048x2048x1405): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1405): 74.568

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2237.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1406x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1406x2048): 83.835
Elapsed time for attention_prob_times_values (80x2048x2048x1406): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1406): 81.414

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2351.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1407x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1407x2048): 82.669
Elapsed time for attention_prob_times_values (80x2048x2048x1407): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1407): 75.313

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2244.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1408x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1408x2048): 92.877
Elapsed time for attention_prob_times_values (80x2048x2048x1408): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1408): 96.474

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2697.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1409x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1409x2048): 83.869
Elapsed time for attention_prob_times_values (80x2048x2048x1409): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1409): 66.890

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2122.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1410x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1410x2048): 84.823
Elapsed time for attention_prob_times_values (80x2048x2048x1410): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1410): 73.790

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2252.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1411x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1411x2048): 78.342
Elapsed time for attention_prob_times_values (80x2048x2048x1411): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1411): 68.777

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2091.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1412x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1412x2048): 80.751
Elapsed time for attention_prob_times_values (80x2048x2048x1412): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1412): 71.267

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2163.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1413x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1413x2048): 84.324
Elapsed time for attention_prob_times_values (80x2048x2048x1413): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1413): 68.072

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2154.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1414x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1414x2048): 82.921
Elapsed time for attention_prob_times_values (80x2048x2048x1414): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1414): 74.962

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2253.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1415x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1415x2048): 84.233
Elapsed time for attention_prob_times_values (80x2048x2048x1415): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1415): 69.387

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2179.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1416x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1416x2048): 86.315
Elapsed time for attention_prob_times_values (80x2048x2048x1416): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1416): 87.442

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2489.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1417x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1417x2048): 83.894
Elapsed time for attention_prob_times_values (80x2048x2048x1417): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1417): 69.757

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2184.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1418x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1418x2048): 84.283
Elapsed time for attention_prob_times_values (80x2048x2048x1418): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1418): 74.603

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2271.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1419x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1419x2048): 83.366
Elapsed time for attention_prob_times_values (80x2048x2048x1419): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1419): 69.913

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2183.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1420x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1420x2048): 85.562
Elapsed time for attention_prob_times_values (80x2048x2048x1420): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1420): 75.398

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2303.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1421x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1421x2048): 79.327
Elapsed time for attention_prob_times_values (80x2048x2048x1421): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1421): 69.537

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2130.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1422x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1422x2048): 85.038
Elapsed time for attention_prob_times_values (80x2048x2048x1422): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1422): 74.437

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2284.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1423x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1423x2048): 83.986
Elapsed time for attention_prob_times_values (80x2048x2048x1423): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1423): 69.884

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2196.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1424x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1424x2048): 86.887
Elapsed time for attention_prob_times_values (80x2048x2048x1424): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1424): 88.215

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2522.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1425x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1425x2048): 83.737
Elapsed time for attention_prob_times_values (80x2048x2048x1425): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1425): 70.460

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2206.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1426x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1426x2048): 84.967
Elapsed time for attention_prob_times_values (80x2048x2048x1426): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1426): 75.295

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2303.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1427x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1427x2048): 83.719
Elapsed time for attention_prob_times_values (80x2048x2048x1427): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1427): 70.369

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2207.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1428x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1428x2048): 85.171
Elapsed time for attention_prob_times_values (80x2048x2048x1428): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1428): 74.848

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2301.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1429x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1429x2048): 83.836
Elapsed time for attention_prob_times_values (80x2048x2048x1429): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1429): 70.336

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2211.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1430x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1430x2048): 84.782
Elapsed time for attention_prob_times_values (80x2048x2048x1430): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1430): 72.638

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2263.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1431x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1431x2048): 82.913
Elapsed time for attention_prob_times_values (80x2048x2048x1431): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1431): 70.138

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2199.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1432x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1432x2048): 82.265
Elapsed time for attention_prob_times_values (80x2048x2048x1432): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1432): 87.064

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2450.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1433x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1433x2048): 83.496
Elapsed time for attention_prob_times_values (80x2048x2048x1433): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1433): 70.118

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2209.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1434x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1434x2048): 83.853
Elapsed time for attention_prob_times_values (80x2048x2048x1434): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1434): 75.555

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2305.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1435x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1435x2048): 83.357
Elapsed time for attention_prob_times_values (80x2048x2048x1435): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1435): 70.489

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2217.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1436x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1436x2048): 85.128
Elapsed time for attention_prob_times_values (80x2048x2048x1436): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1436): 74.802

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2313.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1437x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1437x2048): 83.021
Elapsed time for attention_prob_times_values (80x2048x2048x1437): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1437): 70.959

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2224.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1438x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1438x2048): 84.311
Elapsed time for attention_prob_times_values (80x2048x2048x1438): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1438): 75.559

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2318.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1439x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1439x2048): 83.509
Elapsed time for attention_prob_times_values (80x2048x2048x1439): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1439): 71.308

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2239.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1440x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1440x2048): 96.949
Elapsed time for attention_prob_times_values (80x2048x2048x1440): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1440): 89.745

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2714.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1441x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1441x2048): 84.655
Elapsed time for attention_prob_times_values (80x2048x2048x1441): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1441): 71.251

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2255.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1442x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1442x2048): 86.143
Elapsed time for attention_prob_times_values (80x2048x2048x1442): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1442): 75.877

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2353.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1443x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1443x2048): 84.946
Elapsed time for attention_prob_times_values (80x2048x2048x1443): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1443): 71.230

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2261.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1444x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1444x2048): 86.615
Elapsed time for attention_prob_times_values (80x2048x2048x1444): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1444): 76.178

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2367.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1445x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1445x2048): 84.716
Elapsed time for attention_prob_times_values (80x2048x2048x1445): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1445): 71.093

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2259.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1446x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1446x2048): 85.855
Elapsed time for attention_prob_times_values (80x2048x2048x1446): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1446): 75.995

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2357.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1447x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1447x2048): 84.652
Elapsed time for attention_prob_times_values (80x2048x2048x1447): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1447): 70.909

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2258.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1448x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1448x2048): 86.857
Elapsed time for attention_prob_times_values (80x2048x2048x1448): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1448): 89.261

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2577.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 28980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1449x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1449x2048): 84.275
Elapsed time for attention_prob_times_values (80x2048x2048x1449): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1449): 70.995

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2258.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1450x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1450x2048): 84.829
Elapsed time for attention_prob_times_values (80x2048x2048x1450): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1450): 76.118

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2352.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1451x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1451x2048): 83.753
Elapsed time for attention_prob_times_values (80x2048x2048x1451): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1451): 71.043

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2255.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1452x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1452x2048): 85.692
Elapsed time for attention_prob_times_values (80x2048x2048x1452): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1452): 76.484

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2373.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1453x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1453x2048): 84.537
Elapsed time for attention_prob_times_values (80x2048x2048x1453): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1453): 71.241

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2271.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1454x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1454x2048): 85.534
Elapsed time for attention_prob_times_values (80x2048x2048x1454): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1454): 76.375

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2372.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1455x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1455x2048): 84.530
Elapsed time for attention_prob_times_values (80x2048x2048x1455): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1455): 71.367

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2276.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1456x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1456x2048): 86.668
Elapsed time for attention_prob_times_values (80x2048x2048x1456): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1456): 89.959

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2598.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1457x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1457x2048): 84.258
Elapsed time for attention_prob_times_values (80x2048x2048x1457): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1457): 71.242

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2274.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1458x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1458x2048): 84.993
Elapsed time for attention_prob_times_values (80x2048x2048x1458): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1458): 76.429

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2372.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1459x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1459x2048): 84.011
Elapsed time for attention_prob_times_values (80x2048x2048x1459): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1459): 71.280

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2274.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1460x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1460x2048): 85.783
Elapsed time for attention_prob_times_values (80x2048x2048x1460): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1460): 76.830

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2392.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1461x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1461x2048): 84.328
Elapsed time for attention_prob_times_values (80x2048x2048x1461): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1461): 71.191

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2280.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1462x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1462x2048): 84.998
Elapsed time for attention_prob_times_values (80x2048x2048x1462): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1462): 76.621

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2381.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1463x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1463x2048): 84.430
Elapsed time for attention_prob_times_values (80x2048x2048x1463): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1463): 71.245

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2285.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1464x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1464x2048): 86.330
Elapsed time for attention_prob_times_values (80x2048x2048x1464): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1464): 90.135

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2609.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1465x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1465x2048): 83.977
Elapsed time for attention_prob_times_values (80x2048x2048x1465): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1465): 71.183

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2281.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1466x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1466x2048): 84.829
Elapsed time for attention_prob_times_values (80x2048x2048x1466): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1466): 76.811

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2389.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1467x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1467x2048): 83.957
Elapsed time for attention_prob_times_values (80x2048x2048x1467): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1467): 71.314

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2286.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1468x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1468x2048): 85.318
Elapsed time for attention_prob_times_values (80x2048x2048x1468): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1468): 77.201

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2405.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1469x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1469x2048): 83.965
Elapsed time for attention_prob_times_values (80x2048x2048x1469): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1469): 71.334

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2290.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1470x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1470x2048): 85.280
Elapsed time for attention_prob_times_values (80x2048x2048x1470): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1470): 77.152

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2406.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1471x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1471x2048): 84.360
Elapsed time for attention_prob_times_values (80x2048x2048x1471): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1471): 71.462

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2300.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1472x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1472x2048): 92.739
Elapsed time for attention_prob_times_values (80x2048x2048x1472): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1472): 84.781

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2635.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1473x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1473x2048): 85.712
Elapsed time for attention_prob_times_values (80x2048x2048x1473): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1473): 70.061

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2295.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1474x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1474x2048): 83.976
Elapsed time for attention_prob_times_values (80x2048x2048x1474): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1474): 77.384

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2399.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1475x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1475x2048): 83.442
Elapsed time for attention_prob_times_values (80x2048x2048x1475): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1475): 71.203

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2290.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1476x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1476x2048): 87.147
Elapsed time for attention_prob_times_values (80x2048x2048x1476): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1476): 77.297

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2443.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1477x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1477x2048): 82.831
Elapsed time for attention_prob_times_values (80x2048x2048x1477): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1477): 71.700

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2294.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1478x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1478x2048): 86.440
Elapsed time for attention_prob_times_values (80x2048x2048x1478): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1478): 77.559

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2441.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1479x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1479x2048): 85.182
Elapsed time for attention_prob_times_values (80x2048x2048x1479): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1479): 71.960

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2331.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1480x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1480x2048): 84.532
Elapsed time for attention_prob_times_values (80x2048x2048x1480): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1480): 84.563

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2528.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1481x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1481x2048): 83.488
Elapsed time for attention_prob_times_values (80x2048x2048x1481): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1481): 71.154

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2299.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1482x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1482x2048): 84.862
Elapsed time for attention_prob_times_values (80x2048x2048x1482): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1482): 76.037

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2401.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1483x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1483x2048): 84.772
Elapsed time for attention_prob_times_values (80x2048x2048x1483): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1483): 72.351

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2339.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1484x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1484x2048): 84.495
Elapsed time for attention_prob_times_values (80x2048x2048x1484): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1484): 77.900

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2430.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1485x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1485x2048): 83.850
Elapsed time for attention_prob_times_values (80x2048x2048x1485): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1485): 70.459

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2297.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1486x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1486x2048): 85.985
Elapsed time for attention_prob_times_values (80x2048x2048x1486): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1486): 77.999

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2455.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1487x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1487x2048): 83.863
Elapsed time for attention_prob_times_values (80x2048x2048x1487): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1487): 72.204

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2331.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1488x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1488x2048): 84.962
Elapsed time for attention_prob_times_values (80x2048x2048x1488): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1488): 82.892

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2522.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1489x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1489x2048): 84.797
Elapsed time for attention_prob_times_values (80x2048x2048x1489): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1489): 71.481

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2333.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1490x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1490x2048): 85.264
Elapsed time for attention_prob_times_values (80x2048x2048x1490): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1490): 78.773

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2465.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1491x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1491x2048): 83.502
Elapsed time for attention_prob_times_values (80x2048x2048x1491): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1491): 71.242

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2315.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1492x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1492x2048): 86.296
Elapsed time for attention_prob_times_values (80x2048x2048x1492): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1492): 78.080

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2471.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1493x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1493x2048): 84.592
Elapsed time for attention_prob_times_values (80x2048x2048x1493): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1493): 73.205

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2367.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1494x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1494x2048): 83.356
Elapsed time for attention_prob_times_values (80x2048x2048x1494): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1494): 78.405

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2438.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1495x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1495x2048): 83.928
Elapsed time for attention_prob_times_values (80x2048x2048x1495): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1495): 71.842

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2337.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1496x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1496x2048): 86.922
Elapsed time for attention_prob_times_values (80x2048x2048x1496): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1496): 83.663

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2576.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1497x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1497x2048): 82.379
Elapsed time for attention_prob_times_values (80x2048x2048x1497): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1497): 73.662

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2351.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1498x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1498x2048): 84.200
Elapsed time for attention_prob_times_values (80x2048x2048x1498): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1498): 77.108

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2435.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 29980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1499x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1499x2048): 84.571
Elapsed time for attention_prob_times_values (80x2048x2048x1499): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1499): 73.915

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2388.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1500x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1500x2048): 86.069
Elapsed time for attention_prob_times_values (80x2048x2048x1500): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1500): 79.875

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2510.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1501x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1501x2048): 80.301
Elapsed time for attention_prob_times_values (80x2048x2048x1501): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1501): 73.247

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2322.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1502x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1502x2048): 81.436
Elapsed time for attention_prob_times_values (80x2048x2048x1502): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1502): 77.810

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2414.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1503x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1503x2048): 84.822
Elapsed time for attention_prob_times_values (80x2048x2048x1503): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1503): 73.867

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2397.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1504x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1504x2048): 95.224
Elapsed time for attention_prob_times_values (80x2048x2048x1504): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1504): 85.995

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2745.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1505x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1505x2048): 81.537
Elapsed time for attention_prob_times_values (80x2048x2048x1505): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1505): 74.674

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2369.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1506x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1506x2048): 82.348
Elapsed time for attention_prob_times_values (80x2048x2048x1506): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1506): 77.014

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2420.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1507x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1507x2048): 80.642
Elapsed time for attention_prob_times_values (80x2048x2048x1507): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1507): 71.162

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2300.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1508x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1508x2048): 87.437
Elapsed time for attention_prob_times_values (80x2048x2048x1508): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1508): 76.012

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2476.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1509x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1509x2048): 83.012
Elapsed time for attention_prob_times_values (80x2048x2048x1509): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1509): 73.916

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2382.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1510x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1510x2048): 81.824
Elapsed time for attention_prob_times_values (80x2048x2048x1510): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1510): 75.656

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2397.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1511x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1511x2048): 85.355
Elapsed time for attention_prob_times_values (80x2048x2048x1511): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1511): 70.888

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2363.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1512x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1512x2048): 85.694
Elapsed time for attention_prob_times_values (80x2048x2048x1512): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1512): 86.558

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2629.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1513x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1513x2048): 82.264
Elapsed time for attention_prob_times_values (80x2048x2048x1513): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1513): 70.863

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2326.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1514x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1514x2048): 83.134
Elapsed time for attention_prob_times_values (80x2048x2048x1514): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1514): 75.633

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2421.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1515x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1515x2048): 84.917
Elapsed time for attention_prob_times_values (80x2048x2048x1515): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1515): 72.982

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2401.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1516x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1516x2048): 80.963
Elapsed time for attention_prob_times_values (80x2048x2048x1516): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1516): 80.683

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2473.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1517x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1517x2048): 82.386
Elapsed time for attention_prob_times_values (80x2048x2048x1517): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1517): 72.620

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2364.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1518x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1518x2048): 80.031
Elapsed time for attention_prob_times_values (80x2048x2048x1518): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1518): 74.462

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2364.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1519x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1519x2048): 85.243
Elapsed time for attention_prob_times_values (80x2048x2048x1519): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1519): 71.241

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2380.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1520x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1520x2048): 85.527
Elapsed time for attention_prob_times_values (80x2048x2048x1520): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1520): 87.538

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2655.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1521x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1521x2048): 79.146
Elapsed time for attention_prob_times_values (80x2048x2048x1521): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1521): 73.305

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2337.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1522x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1522x2048): 85.134
Elapsed time for attention_prob_times_values (80x2048x2048x1522): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1522): 78.452

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2509.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1523x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1523x2048): 84.808
Elapsed time for attention_prob_times_values (80x2048x2048x1523): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1523): 71.370

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2383.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1524x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1524x2048): 81.103
Elapsed time for attention_prob_times_values (80x2048x2048x1524): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1524): 80.872

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2491.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1525x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1525x2048): 80.240
Elapsed time for attention_prob_times_values (80x2048x2048x1525): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1525): 72.537

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2345.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1526x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1526x2048): 79.789
Elapsed time for attention_prob_times_values (80x2048x2048x1526): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1526): 75.069

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2382.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1527x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1527x2048): 84.806
Elapsed time for attention_prob_times_values (80x2048x2048x1527): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1527): 73.809

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2432.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1528x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1528x2048): 82.680
Elapsed time for attention_prob_times_values (80x2048x2048x1528): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1528): 80.391

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2514.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1529x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1529x2048): 79.297
Elapsed time for attention_prob_times_values (80x2048x2048x1529): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1529): 69.336

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2283.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1530x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1530x2048): 85.419
Elapsed time for attention_prob_times_values (80x2048x2048x1530): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1530): 75.433

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2474.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1531x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1531x2048): 79.515
Elapsed time for attention_prob_times_values (80x2048x2048x1531): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1531): 73.511

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2360.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1532x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1532x2048): 81.454
Elapsed time for attention_prob_times_values (80x2048x2048x1532): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1532): 74.572

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2407.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1533x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1533x2048): 84.385
Elapsed time for attention_prob_times_values (80x2048x2048x1533): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1533): 71.628

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2397.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1534x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1534x2048): 80.687
Elapsed time for attention_prob_times_values (80x2048x2048x1534): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1534): 77.604

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2449.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1535x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1535x2048): 81.517
Elapsed time for attention_prob_times_values (80x2048x2048x1535): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1535): 73.706

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2398.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1536x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1536x2048): 93.389
Elapsed time for attention_prob_times_values (80x2048x2048x1536): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1536): 91.308

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2862.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1537x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1537x2048): 85.163
Elapsed time for attention_prob_times_values (80x2048x2048x1537): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1537): 69.572

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2375.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1538x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1538x2048): 85.011
Elapsed time for attention_prob_times_values (80x2048x2048x1538): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1538): 76.179

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2494.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1539x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1539x2048): 83.355
Elapsed time for attention_prob_times_values (80x2048x2048x1539): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1539): 69.883

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2361.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1540x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1540x2048): 87.083
Elapsed time for attention_prob_times_values (80x2048x2048x1540): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1540): 76.327

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2528.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1541x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1541x2048): 85.265
Elapsed time for attention_prob_times_values (80x2048x2048x1541): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1541): 70.707

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2404.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1542x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1542x2048): 86.487
Elapsed time for attention_prob_times_values (80x2048x2048x1542): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1542): 76.197

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2521.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1543x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1543x2048): 85.420
Elapsed time for attention_prob_times_values (80x2048x2048x1543): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1543): 70.799

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2410.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1544x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1544x2048): 87.130
Elapsed time for attention_prob_times_values (80x2048x2048x1544): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1544): 81.569

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2625.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1545x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1545x2048): 85.000
Elapsed time for attention_prob_times_values (80x2048x2048x1545): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1545): 71.075

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2413.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1546x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1546x2048): 84.630
Elapsed time for attention_prob_times_values (80x2048x2048x1546): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1546): 74.853

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2478.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1547x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1547x2048): 84.978
Elapsed time for attention_prob_times_values (80x2048x2048x1547): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1547): 70.029

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2396.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1548x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1548x2048): 85.960
Elapsed time for attention_prob_times_values (80x2048x2048x1548): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1548): 76.716

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2532.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 30980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1549x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1549x2048): 82.996
Elapsed time for attention_prob_times_values (80x2048x2048x1549): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1549): 69.895

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2371.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1550x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1550x2048): 86.349
Elapsed time for attention_prob_times_values (80x2048x2048x1550): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1550): 75.103

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2512.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1551x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1551x2048): 84.519
Elapsed time for attention_prob_times_values (80x2048x2048x1551): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1551): 71.776

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2429.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1552x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1552x2048): 85.182
Elapsed time for attention_prob_times_values (80x2048x2048x1552): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1552): 79.778

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2579.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1553x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1553x2048): 82.353
Elapsed time for attention_prob_times_values (80x2048x2048x1553): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1553): 70.230

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2375.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1554x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1554x2048): 85.294
Elapsed time for attention_prob_times_values (80x2048x2048x1554): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1554): 76.829

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2534.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1555x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1555x2048): 83.035
Elapsed time for attention_prob_times_values (80x2048x2048x1555): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1555): 69.469

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2373.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1556x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1556x2048): 86.541
Elapsed time for attention_prob_times_values (80x2048x2048x1556): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1556): 75.436

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2530.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1557x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1557x2048): 83.798
Elapsed time for attention_prob_times_values (80x2048x2048x1557): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1557): 71.935

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2431.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1558x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1558x2048): 83.428
Elapsed time for attention_prob_times_values (80x2048x2048x1558): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1558): 75.580

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2492.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1559x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1559x2048): 85.151
Elapsed time for attention_prob_times_values (80x2048x2048x1559): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1559): 70.913

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2433.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1560x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1560x2048): 86.049
Elapsed time for attention_prob_times_values (80x2048x2048x1560): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1560): 82.522

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2651.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1561x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1561x2048): 81.835
Elapsed time for attention_prob_times_values (80x2048x2048x1561): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1561): 72.236

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2416.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1562x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1562x2048): 85.835
Elapsed time for attention_prob_times_values (80x2048x2048x1562): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1562): 77.062

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2558.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1563x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1563x2048): 84.856
Elapsed time for attention_prob_times_values (80x2048x2048x1563): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1563): 72.575

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2466.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1564x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1564x2048): 86.438
Elapsed time for attention_prob_times_values (80x2048x2048x1564): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1564): 77.417

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2576.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1565x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1565x2048): 85.004
Elapsed time for attention_prob_times_values (80x2048x2048x1565): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1565): 72.531

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2470.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1566x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1566x2048): 86.132
Elapsed time for attention_prob_times_values (80x2048x2048x1566): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1566): 77.321

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2573.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1567x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1567x2048): 84.900
Elapsed time for attention_prob_times_values (80x2048x2048x1567): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1567): 72.849

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2478.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1568x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1568x2048): 96.479
Elapsed time for attention_prob_times_values (80x2048x2048x1568): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1568): 84.685

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2852.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1569x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1569x2048): 86.850
Elapsed time for attention_prob_times_values (80x2048x2048x1569): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1569): 72.948

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2509.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1570x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1570x2048): 87.758
Elapsed time for attention_prob_times_values (80x2048x2048x1570): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1570): 77.450

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2605.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1571x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1571x2048): 86.541
Elapsed time for attention_prob_times_values (80x2048x2048x1571): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1571): 72.821

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2505.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1572x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1572x2048): 87.994
Elapsed time for attention_prob_times_values (80x2048x2048x1572): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1572): 77.786

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2617.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1573x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1573x2048): 86.372
Elapsed time for attention_prob_times_values (80x2048x2048x1573): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1573): 71.773

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2487.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1574x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1574x2048): 84.302
Elapsed time for attention_prob_times_values (80x2048x2048x1574): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1574): 76.921

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2553.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1575x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1575x2048): 86.062
Elapsed time for attention_prob_times_values (80x2048x2048x1575): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1575): 72.599

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2501.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1576x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1576x2048): 87.968
Elapsed time for attention_prob_times_values (80x2048x2048x1576): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1576): 80.334

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2668.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1577x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1577x2048): 85.613
Elapsed time for attention_prob_times_values (80x2048x2048x1577): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1577): 72.510

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2496.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1578x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1578x2048): 86.515
Elapsed time for attention_prob_times_values (80x2048x2048x1578): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1578): 77.722

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2605.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1579x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1579x2048): 85.441
Elapsed time for attention_prob_times_values (80x2048x2048x1579): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1579): 72.582

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2499.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1580x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1580x2048): 87.233
Elapsed time for attention_prob_times_values (80x2048x2048x1580): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1580): 78.093

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2625.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1581x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1581x2048): 85.652
Elapsed time for attention_prob_times_values (80x2048x2048x1581): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1581): 72.839

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2509.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1582x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1582x2048): 86.836
Elapsed time for attention_prob_times_values (80x2048x2048x1582): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1582): 77.800

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2617.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1583x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1583x2048): 85.858
Elapsed time for attention_prob_times_values (80x2048x2048x1583): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1583): 72.872

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2516.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1584x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1584x2048): 88.048
Elapsed time for attention_prob_times_values (80x2048x2048x1584): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1584): 84.355

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2751.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1585x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1585x2048): 85.377
Elapsed time for attention_prob_times_values (80x2048x2048x1585): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1585): 72.738

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2510.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1586x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1586x2048): 86.601
Elapsed time for attention_prob_times_values (80x2048x2048x1586): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1586): 78.123

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2626.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1587x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1587x2048): 85.322
Elapsed time for attention_prob_times_values (80x2048x2048x1587): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1587): 72.839

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2514.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1588x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1588x2048): 87.029
Elapsed time for attention_prob_times_values (80x2048x2048x1588): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1588): 78.403

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2641.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1589x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1589x2048): 85.675
Elapsed time for attention_prob_times_values (80x2048x2048x1589): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1589): 71.646

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2499.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1590x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1590x2048): 85.974
Elapsed time for attention_prob_times_values (80x2048x2048x1590): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1590): 76.184

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2589.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1591x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1591x2048): 85.525
Elapsed time for attention_prob_times_values (80x2048x2048x1591): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1591): 72.287

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2513.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1592x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1592x2048): 85.414
Elapsed time for attention_prob_times_values (80x2048x2048x1592): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1592): 84.044

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2719.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1593x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1593x2048): 83.731
Elapsed time for attention_prob_times_values (80x2048x2048x1593): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1593): 71.558

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2478.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1594x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1594x2048): 84.940
Elapsed time for attention_prob_times_values (80x2048x2048x1594): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1594): 76.185

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2581.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1595x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1595x2048): 85.375
Elapsed time for attention_prob_times_values (80x2048x2048x1595): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1595): 71.648

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2505.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1596x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1596x2048): 85.479
Elapsed time for attention_prob_times_values (80x2048x2048x1596): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1596): 78.690

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2636.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1597x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1597x2048): 84.968
Elapsed time for attention_prob_times_values (80x2048x2048x1597): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1597): 72.471

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2518.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1598x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1598x2048): 86.285
Elapsed time for attention_prob_times_values (80x2048x2048x1598): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1598): 76.569

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2613.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 31980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1599x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1599x2048): 85.418
Elapsed time for attention_prob_times_values (80x2048x2048x1599): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1599): 72.230

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2522.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1600x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1600x2048): 94.518
Elapsed time for attention_prob_times_values (80x2048x2048x1600): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1600): 87.675

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2933.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1601x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1601x2048): 86.807
Elapsed time for attention_prob_times_values (80x2048x2048x1601): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1601): 71.151

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2523.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1602x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1602x2048): 86.793
Elapsed time for attention_prob_times_values (80x2048x2048x1602): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1602): 77.876

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2650.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1603x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1603x2048): 86.919
Elapsed time for attention_prob_times_values (80x2048x2048x1603): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1603): 72.693

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2557.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1604x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1604x2048): 86.975
Elapsed time for attention_prob_times_values (80x2048x2048x1604): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1604): 78.910

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2675.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1605x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1605x2048): 86.210
Elapsed time for attention_prob_times_values (80x2048x2048x1605): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1605): 72.752

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2552.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1606x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1606x2048): 87.700
Elapsed time for attention_prob_times_values (80x2048x2048x1606): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1606): 78.054

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2673.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1607x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1607x2048): 86.253
Elapsed time for attention_prob_times_values (80x2048x2048x1607): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1607): 72.136

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2544.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1608x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1608x2048): 87.568
Elapsed time for attention_prob_times_values (80x2048x2048x1608): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1608): 84.706

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2790.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1609x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1609x2048): 85.199
Elapsed time for attention_prob_times_values (80x2048x2048x1609): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1609): 73.299

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2555.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1610x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1610x2048): 86.870
Elapsed time for attention_prob_times_values (80x2048x2048x1610): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1610): 78.825

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2681.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1611x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1611x2048): 85.767
Elapsed time for attention_prob_times_values (80x2048x2048x1611): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1611): 73.464

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2569.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1612x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1612x2048): 85.295
Elapsed time for attention_prob_times_values (80x2048x2048x1612): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1612): 79.091

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2666.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1613x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1613x2048): 84.488
Elapsed time for attention_prob_times_values (80x2048x2048x1613): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1613): 70.959

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2507.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1614x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1614x2048): 83.006
Elapsed time for attention_prob_times_values (80x2048x2048x1614): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1614): 75.004

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2562.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1615x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1615x2048): 85.767
Elapsed time for attention_prob_times_values (80x2048x2048x1615): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1615): 72.561

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2558.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1616x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1616x2048): 86.783
Elapsed time for attention_prob_times_values (80x2048x2048x1616): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1616): 85.739

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2808.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1617x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1617x2048): 82.995
Elapsed time for attention_prob_times_values (80x2048x2048x1617): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1617): 71.300

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2499.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1618x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1618x2048): 84.105
Elapsed time for attention_prob_times_values (80x2048x2048x1618): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1618): 75.528

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2594.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1619x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1619x2048): 85.415
Elapsed time for attention_prob_times_values (80x2048x2048x1619): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1619): 72.848

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2565.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1620x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1620x2048): 84.335
Elapsed time for attention_prob_times_values (80x2048x2048x1620): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1620): 79.475

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2671.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1621x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1621x2048): 83.072
Elapsed time for attention_prob_times_values (80x2048x2048x1621): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1621): 71.754

Attention duration (in seconds): 0.0283
Attention throughput (in TFLOP/s): 2514.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1622x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1622x2048): 83.954
Elapsed time for attention_prob_times_values (80x2048x2048x1622): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1622): 76.845

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2622.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1623x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1623x2048): 85.699
Elapsed time for attention_prob_times_values (80x2048x2048x1623): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1623): 71.518

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 2549.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1624x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1624x2048): 86.514
Elapsed time for attention_prob_times_values (80x2048x2048x1624): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1624): 85.484

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2813.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1625x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1625x2048): 83.772
Elapsed time for attention_prob_times_values (80x2048x2048x1625): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1625): 73.050

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 2555.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1626x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1626x2048): 86.299
Elapsed time for attention_prob_times_values (80x2048x2048x1626): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1626): 75.956

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2646.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1627x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1627x2048): 85.600
Elapsed time for attention_prob_times_values (80x2048x2048x1627): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1627): 72.023

Attention duration (in seconds): 0.0279
Attention throughput (in TFLOP/s): 2564.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1628x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1628x2048): 82.872
Elapsed time for attention_prob_times_values (80x2048x2048x1628): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1628): 79.903

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2668.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1629x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1629x2048): 84.086
Elapsed time for attention_prob_times_values (80x2048x2048x1629): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1629): 72.561

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2556.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1630x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1630x2048): 84.937
Elapsed time for attention_prob_times_values (80x2048x2048x1630): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1630): 77.313

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2657.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1631x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1631x2048): 85.566
Elapsed time for attention_prob_times_values (80x2048x2048x1631): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1631): 74.993

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2626.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1632x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1632x2048): 94.877
Elapsed time for attention_prob_times_values (80x2048x2048x1632): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1632): 87.694

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2996.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1633x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1633x2048): 85.446
Elapsed time for attention_prob_times_values (80x2048x2048x1633): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1633): 74.419

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2616.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1634x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1634x2048): 89.239
Elapsed time for attention_prob_times_values (80x2048x2048x1634): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1634): 79.784

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2772.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1635x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1635x2048): 89.494
Elapsed time for attention_prob_times_values (80x2048x2048x1635): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1635): 74.511

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2678.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1636x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1636x2048): 87.415
Elapsed time for attention_prob_times_values (80x2048x2048x1636): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1636): 80.231

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2757.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1637x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1637x2048): 86.221
Elapsed time for attention_prob_times_values (80x2048x2048x1637): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1637): 74.662

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2638.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 20, hidden_size: 32760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (80x2048x1638x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (80x2048x1638x2048): 87.444
Elapsed time for attention_prob_times_values (80x2048x2048x1638): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (80x2048x2048x1638): 79.828

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2753.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1x2048): 1.000
Elapsed time for attention_prob_times_values (96x2048x2048x1): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1): 0.211

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 0.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 48, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x2x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x2x2048): 1.841
Elapsed time for attention_prob_times_values (96x2048x2048x2): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x2): 1.772

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 1.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 72, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x3x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x3x2048): 2.588
Elapsed time for attention_prob_times_values (96x2048x2048x3): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x3): 2.415

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 2.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x4x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x4x2048): 3.334
Elapsed time for attention_prob_times_values (96x2048x2048x4): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x4): 3.078

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 3.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x5x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x5x2048): 4.123
Elapsed time for attention_prob_times_values (96x2048x2048x5): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x5): 3.127

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 3.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x6x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x6x2048): 4.945
Elapsed time for attention_prob_times_values (96x2048x2048x6): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x6): 4.463

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 5.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x7x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x7x2048): 5.749
Elapsed time for attention_prob_times_values (96x2048x2048x7): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x7): 5.184

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 6.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x8x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x8x2048): 6.541
Elapsed time for attention_prob_times_values (96x2048x2048x8): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x8): 6.982

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 8.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x9x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x9x2048): 7.056
Elapsed time for attention_prob_times_values (96x2048x2048x9): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x9): 7.519

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 8.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x10x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x10x2048): 7.810
Elapsed time for attention_prob_times_values (96x2048x2048x10): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x10): 8.577

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 10.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x11x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x11x2048): 8.569
Elapsed time for attention_prob_times_values (96x2048x2048x11): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x11): 9.064

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 11.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x12x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x12x2048): 9.316
Elapsed time for attention_prob_times_values (96x2048x2048x12): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x12): 10.272

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 12.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x13x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x13x2048): 10.067
Elapsed time for attention_prob_times_values (96x2048x2048x13): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x13): 10.746

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 13.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x14x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x14x2048): 10.826
Elapsed time for attention_prob_times_values (96x2048x2048x14): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x14): 11.860

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 15.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x15x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x15x2048): 11.594
Elapsed time for attention_prob_times_values (96x2048x2048x15): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x15): 12.339

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 16.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x16x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x16x2048): 12.415
Elapsed time for attention_prob_times_values (96x2048x2048x16): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x16): 13.599

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 17.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x17x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x17x2048): 12.642
Elapsed time for attention_prob_times_values (96x2048x2048x17): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x17): 13.826

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 18.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x18x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x18x2048): 13.463
Elapsed time for attention_prob_times_values (96x2048x2048x18): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x18): 14.946

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 20.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x19x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x19x2048): 14.117
Elapsed time for attention_prob_times_values (96x2048x2048x19): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x19): 15.453

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 21.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x20x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x20x2048): 14.801
Elapsed time for attention_prob_times_values (96x2048x2048x20): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x20): 16.594

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 22.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x21x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x21x2048): 15.586
Elapsed time for attention_prob_times_values (96x2048x2048x21): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x21): 16.941

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 24.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x22x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x22x2048): 16.339
Elapsed time for attention_prob_times_values (96x2048x2048x22): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x22): 16.196

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 24.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x23x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x23x2048): 17.059
Elapsed time for attention_prob_times_values (96x2048x2048x23): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x23): 18.471

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 27.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x24x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x24x2048): 17.785
Elapsed time for attention_prob_times_values (96x2048x2048x24): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x24): 20.071

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 29.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x25x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x25x2048): 18.043
Elapsed time for attention_prob_times_values (96x2048x2048x25): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x25): 19.869

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 29.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x26x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x26x2048): 18.745
Elapsed time for attention_prob_times_values (96x2048x2048x26): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x26): 21.000

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 31.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x27x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x27x2048): 19.416
Elapsed time for attention_prob_times_values (96x2048x2048x27): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x27): 18.203

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 30.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x28x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x28x2048): 20.050
Elapsed time for attention_prob_times_values (96x2048x2048x28): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x28): 22.626

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 35.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x29x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x29x2048): 20.709
Elapsed time for attention_prob_times_values (96x2048x2048x29): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x29): 22.833

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 36.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x30x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x30x2048): 21.491
Elapsed time for attention_prob_times_values (96x2048x2048x30): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x30): 23.995

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 38.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x31x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x31x2048): 22.132
Elapsed time for attention_prob_times_values (96x2048x2048x31): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x31): 24.317

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 40.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x32x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x32x2048): 26.222
Elapsed time for attention_prob_times_values (96x2048x2048x32): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x32): 26.127

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 45.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x33x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x33x2048): 26.013
Elapsed time for attention_prob_times_values (96x2048x2048x33): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x33): 25.844

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 45.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x34x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x34x2048): 26.196
Elapsed time for attention_prob_times_values (96x2048x2048x34): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x34): 26.813

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 47.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x35x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x35x2048): 25.207
Elapsed time for attention_prob_times_values (96x2048x2048x35): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x35): 27.391

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 47.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x36x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x36x2048): 25.828
Elapsed time for attention_prob_times_values (96x2048x2048x36): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x36): 24.710

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 46.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x37x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x37x2048): 25.752
Elapsed time for attention_prob_times_values (96x2048x2048x37): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x37): 28.730

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 50.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x38x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x38x2048): 26.585
Elapsed time for attention_prob_times_values (96x2048x2048x38): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x38): 29.794

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 53.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x39x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x39x2048): 26.889
Elapsed time for attention_prob_times_values (96x2048x2048x39): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x39): 30.200

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 54.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x40x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x40x2048): 27.749
Elapsed time for attention_prob_times_values (96x2048x2048x40): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x40): 32.046

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 57.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x41x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x41x2048): 27.396
Elapsed time for attention_prob_times_values (96x2048x2048x41): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x41): 27.977

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 54.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x42x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x42x2048): 28.192
Elapsed time for attention_prob_times_values (96x2048x2048x42): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x42): 32.315

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 59.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x43x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x43x2048): 28.351
Elapsed time for attention_prob_times_values (96x2048x2048x43): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x43): 32.580

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 60.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x44x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x44x2048): 29.541
Elapsed time for attention_prob_times_values (96x2048x2048x44): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x44): 33.854

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 64.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x45x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x45x2048): 27.166
Elapsed time for attention_prob_times_values (96x2048x2048x45): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x45): 30.357

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 58.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x46x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x46x2048): 30.871
Elapsed time for attention_prob_times_values (96x2048x2048x46): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x46): 35.288

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 68.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x47x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x47x2048): 31.158
Elapsed time for attention_prob_times_values (96x2048x2048x47): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x47): 35.239

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 69.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x48x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x48x2048): 32.374
Elapsed time for attention_prob_times_values (96x2048x2048x48): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x48): 37.978

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 74.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x49x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x49x2048): 32.735
Elapsed time for attention_prob_times_values (96x2048x2048x49): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x49): 36.870

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 74.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x50x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x50x2048): 33.469
Elapsed time for attention_prob_times_values (96x2048x2048x50): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x50): 37.757

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 77.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x51x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x51x2048): 34.017
Elapsed time for attention_prob_times_values (96x2048x2048x51): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x51): 37.769

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 78.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x52x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x52x2048): 32.787
Elapsed time for attention_prob_times_values (96x2048x2048x52): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x52): 39.523

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 79.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x53x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x53x2048): 35.393
Elapsed time for attention_prob_times_values (96x2048x2048x53): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x53): 39.138

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 83.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x54x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x54x2048): 36.089
Elapsed time for attention_prob_times_values (96x2048x2048x54): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x54): 40.149

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 86.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x55x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x55x2048): 36.721
Elapsed time for attention_prob_times_values (96x2048x2048x55): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x55): 33.618

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 80.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x56x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x56x2048): 37.433
Elapsed time for attention_prob_times_values (96x2048x2048x56): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x56): 43.856

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 93.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x57x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x57x2048): 36.876
Elapsed time for attention_prob_times_values (96x2048x2048x57): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x57): 42.083

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 91.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x58x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x58x2048): 37.552
Elapsed time for attention_prob_times_values (96x2048x2048x58): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x58): 43.381

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 94.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x59x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x59x2048): 38.039
Elapsed time for attention_prob_times_values (96x2048x2048x59): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x59): 43.182

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 96.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x60x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x60x2048): 38.772
Elapsed time for attention_prob_times_values (96x2048x2048x60): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x60): 44.906

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 100.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x61x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x61x2048): 39.208
Elapsed time for attention_prob_times_values (96x2048x2048x61): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x61): 44.253

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 101.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x62x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x62x2048): 36.785
Elapsed time for attention_prob_times_values (96x2048x2048x62): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x62): 46.290

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 100.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x63x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x63x2048): 40.007
Elapsed time for attention_prob_times_values (96x2048x2048x63): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x63): 45.812

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 105.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x64x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x64x2048): 55.457
Elapsed time for attention_prob_times_values (96x2048x2048x64): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x64): 49.821

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 131.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x65x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x65x2048): 42.392
Elapsed time for attention_prob_times_values (96x2048x2048x65): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x65): 26.735

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 82.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x66x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x66x2048): 43.095
Elapsed time for attention_prob_times_values (96x2048x2048x66): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x66): 34.270

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 97.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x67x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x67x2048): 42.861
Elapsed time for attention_prob_times_values (96x2048x2048x67): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x67): 33.888

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 97.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x68x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x68x2048): 43.858
Elapsed time for attention_prob_times_values (96x2048x2048x68): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x68): 35.385

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 101.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x69x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x69x2048): 43.490
Elapsed time for attention_prob_times_values (96x2048x2048x69): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x69): 34.853

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 101.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x70x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x70x2048): 44.468
Elapsed time for attention_prob_times_values (96x2048x2048x70): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x70): 35.840

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 104.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x71x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x71x2048): 44.304
Elapsed time for attention_prob_times_values (96x2048x2048x71): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x71): 35.581

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 105.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x72x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x72x2048): 45.474
Elapsed time for attention_prob_times_values (96x2048x2048x72): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x72): 35.560

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 107.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x73x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x73x2048): 44.141
Elapsed time for attention_prob_times_values (96x2048x2048x73): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x73): 36.535

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 108.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x74x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x74x2048): 39.518
Elapsed time for attention_prob_times_values (96x2048x2048x74): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x74): 38.073

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 106.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x75x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x75x2048): 38.932
Elapsed time for attention_prob_times_values (96x2048x2048x75): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x75): 37.183

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 104.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x76x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x76x2048): 46.187
Elapsed time for attention_prob_times_values (96x2048x2048x76): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x76): 39.054

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 117.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x77x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x77x2048): 45.838
Elapsed time for attention_prob_times_values (96x2048x2048x77): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x77): 38.102

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 116.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x78x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x78x2048): 41.720
Elapsed time for attention_prob_times_values (96x2048x2048x78): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x78): 39.505

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 114.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x79x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x79x2048): 46.904
Elapsed time for attention_prob_times_values (96x2048x2048x79): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x79): 39.029

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 121.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x80x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x80x2048): 48.617
Elapsed time for attention_prob_times_values (96x2048x2048x80): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x80): 36.290

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 119.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x81x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x81x2048): 46.013
Elapsed time for attention_prob_times_values (96x2048x2048x81): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x81): 39.784

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 123.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x82x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x82x2048): 46.409
Elapsed time for attention_prob_times_values (96x2048x2048x82): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x82): 41.475

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 127.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 1992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x83x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x83x2048): 41.950
Elapsed time for attention_prob_times_values (96x2048x2048x83): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x83): 40.624

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 121.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x84x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x84x2048): 47.725
Elapsed time for attention_prob_times_values (96x2048x2048x84): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x84): 42.430

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 133.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x85x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x85x2048): 47.369
Elapsed time for attention_prob_times_values (96x2048x2048x85): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x85): 41.501

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 132.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x86x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x86x2048): 48.357
Elapsed time for attention_prob_times_values (96x2048x2048x86): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x86): 42.618

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 136.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x87x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x87x2048): 48.564
Elapsed time for attention_prob_times_values (96x2048x2048x87): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x87): 42.288

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 137.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x88x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x88x2048): 49.664
Elapsed time for attention_prob_times_values (96x2048x2048x88): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x88): 42.505

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 140.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x89x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x89x2048): 48.098
Elapsed time for attention_prob_times_values (96x2048x2048x89): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x89): 43.093

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 140.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x90x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x90x2048): 48.897
Elapsed time for attention_prob_times_values (96x2048x2048x90): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x90): 43.496

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 143.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x91x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x91x2048): 48.828
Elapsed time for attention_prob_times_values (96x2048x2048x91): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x91): 43.923

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 144.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x92x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x92x2048): 48.646
Elapsed time for attention_prob_times_values (96x2048x2048x92): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x92): 45.942

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 149.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x93x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x93x2048): 49.649
Elapsed time for attention_prob_times_values (96x2048x2048x93): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x93): 44.525

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 149.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x94x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x94x2048): 50.397
Elapsed time for attention_prob_times_values (96x2048x2048x94): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x94): 45.209

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 152.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x95x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x95x2048): 49.268
Elapsed time for attention_prob_times_values (96x2048x2048x95): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x95): 45.470

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 152.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x96x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x96x2048): 64.222
Elapsed time for attention_prob_times_values (96x2048x2048x96): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x96): 47.008

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 176.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x97x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x97x2048): 50.939
Elapsed time for attention_prob_times_values (96x2048x2048x97): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x97): 46.467

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 159.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x98x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x98x2048): 51.989
Elapsed time for attention_prob_times_values (96x2048x2048x98): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x98): 48.148

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 164.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x99x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x99x2048): 51.485
Elapsed time for attention_prob_times_values (96x2048x2048x99): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x99): 46.553

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 162.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x100x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x100x2048): 52.801
Elapsed time for attention_prob_times_values (96x2048x2048x100): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x100): 49.066

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 170.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x101x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x101x2048): 52.076
Elapsed time for attention_prob_times_values (96x2048x2048x101): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x101): 48.246

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 168.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x102x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x102x2048): 53.110
Elapsed time for attention_prob_times_values (96x2048x2048x102): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x102): 49.859

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 174.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x103x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x103x2048): 52.721
Elapsed time for attention_prob_times_values (96x2048x2048x103): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x103): 48.849

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 173.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x104x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x104x2048): 54.101
Elapsed time for attention_prob_times_values (96x2048x2048x104): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x104): 48.898

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 176.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x105x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x105x2048): 52.379
Elapsed time for attention_prob_times_values (96x2048x2048x105): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x105): 49.361

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 175.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x106x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x106x2048): 53.200
Elapsed time for attention_prob_times_values (96x2048x2048x106): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x106): 51.346

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 182.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x107x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x107x2048): 53.157
Elapsed time for attention_prob_times_values (96x2048x2048x107): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x107): 50.134

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 181.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x108x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x108x2048): 54.430
Elapsed time for attention_prob_times_values (96x2048x2048x108): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x108): 50.602

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 185.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x109x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x109x2048): 53.747
Elapsed time for attention_prob_times_values (96x2048x2048x109): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x109): 50.817

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 185.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x110x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x110x2048): 50.216
Elapsed time for attention_prob_times_values (96x2048x2048x110): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x110): 52.777

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 184.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x111x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x111x2048): 53.584
Elapsed time for attention_prob_times_values (96x2048x2048x111): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x111): 51.388

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 188.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x112x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x112x2048): 56.438
Elapsed time for attention_prob_times_values (96x2048x2048x112): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x112): 53.267

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 198.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x113x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x113x2048): 54.316
Elapsed time for attention_prob_times_values (96x2048x2048x113): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x113): 51.641

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 193.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x114x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x114x2048): 55.436
Elapsed time for attention_prob_times_values (96x2048x2048x114): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x114): 54.387

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 201.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x115x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x115x2048): 55.074
Elapsed time for attention_prob_times_values (96x2048x2048x115): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x115): 52.343

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 198.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x116x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x116x2048): 56.198
Elapsed time for attention_prob_times_values (96x2048x2048x116): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x116): 54.930

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 206.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x117x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x117x2048): 55.708
Elapsed time for attention_prob_times_values (96x2048x2048x117): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x117): 53.484

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 204.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x118x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x118x2048): 56.687
Elapsed time for attention_prob_times_values (96x2048x2048x118): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x118): 55.728

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 211.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x119x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x119x2048): 56.303
Elapsed time for attention_prob_times_values (96x2048x2048x119): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x119): 54.112

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 209.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x120x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x120x2048): 57.879
Elapsed time for attention_prob_times_values (96x2048x2048x120): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x120): 56.023

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 217.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x121x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x121x2048): 48.701
Elapsed time for attention_prob_times_values (96x2048x2048x121): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x121): 43.725

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 176.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x122x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x122x2048): 56.612
Elapsed time for attention_prob_times_values (96x2048x2048x122): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x122): 56.511

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 218.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x123x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x123x2048): 55.751
Elapsed time for attention_prob_times_values (96x2048x2048x123): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x123): 44.048

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 191.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x124x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x124x2048): 57.509
Elapsed time for attention_prob_times_values (96x2048x2048x124): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x124): 57.667

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 224.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x125x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x125x2048): 56.696
Elapsed time for attention_prob_times_values (96x2048x2048x125): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x125): 44.026

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 194.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x126x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x126x2048): 55.735
Elapsed time for attention_prob_times_values (96x2048x2048x126): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x126): 58.525

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 225.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x127x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x127x2048): 56.802
Elapsed time for attention_prob_times_values (96x2048x2048x127): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x127): 43.563

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 196.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x128x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x128x2048): 67.883
Elapsed time for attention_prob_times_values (96x2048x2048x128): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x128): 61.994

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 259.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x129x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x129x2048): 56.477
Elapsed time for attention_prob_times_values (96x2048x2048x129): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x129): 43.986

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 198.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x130x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x130x2048): 57.234
Elapsed time for attention_prob_times_values (96x2048x2048x130): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x130): 46.703

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 208.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x131x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x131x2048): 57.146
Elapsed time for attention_prob_times_values (96x2048x2048x131): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x131): 44.772

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 204.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x132x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x132x2048): 58.712
Elapsed time for attention_prob_times_values (96x2048x2048x132): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x132): 47.231

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 214.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x133x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x133x2048): 57.601
Elapsed time for attention_prob_times_values (96x2048x2048x133): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x133): 45.391

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 209.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x134x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x134x2048): 58.880
Elapsed time for attention_prob_times_values (96x2048x2048x134): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x134): 47.840

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 218.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x135x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x135x2048): 58.355
Elapsed time for attention_prob_times_values (96x2048x2048x135): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x135): 45.981

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 214.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x136x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x136x2048): 60.142
Elapsed time for attention_prob_times_values (96x2048x2048x136): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x136): 45.236

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 216.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x137x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x137x2048): 58.229
Elapsed time for attention_prob_times_values (96x2048x2048x137): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x137): 46.418

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 217.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x138x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x138x2048): 58.713
Elapsed time for attention_prob_times_values (96x2048x2048x138): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x138): 49.045

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 226.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x139x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x139x2048): 58.505
Elapsed time for attention_prob_times_values (96x2048x2048x139): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x139): 46.898

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 221.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x140x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x140x2048): 60.071
Elapsed time for attention_prob_times_values (96x2048x2048x140): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x140): 49.730

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 232.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x141x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x141x2048): 58.899
Elapsed time for attention_prob_times_values (96x2048x2048x141): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x141): 47.822

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 227.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x142x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x142x2048): 60.544
Elapsed time for attention_prob_times_values (96x2048x2048x142): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x142): 50.297

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 237.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x143x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x143x2048): 56.302
Elapsed time for attention_prob_times_values (96x2048x2048x143): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x143): 48.495

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 226.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x144x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x144x2048): 59.871
Elapsed time for attention_prob_times_values (96x2048x2048x144): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x144): 47.309

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 231.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x145x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x145x2048): 59.394
Elapsed time for attention_prob_times_values (96x2048x2048x145): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x145): 48.777

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 235.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x146x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x146x2048): 60.575
Elapsed time for attention_prob_times_values (96x2048x2048x146): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x146): 51.241

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 245.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x147x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x147x2048): 56.862
Elapsed time for attention_prob_times_values (96x2048x2048x147): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x147): 49.494

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 235.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x148x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x148x2048): 61.564
Elapsed time for attention_prob_times_values (96x2048x2048x148): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x148): 52.247

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 252.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x149x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x149x2048): 60.775
Elapsed time for attention_prob_times_values (96x2048x2048x149): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x149): 50.232

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 247.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x150x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x150x2048): 60.692
Elapsed time for attention_prob_times_values (96x2048x2048x150): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x150): 51.784

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 252.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x151x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x151x2048): 61.472
Elapsed time for attention_prob_times_values (96x2048x2048x151): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x151): 50.714

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 252.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x152x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x152x2048): 63.306
Elapsed time for attention_prob_times_values (96x2048x2048x152): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x152): 46.534

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 244.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x153x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x153x2048): 61.180
Elapsed time for attention_prob_times_values (96x2048x2048x153): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x153): 51.258

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 255.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x154x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x154x2048): 62.109
Elapsed time for attention_prob_times_values (96x2048x2048x154): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x154): 53.697

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 265.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x155x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x155x2048): 61.564
Elapsed time for attention_prob_times_values (96x2048x2048x155): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x155): 51.996

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 261.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x156x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x156x2048): 62.954
Elapsed time for attention_prob_times_values (96x2048x2048x156): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x156): 54.654

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 272.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x157x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x157x2048): 62.417
Elapsed time for attention_prob_times_values (96x2048x2048x157): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x157): 52.349

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 266.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x158x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x158x2048): 63.427
Elapsed time for attention_prob_times_values (96x2048x2048x158): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x158): 55.001

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 277.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x159x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x159x2048): 62.784
Elapsed time for attention_prob_times_values (96x2048x2048x159): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x159): 53.079

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 271.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x160x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x160x2048): 76.182
Elapsed time for attention_prob_times_values (96x2048x2048x160): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x160): 55.079

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 303.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x161x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x161x2048): 61.747
Elapsed time for attention_prob_times_values (96x2048x2048x161): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x161): 54.008

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 275.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x162x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x162x2048): 63.135
Elapsed time for attention_prob_times_values (96x2048x2048x162): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x162): 56.070

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 284.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x163x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x163x2048): 62.373
Elapsed time for attention_prob_times_values (96x2048x2048x163): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x163): 54.571

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 280.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x164x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x164x2048): 64.129
Elapsed time for attention_prob_times_values (96x2048x2048x164): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x164): 53.985

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 283.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x165x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x165x2048): 62.889
Elapsed time for attention_prob_times_values (96x2048x2048x165): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x165): 55.197

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 286.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 3984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x166x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x166x2048): 63.963
Elapsed time for attention_prob_times_values (96x2048x2048x166): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x166): 57.214

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 295.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x167x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x167x2048): 63.223
Elapsed time for attention_prob_times_values (96x2048x2048x167): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x167): 51.547

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 279.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x168x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x168x2048): 60.833
Elapsed time for attention_prob_times_values (96x2048x2048x168): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x168): 55.616

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 286.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x169x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x169x2048): 63.047
Elapsed time for attention_prob_times_values (96x2048x2048x169): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x169): 55.838

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 293.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x170x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x170x2048): 56.729
Elapsed time for attention_prob_times_values (96x2048x2048x170): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x170): 58.462

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 287.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x171x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x171x2048): 63.646
Elapsed time for attention_prob_times_values (96x2048x2048x171): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x171): 56.011

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 298.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x172x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x172x2048): 65.182
Elapsed time for attention_prob_times_values (96x2048x2048x172): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x172): 59.179

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 312.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x173x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x173x2048): 64.199
Elapsed time for attention_prob_times_values (96x2048x2048x173): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x173): 57.272

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 306.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x174x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x174x2048): 65.455
Elapsed time for attention_prob_times_values (96x2048x2048x174): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x174): 59.631

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 316.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x175x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x175x2048): 64.742
Elapsed time for attention_prob_times_values (96x2048x2048x175): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x175): 57.736

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 311.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x176x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x176x2048): 67.493
Elapsed time for attention_prob_times_values (96x2048x2048x176): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x176): 51.572

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 299.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x177x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x177x2048): 64.396
Elapsed time for attention_prob_times_values (96x2048x2048x177): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x177): 58.236

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 314.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x178x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x178x2048): 59.404
Elapsed time for attention_prob_times_values (96x2048x2048x178): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x178): 60.779

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 310.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x179x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x179x2048): 57.200
Elapsed time for attention_prob_times_values (96x2048x2048x179): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x179): 59.090

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 302.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x180x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x180x2048): 66.425
Elapsed time for attention_prob_times_values (96x2048x2048x180): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x180): 61.332

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 332.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x181x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x181x2048): 65.471
Elapsed time for attention_prob_times_values (96x2048x2048x181): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x181): 59.454

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 326.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x182x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x182x2048): 66.849
Elapsed time for attention_prob_times_values (96x2048x2048x182): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x182): 60.470

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 334.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x183x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x183x2048): 65.876
Elapsed time for attention_prob_times_values (96x2048x2048x183): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x183): 58.913

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 328.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x184x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x184x2048): 67.886
Elapsed time for attention_prob_times_values (96x2048x2048x184): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x184): 60.398

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 339.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x185x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x185x2048): 64.673
Elapsed time for attention_prob_times_values (96x2048x2048x185): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x185): 60.433

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 333.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x186x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x186x2048): 63.441
Elapsed time for attention_prob_times_values (96x2048x2048x186): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x186): 62.769

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 338.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x187x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x187x2048): 66.125
Elapsed time for attention_prob_times_values (96x2048x2048x187): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x187): 58.172

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 333.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x188x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x188x2048): 65.337
Elapsed time for attention_prob_times_values (96x2048x2048x188): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x188): 63.204

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 347.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x189x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x189x2048): 66.464
Elapsed time for attention_prob_times_values (96x2048x2048x189): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x189): 61.590

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 347.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x190x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x190x2048): 64.434
Elapsed time for attention_prob_times_values (96x2048x2048x190): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x190): 64.035

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 350.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x191x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x191x2048): 67.118
Elapsed time for attention_prob_times_values (96x2048x2048x191): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x191): 61.783

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 352.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x192x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x192x2048): 77.278
Elapsed time for attention_prob_times_values (96x2048x2048x192): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x192): 65.573

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 390.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x193x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x193x2048): 65.961
Elapsed time for attention_prob_times_values (96x2048x2048x193): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x193): 50.909

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 317.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x194x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x194x2048): 67.325
Elapsed time for attention_prob_times_values (96x2048x2048x194): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x194): 52.649

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 327.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x195x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x195x2048): 66.316
Elapsed time for attention_prob_times_values (96x2048x2048x195): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x195): 51.062

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 321.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x196x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x196x2048): 67.859
Elapsed time for attention_prob_times_values (96x2048x2048x196): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x196): 52.916

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 332.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x197x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x197x2048): 66.701
Elapsed time for attention_prob_times_values (96x2048x2048x197): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x197): 51.542

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 326.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x198x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x198x2048): 68.077
Elapsed time for attention_prob_times_values (96x2048x2048x198): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x198): 53.260

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 337.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x199x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x199x2048): 67.069
Elapsed time for attention_prob_times_values (96x2048x2048x199): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x199): 51.717

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 330.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x200x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x200x2048): 69.379
Elapsed time for attention_prob_times_values (96x2048x2048x200): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x200): 51.201

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 335.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x201x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x201x2048): 63.085
Elapsed time for attention_prob_times_values (96x2048x2048x201): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x201): 51.480

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 323.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x202x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x202x2048): 66.378
Elapsed time for attention_prob_times_values (96x2048x2048x202): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x202): 54.258

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 342.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x203x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x203x2048): 59.396
Elapsed time for attention_prob_times_values (96x2048x2048x203): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x203): 52.028

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 319.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x204x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x204x2048): 66.217
Elapsed time for attention_prob_times_values (96x2048x2048x204): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x204): 54.931

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 347.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x205x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x205x2048): 67.660
Elapsed time for attention_prob_times_values (96x2048x2048x205): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x205): 52.524

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 343.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x206x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x206x2048): 68.994
Elapsed time for attention_prob_times_values (96x2048x2048x206): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x206): 55.087

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 357.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x207x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x207x2048): 68.140
Elapsed time for attention_prob_times_values (96x2048x2048x207): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x207): 52.896

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 348.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x208x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x208x2048): 71.102
Elapsed time for attention_prob_times_values (96x2048x2048x208): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x208): 53.886

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 360.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x209x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x209x2048): 67.729
Elapsed time for attention_prob_times_values (96x2048x2048x209): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x209): 50.256

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 340.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x210x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x210x2048): 68.973
Elapsed time for attention_prob_times_values (96x2048x2048x210): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x210): 56.033

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 366.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x211x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x211x2048): 68.031
Elapsed time for attention_prob_times_values (96x2048x2048x211): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x211): 50.197

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 343.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x212x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x212x2048): 69.576
Elapsed time for attention_prob_times_values (96x2048x2048x212): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x212): 50.357

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 348.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x213x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x213x2048): 68.604
Elapsed time for attention_prob_times_values (96x2048x2048x213): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x213): 53.875

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 361.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x214x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x214x2048): 69.751
Elapsed time for attention_prob_times_values (96x2048x2048x214): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x214): 56.750

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 376.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x215x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x215x2048): 68.920
Elapsed time for attention_prob_times_values (96x2048x2048x215): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x215): 52.437

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 359.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x216x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x216x2048): 70.978
Elapsed time for attention_prob_times_values (96x2048x2048x216): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x216): 55.242

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 376.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x217x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x217x2048): 68.616
Elapsed time for attention_prob_times_values (96x2048x2048x217): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x217): 54.842

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 371.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x218x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x218x2048): 66.475
Elapsed time for attention_prob_times_values (96x2048x2048x218): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x218): 57.612

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 377.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x219x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x219x2048): 68.638
Elapsed time for attention_prob_times_values (96x2048x2048x219): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x219): 54.212

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 371.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x220x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x220x2048): 70.489
Elapsed time for attention_prob_times_values (96x2048x2048x220): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x220): 58.314

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 392.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x221x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x221x2048): 69.497
Elapsed time for attention_prob_times_values (96x2048x2048x221): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x221): 55.836

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 382.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x222x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x222x2048): 67.804
Elapsed time for attention_prob_times_values (96x2048x2048x222): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x222): 51.785

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 364.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x223x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x223x2048): 64.175
Elapsed time for attention_prob_times_values (96x2048x2048x223): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x223): 56.156

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 372.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x224x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x224x2048): 79.651
Elapsed time for attention_prob_times_values (96x2048x2048x224): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x224): 56.124

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 411.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x225x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x225x2048): 68.442
Elapsed time for attention_prob_times_values (96x2048x2048x225): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x225): 54.312

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 379.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x226x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x226x2048): 65.553
Elapsed time for attention_prob_times_values (96x2048x2048x226): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x226): 57.499

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 385.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x227x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x227x2048): 64.394
Elapsed time for attention_prob_times_values (96x2048x2048x227): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x227): 57.356

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 383.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x228x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x228x2048): 63.614
Elapsed time for attention_prob_times_values (96x2048x2048x228): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x228): 59.847

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 391.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x229x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x229x2048): 69.099
Elapsed time for attention_prob_times_values (96x2048x2048x229): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x229): 57.702

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 400.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x230x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x230x2048): 70.509
Elapsed time for attention_prob_times_values (96x2048x2048x230): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x230): 58.765

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 409.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x231x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x231x2048): 69.392
Elapsed time for attention_prob_times_values (96x2048x2048x231): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x231): 54.207

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 390.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x232x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x232x2048): 71.813
Elapsed time for attention_prob_times_values (96x2048x2048x232): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x232): 58.506

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 415.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x233x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x233x2048): 68.830
Elapsed time for attention_prob_times_values (96x2048x2048x233): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x233): 58.017

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 406.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x234x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x234x2048): 70.375
Elapsed time for attention_prob_times_values (96x2048x2048x234): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x234): 60.580

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 422.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x235x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x235x2048): 69.161
Elapsed time for attention_prob_times_values (96x2048x2048x235): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x235): 58.464

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 412.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x236x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x236x2048): 71.052
Elapsed time for attention_prob_times_values (96x2048x2048x236): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x236): 59.170

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 421.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x237x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x237x2048): 69.507
Elapsed time for attention_prob_times_values (96x2048x2048x237): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x237): 58.818

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 417.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x238x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x238x2048): 71.097
Elapsed time for attention_prob_times_values (96x2048x2048x238): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x238): 61.601

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 434.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x239x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x239x2048): 69.984
Elapsed time for attention_prob_times_values (96x2048x2048x239): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x239): 59.328

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 423.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x240x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x240x2048): 55.665
Elapsed time for attention_prob_times_values (96x2048x2048x240): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x240): 61.660

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 387.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x241x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x241x2048): 53.246
Elapsed time for attention_prob_times_values (96x2048x2048x241): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x241): 59.864

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 374.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x242x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x242x2048): 53.692
Elapsed time for attention_prob_times_values (96x2048x2048x242): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x242): 62.236

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 384.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x243x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x243x2048): 52.007
Elapsed time for attention_prob_times_values (96x2048x2048x243): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x243): 59.615

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 371.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x244x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x244x2048): 54.725
Elapsed time for attention_prob_times_values (96x2048x2048x244): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x244): 62.990

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 393.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x245x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x245x2048): 53.510
Elapsed time for attention_prob_times_values (96x2048x2048x245): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x245): 60.563

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 383.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x246x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x246x2048): 54.687
Elapsed time for attention_prob_times_values (96x2048x2048x246): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x246): 63.139

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 396.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x247x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x247x2048): 52.137
Elapsed time for attention_prob_times_values (96x2048x2048x247): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x247): 60.973

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 381.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x248x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x248x2048): 55.711
Elapsed time for attention_prob_times_values (96x2048x2048x248): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x248): 60.503

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 395.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 5976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x249x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x249x2048): 53.652
Elapsed time for attention_prob_times_values (96x2048x2048x249): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x249): 62.760

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 395.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x250x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x250x2048): 54.130
Elapsed time for attention_prob_times_values (96x2048x2048x250): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x250): 64.006

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 402.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x251x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x251x2048): 53.873
Elapsed time for attention_prob_times_values (96x2048x2048x251): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x251): 59.192

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 388.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x252x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x252x2048): 51.908
Elapsed time for attention_prob_times_values (96x2048x2048x252): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x252): 64.435

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 397.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x253x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x253x2048): 53.803
Elapsed time for attention_prob_times_values (96x2048x2048x253): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x253): 60.427

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 394.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x254x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x254x2048): 54.576
Elapsed time for attention_prob_times_values (96x2048x2048x254): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x254): 65.010

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 412.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x255x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x255x2048): 53.325
Elapsed time for attention_prob_times_values (96x2048x2048x255): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x255): 61.827

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 399.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x256x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x256x2048): 76.433
Elapsed time for attention_prob_times_values (96x2048x2048x256): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x256): 69.098

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 508.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x257x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x257x2048): 55.032
Elapsed time for attention_prob_times_values (96x2048x2048x257): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x257): 53.357

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 380.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x258x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x258x2048): 57.182
Elapsed time for attention_prob_times_values (96x2048x2048x258): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x258): 55.851

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 398.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x259x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x259x2048): 55.828
Elapsed time for attention_prob_times_values (96x2048x2048x259): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x259): 54.045

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 388.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x260x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x260x2048): 57.693
Elapsed time for attention_prob_times_values (96x2048x2048x260): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x260): 55.663

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 401.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x261x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x261x2048): 55.092
Elapsed time for attention_prob_times_values (96x2048x2048x261): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x261): 54.355

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 389.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x262x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x262x2048): 56.794
Elapsed time for attention_prob_times_values (96x2048x2048x262): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x262): 57.142

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 406.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x263x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x263x2048): 55.615
Elapsed time for attention_prob_times_values (96x2048x2048x263): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x263): 55.176

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 396.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x264x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x264x2048): 57.653
Elapsed time for attention_prob_times_values (96x2048x2048x264): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x264): 54.254

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 401.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x265x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x265x2048): 55.259
Elapsed time for attention_prob_times_values (96x2048x2048x265): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x265): 55.011

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 397.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x266x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x266x2048): 56.018
Elapsed time for attention_prob_times_values (96x2048x2048x266): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x266): 57.835

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 411.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x267x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x267x2048): 55.800
Elapsed time for attention_prob_times_values (96x2048x2048x267): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x267): 55.491

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 403.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x268x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x268x2048): 56.807
Elapsed time for attention_prob_times_values (96x2048x2048x268): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x268): 58.549

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 419.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x269x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x269x2048): 56.170
Elapsed time for attention_prob_times_values (96x2048x2048x269): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x269): 55.976

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 409.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x270x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x270x2048): 56.726
Elapsed time for attention_prob_times_values (96x2048x2048x270): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x270): 58.758

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 423.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x271x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x271x2048): 56.344
Elapsed time for attention_prob_times_values (96x2048x2048x271): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x271): 56.267

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 413.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x272x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x272x2048): 58.258
Elapsed time for attention_prob_times_values (96x2048x2048x272): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x272): 73.223

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 478.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x273x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x273x2048): 55.673
Elapsed time for attention_prob_times_values (96x2048x2048x273): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x273): 54.064

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 405.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x274x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x274x2048): 55.144
Elapsed time for attention_prob_times_values (96x2048x2048x274): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x274): 59.247

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 423.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x275x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x275x2048): 55.990
Elapsed time for attention_prob_times_values (96x2048x2048x275): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x275): 56.905

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 420.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x276x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x276x2048): 55.786
Elapsed time for attention_prob_times_values (96x2048x2048x276): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x276): 58.757

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 427.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x277x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x277x2048): 55.105
Elapsed time for attention_prob_times_values (96x2048x2048x277): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x277): 57.376

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 421.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x278x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x278x2048): 56.906
Elapsed time for attention_prob_times_values (96x2048x2048x278): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x278): 56.682

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 426.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x279x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x279x2048): 56.780
Elapsed time for attention_prob_times_values (96x2048x2048x279): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x279): 57.712

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 431.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x280x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x280x2048): 58.068
Elapsed time for attention_prob_times_values (96x2048x2048x280): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x280): 70.526

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 481.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x281x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x281x2048): 56.075
Elapsed time for attention_prob_times_values (96x2048x2048x281): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x281): 58.101

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 432.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x282x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x282x2048): 56.519
Elapsed time for attention_prob_times_values (96x2048x2048x282): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x282): 60.778

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 445.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x283x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x283x2048): 56.298
Elapsed time for attention_prob_times_values (96x2048x2048x283): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x283): 58.554

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 438.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x284x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x284x2048): 57.451
Elapsed time for attention_prob_times_values (96x2048x2048x284): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x284): 61.030

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 453.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x285x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x285x2048): 55.433
Elapsed time for attention_prob_times_values (96x2048x2048x285): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x285): 58.222

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 436.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x286x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x286x2048): 54.888
Elapsed time for attention_prob_times_values (96x2048x2048x286): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x286): 61.660

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 447.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x287x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x287x2048): 56.446
Elapsed time for attention_prob_times_values (96x2048x2048x287): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x287): 59.421

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 447.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x288x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x288x2048): 77.370
Elapsed time for attention_prob_times_values (96x2048x2048x288): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x288): 76.254

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 595.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x289x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x289x2048): 60.398
Elapsed time for attention_prob_times_values (96x2048x2048x289): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x289): 59.872

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 467.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x290x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x290x2048): 60.223
Elapsed time for attention_prob_times_values (96x2048x2048x290): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x290): 62.153

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 476.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 6984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x291x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x291x2048): 60.442
Elapsed time for attention_prob_times_values (96x2048x2048x291): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x291): 60.102

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 471.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x292x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x292x2048): 61.386
Elapsed time for attention_prob_times_values (96x2048x2048x292): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x292): 62.949

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 487.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x293x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x293x2048): 60.467
Elapsed time for attention_prob_times_values (96x2048x2048x293): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x293): 60.608

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 476.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x294x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x294x2048): 58.673
Elapsed time for attention_prob_times_values (96x2048x2048x294): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x294): 62.986

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 479.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x295x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x295x2048): 58.386
Elapsed time for attention_prob_times_values (96x2048x2048x295): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x295): 61.141

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 472.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x296x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x296x2048): 59.505
Elapsed time for attention_prob_times_values (96x2048x2048x296): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x296): 78.917

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 538.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x297x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x297x2048): 59.188
Elapsed time for attention_prob_times_values (96x2048x2048x297): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x297): 58.192

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 467.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x298x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x298x2048): 56.311
Elapsed time for attention_prob_times_values (96x2048x2048x298): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x298): 61.518

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 469.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x299x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x299x2048): 56.298
Elapsed time for attention_prob_times_values (96x2048x2048x299): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x299): 61.604

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 471.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x300x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x300x2048): 60.958
Elapsed time for attention_prob_times_values (96x2048x2048x300): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x300): 64.852

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 504.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x301x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x301x2048): 59.869
Elapsed time for attention_prob_times_values (96x2048x2048x301): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x301): 62.316

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 491.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x302x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x302x2048): 55.441
Elapsed time for attention_prob_times_values (96x2048x2048x302): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x302): 65.018

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 483.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x303x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x303x2048): 60.033
Elapsed time for attention_prob_times_values (96x2048x2048x303): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x303): 63.137

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 498.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x304x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x304x2048): 61.350
Elapsed time for attention_prob_times_values (96x2048x2048x304): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x304): 80.820

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 566.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x305x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x305x2048): 59.227
Elapsed time for attention_prob_times_values (96x2048x2048x305): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x305): 63.121

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 497.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x306x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x306x2048): 60.057
Elapsed time for attention_prob_times_values (96x2048x2048x306): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x306): 65.678

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 512.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x307x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x307x2048): 59.674
Elapsed time for attention_prob_times_values (96x2048x2048x307): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x307): 63.492

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 504.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x308x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x308x2048): 61.189
Elapsed time for attention_prob_times_values (96x2048x2048x308): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x308): 64.770

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 517.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x309x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x309x2048): 60.059
Elapsed time for attention_prob_times_values (96x2048x2048x309): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x309): 64.907

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 514.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x310x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x310x2048): 60.571
Elapsed time for attention_prob_times_values (96x2048x2048x310): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x310): 66.465

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 523.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x311x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x311x2048): 60.306
Elapsed time for attention_prob_times_values (96x2048x2048x311): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x311): 63.858

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 514.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x312x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x312x2048): 61.969
Elapsed time for attention_prob_times_values (96x2048x2048x312): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x312): 81.296

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 584.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x313x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x313x2048): 59.521
Elapsed time for attention_prob_times_values (96x2048x2048x313): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x313): 64.575

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 516.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x314x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x314x2048): 59.541
Elapsed time for attention_prob_times_values (96x2048x2048x314): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x314): 67.097

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 527.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x315x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x315x2048): 59.804
Elapsed time for attention_prob_times_values (96x2048x2048x315): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x315): 65.347

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 523.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x316x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x316x2048): 59.268
Elapsed time for attention_prob_times_values (96x2048x2048x316): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x316): 67.218

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 529.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x317x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x317x2048): 57.554
Elapsed time for attention_prob_times_values (96x2048x2048x317): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x317): 65.520

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 516.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x318x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x318x2048): 60.657
Elapsed time for attention_prob_times_values (96x2048x2048x318): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x318): 67.908

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 541.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x319x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x319x2048): 59.164
Elapsed time for attention_prob_times_values (96x2048x2048x319): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x319): 66.099

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 529.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x320x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x320x2048): 78.380
Elapsed time for attention_prob_times_values (96x2048x2048x320): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x320): 81.840

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 680.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x321x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x321x2048): 62.869
Elapsed time for attention_prob_times_values (96x2048x2048x321): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x321): 58.227

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 515.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x322x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x322x2048): 63.726
Elapsed time for attention_prob_times_values (96x2048x2048x322): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x322): 60.578

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 530.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x323x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x323x2048): 62.903
Elapsed time for attention_prob_times_values (96x2048x2048x323): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x323): 58.524

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 519.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x324x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x324x2048): 64.002
Elapsed time for attention_prob_times_values (96x2048x2048x324): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x324): 59.995

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 532.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x325x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x325x2048): 62.783
Elapsed time for attention_prob_times_values (96x2048x2048x325): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x325): 58.820

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 523.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x326x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x326x2048): 63.167
Elapsed time for attention_prob_times_values (96x2048x2048x326): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x326): 61.194

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 537.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x327x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x327x2048): 62.451
Elapsed time for attention_prob_times_values (96x2048x2048x327): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x327): 59.204

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 526.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x328x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x328x2048): 62.571
Elapsed time for attention_prob_times_values (96x2048x2048x328): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x328): 73.007

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 585.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x329x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x329x2048): 61.474
Elapsed time for attention_prob_times_values (96x2048x2048x329): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x329): 57.626

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 518.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x330x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x330x2048): 57.482
Elapsed time for attention_prob_times_values (96x2048x2048x330): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x330): 61.295

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 518.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x331x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x331x2048): 61.054
Elapsed time for attention_prob_times_values (96x2048x2048x331): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x331): 59.422

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 527.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x332x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x332x2048): 63.485
Elapsed time for attention_prob_times_values (96x2048x2048x332): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x332): 62.627

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 553.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 7992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x333x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x333x2048): 60.293
Elapsed time for attention_prob_times_values (96x2048x2048x333): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x333): 57.874

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 519.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x334x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x334x2048): 62.850
Elapsed time for attention_prob_times_values (96x2048x2048x334): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x334): 62.724

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 554.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x335x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x335x2048): 57.778
Elapsed time for attention_prob_times_values (96x2048x2048x335): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x335): 58.468

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 514.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x336x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x336x2048): 63.852
Elapsed time for attention_prob_times_values (96x2048x2048x336): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x336): 74.862

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 611.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x337x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x337x2048): 61.294
Elapsed time for attention_prob_times_values (96x2048x2048x337): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x337): 59.347

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 536.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x338x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x338x2048): 62.131
Elapsed time for attention_prob_times_values (96x2048x2048x338): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x338): 63.254

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 559.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x339x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x339x2048): 61.874
Elapsed time for attention_prob_times_values (96x2048x2048x339): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x339): 56.091

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 526.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x340x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x340x2048): 62.858
Elapsed time for attention_prob_times_values (96x2048x2048x340): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x340): 63.684

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 567.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x341x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x341x2048): 62.252
Elapsed time for attention_prob_times_values (96x2048x2048x341): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x341): 59.484

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 547.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x342x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x342x2048): 60.269
Elapsed time for attention_prob_times_values (96x2048x2048x342): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x342): 62.503

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 553.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x343x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x343x2048): 60.225
Elapsed time for attention_prob_times_values (96x2048x2048x343): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x343): 60.883

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 547.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x344x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x344x2048): 61.752
Elapsed time for attention_prob_times_values (96x2048x2048x344): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x344): 76.488

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 619.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x345x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x345x2048): 59.039
Elapsed time for attention_prob_times_values (96x2048x2048x345): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x345): 59.664

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 539.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x346x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x346x2048): 61.947
Elapsed time for attention_prob_times_values (96x2048x2048x346): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x346): 62.507

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 566.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x347x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x347x2048): 60.103
Elapsed time for attention_prob_times_values (96x2048x2048x347): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x347): 60.547

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 550.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x348x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x348x2048): 63.101
Elapsed time for attention_prob_times_values (96x2048x2048x348): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x348): 63.114

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 577.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x349x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x349x2048): 61.893
Elapsed time for attention_prob_times_values (96x2048x2048x349): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x349): 60.319

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 560.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x350x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x350x2048): 62.659
Elapsed time for attention_prob_times_values (96x2048x2048x350): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x350): 63.020

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 578.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x351x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x351x2048): 62.098
Elapsed time for attention_prob_times_values (96x2048x2048x351): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x351): 61.096

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 568.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x352x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x352x2048): 80.799
Elapsed time for attention_prob_times_values (96x2048x2048x352): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x352): 75.957

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 724.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x353x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x353x2048): 64.304
Elapsed time for attention_prob_times_values (96x2048x2048x353): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x353): 62.883

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 589.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x354x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x354x2048): 63.846
Elapsed time for attention_prob_times_values (96x2048x2048x354): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x354): 63.625

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 592.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x355x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x355x2048): 61.930
Elapsed time for attention_prob_times_values (96x2048x2048x355): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x355): 63.326

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 583.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x356x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x356x2048): 64.073
Elapsed time for attention_prob_times_values (96x2048x2048x356): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x356): 61.871

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 588.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x357x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x357x2048): 61.965
Elapsed time for attention_prob_times_values (96x2048x2048x357): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x357): 62.501

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 582.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x358x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x358x2048): 65.117
Elapsed time for attention_prob_times_values (96x2048x2048x358): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x358): 62.251

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 597.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x359x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x359x2048): 64.013
Elapsed time for attention_prob_times_values (96x2048x2048x359): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x359): 64.182

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 603.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x360x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x360x2048): 62.447
Elapsed time for attention_prob_times_values (96x2048x2048x360): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x360): 79.361

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 659.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x361x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x361x2048): 63.159
Elapsed time for attention_prob_times_values (96x2048x2048x361): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x361): 62.269

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 593.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x362x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x362x2048): 63.867
Elapsed time for attention_prob_times_values (96x2048x2048x362): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x362): 62.083

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 597.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x363x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x363x2048): 63.483
Elapsed time for attention_prob_times_values (96x2048x2048x363): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x363): 62.166

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 597.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x364x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x364x2048): 64.682
Elapsed time for attention_prob_times_values (96x2048x2048x364): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x364): 65.398

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 619.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x365x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x365x2048): 60.580
Elapsed time for attention_prob_times_values (96x2048x2048x365): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x365): 62.969

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 590.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x366x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x366x2048): 64.444
Elapsed time for attention_prob_times_values (96x2048x2048x366): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x366): 62.295

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 606.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x367x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x367x2048): 63.717
Elapsed time for attention_prob_times_values (96x2048x2048x367): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x367): 63.766

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 612.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x368x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x368x2048): 65.690
Elapsed time for attention_prob_times_values (96x2048x2048x368): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x368): 80.552

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 696.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x369x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x369x2048): 62.853
Elapsed time for attention_prob_times_values (96x2048x2048x369): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x369): 63.675

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 610.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x370x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x370x2048): 63.923
Elapsed time for attention_prob_times_values (96x2048x2048x370): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x370): 65.787

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 627.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x371x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x371x2048): 63.222
Elapsed time for attention_prob_times_values (96x2048x2048x371): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x371): 63.902

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 616.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x372x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x372x2048): 64.463
Elapsed time for attention_prob_times_values (96x2048x2048x372): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x372): 63.176

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 620.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x373x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x373x2048): 63.190
Elapsed time for attention_prob_times_values (96x2048x2048x373): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x373): 63.413

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 616.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 8976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x374x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x374x2048): 64.242
Elapsed time for attention_prob_times_values (96x2048x2048x374): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x374): 64.298

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 627.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x375x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x375x2048): 63.775
Elapsed time for attention_prob_times_values (96x2048x2048x375): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x375): 64.424

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 627.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x376x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x376x2048): 61.565
Elapsed time for attention_prob_times_values (96x2048x2048x376): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x376): 80.036

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 682.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x377x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x377x2048): 62.111
Elapsed time for attention_prob_times_values (96x2048x2048x377): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x377): 64.692

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 623.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x378x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x378x2048): 60.126
Elapsed time for attention_prob_times_values (96x2048x2048x378): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x378): 67.142

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 625.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x379x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x379x2048): 63.223
Elapsed time for attention_prob_times_values (96x2048x2048x379): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x379): 62.854

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 622.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x380x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x380x2048): 64.476
Elapsed time for attention_prob_times_values (96x2048x2048x380): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x380): 67.409

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 652.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x381x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x381x2048): 62.954
Elapsed time for attention_prob_times_values (96x2048x2048x381): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x381): 65.209

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 636.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x382x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x382x2048): 60.491
Elapsed time for attention_prob_times_values (96x2048x2048x382): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x382): 67.749

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 636.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x383x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x383x2048): 62.998
Elapsed time for attention_prob_times_values (96x2048x2048x383): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x383): 60.849

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 617.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x384x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x384x2048): 77.909
Elapsed time for attention_prob_times_values (96x2048x2048x384): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x384): 82.320

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 800.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x385x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x385x2048): 63.618
Elapsed time for attention_prob_times_values (96x2048x2048x385): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x385): 57.948

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 607.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x386x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x386x2048): 66.428
Elapsed time for attention_prob_times_values (96x2048x2048x386): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x386): 60.877

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 638.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x387x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x387x2048): 65.483
Elapsed time for attention_prob_times_values (96x2048x2048x387): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x387): 58.431

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 621.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x388x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x388x2048): 66.834
Elapsed time for attention_prob_times_values (96x2048x2048x388): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x388): 61.859

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 648.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x389x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x389x2048): 60.488
Elapsed time for attention_prob_times_values (96x2048x2048x389): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x389): 58.899

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 603.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x390x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x390x2048): 63.298
Elapsed time for attention_prob_times_values (96x2048x2048x390): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x390): 62.008

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 635.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x391x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x391x2048): 65.568
Elapsed time for attention_prob_times_values (96x2048x2048x391): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x391): 59.298

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 632.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x392x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x392x2048): 66.848
Elapsed time for attention_prob_times_values (96x2048x2048x392): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x392): 74.364

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 717.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x393x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x393x2048): 64.417
Elapsed time for attention_prob_times_values (96x2048x2048x393): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x393): 56.036

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 611.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x394x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x394x2048): 65.236
Elapsed time for attention_prob_times_values (96x2048x2048x394): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x394): 62.553

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 653.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x395x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x395x2048): 64.861
Elapsed time for attention_prob_times_values (96x2048x2048x395): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x395): 59.451

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 636.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x396x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x396x2048): 65.880
Elapsed time for attention_prob_times_values (96x2048x2048x396): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x396): 63.036

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 662.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x397x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x397x2048): 64.899
Elapsed time for attention_prob_times_values (96x2048x2048x397): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x397): 59.733

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 641.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x398x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x398x2048): 65.808
Elapsed time for attention_prob_times_values (96x2048x2048x398): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x398): 63.061

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 665.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x399x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x399x2048): 65.188
Elapsed time for attention_prob_times_values (96x2048x2048x399): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x399): 60.037

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 647.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x400x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x400x2048): 67.038
Elapsed time for attention_prob_times_values (96x2048x2048x400): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x400): 71.042

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 715.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x401x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x401x2048): 64.532
Elapsed time for attention_prob_times_values (96x2048x2048x401): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x401): 60.268

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 648.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x402x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x402x2048): 65.215
Elapsed time for attention_prob_times_values (96x2048x2048x402): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x402): 61.178

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 657.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x403x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x403x2048): 64.885
Elapsed time for attention_prob_times_values (96x2048x2048x403): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x403): 60.453

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 653.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x404x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x404x2048): 65.754
Elapsed time for attention_prob_times_values (96x2048x2048x404): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x404): 63.458

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 676.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x405x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x405x2048): 65.033
Elapsed time for attention_prob_times_values (96x2048x2048x405): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x405): 60.790

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 659.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x406x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x406x2048): 63.932
Elapsed time for attention_prob_times_values (96x2048x2048x406): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x406): 62.883

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 666.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x407x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x407x2048): 65.342
Elapsed time for attention_prob_times_values (96x2048x2048x407): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x407): 60.909

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 664.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x408x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x408x2048): 66.778
Elapsed time for attention_prob_times_values (96x2048x2048x408): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x408): 77.006

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 755.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x409x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x409x2048): 64.776
Elapsed time for attention_prob_times_values (96x2048x2048x409): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x409): 61.317

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 666.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x410x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x410x2048): 65.435
Elapsed time for attention_prob_times_values (96x2048x2048x410): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x410): 61.372

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 671.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x411x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x411x2048): 65.053
Elapsed time for attention_prob_times_values (96x2048x2048x411): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x411): 61.027

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 669.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x412x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x412x2048): 62.920
Elapsed time for attention_prob_times_values (96x2048x2048x412): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x412): 63.841

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 675.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x413x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x413x2048): 61.873
Elapsed time for attention_prob_times_values (96x2048x2048x413): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x413): 61.868

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 660.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x414x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x414x2048): 65.663
Elapsed time for attention_prob_times_values (96x2048x2048x414): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x414): 65.083

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 699.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x415x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x415x2048): 65.170
Elapsed time for attention_prob_times_values (96x2048x2048x415): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x415): 62.120

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 682.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x416x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x416x2048): 83.217
Elapsed time for attention_prob_times_values (96x2048x2048x416): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x416): 77.023

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 860.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x417x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x417x2048): 68.241
Elapsed time for attention_prob_times_values (96x2048x2048x417): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x417): 62.564

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 703.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x418x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x418x2048): 69.152
Elapsed time for attention_prob_times_values (96x2048x2048x418): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x418): 65.596

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 726.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x419x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x419x2048): 68.192
Elapsed time for attention_prob_times_values (96x2048x2048x419): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x419): 62.661

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 706.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x420x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x420x2048): 69.283
Elapsed time for attention_prob_times_values (96x2048x2048x420): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x420): 65.459

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 729.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x421x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x421x2048): 68.114
Elapsed time for attention_prob_times_values (96x2048x2048x421): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x421): 62.893

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 710.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x422x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x422x2048): 68.650
Elapsed time for attention_prob_times_values (96x2048x2048x422): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x422): 66.094

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 733.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x423x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x423x2048): 65.997
Elapsed time for attention_prob_times_values (96x2048x2048x423): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x423): 63.425

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 705.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x424x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x424x2048): 69.633
Elapsed time for attention_prob_times_values (96x2048x2048x424): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x424): 79.440

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 811.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x425x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x425x2048): 66.887
Elapsed time for attention_prob_times_values (96x2048x2048x425): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x425): 63.434

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 713.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x426x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x426x2048): 66.068
Elapsed time for attention_prob_times_values (96x2048x2048x426): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x426): 66.603

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 728.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x427x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x427x2048): 67.158
Elapsed time for attention_prob_times_values (96x2048x2048x427): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x427): 63.663

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 719.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x428x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x428x2048): 68.664
Elapsed time for attention_prob_times_values (96x2048x2048x428): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x428): 66.969

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 747.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x429x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x429x2048): 64.467
Elapsed time for attention_prob_times_values (96x2048x2048x429): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x429): 63.854

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 709.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x430x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x430x2048): 68.272
Elapsed time for attention_prob_times_values (96x2048x2048x430): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x430): 67.269

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 750.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x431x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x431x2048): 67.426
Elapsed time for attention_prob_times_values (96x2048x2048x431): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x431): 62.429

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 719.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x432x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x432x2048): 68.103
Elapsed time for attention_prob_times_values (96x2048x2048x432): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x432): 80.957

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 822.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x433x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x433x2048): 64.107
Elapsed time for attention_prob_times_values (96x2048x2048x433): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x433): 64.586

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 717.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x434x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x434x2048): 67.542
Elapsed time for attention_prob_times_values (96x2048x2048x434): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x434): 67.748

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 755.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x435x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x435x2048): 63.949
Elapsed time for attention_prob_times_values (96x2048x2048x435): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x435): 64.735

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 720.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x436x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x436x2048): 65.249
Elapsed time for attention_prob_times_values (96x2048x2048x436): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x436): 65.528

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 733.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x437x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x437x2048): 67.341
Elapsed time for attention_prob_times_values (96x2048x2048x437): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x437): 65.174

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 744.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x438x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x438x2048): 67.995
Elapsed time for attention_prob_times_values (96x2048x2048x438): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x438): 68.136

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 766.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x439x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x439x2048): 67.520
Elapsed time for attention_prob_times_values (96x2048x2048x439): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x439): 65.513

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 750.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x440x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x440x2048): 65.255
Elapsed time for attention_prob_times_values (96x2048x2048x440): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x440): 82.406

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 823.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x441x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x441x2048): 63.594
Elapsed time for attention_prob_times_values (96x2048x2048x441): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x441): 65.686

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 732.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x442x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x442x2048): 64.739
Elapsed time for attention_prob_times_values (96x2048x2048x442): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x442): 68.790

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 757.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x443x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x443x2048): 66.633
Elapsed time for attention_prob_times_values (96x2048x2048x443): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x443): 66.063

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 755.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x444x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x444x2048): 68.486
Elapsed time for attention_prob_times_values (96x2048x2048x444): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x444): 66.180

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 767.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x445x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x445x2048): 66.899
Elapsed time for attention_prob_times_values (96x2048x2048x445): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x445): 66.335

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 761.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x446x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x446x2048): 67.867
Elapsed time for attention_prob_times_values (96x2048x2048x446): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x446): 69.390

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 785.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x447x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x447x2048): 67.235
Elapsed time for attention_prob_times_values (96x2048x2048x447): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x447): 66.527

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 767.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x448x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x448x2048): 80.619
Elapsed time for attention_prob_times_values (96x2048x2048x448): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x448): 80.187

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 924.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x449x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x449x2048): 69.805
Elapsed time for attention_prob_times_values (96x2048x2048x449): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x449): 60.768

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 748.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x450x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x450x2048): 70.815
Elapsed time for attention_prob_times_values (96x2048x2048x450): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x450): 61.445

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 759.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x451x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x451x2048): 69.696
Elapsed time for attention_prob_times_values (96x2048x2048x451): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x451): 59.371

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 741.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x452x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x452x2048): 70.171
Elapsed time for attention_prob_times_values (96x2048x2048x452): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x452): 63.861

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 775.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x453x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x453x2048): 69.553
Elapsed time for attention_prob_times_values (96x2048x2048x453): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x453): 60.591

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 752.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x454x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x454x2048): 68.088
Elapsed time for attention_prob_times_values (96x2048x2048x454): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x454): 62.524

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 758.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x455x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x455x2048): 69.406
Elapsed time for attention_prob_times_values (96x2048x2048x455): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x455): 60.668

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 755.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x456x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x456x2048): 70.701
Elapsed time for attention_prob_times_values (96x2048x2048x456): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x456): 82.106

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 887.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x457x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x457x2048): 68.060
Elapsed time for attention_prob_times_values (96x2048x2048x457): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x457): 59.160

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 741.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 10992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x458x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x458x2048): 69.083
Elapsed time for attention_prob_times_values (96x2048x2048x458): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x458): 64.393

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 782.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x459x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x459x2048): 68.470
Elapsed time for attention_prob_times_values (96x2048x2048x459): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x459): 61.031

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 758.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x460x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x460x2048): 70.235
Elapsed time for attention_prob_times_values (96x2048x2048x460): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x460): 64.885

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 794.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x461x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x461x2048): 67.777
Elapsed time for attention_prob_times_values (96x2048x2048x461): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x461): 61.580

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 761.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x462x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x462x2048): 67.907
Elapsed time for attention_prob_times_values (96x2048x2048x462): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x462): 64.937

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 785.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x463x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x463x2048): 66.984
Elapsed time for attention_prob_times_values (96x2048x2048x463): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x463): 62.039

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 763.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x464x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x464x2048): 69.668
Elapsed time for attention_prob_times_values (96x2048x2048x464): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x464): 83.824

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 903.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x465x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x465x2048): 67.644
Elapsed time for attention_prob_times_values (96x2048x2048x465): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x465): 62.302

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 771.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x466x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x466x2048): 68.878
Elapsed time for attention_prob_times_values (96x2048x2048x466): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x466): 63.119

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 785.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x467x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x467x2048): 67.568
Elapsed time for attention_prob_times_values (96x2048x2048x467): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x467): 59.661

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 756.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x468x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x468x2048): 67.667
Elapsed time for attention_prob_times_values (96x2048x2048x468): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x468): 65.857

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 798.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x469x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x469x2048): 67.244
Elapsed time for attention_prob_times_values (96x2048x2048x469): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x469): 62.733

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 778.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x470x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x470x2048): 69.116
Elapsed time for attention_prob_times_values (96x2048x2048x470): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x470): 65.858

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 810.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x471x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x471x2048): 68.671
Elapsed time for attention_prob_times_values (96x2048x2048x471): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x471): 63.021

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 791.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x472x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x472x2048): 70.574
Elapsed time for attention_prob_times_values (96x2048x2048x472): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x472): 83.235

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 921.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x473x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x473x2048): 68.026
Elapsed time for attention_prob_times_values (96x2048x2048x473): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x473): 63.123

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 791.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x474x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x474x2048): 68.385
Elapsed time for attention_prob_times_values (96x2048x2048x474): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x474): 66.216

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 814.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x475x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x475x2048): 68.110
Elapsed time for attention_prob_times_values (96x2048x2048x475): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x475): 63.378

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 796.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x476x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x476x2048): 69.559
Elapsed time for attention_prob_times_values (96x2048x2048x476): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x476): 66.741

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 828.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x477x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x477x2048): 68.275
Elapsed time for attention_prob_times_values (96x2048x2048x477): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x477): 63.574

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 801.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x478x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x478x2048): 68.853
Elapsed time for attention_prob_times_values (96x2048x2048x478): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x478): 66.620

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 826.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x479x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x479x2048): 68.370
Elapsed time for attention_prob_times_values (96x2048x2048x479): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x479): 61.668

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 792.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x480x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x480x2048): 85.354
Elapsed time for attention_prob_times_values (96x2048x2048x480): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x480): 87.510

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1058.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x481x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x481x2048): 70.641
Elapsed time for attention_prob_times_values (96x2048x2048x481): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x481): 64.204

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 825.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x482x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x482x2048): 69.391
Elapsed time for attention_prob_times_values (96x2048x2048x482): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x482): 65.091

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 826.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x483x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x483x2048): 70.500
Elapsed time for attention_prob_times_values (96x2048x2048x483): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x483): 64.364

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 829.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x484x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x484x2048): 72.062
Elapsed time for attention_prob_times_values (96x2048x2048x484): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x484): 66.058

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 850.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x485x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x485x2048): 70.206
Elapsed time for attention_prob_times_values (96x2048x2048x485): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x485): 62.655

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 818.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x486x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x486x2048): 70.990
Elapsed time for attention_prob_times_values (96x2048x2048x486): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x486): 65.550

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 844.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x487x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x487x2048): 69.380
Elapsed time for attention_prob_times_values (96x2048x2048x487): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x487): 64.700

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 831.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x488x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x488x2048): 71.635
Elapsed time for attention_prob_times_values (96x2048x2048x488): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x488): 87.789

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 981.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x489x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x489x2048): 68.743
Elapsed time for attention_prob_times_values (96x2048x2048x489): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x489): 64.744

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 830.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x490x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x490x2048): 69.902
Elapsed time for attention_prob_times_values (96x2048x2048x490): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x490): 67.811

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 859.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x491x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x491x2048): 67.755
Elapsed time for attention_prob_times_values (96x2048x2048x491): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x491): 65.076

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 830.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x492x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x492x2048): 67.772
Elapsed time for attention_prob_times_values (96x2048x2048x492): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x492): 66.119

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 838.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x493x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x493x2048): 69.476
Elapsed time for attention_prob_times_values (96x2048x2048x493): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x493): 65.065

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 843.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x494x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x494x2048): 70.333
Elapsed time for attention_prob_times_values (96x2048x2048x494): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x494): 65.540

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 853.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x495x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x495x2048): 67.768
Elapsed time for attention_prob_times_values (96x2048x2048x495): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x495): 65.444

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 839.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x496x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x496x2048): 71.792
Elapsed time for attention_prob_times_values (96x2048x2048x496): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x496): 89.442

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1005.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x497x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x497x2048): 66.918
Elapsed time for attention_prob_times_values (96x2048x2048x497): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x497): 63.961

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 827.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x498x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x498x2048): 69.574
Elapsed time for attention_prob_times_values (96x2048x2048x498): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x498): 68.495

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 874.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 11976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x499x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x499x2048): 68.834
Elapsed time for attention_prob_times_values (96x2048x2048x499): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x499): 65.839

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 854.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x500x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x500x2048): 70.243
Elapsed time for attention_prob_times_values (96x2048x2048x500): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x500): 69.079

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 885.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x501x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x501x2048): 69.249
Elapsed time for attention_prob_times_values (96x2048x2048x501): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x501): 66.095

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 861.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x502x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x502x2048): 70.121
Elapsed time for attention_prob_times_values (96x2048x2048x502): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x502): 66.152

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 869.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x503x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x503x2048): 69.403
Elapsed time for attention_prob_times_values (96x2048x2048x503): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x503): 63.190

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 846.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x504x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x504x2048): 71.130
Elapsed time for attention_prob_times_values (96x2048x2048x504): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x504): 90.556

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1020.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x505x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x505x2048): 68.852
Elapsed time for attention_prob_times_values (96x2048x2048x505): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x505): 57.967

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 807.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x506x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x506x2048): 69.468
Elapsed time for attention_prob_times_values (96x2048x2048x506): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x506): 67.766

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 882.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x507x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x507x2048): 68.789
Elapsed time for attention_prob_times_values (96x2048x2048x507): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x507): 60.469

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 829.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x508x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x508x2048): 69.996
Elapsed time for attention_prob_times_values (96x2048x2048x508): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x508): 67.047

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 883.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x509x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x509x2048): 68.800
Elapsed time for attention_prob_times_values (96x2048x2048x509): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x509): 60.516

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 832.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x510x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x510x2048): 69.010
Elapsed time for attention_prob_times_values (96x2048x2048x510): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x510): 67.528

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 884.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x511x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x511x2048): 67.913
Elapsed time for attention_prob_times_values (96x2048x2048x511): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x511): 60.743

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 832.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x512x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x512x2048): 81.888
Elapsed time for attention_prob_times_values (96x2048x2048x512): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x512): 94.761

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 1142.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x513x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x513x2048): 68.502
Elapsed time for attention_prob_times_values (96x2048x2048x513): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x513): 59.414

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 828.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x514x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x514x2048): 73.512
Elapsed time for attention_prob_times_values (96x2048x2048x514): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x514): 61.119

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 870.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x515x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x515x2048): 70.847
Elapsed time for attention_prob_times_values (96x2048x2048x515): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x515): 61.169

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 858.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x516x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x516x2048): 71.931
Elapsed time for attention_prob_times_values (96x2048x2048x516): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x516): 64.650

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 891.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x517x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x517x2048): 71.015
Elapsed time for attention_prob_times_values (96x2048x2048x517): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x517): 61.942

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 867.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x518x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x518x2048): 71.860
Elapsed time for attention_prob_times_values (96x2048x2048x518): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x518): 64.847

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 895.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x519x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x519x2048): 70.374
Elapsed time for attention_prob_times_values (96x2048x2048x519): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x519): 61.958

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 867.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x520x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x520x2048): 72.084
Elapsed time for attention_prob_times_values (96x2048x2048x520): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x520): 75.118

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 970.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x521x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x521x2048): 69.730
Elapsed time for attention_prob_times_values (96x2048x2048x521): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x521): 60.628

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 856.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x522x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x522x2048): 70.781
Elapsed time for attention_prob_times_values (96x2048x2048x522): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x522): 63.214

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 883.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x523x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x523x2048): 67.769
Elapsed time for attention_prob_times_values (96x2048x2048x523): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x523): 61.438

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 854.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x524x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x524x2048): 66.997
Elapsed time for attention_prob_times_values (96x2048x2048x524): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x524): 62.196

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 856.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x525x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x525x2048): 70.489
Elapsed time for attention_prob_times_values (96x2048x2048x525): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x525): 62.256

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 879.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x526x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x526x2048): 68.965
Elapsed time for attention_prob_times_values (96x2048x2048x526): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x526): 65.737

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 897.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x527x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x527x2048): 70.572
Elapsed time for attention_prob_times_values (96x2048x2048x527): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x527): 62.285

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 883.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x528x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x528x2048): 72.107
Elapsed time for attention_prob_times_values (96x2048x2048x528): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x528): 73.558

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 974.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x529x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x529x2048): 69.854
Elapsed time for attention_prob_times_values (96x2048x2048x529): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x529): 62.332

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 882.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x530x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x530x2048): 70.516
Elapsed time for attention_prob_times_values (96x2048x2048x530): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x530): 66.041

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 915.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x531x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x531x2048): 69.979
Elapsed time for attention_prob_times_values (96x2048x2048x531): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x531): 59.751

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 866.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x532x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x532x2048): 71.160
Elapsed time for attention_prob_times_values (96x2048x2048x532): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x532): 66.617

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 926.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x533x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x533x2048): 70.295
Elapsed time for attention_prob_times_values (96x2048x2048x533): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x533): 62.292

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 891.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x534x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x534x2048): 71.081
Elapsed time for attention_prob_times_values (96x2048x2048x534): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x534): 66.300

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 927.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x535x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x535x2048): 67.200
Elapsed time for attention_prob_times_values (96x2048x2048x535): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x535): 59.345

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 853.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x536x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x536x2048): 71.989
Elapsed time for attention_prob_times_values (96x2048x2048x536): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x536): 72.975

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 982.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x537x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x537x2048): 67.766
Elapsed time for attention_prob_times_values (96x2048x2048x537): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x537): 58.866

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 855.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x538x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x538x2048): 67.073
Elapsed time for attention_prob_times_values (96x2048x2048x538): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x538): 63.986

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 891.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x539x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x539x2048): 70.038
Elapsed time for attention_prob_times_values (96x2048x2048x539): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x539): 60.619

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 885.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x540x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x540x2048): 66.707
Elapsed time for attention_prob_times_values (96x2048x2048x540): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x540): 67.208

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 914.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 12984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x541x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x541x2048): 70.207
Elapsed time for attention_prob_times_values (96x2048x2048x541): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x541): 62.531

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 904.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x542x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x542x2048): 67.950
Elapsed time for attention_prob_times_values (96x2048x2048x542): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x542): 64.811

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 909.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x543x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x543x2048): 70.428
Elapsed time for attention_prob_times_values (96x2048x2048x543): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x543): 62.552

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 909.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x544x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x544x2048): 86.759
Elapsed time for attention_prob_times_values (96x2048x2048x544): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x544): 79.297

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1139.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x545x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x545x2048): 70.018
Elapsed time for attention_prob_times_values (96x2048x2048x545): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x545): 62.610

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 910.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x546x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x546x2048): 73.807
Elapsed time for attention_prob_times_values (96x2048x2048x546): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x546): 65.832

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 960.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x547x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x547x2048): 72.629
Elapsed time for attention_prob_times_values (96x2048x2048x547): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x547): 63.858

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 939.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x548x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x548x2048): 74.414
Elapsed time for attention_prob_times_values (96x2048x2048x548): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x548): 66.274

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 970.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x549x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x549x2048): 72.572
Elapsed time for attention_prob_times_values (96x2048x2048x549): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x549): 64.118

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 944.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x550x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x550x2048): 73.478
Elapsed time for attention_prob_times_values (96x2048x2048x550): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x550): 66.367

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 968.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x551x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x551x2048): 72.530
Elapsed time for attention_prob_times_values (96x2048x2048x551): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x551): 64.033

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 946.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x552x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x552x2048): 74.371
Elapsed time for attention_prob_times_values (96x2048x2048x552): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x552): 79.479

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1070.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x553x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x553x2048): 71.775
Elapsed time for attention_prob_times_values (96x2048x2048x553): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x553): 64.407

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 947.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x554x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x554x2048): 72.411
Elapsed time for attention_prob_times_values (96x2048x2048x554): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x554): 66.595

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 970.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x555x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x555x2048): 69.805
Elapsed time for attention_prob_times_values (96x2048x2048x555): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x555): 64.009

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 935.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x556x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x556x2048): 73.428
Elapsed time for attention_prob_times_values (96x2048x2048x556): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x556): 68.862

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 997.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x557x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x557x2048): 72.122
Elapsed time for attention_prob_times_values (96x2048x2048x557): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x557): 65.357

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 963.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x558x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x558x2048): 73.135
Elapsed time for attention_prob_times_values (96x2048x2048x558): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x558): 68.644

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 996.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x559x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x559x2048): 72.176
Elapsed time for attention_prob_times_values (96x2048x2048x559): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x559): 65.747

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 970.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x560x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x560x2048): 72.706
Elapsed time for attention_prob_times_values (96x2048x2048x560): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x560): 79.716

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1074.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x561x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x561x2048): 71.357
Elapsed time for attention_prob_times_values (96x2048x2048x561): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x561): 66.132

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 971.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x562x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x562x2048): 72.265
Elapsed time for attention_prob_times_values (96x2048x2048x562): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x562): 68.082

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 993.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x563x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x563x2048): 71.676
Elapsed time for attention_prob_times_values (96x2048x2048x563): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x563): 66.248

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 977.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x564x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x564x2048): 73.262
Elapsed time for attention_prob_times_values (96x2048x2048x564): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x564): 69.958

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1017.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x565x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x565x2048): 71.907
Elapsed time for attention_prob_times_values (96x2048x2048x565): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x565): 66.016

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 980.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x566x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x566x2048): 72.578
Elapsed time for attention_prob_times_values (96x2048x2048x566): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x566): 69.747

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1014.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x567x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x567x2048): 72.036
Elapsed time for attention_prob_times_values (96x2048x2048x567): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x567): 64.273

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 970.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x568x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x568x2048): 73.992
Elapsed time for attention_prob_times_values (96x2048x2048x568): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x568): 81.933

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1112.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x569x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x569x2048): 71.187
Elapsed time for attention_prob_times_values (96x2048x2048x569): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x569): 66.006

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 981.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x570x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x570x2048): 70.189
Elapsed time for attention_prob_times_values (96x2048x2048x570): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x570): 68.746

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 997.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x571x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x571x2048): 71.477
Elapsed time for attention_prob_times_values (96x2048x2048x571): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x571): 65.901

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 986.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x572x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x572x2048): 73.106
Elapsed time for attention_prob_times_values (96x2048x2048x572): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x572): 68.768

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1020.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x573x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x573x2048): 69.135
Elapsed time for attention_prob_times_values (96x2048x2048x573): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x573): 66.411

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 977.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x574x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x574x2048): 72.368
Elapsed time for attention_prob_times_values (96x2048x2048x574): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x574): 69.596

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1025.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x575x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x575x2048): 71.293
Elapsed time for attention_prob_times_values (96x2048x2048x575): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x575): 65.779

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 990.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x576x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x576x2048): 87.225
Elapsed time for attention_prob_times_values (96x2048x2048x576): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x576): 84.619

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1245.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x577x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x577x2048): 70.935
Elapsed time for attention_prob_times_values (96x2048x2048x577): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x577): 58.754

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 933.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x578x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x578x2048): 75.040
Elapsed time for attention_prob_times_values (96x2048x2048x578): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x578): 64.732

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1011.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x579x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x579x2048): 73.883
Elapsed time for attention_prob_times_values (96x2048x2048x579): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x579): 62.198

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 984.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x580x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x580x2048): 75.520
Elapsed time for attention_prob_times_values (96x2048x2048x580): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x580): 60.418

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 979.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x581x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x581x2048): 73.779
Elapsed time for attention_prob_times_values (96x2048x2048x581): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x581): 62.383

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 988.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x582x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x582x2048): 71.792
Elapsed time for attention_prob_times_values (96x2048x2048x582): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x582): 62.969

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 982.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 13992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x583x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x583x2048): 75.278
Elapsed time for attention_prob_times_values (96x2048x2048x583): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x583): 62.566

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1002.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x584x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x584x2048): 75.845
Elapsed time for attention_prob_times_values (96x2048x2048x584): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x584): 84.316

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1172.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x585x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x585x2048): 72.462
Elapsed time for attention_prob_times_values (96x2048x2048x585): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x585): 62.445

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 986.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x586x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x586x2048): 73.366
Elapsed time for attention_prob_times_values (96x2048x2048x586): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x586): 65.445

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1019.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x587x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x587x2048): 72.974
Elapsed time for attention_prob_times_values (96x2048x2048x587): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x587): 62.708

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 995.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x588x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x588x2048): 76.037
Elapsed time for attention_prob_times_values (96x2048x2048x588): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x588): 65.630

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1041.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x589x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x589x2048): 72.962
Elapsed time for attention_prob_times_values (96x2048x2048x589): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x589): 62.986

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1000.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x590x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x590x2048): 71.400
Elapsed time for attention_prob_times_values (96x2048x2048x590): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x590): 65.838

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1015.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x591x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x591x2048): 72.810
Elapsed time for attention_prob_times_values (96x2048x2048x591): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x591): 63.076

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1003.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x592x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x592x2048): 74.947
Elapsed time for attention_prob_times_values (96x2048x2048x592): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x592): 85.720

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1189.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x593x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x593x2048): 72.344
Elapsed time for attention_prob_times_values (96x2048x2048x593): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x593): 63.241

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1005.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x594x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x594x2048): 72.974
Elapsed time for attention_prob_times_values (96x2048x2048x594): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x594): 65.625

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1031.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x595x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x595x2048): 72.480
Elapsed time for attention_prob_times_values (96x2048x2048x595): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x595): 63.438

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1011.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x596x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x596x2048): 73.649
Elapsed time for attention_prob_times_values (96x2048x2048x596): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x596): 66.569

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1046.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x597x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x597x2048): 72.614
Elapsed time for attention_prob_times_values (96x2048x2048x597): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x597): 63.748

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1017.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x598x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x598x2048): 73.282
Elapsed time for attention_prob_times_values (96x2048x2048x598): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x598): 66.561

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1047.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x599x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x599x2048): 72.865
Elapsed time for attention_prob_times_values (96x2048x2048x599): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x599): 63.948

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1024.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x600x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x600x2048): 72.485
Elapsed time for attention_prob_times_values (96x2048x2048x600): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x600): 83.988

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1172.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x601x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x601x2048): 71.970
Elapsed time for attention_prob_times_values (96x2048x2048x601): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x601): 63.921

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1021.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x602x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x602x2048): 72.852
Elapsed time for attention_prob_times_values (96x2048x2048x602): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x602): 66.896

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1053.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x603x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x603x2048): 71.377
Elapsed time for attention_prob_times_values (96x2048x2048x603): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x603): 62.552

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1008.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x604x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x604x2048): 73.752
Elapsed time for attention_prob_times_values (96x2048x2048x604): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x604): 65.443

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1051.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x605x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x605x2048): 72.352
Elapsed time for attention_prob_times_values (96x2048x2048x605): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x605): 61.538

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1009.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x606x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x606x2048): 73.145
Elapsed time for attention_prob_times_values (96x2048x2048x606): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x606): 66.587

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1059.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x607x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x607x2048): 70.601
Elapsed time for attention_prob_times_values (96x2048x2048x607): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x607): 64.563

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1026.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x608x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x608x2048): 88.431
Elapsed time for attention_prob_times_values (96x2048x2048x608): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x608): 84.522

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1318.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x609x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x609x2048): 72.458
Elapsed time for attention_prob_times_values (96x2048x2048x609): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x609): 64.790

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1044.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x610x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x610x2048): 75.271
Elapsed time for attention_prob_times_values (96x2048x2048x610): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x610): 66.785

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1082.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x611x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x611x2048): 74.098
Elapsed time for attention_prob_times_values (96x2048x2048x611): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x611): 61.879

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1033.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x612x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x612x2048): 75.648
Elapsed time for attention_prob_times_values (96x2048x2048x612): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x612): 66.827

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1088.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x613x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x613x2048): 73.887
Elapsed time for attention_prob_times_values (96x2048x2048x613): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x613): 65.273

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1065.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x614x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x614x2048): 74.743
Elapsed time for attention_prob_times_values (96x2048x2048x614): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x614): 66.354

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1081.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x615x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x615x2048): 73.733
Elapsed time for attention_prob_times_values (96x2048x2048x615): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x615): 65.379

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1068.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x616x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x616x2048): 74.077
Elapsed time for attention_prob_times_values (96x2048x2048x616): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x616): 88.728

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1246.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x617x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x617x2048): 72.810
Elapsed time for attention_prob_times_values (96x2048x2048x617): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x617): 63.777

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1051.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x618x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x618x2048): 73.620
Elapsed time for attention_prob_times_values (96x2048x2048x618): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x618): 68.127

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1095.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x619x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x619x2048): 73.051
Elapsed time for attention_prob_times_values (96x2048x2048x619): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x619): 65.662

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1072.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x620x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x620x2048): 74.254
Elapsed time for attention_prob_times_values (96x2048x2048x620): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x620): 68.540

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1107.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x621x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x621x2048): 70.582
Elapsed time for attention_prob_times_values (96x2048x2048x621): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x621): 65.911

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1060.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x622x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x622x2048): 73.997
Elapsed time for attention_prob_times_values (96x2048x2048x622): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x622): 68.436

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1107.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x623x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x623x2048): 73.234
Elapsed time for attention_prob_times_values (96x2048x2048x623): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x623): 65.957

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1082.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x624x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x624x2048): 73.176
Elapsed time for attention_prob_times_values (96x2048x2048x624): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x624): 90.246

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1262.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x625x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x625x2048): 71.827
Elapsed time for attention_prob_times_values (96x2048x2048x625): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x625): 65.563

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1072.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x626x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x626x2048): 73.446
Elapsed time for attention_prob_times_values (96x2048x2048x626): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x626): 68.841

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1113.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x627x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x627x2048): 72.692
Elapsed time for attention_prob_times_values (96x2048x2048x627): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x627): 66.502

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1090.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x628x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x628x2048): 74.005
Elapsed time for attention_prob_times_values (96x2048x2048x628): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x628): 68.632

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1119.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x629x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x629x2048): 71.958
Elapsed time for attention_prob_times_values (96x2048x2048x629): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x629): 66.771

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1090.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x630x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x630x2048): 73.849
Elapsed time for attention_prob_times_values (96x2048x2048x630): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x630): 69.256

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1126.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x631x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x631x2048): 73.121
Elapsed time for attention_prob_times_values (96x2048x2048x631): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x631): 66.737

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1101.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x632x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x632x2048): 74.815
Elapsed time for attention_prob_times_values (96x2048x2048x632): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x632): 91.013

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1298.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x633x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x633x2048): 72.522
Elapsed time for attention_prob_times_values (96x2048x2048x633): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x633): 67.155

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1104.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x634x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x634x2048): 72.995
Elapsed time for attention_prob_times_values (96x2048x2048x634): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x634): 69.042

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1125.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x635x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x635x2048): 72.463
Elapsed time for attention_prob_times_values (96x2048x2048x635): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x635): 67.119

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1106.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x636x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x636x2048): 73.708
Elapsed time for attention_prob_times_values (96x2048x2048x636): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x636): 68.434

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1128.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x637x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x637x2048): 72.284
Elapsed time for attention_prob_times_values (96x2048x2048x637): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x637): 65.601

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1095.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x638x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x638x2048): 73.214
Elapsed time for attention_prob_times_values (96x2048x2048x638): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x638): 69.094

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1134.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x639x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x639x2048): 72.319
Elapsed time for attention_prob_times_values (96x2048x2048x639): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x639): 65.148

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1095.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x640x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x640x2048): 82.834
Elapsed time for attention_prob_times_values (96x2048x2048x640): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x640): 93.626

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1406.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x641x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x641x2048): 74.226
Elapsed time for attention_prob_times_values (96x2048x2048x641): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x641): 61.063

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1073.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x642x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x642x2048): 73.673
Elapsed time for attention_prob_times_values (96x2048x2048x642): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x642): 64.600

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1104.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x643x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x643x2048): 74.072
Elapsed time for attention_prob_times_values (96x2048x2048x643): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x643): 60.234

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1067.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x644x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x644x2048): 75.808
Elapsed time for attention_prob_times_values (96x2048x2048x644): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x644): 64.497

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1121.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x645x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x645x2048): 74.534
Elapsed time for attention_prob_times_values (96x2048x2048x645): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x645): 60.706

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1078.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x646x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x646x2048): 75.210
Elapsed time for attention_prob_times_values (96x2048x2048x646): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x646): 64.985

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1125.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x647x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x647x2048): 74.211
Elapsed time for attention_prob_times_values (96x2048x2048x647): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x647): 62.345

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1095.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x648x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x648x2048): 75.725
Elapsed time for attention_prob_times_values (96x2048x2048x648): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x648): 77.634

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1241.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x649x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x649x2048): 73.468
Elapsed time for attention_prob_times_values (96x2048x2048x649): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x649): 62.505

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1094.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x650x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x650x2048): 74.165
Elapsed time for attention_prob_times_values (96x2048x2048x650): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x650): 65.288

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1127.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x651x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x651x2048): 73.625
Elapsed time for attention_prob_times_values (96x2048x2048x651): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x651): 62.730

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1101.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x652x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x652x2048): 74.764
Elapsed time for attention_prob_times_values (96x2048x2048x652): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x652): 65.561

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1137.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x653x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x653x2048): 73.836
Elapsed time for attention_prob_times_values (96x2048x2048x653): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x653): 62.855

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1107.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x654x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x654x2048): 74.556
Elapsed time for attention_prob_times_values (96x2048x2048x654): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x654): 65.078

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1134.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x655x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x655x2048): 73.636
Elapsed time for attention_prob_times_values (96x2048x2048x655): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x655): 63.044

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1110.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x656x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x656x2048): 75.903
Elapsed time for attention_prob_times_values (96x2048x2048x656): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x656): 78.968

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1267.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x657x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x657x2048): 73.331
Elapsed time for attention_prob_times_values (96x2048x2048x657): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x657): 63.375

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1114.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x658x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x658x2048): 74.183
Elapsed time for attention_prob_times_values (96x2048x2048x658): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x658): 65.812

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1145.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x659x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x659x2048): 73.557
Elapsed time for attention_prob_times_values (96x2048x2048x659): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x659): 63.353

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1119.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x660x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x660x2048): 72.075
Elapsed time for attention_prob_times_values (96x2048x2048x660): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x660): 65.419

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1129.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x661x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x661x2048): 73.773
Elapsed time for attention_prob_times_values (96x2048x2048x661): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x661): 63.596

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1126.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x662x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x662x2048): 74.430
Elapsed time for attention_prob_times_values (96x2048x2048x662): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x662): 66.059

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1156.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x663x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x663x2048): 73.911
Elapsed time for attention_prob_times_values (96x2048x2048x663): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x663): 63.445

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1129.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x664x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x664x2048): 73.019
Elapsed time for attention_prob_times_values (96x2048x2048x664): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x664): 79.580

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1261.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x665x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x665x2048): 73.385
Elapsed time for attention_prob_times_values (96x2048x2048x665): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x665): 63.930

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1133.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 15984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x666x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x666x2048): 74.064
Elapsed time for attention_prob_times_values (96x2048x2048x666): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x666): 66.045

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1159.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x667x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x667x2048): 70.900
Elapsed time for attention_prob_times_values (96x2048x2048x667): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x667): 63.924

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1118.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x668x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x668x2048): 74.682
Elapsed time for attention_prob_times_values (96x2048x2048x668): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x668): 66.593

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1172.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x669x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x669x2048): 72.652
Elapsed time for attention_prob_times_values (96x2048x2048x669): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x669): 64.097

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1136.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x670x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x670x2048): 74.250
Elapsed time for attention_prob_times_values (96x2048x2048x670): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x670): 66.449

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1171.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x671x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x671x2048): 73.708
Elapsed time for attention_prob_times_values (96x2048x2048x671): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x671): 64.296

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1148.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x672x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x672x2048): 89.072
Elapsed time for attention_prob_times_values (96x2048x2048x672): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x672): 81.091

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1421.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x673x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x673x2048): 75.776
Elapsed time for attention_prob_times_values (96x2048x2048x673): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x673): 64.555

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1169.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x674x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x674x2048): 76.789
Elapsed time for attention_prob_times_values (96x2048x2048x674): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x674): 66.652

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1198.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x675x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x675x2048): 69.420
Elapsed time for attention_prob_times_values (96x2048x2048x675): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x675): 61.324

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1095.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x676x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x676x2048): 77.417
Elapsed time for attention_prob_times_values (96x2048x2048x676): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x676): 67.076

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1210.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x677x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x677x2048): 75.706
Elapsed time for attention_prob_times_values (96x2048x2048x677): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x677): 58.695

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1115.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x678x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x678x2048): 76.479
Elapsed time for attention_prob_times_values (96x2048x2048x678): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x678): 66.989

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1206.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x679x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x679x2048): 75.757
Elapsed time for attention_prob_times_values (96x2048x2048x679): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x679): 64.761

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1181.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x680x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x680x2048): 81.132
Elapsed time for attention_prob_times_values (96x2048x2048x680): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x680): 81.474

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1377.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x681x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x681x2048): 75.062
Elapsed time for attention_prob_times_values (96x2048x2048x681): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x681): 64.528

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1177.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x682x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x682x2048): 75.761
Elapsed time for attention_prob_times_values (96x2048x2048x682): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x682): 66.864

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1206.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x683x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x683x2048): 75.128
Elapsed time for attention_prob_times_values (96x2048x2048x683): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x683): 61.939

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1154.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x684x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x684x2048): 76.779
Elapsed time for attention_prob_times_values (96x2048x2048x684): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x684): 67.459

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1223.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x685x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x685x2048): 75.383
Elapsed time for attention_prob_times_values (96x2048x2048x685): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x685): 64.916

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1189.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x686x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x686x2048): 76.230
Elapsed time for attention_prob_times_values (96x2048x2048x686): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x686): 67.831

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1225.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x687x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x687x2048): 75.581
Elapsed time for attention_prob_times_values (96x2048x2048x687): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x687): 65.320

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1198.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x688x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x688x2048): 77.036
Elapsed time for attention_prob_times_values (96x2048x2048x688): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x688): 82.838

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1367.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x689x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x689x2048): 74.541
Elapsed time for attention_prob_times_values (96x2048x2048x689): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x689): 65.554

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1196.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x690x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x690x2048): 74.728
Elapsed time for attention_prob_times_values (96x2048x2048x690): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x690): 68.053

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1223.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x691x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x691x2048): 74.779
Elapsed time for attention_prob_times_values (96x2048x2048x691): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x691): 65.418

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1199.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x692x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x692x2048): 76.243
Elapsed time for attention_prob_times_values (96x2048x2048x692): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x692): 68.541

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1242.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x693x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x693x2048): 75.046
Elapsed time for attention_prob_times_values (96x2048x2048x693): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x693): 65.545

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1206.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x694x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x694x2048): 75.612
Elapsed time for attention_prob_times_values (96x2048x2048x694): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x694): 68.610

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1242.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x695x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x695x2048): 75.070
Elapsed time for attention_prob_times_values (96x2048x2048x695): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x695): 63.012

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1184.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x696x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x696x2048): 76.924
Elapsed time for attention_prob_times_values (96x2048x2048x696): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x696): 83.462

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1386.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x697x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x697x2048): 74.363
Elapsed time for attention_prob_times_values (96x2048x2048x697): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x697): 65.951

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1211.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x698x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x698x2048): 75.086
Elapsed time for attention_prob_times_values (96x2048x2048x698): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x698): 66.881

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1228.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x699x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x699x2048): 74.595
Elapsed time for attention_prob_times_values (96x2048x2048x699): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x699): 66.085

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1218.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x700x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x700x2048): 76.258
Elapsed time for attention_prob_times_values (96x2048x2048x700): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x700): 65.427

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1225.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x701x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x701x2048): 74.655
Elapsed time for attention_prob_times_values (96x2048x2048x701): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x701): 66.391

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1224.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x702x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x702x2048): 75.574
Elapsed time for attention_prob_times_values (96x2048x2048x702): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x702): 68.939

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1258.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x703x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x703x2048): 74.565
Elapsed time for attention_prob_times_values (96x2048x2048x703): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x703): 65.020

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1214.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x704x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x704x2048): 89.003
Elapsed time for attention_prob_times_values (96x2048x2048x704): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x704): 85.826

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1529.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x705x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x705x2048): 76.863
Elapsed time for attention_prob_times_values (96x2048x2048x705): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x705): 63.302

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1216.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x706x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x706x2048): 77.775
Elapsed time for attention_prob_times_values (96x2048x2048x706): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x706): 63.615

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1228.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x707x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x707x2048): 76.655
Elapsed time for attention_prob_times_values (96x2048x2048x707): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x707): 63.432

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1219.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x708x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x708x2048): 78.381
Elapsed time for attention_prob_times_values (96x2048x2048x708): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x708): 65.810

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1258.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x709x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x709x2048): 76.296
Elapsed time for attention_prob_times_values (96x2048x2048x709): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x709): 63.575

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1221.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x710x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x710x2048): 74.870
Elapsed time for attention_prob_times_values (96x2048x2048x710): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x710): 66.378

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1241.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x711x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x711x2048): 76.278
Elapsed time for attention_prob_times_values (96x2048x2048x711): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x711): 63.771

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1227.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x712x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x712x2048): 78.244
Elapsed time for attention_prob_times_values (96x2048x2048x712): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x712): 85.187

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1442.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x713x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x713x2048): 75.446
Elapsed time for attention_prob_times_values (96x2048x2048x713): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x713): 62.919

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1215.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x714x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x714x2048): 76.300
Elapsed time for attention_prob_times_values (96x2048x2048x714): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x714): 66.612

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1261.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x715x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x715x2048): 72.962
Elapsed time for attention_prob_times_values (96x2048x2048x715): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x715): 63.068

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1201.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x716x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x716x2048): 77.205
Elapsed time for attention_prob_times_values (96x2048x2048x716): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x716): 66.957

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1275.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x717x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x717x2048): 75.798
Elapsed time for attention_prob_times_values (96x2048x2048x717): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x717): 63.660

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1232.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x718x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x718x2048): 74.201
Elapsed time for attention_prob_times_values (96x2048x2048x718): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x718): 66.863

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1254.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x719x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x719x2048): 75.798
Elapsed time for attention_prob_times_values (96x2048x2048x719): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x719): 61.753

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1214.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x720x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x720x2048): 77.882
Elapsed time for attention_prob_times_values (96x2048x2048x720): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x720): 86.380

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1464.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x721x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x721x2048): 69.906
Elapsed time for attention_prob_times_values (96x2048x2048x721): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x721): 60.039

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1156.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x722x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x722x2048): 76.000
Elapsed time for attention_prob_times_values (96x2048x2048x722): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x722): 67.130

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1277.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x723x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x723x2048): 75.405
Elapsed time for attention_prob_times_values (96x2048x2048x723): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x723): 64.049

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1242.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x724x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x724x2048): 76.713
Elapsed time for attention_prob_times_values (96x2048x2048x724): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x724): 66.303

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1278.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x725x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x725x2048): 74.716
Elapsed time for attention_prob_times_values (96x2048x2048x725): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x725): 64.303

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1243.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x726x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x726x2048): 76.061
Elapsed time for attention_prob_times_values (96x2048x2048x726): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x726): 67.341

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1286.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x727x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x727x2048): 75.378
Elapsed time for attention_prob_times_values (96x2048x2048x727): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x727): 63.472

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1243.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x728x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x728x2048): 76.009
Elapsed time for attention_prob_times_values (96x2048x2048x728): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x728): 86.140

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1458.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x729x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x729x2048): 74.902
Elapsed time for attention_prob_times_values (96x2048x2048x729): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x729): 64.362

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1252.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x730x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x730x2048): 75.668
Elapsed time for attention_prob_times_values (96x2048x2048x730): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x730): 67.576

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1292.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x731x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x731x2048): 75.195
Elapsed time for attention_prob_times_values (96x2048x2048x731): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x731): 64.358

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1257.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x732x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x732x2048): 76.516
Elapsed time for attention_prob_times_values (96x2048x2048x732): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x732): 66.609

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1293.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x733x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x733x2048): 75.265
Elapsed time for attention_prob_times_values (96x2048x2048x733): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x733): 64.777

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1265.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x734x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x734x2048): 75.945
Elapsed time for attention_prob_times_values (96x2048x2048x734): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x734): 66.985

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1295.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x735x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x735x2048): 75.215
Elapsed time for attention_prob_times_values (96x2048x2048x735): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x735): 64.688

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1267.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x736x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x736x2048): 89.821
Elapsed time for attention_prob_times_values (96x2048x2048x736): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x736): 89.027

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1631.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x737x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x737x2048): 77.094
Elapsed time for attention_prob_times_values (96x2048x2048x737): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x737): 65.063

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1289.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x738x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x738x2048): 77.940
Elapsed time for attention_prob_times_values (96x2048x2048x738): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x738): 64.056

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1286.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x739x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x739x2048): 76.213
Elapsed time for attention_prob_times_values (96x2048x2048x739): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x739): 65.193

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1287.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x740x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x740x2048): 78.517
Elapsed time for attention_prob_times_values (96x2048x2048x740): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x740): 66.824

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1324.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x741x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x741x2048): 76.771
Elapsed time for attention_prob_times_values (96x2048x2048x741): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x741): 65.339

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1296.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x742x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x742x2048): 77.583
Elapsed time for attention_prob_times_values (96x2048x2048x742): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x742): 67.296

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1325.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x743x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x743x2048): 76.672
Elapsed time for attention_prob_times_values (96x2048x2048x743): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x743): 64.221

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1287.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x744x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x744x2048): 78.529
Elapsed time for attention_prob_times_values (96x2048x2048x744): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x744): 89.022

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1538.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x745x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x745x2048): 72.901
Elapsed time for attention_prob_times_values (96x2048x2048x745): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x745): 65.466

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1273.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x746x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x746x2048): 76.679
Elapsed time for attention_prob_times_values (96x2048x2048x746): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x746): 60.423

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1249.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x747x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x747x2048): 76.136
Elapsed time for attention_prob_times_values (96x2048x2048x747): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x747): 65.659

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1304.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x748x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x748x2048): 72.399
Elapsed time for attention_prob_times_values (96x2048x2048x748): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x748): 63.184

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1250.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 17976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x749x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x749x2048): 75.786
Elapsed time for attention_prob_times_values (96x2048x2048x749): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x749): 61.991

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1265.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x750x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x750x2048): 76.981
Elapsed time for attention_prob_times_values (96x2048x2048x750): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x750): 67.381

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1335.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x751x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x751x2048): 76.046
Elapsed time for attention_prob_times_values (96x2048x2048x751): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x751): 60.233

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1250.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x752x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x752x2048): 70.891
Elapsed time for attention_prob_times_values (96x2048x2048x752): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x752): 89.782

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1475.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x753x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x753x2048): 75.396
Elapsed time for attention_prob_times_values (96x2048x2048x753): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x753): 66.061

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1313.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x754x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x754x2048): 76.164
Elapsed time for attention_prob_times_values (96x2048x2048x754): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x754): 65.797

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1318.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x755x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x755x2048): 75.610
Elapsed time for attention_prob_times_values (96x2048x2048x755): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x755): 64.361

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1299.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x756x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x756x2048): 76.963
Elapsed time for attention_prob_times_values (96x2048x2048x756): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x756): 67.991

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1351.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x757x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x757x2048): 75.841
Elapsed time for attention_prob_times_values (96x2048x2048x757): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x757): 66.574

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1328.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x758x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x758x2048): 76.520
Elapsed time for attention_prob_times_values (96x2048x2048x758): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x758): 67.804

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1349.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x759x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x759x2048): 75.851
Elapsed time for attention_prob_times_values (96x2048x2048x759): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x759): 66.103

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1327.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x760x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x760x2048): 77.601
Elapsed time for attention_prob_times_values (96x2048x2048x760): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x760): 90.959

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1575.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x761x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x761x2048): 55.857
Elapsed time for attention_prob_times_values (96x2048x2048x761): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x761): 66.136

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1140.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x762x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x762x2048): 71.632
Elapsed time for attention_prob_times_values (96x2048x2048x762): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x762): 65.647

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1292.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x763x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x763x2048): 75.321
Elapsed time for attention_prob_times_values (96x2048x2048x763): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x763): 66.071

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1329.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x764x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x764x2048): 74.678
Elapsed time for attention_prob_times_values (96x2048x2048x764): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x764): 67.071

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1336.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x765x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x765x2048): 75.375
Elapsed time for attention_prob_times_values (96x2048x2048x765): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x765): 64.642

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1317.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x766x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x766x2048): 76.187
Elapsed time for attention_prob_times_values (96x2048x2048x766): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x766): 64.641

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1325.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x767x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x767x2048): 75.282
Elapsed time for attention_prob_times_values (96x2048x2048x767): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x767): 65.206

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1326.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x768x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x768x2048): 86.443
Elapsed time for attention_prob_times_values (96x2048x2048x768): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x768): 87.266

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1650.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x769x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x769x2048): 77.183
Elapsed time for attention_prob_times_values (96x2048x2048x769): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x769): 60.904

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1295.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x770x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x770x2048): 78.162
Elapsed time for attention_prob_times_values (96x2048x2048x770): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x770): 64.762

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1349.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x771x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x771x2048): 76.895
Elapsed time for attention_prob_times_values (96x2048x2048x771): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x771): 60.601

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1292.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x772x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x772x2048): 78.686
Elapsed time for attention_prob_times_values (96x2048x2048x772): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x772): 64.838

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1357.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x773x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x773x2048): 77.215
Elapsed time for attention_prob_times_values (96x2048x2048x773): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x773): 61.971

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1314.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x774x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x774x2048): 78.056
Elapsed time for attention_prob_times_values (96x2048x2048x774): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x774): 64.890

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1356.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x775x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x775x2048): 74.433
Elapsed time for attention_prob_times_values (96x2048x2048x775): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x775): 62.128

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1297.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x776x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x776x2048): 78.474
Elapsed time for attention_prob_times_values (96x2048x2048x776): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x776): 82.238

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1540.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x777x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x777x2048): 73.077
Elapsed time for attention_prob_times_values (96x2048x2048x777): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x777): 58.403

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1247.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x778x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x778x2048): 76.986
Elapsed time for attention_prob_times_values (96x2048x2048x778): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x778): 65.059

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1356.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x779x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x779x2048): 76.536
Elapsed time for attention_prob_times_values (96x2048x2048x779): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x779): 59.482

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1289.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x780x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x780x2048): 77.676
Elapsed time for attention_prob_times_values (96x2048x2048x780): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x780): 65.572

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1371.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x781x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x781x2048): 76.712
Elapsed time for attention_prob_times_values (96x2048x2048x781): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x781): 62.070

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1324.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x782x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x782x2048): 77.512
Elapsed time for attention_prob_times_values (96x2048x2048x782): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x782): 63.849

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1353.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x783x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x783x2048): 74.051
Elapsed time for attention_prob_times_values (96x2048x2048x783): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x783): 62.389

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1310.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x784x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x784x2048): 78.724
Elapsed time for attention_prob_times_values (96x2048x2048x784): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x784): 83.384

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1569.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x785x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x785x2048): 73.555
Elapsed time for attention_prob_times_values (96x2048x2048x785): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x785): 62.593

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1311.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x786x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x786x2048): 74.625
Elapsed time for attention_prob_times_values (96x2048x2048x786): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x786): 65.580

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1355.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x787x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x787x2048): 76.297
Elapsed time for attention_prob_times_values (96x2048x2048x787): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x787): 62.571

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1336.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x788x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x788x2048): 77.562
Elapsed time for attention_prob_times_values (96x2048x2048x788): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x788): 65.931

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1387.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x789x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x789x2048): 76.484
Elapsed time for attention_prob_times_values (96x2048x2048x789): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x789): 62.572

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1341.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x790x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x790x2048): 77.154
Elapsed time for attention_prob_times_values (96x2048x2048x790): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x790): 65.746

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1385.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 18984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x791x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x791x2048): 76.498
Elapsed time for attention_prob_times_values (96x2048x2048x791): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x791): 62.846

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1348.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x792x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x792x2048): 78.115
Elapsed time for attention_prob_times_values (96x2048x2048x792): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x792): 83.750

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1581.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x793x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x793x2048): 75.963
Elapsed time for attention_prob_times_values (96x2048x2048x793): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x793): 62.981

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1348.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x794x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x794x2048): 76.584
Elapsed time for attention_prob_times_values (96x2048x2048x794): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x794): 66.071

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1391.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x795x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x795x2048): 76.100
Elapsed time for attention_prob_times_values (96x2048x2048x795): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x795): 63.152

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1355.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x796x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x796x2048): 77.516
Elapsed time for attention_prob_times_values (96x2048x2048x796): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x796): 66.498

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1407.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x797x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x797x2048): 76.214
Elapsed time for attention_prob_times_values (96x2048x2048x797): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x797): 63.205

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1359.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x798x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x798x2048): 76.943
Elapsed time for attention_prob_times_values (96x2048x2048x798): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x798): 66.309

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1403.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x799x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x799x2048): 76.342
Elapsed time for attention_prob_times_values (96x2048x2048x799): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x799): 62.904

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1360.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x800x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x800x2048): 90.731
Elapsed time for attention_prob_times_values (96x2048x2048x800): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x800): 85.306

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1736.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x801x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x801x2048): 78.469
Elapsed time for attention_prob_times_values (96x2048x2048x801): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x801): 62.608

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1377.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x802x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x802x2048): 79.324
Elapsed time for attention_prob_times_values (96x2048x2048x802): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x802): 65.206

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1416.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x803x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x803x2048): 78.259
Elapsed time for attention_prob_times_values (96x2048x2048x803): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x803): 63.811

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1393.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x804x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x804x2048): 79.938
Elapsed time for attention_prob_times_values (96x2048x2048x804): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x804): 64.511

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1416.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x805x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x805x2048): 75.399
Elapsed time for attention_prob_times_values (96x2048x2048x805): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x805): 63.948

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1374.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x806x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x806x2048): 78.943
Elapsed time for attention_prob_times_values (96x2048x2048x806): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x806): 64.692

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1414.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x807x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x807x2048): 77.800
Elapsed time for attention_prob_times_values (96x2048x2048x807): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x807): 64.128

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1400.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x808x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x808x2048): 79.946
Elapsed time for attention_prob_times_values (96x2048x2048x808): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x808): 85.581

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1648.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x809x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x809x2048): 74.408
Elapsed time for attention_prob_times_values (96x2048x2048x809): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x809): 63.999

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1373.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x810x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x810x2048): 75.136
Elapsed time for attention_prob_times_values (96x2048x2048x810): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x810): 65.380

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1397.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x811x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x811x2048): 77.305
Elapsed time for attention_prob_times_values (96x2048x2048x811): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x811): 64.159

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1402.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x812x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x812x2048): 79.140
Elapsed time for attention_prob_times_values (96x2048x2048x812): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x812): 64.537

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1424.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x813x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x813x2048): 77.623
Elapsed time for attention_prob_times_values (96x2048x2048x813): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x813): 64.377

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1411.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x814x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x814x2048): 78.551
Elapsed time for attention_prob_times_values (96x2048x2048x814): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x814): 80.061

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1592.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x815x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x815x2048): 78.032
Elapsed time for attention_prob_times_values (96x2048x2048x815): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x815): 77.149

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1559.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x816x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x816x2048): 80.309
Elapsed time for attention_prob_times_values (96x2048x2048x816): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x816): 86.407

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1675.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x817x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x817x2048): 77.512
Elapsed time for attention_prob_times_values (96x2048x2048x817): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x817): 78.040

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1567.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x818x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x818x2048): 76.911
Elapsed time for attention_prob_times_values (96x2048x2048x818): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x818): 80.414

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1585.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x819x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x819x2048): 76.055
Elapsed time for attention_prob_times_values (96x2048x2048x819): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x819): 77.997

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1555.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x820x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x820x2048): 79.266
Elapsed time for attention_prob_times_values (96x2048x2048x820): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x820): 80.590

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1615.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x821x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x821x2048): 79.951
Elapsed time for attention_prob_times_values (96x2048x2048x821): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x821): 78.166

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1600.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x822x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x822x2048): 78.476
Elapsed time for attention_prob_times_values (96x2048x2048x822): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x822): 80.711

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1612.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x823x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x823x2048): 77.751
Elapsed time for attention_prob_times_values (96x2048x2048x823): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x823): 78.303

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1583.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x824x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x824x2048): 79.589
Elapsed time for attention_prob_times_values (96x2048x2048x824): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x824): 87.147

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1689.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x825x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x825x2048): 76.741
Elapsed time for attention_prob_times_values (96x2048x2048x825): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x825): 78.613

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1579.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x826x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x826x2048): 77.640
Elapsed time for attention_prob_times_values (96x2048x2048x826): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x826): 81.083

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1614.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x827x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x827x2048): 70.833
Elapsed time for attention_prob_times_values (96x2048x2048x827): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x827): 78.810

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1520.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x828x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x828x2048): 74.534
Elapsed time for attention_prob_times_values (96x2048x2048x828): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x828): 74.725

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1522.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x829x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x829x2048): 77.074
Elapsed time for attention_prob_times_values (96x2048x2048x829): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x829): 78.981

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1593.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x830x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x830x2048): 73.217
Elapsed time for attention_prob_times_values (96x2048x2048x830): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x830): 74.648

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1512.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x831x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x831x2048): 75.148
Elapsed time for attention_prob_times_values (96x2048x2048x831): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x831): 72.071

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1506.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x832x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x832x2048): 90.961
Elapsed time for attention_prob_times_values (96x2048x2048x832): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x832): 88.971

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1844.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 19992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x833x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x833x2048): 75.112
Elapsed time for attention_prob_times_values (96x2048x2048x833): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x833): 76.455

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1555.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x834x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x834x2048): 79.660
Elapsed time for attention_prob_times_values (96x2048x2048x834): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x834): 81.852

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1658.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x835x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x835x2048): 78.716
Elapsed time for attention_prob_times_values (96x2048x2048x835): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x835): 79.624

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1628.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x836x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x836x2048): 77.994
Elapsed time for attention_prob_times_values (96x2048x2048x836): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x836): 78.632

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1612.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x837x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x837x2048): 75.709
Elapsed time for attention_prob_times_values (96x2048x2048x837): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x837): 79.820

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1602.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x838x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x838x2048): 79.471
Elapsed time for attention_prob_times_values (96x2048x2048x838): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x838): 82.211

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1668.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x839x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x839x2048): 76.592
Elapsed time for attention_prob_times_values (96x2048x2048x839): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x839): 76.862

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1585.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x840x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x840x2048): 80.378
Elapsed time for attention_prob_times_values (96x2048x2048x840): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x840): 85.713

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1716.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x841x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x841x2048): 77.571
Elapsed time for attention_prob_times_values (96x2048x2048x841): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x841): 78.115

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1612.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x842x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x842x2048): 78.510
Elapsed time for attention_prob_times_values (96x2048x2048x842): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x842): 80.544

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1648.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x843x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x843x2048): 77.911
Elapsed time for attention_prob_times_values (96x2048x2048x843): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x843): 80.410

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1642.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x844x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x844x2048): 79.524
Elapsed time for attention_prob_times_values (96x2048x2048x844): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x844): 82.852

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1686.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x845x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x845x2048): 77.948
Elapsed time for attention_prob_times_values (96x2048x2048x845): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x845): 80.564

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1648.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x846x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x846x2048): 78.090
Elapsed time for attention_prob_times_values (96x2048x2048x846): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x846): 82.808

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1674.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x847x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x847x2048): 78.005
Elapsed time for attention_prob_times_values (96x2048x2048x847): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x847): 77.797

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1624.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x848x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x848x2048): 79.734
Elapsed time for attention_prob_times_values (96x2048x2048x848): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x848): 89.613

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1761.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x849x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x849x2048): 77.327
Elapsed time for attention_prob_times_values (96x2048x2048x849): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x849): 79.859

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1642.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x850x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x850x2048): 78.196
Elapsed time for attention_prob_times_values (96x2048x2048x850): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x850): 83.245

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1687.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x851x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x851x2048): 75.080
Elapsed time for attention_prob_times_values (96x2048x2048x851): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x851): 81.210

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1634.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x852x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x852x2048): 79.252
Elapsed time for attention_prob_times_values (96x2048x2048x852): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x852): 83.550

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1705.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x853x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x853x2048): 74.789
Elapsed time for attention_prob_times_values (96x2048x2048x853): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x853): 81.290

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1635.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x854x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x854x2048): 76.094
Elapsed time for attention_prob_times_values (96x2048x2048x854): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x854): 83.552

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1673.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x855x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x855x2048): 75.535
Elapsed time for attention_prob_times_values (96x2048x2048x855): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x855): 81.387

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1648.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x856x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x856x2048): 79.758
Elapsed time for attention_prob_times_values (96x2048x2048x856): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x856): 90.300

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1784.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x857x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x857x2048): 74.742
Elapsed time for attention_prob_times_values (96x2048x2048x857): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x857): 77.058

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1600.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x858x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x858x2048): 77.997
Elapsed time for attention_prob_times_values (96x2048x2048x858): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x858): 76.917

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1634.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x859x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x859x2048): 77.374
Elapsed time for attention_prob_times_values (96x2048x2048x859): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x859): 81.777

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1680.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x860x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x860x2048): 79.041
Elapsed time for attention_prob_times_values (96x2048x2048x860): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x860): 79.222

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1674.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x861x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x861x2048): 73.579
Elapsed time for attention_prob_times_values (96x2048x2048x861): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x861): 81.882

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1641.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x862x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x862x2048): 78.193
Elapsed time for attention_prob_times_values (96x2048x2048x862): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x862): 84.201

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1719.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x863x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x863x2048): 72.028
Elapsed time for attention_prob_times_values (96x2048x2048x863): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x863): 82.051

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1628.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x864x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x864x2048): 83.976
Elapsed time for attention_prob_times_values (96x2048x2048x864): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x864): 91.766

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1863.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x865x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x865x2048): 79.138
Elapsed time for attention_prob_times_values (96x2048x2048x865): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x865): 82.232

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1715.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x866x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x866x2048): 79.918
Elapsed time for attention_prob_times_values (96x2048x2048x866): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x866): 84.406

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1748.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x867x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x867x2048): 78.849
Elapsed time for attention_prob_times_values (96x2048x2048x867): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x867): 82.320

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1717.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x868x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x868x2048): 78.730
Elapsed time for attention_prob_times_values (96x2048x2048x868): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x868): 84.755

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1742.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x869x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x869x2048): 78.688
Elapsed time for attention_prob_times_values (96x2048x2048x869): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x869): 82.441

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1720.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x870x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x870x2048): 79.323
Elapsed time for attention_prob_times_values (96x2048x2048x870): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x870): 84.676

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1752.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x871x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x871x2048): 78.440
Elapsed time for attention_prob_times_values (96x2048x2048x871): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x871): 82.593

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1723.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x872x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x872x2048): 77.577
Elapsed time for attention_prob_times_values (96x2048x2048x872): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x872): 91.767

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1802.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x873x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x873x2048): 77.521
Elapsed time for attention_prob_times_values (96x2048x2048x873): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x873): 82.637

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1716.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 20976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x874x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x874x2048): 78.614
Elapsed time for attention_prob_times_values (96x2048x2048x874): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x874): 85.144

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1756.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x875x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x875x2048): 76.553
Elapsed time for attention_prob_times_values (96x2048x2048x875): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x875): 82.775

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1710.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x876x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x876x2048): 79.350
Elapsed time for attention_prob_times_values (96x2048x2048x876): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x876): 85.373

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1770.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x877x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x877x2048): 78.046
Elapsed time for attention_prob_times_values (96x2048x2048x877): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x877): 82.950

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1733.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x878x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x878x2048): 78.898
Elapsed time for attention_prob_times_values (96x2048x2048x878): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x878): 85.194

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1767.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x879x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x879x2048): 77.939
Elapsed time for attention_prob_times_values (96x2048x2048x879): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x879): 83.093

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1737.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x880x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x880x2048): 80.515
Elapsed time for attention_prob_times_values (96x2048x2048x880): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x880): 92.858

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1865.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x881x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x881x2048): 77.653
Elapsed time for attention_prob_times_values (96x2048x2048x881): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x881): 83.266

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1739.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x882x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x882x2048): 78.530
Elapsed time for attention_prob_times_values (96x2048x2048x882): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x882): 85.617

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1775.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x883x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x883x2048): 77.750
Elapsed time for attention_prob_times_values (96x2048x2048x883): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x883): 83.495

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1746.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x884x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x884x2048): 76.291
Elapsed time for attention_prob_times_values (96x2048x2048x884): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x884): 82.734

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1724.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x885x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x885x2048): 77.773
Elapsed time for attention_prob_times_values (96x2048x2048x885): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x885): 83.616

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1752.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x886x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x886x2048): 78.378
Elapsed time for attention_prob_times_values (96x2048x2048x886): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x886): 86.020

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1785.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x887x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x887x2048): 77.900
Elapsed time for attention_prob_times_values (96x2048x2048x887): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x887): 83.531

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1756.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x888x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x888x2048): 79.733
Elapsed time for attention_prob_times_values (96x2048x2048x888): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x888): 93.445

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1876.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x889x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x889x2048): 77.635
Elapsed time for attention_prob_times_values (96x2048x2048x889): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x889): 83.480

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1756.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x890x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x890x2048): 78.234
Elapsed time for attention_prob_times_values (96x2048x2048x890): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x890): 86.543

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1796.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x891x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x891x2048): 77.599
Elapsed time for attention_prob_times_values (96x2048x2048x891): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x891): 84.131

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1766.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x892x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x892x2048): 78.872
Elapsed time for attention_prob_times_values (96x2048x2048x892): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x892): 86.747

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1809.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x893x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x893x2048): 77.036
Elapsed time for attention_prob_times_values (96x2048x2048x893): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x893): 84.315

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1765.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x894x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x894x2048): 78.360
Elapsed time for attention_prob_times_values (96x2048x2048x894): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x894): 86.784

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1807.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x895x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x895x2048): 77.506
Elapsed time for attention_prob_times_values (96x2048x2048x895): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x895): 84.320

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1775.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x896x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x896x2048): 88.018
Elapsed time for attention_prob_times_values (96x2048x2048x896): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x896): 95.585

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 2016.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x897x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x897x2048): 79.012
Elapsed time for attention_prob_times_values (96x2048x2048x897): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x897): 75.682

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1702.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x898x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x898x2048): 79.959
Elapsed time for attention_prob_times_values (96x2048x2048x898): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x898): 77.379

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1733.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x899x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x899x2048): 78.763
Elapsed time for attention_prob_times_values (96x2048x2048x899): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x899): 73.646

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1679.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x900x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x900x2048): 80.485
Elapsed time for attention_prob_times_values (96x2048x2048x900): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x900): 78.369

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1754.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x901x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x901x2048): 78.748
Elapsed time for attention_prob_times_values (96x2048x2048x901): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x901): 76.231

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1713.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x902x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x902x2048): 79.894
Elapsed time for attention_prob_times_values (96x2048x2048x902): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x902): 78.436

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1752.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x903x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x903x2048): 76.080
Elapsed time for attention_prob_times_values (96x2048x2048x903): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x903): 76.370

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1689.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x904x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x904x2048): 80.502
Elapsed time for attention_prob_times_values (96x2048x2048x904): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x904): 82.967

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1813.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x905x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x905x2048): 78.596
Elapsed time for attention_prob_times_values (96x2048x2048x905): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x905): 76.597

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1723.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x906x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x906x2048): 79.558
Elapsed time for attention_prob_times_values (96x2048x2048x906): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x906): 73.321

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1696.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x907x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x907x2048): 78.919
Elapsed time for attention_prob_times_values (96x2048x2048x907): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x907): 75.552

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1718.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x908x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x908x2048): 80.042
Elapsed time for attention_prob_times_values (96x2048x2048x908): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x908): 78.905

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1770.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x909x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x909x2048): 78.967
Elapsed time for attention_prob_times_values (96x2048x2048x909): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x909): 76.002

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1727.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x910x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x910x2048): 78.531
Elapsed time for attention_prob_times_values (96x2048x2048x910): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x910): 78.882

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1757.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x911x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x911x2048): 78.938
Elapsed time for attention_prob_times_values (96x2048x2048x911): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x911): 77.027

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1742.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x912x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x912x2048): 78.107
Elapsed time for attention_prob_times_values (96x2048x2048x912): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x912): 85.277

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1824.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x913x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x913x2048): 71.212
Elapsed time for attention_prob_times_values (96x2048x2048x913): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x913): 70.456

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1586.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x914x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x914x2048): 79.563
Elapsed time for attention_prob_times_values (96x2048x2048x914): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x914): 79.347

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1781.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x915x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x915x2048): 78.445
Elapsed time for attention_prob_times_values (96x2048x2048x915): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x915): 77.395

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1748.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x916x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x916x2048): 79.805
Elapsed time for attention_prob_times_values (96x2048x2048x916): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x916): 79.455

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1789.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x917x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x917x2048): 78.569
Elapsed time for attention_prob_times_values (96x2048x2048x917): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x917): 77.486

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1754.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x918x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x918x2048): 79.331
Elapsed time for attention_prob_times_values (96x2048x2048x918): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x918): 79.598

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1789.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x919x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x919x2048): 77.636
Elapsed time for attention_prob_times_values (96x2048x2048x919): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x919): 77.698

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1750.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x920x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x920x2048): 79.376
Elapsed time for attention_prob_times_values (96x2048x2048x920): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x920): 85.540

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1857.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x921x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x921x2048): 78.217
Elapsed time for attention_prob_times_values (96x2048x2048x921): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x921): 77.828

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1762.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x922x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x922x2048): 78.828
Elapsed time for attention_prob_times_values (96x2048x2048x922): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x922): 79.457

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1789.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x923x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x923x2048): 78.240
Elapsed time for attention_prob_times_values (96x2048x2048x923): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x923): 77.989

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1767.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x924x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x924x2048): 79.571
Elapsed time for attention_prob_times_values (96x2048x2048x924): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x924): 80.178

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1809.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x925x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x925x2048): 78.388
Elapsed time for attention_prob_times_values (96x2048x2048x925): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x925): 71.856

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1700.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x926x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x926x2048): 74.852
Elapsed time for attention_prob_times_values (96x2048x2048x926): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x926): 80.164

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1757.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x927x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x927x2048): 77.488
Elapsed time for attention_prob_times_values (96x2048x2048x927): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x927): 78.264

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1769.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x928x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x928x2048): 85.642
Elapsed time for attention_prob_times_values (96x2048x2048x928): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x928): 86.951

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1963.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x929x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x929x2048): 73.888
Elapsed time for attention_prob_times_values (96x2048x2048x929): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x929): 78.467

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1733.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x930x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x930x2048): 81.188
Elapsed time for attention_prob_times_values (96x2048x2048x930): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x930): 79.624

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1832.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x931x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x931x2048): 74.924
Elapsed time for attention_prob_times_values (96x2048x2048x931): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x931): 75.592

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1717.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x932x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x932x2048): 81.903
Elapsed time for attention_prob_times_values (96x2048x2048x932): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x932): 80.735

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1857.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x933x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x933x2048): 80.021
Elapsed time for attention_prob_times_values (96x2048x2048x933): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x933): 78.481

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1812.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x934x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x934x2048): 78.450
Elapsed time for attention_prob_times_values (96x2048x2048x934): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x934): 80.573

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1819.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x935x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x935x2048): 79.769
Elapsed time for attention_prob_times_values (96x2048x2048x935): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x935): 78.526

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1813.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x936x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x936x2048): 81.820
Elapsed time for attention_prob_times_values (96x2048x2048x936): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x936): 86.934

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1933.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x937x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x937x2048): 79.125
Elapsed time for attention_prob_times_values (96x2048x2048x937): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x937): 76.427

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1785.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x938x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x938x2048): 79.000
Elapsed time for attention_prob_times_values (96x2048x2048x938): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x938): 80.970

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1838.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x939x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x939x2048): 79.254
Elapsed time for attention_prob_times_values (96x2048x2048x939): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x939): 78.970

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1820.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x940x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x940x2048): 81.209
Elapsed time for attention_prob_times_values (96x2048x2048x940): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x940): 81.257

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1870.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x941x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x941x2048): 76.929
Elapsed time for attention_prob_times_values (96x2048x2048x941): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x941): 78.945

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1796.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x942x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x942x2048): 80.356
Elapsed time for attention_prob_times_values (96x2048x2048x942): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x942): 81.280

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1865.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x943x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x943x2048): 79.578
Elapsed time for attention_prob_times_values (96x2048x2048x943): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x943): 79.160

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1833.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x944x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x944x2048): 81.651
Elapsed time for attention_prob_times_values (96x2048x2048x944): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x944): 85.162

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1927.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x945x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x945x2048): 77.776
Elapsed time for attention_prob_times_values (96x2048x2048x945): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x945): 79.334

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1818.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x946x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x946x2048): 80.073
Elapsed time for attention_prob_times_values (96x2048x2048x946): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x946): 81.623

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1873.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x947x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x947x2048): 79.289
Elapsed time for attention_prob_times_values (96x2048x2048x947): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x947): 79.498

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1841.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x948x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x948x2048): 78.619
Elapsed time for attention_prob_times_values (96x2048x2048x948): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x948): 81.792

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1861.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x949x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x949x2048): 79.486
Elapsed time for attention_prob_times_values (96x2048x2048x949): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x949): 79.545

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1848.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x950x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x950x2048): 80.184
Elapsed time for attention_prob_times_values (96x2048x2048x950): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x950): 81.853

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1884.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x951x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x951x2048): 79.516
Elapsed time for attention_prob_times_values (96x2048x2048x951): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x951): 79.587

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1852.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x952x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x952x2048): 81.207
Elapsed time for attention_prob_times_values (96x2048x2048x952): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x952): 88.359

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1972.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x953x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x953x2048): 79.018
Elapsed time for attention_prob_times_values (96x2048x2048x953): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x953): 79.912

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1854.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x954x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x954x2048): 79.796
Elapsed time for attention_prob_times_values (96x2048x2048x954): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x954): 82.242

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1892.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x955x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x955x2048): 78.856
Elapsed time for attention_prob_times_values (96x2048x2048x955): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x955): 79.848

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1855.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x956x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x956x2048): 80.618
Elapsed time for attention_prob_times_values (96x2048x2048x956): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x956): 82.398

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1907.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x957x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x957x2048): 79.190
Elapsed time for attention_prob_times_values (96x2048x2048x957): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x957): 80.182

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1866.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 22992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x958x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x958x2048): 80.063
Elapsed time for attention_prob_times_values (96x2048x2048x958): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x958): 82.305

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1903.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x959x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x959x2048): 79.330
Elapsed time for attention_prob_times_values (96x2048x2048x959): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x959): 80.405

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1874.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x960x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x960x2048): 92.279
Elapsed time for attention_prob_times_values (96x2048x2048x960): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x960): 90.192

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2143.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x961x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x961x2048): 80.939
Elapsed time for attention_prob_times_values (96x2048x2048x961): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x961): 80.396

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1897.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x962x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x962x2048): 81.376
Elapsed time for attention_prob_times_values (96x2048x2048x962): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x962): 82.706

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1931.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x963x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x963x2048): 80.749
Elapsed time for attention_prob_times_values (96x2048x2048x963): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x963): 78.162

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1872.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x964x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x964x2048): 80.364
Elapsed time for attention_prob_times_values (96x2048x2048x964): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x964): 82.928

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1925.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x965x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x965x2048): 80.600
Elapsed time for attention_prob_times_values (96x2048x2048x965): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x965): 80.271

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1899.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x966x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x966x2048): 81.338
Elapsed time for attention_prob_times_values (96x2048x2048x966): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x966): 82.932

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1941.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x967x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x967x2048): 80.493
Elapsed time for attention_prob_times_values (96x2048x2048x967): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x967): 80.751

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1907.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x968x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x968x2048): 82.203
Elapsed time for attention_prob_times_values (96x2048x2048x968): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x968): 89.918

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2034.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x969x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x969x2048): 79.322
Elapsed time for attention_prob_times_values (96x2048x2048x969): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x969): 80.983

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1900.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x970x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x970x2048): 80.554
Elapsed time for attention_prob_times_values (96x2048x2048x970): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x970): 83.334

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1944.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x971x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x971x2048): 79.852
Elapsed time for attention_prob_times_values (96x2048x2048x971): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x971): 81.264

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1913.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x972x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x972x2048): 81.501
Elapsed time for attention_prob_times_values (96x2048x2048x972): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x972): 82.470

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1949.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x973x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x973x2048): 77.894
Elapsed time for attention_prob_times_values (96x2048x2048x973): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x973): 81.428

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1895.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x974x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x974x2048): 79.082
Elapsed time for attention_prob_times_values (96x2048x2048x974): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x974): 83.548

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1936.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x975x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x975x2048): 79.984
Elapsed time for attention_prob_times_values (96x2048x2048x975): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x975): 75.754

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1855.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x976x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x976x2048): 77.506
Elapsed time for attention_prob_times_values (96x2048x2048x976): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x976): 90.982

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1998.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x977x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x977x2048): 79.352
Elapsed time for attention_prob_times_values (96x2048x2048x977): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x977): 78.188

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1882.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x978x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x978x2048): 80.281
Elapsed time for attention_prob_times_values (96x2048x2048x978): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x978): 83.932

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1963.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x979x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x979x2048): 79.047
Elapsed time for attention_prob_times_values (96x2048x2048x979): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x979): 81.903

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1926.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x980x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x980x2048): 81.129
Elapsed time for attention_prob_times_values (96x2048x2048x980): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x980): 84.018

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1978.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x981x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x981x2048): 79.616
Elapsed time for attention_prob_times_values (96x2048x2048x981): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x981): 81.959

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1937.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x982x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x982x2048): 80.427
Elapsed time for attention_prob_times_values (96x2048x2048x982): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x982): 84.083

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1974.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x983x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x983x2048): 79.654
Elapsed time for attention_prob_times_values (96x2048x2048x983): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x983): 82.094

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1943.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x984x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x984x2048): 81.715
Elapsed time for attention_prob_times_values (96x2048x2048x984): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x984): 91.124

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2073.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x985x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x985x2048): 74.557
Elapsed time for attention_prob_times_values (96x2048x2048x985): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x985): 82.106

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1882.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x986x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x986x2048): 76.075
Elapsed time for attention_prob_times_values (96x2048x2048x986): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x986): 84.244

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1927.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x987x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x987x2048): 79.219
Elapsed time for attention_prob_times_values (96x2048x2048x987): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x987): 82.267

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1947.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x988x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x988x2048): 80.907
Elapsed time for attention_prob_times_values (96x2048x2048x988): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x988): 84.206

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1993.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x989x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x989x2048): 78.891
Elapsed time for attention_prob_times_values (96x2048x2048x989): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x989): 82.470

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1949.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x990x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x990x2048): 80.127
Elapsed time for attention_prob_times_values (96x2048x2048x990): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x990): 84.680

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1992.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x991x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x991x2048): 79.269
Elapsed time for attention_prob_times_values (96x2048x2048x991): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x991): 82.647

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1960.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x992x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x992x2048): 92.468
Elapsed time for attention_prob_times_values (96x2048x2048x992): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x992): 92.547

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 2243.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x993x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x993x2048): 80.963
Elapsed time for attention_prob_times_values (96x2048x2048x993): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x993): 82.728

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1986.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x994x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x994x2048): 81.723
Elapsed time for attention_prob_times_values (96x2048x2048x994): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x994): 81.863

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1987.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x995x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x995x2048): 80.680
Elapsed time for attention_prob_times_values (96x2048x2048x995): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x995): 82.924

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1989.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x996x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x996x2048): 82.314
Elapsed time for attention_prob_times_values (96x2048x2048x996): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x996): 85.204

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2038.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x997x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x997x2048): 78.143
Elapsed time for attention_prob_times_values (96x2048x2048x997): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x997): 82.947

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1960.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x998x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x998x2048): 81.288
Elapsed time for attention_prob_times_values (96x2048x2048x998): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x998): 85.198

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2029.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 23976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x999x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x999x2048): 80.266
Elapsed time for attention_prob_times_values (96x2048x2048x999): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x999): 76.665

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1914.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1000x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1000x2048): 82.145
Elapsed time for attention_prob_times_values (96x2048x2048x1000): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1000): 92.527

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2126.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1001x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1001x2048): 79.641
Elapsed time for attention_prob_times_values (96x2048x2048x1001): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1001): 83.085

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1989.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1002x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1002x2048): 80.519
Elapsed time for attention_prob_times_values (96x2048x2048x1002): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1002): 85.475

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2030.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1003x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1003x2048): 79.697
Elapsed time for attention_prob_times_values (96x2048x2048x1003): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1003): 83.186

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1995.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1004x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1004x2048): 81.616
Elapsed time for attention_prob_times_values (96x2048x2048x1004): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1004): 85.715

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2051.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1005x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1005x2048): 79.955
Elapsed time for attention_prob_times_values (96x2048x2048x1005): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1005): 83.303

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2003.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1006x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1006x2048): 80.760
Elapsed time for attention_prob_times_values (96x2048x2048x1006): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1006): 85.812

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2045.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1007x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1007x2048): 79.975
Elapsed time for attention_prob_times_values (96x2048x2048x1007): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1007): 83.358

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2008.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1008x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1008x2048): 82.407
Elapsed time for attention_prob_times_values (96x2048x2048x1008): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1008): 93.696

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2159.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1009x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1009x2048): 79.349
Elapsed time for attention_prob_times_values (96x2048x2048x1009): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1009): 83.602

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2006.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1010x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1010x2048): 80.266
Elapsed time for attention_prob_times_values (96x2048x2048x1010): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1010): 86.007

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2048.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1011x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1011x2048): 79.580
Elapsed time for attention_prob_times_values (96x2048x2048x1011): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1011): 83.665

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2014.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1012x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1012x2048): 81.520
Elapsed time for attention_prob_times_values (96x2048x2048x1012): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1012): 86.199

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2071.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1013x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1013x2048): 79.695
Elapsed time for attention_prob_times_values (96x2048x2048x1013): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1013): 83.607

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2019.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1014x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1014x2048): 80.369
Elapsed time for attention_prob_times_values (96x2048x2048x1014): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1014): 86.230

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2060.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1015x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1015x2048): 79.609
Elapsed time for attention_prob_times_values (96x2048x2048x1015): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1015): 83.723

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2023.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1016x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1016x2048): 81.650
Elapsed time for attention_prob_times_values (96x2048x2048x1016): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1016): 93.899

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2167.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1017x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1017x2048): 79.278
Elapsed time for attention_prob_times_values (96x2048x2048x1017): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1017): 83.921

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2024.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1018x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1018x2048): 80.215
Elapsed time for attention_prob_times_values (96x2048x2048x1018): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1018): 86.490

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2069.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1019x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1019x2048): 80.068
Elapsed time for attention_prob_times_values (96x2048x2048x1019): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1019): 83.948

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2039.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1020x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1020x2048): 84.717
Elapsed time for attention_prob_times_values (96x2048x2048x1020): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1020): 86.198

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2128.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1021x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1021x2048): 79.564
Elapsed time for attention_prob_times_values (96x2048x2048x1021): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1021): 79.794

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1986.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1022x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1022x2048): 80.309
Elapsed time for attention_prob_times_values (96x2048x2048x1022): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1022): 83.097

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2038.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1023x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1023x2048): 79.478
Elapsed time for attention_prob_times_values (96x2048x2048x1023): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1023): 83.419

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2033.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1024x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1024x2048): 90.767
Elapsed time for attention_prob_times_values (96x2048x2048x1024): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1024): 96.296

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 2336.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1025x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1025x2048): 81.033
Elapsed time for attention_prob_times_values (96x2048x2048x1025): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1025): 76.647

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1971.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1026x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1026x2048): 82.036
Elapsed time for attention_prob_times_values (96x2048x2048x1026): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1026): 79.123

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2017.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1027x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1027x2048): 77.990
Elapsed time for attention_prob_times_values (96x2048x2048x1027): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1027): 76.868

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1941.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1028x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1028x2048): 84.599
Elapsed time for attention_prob_times_values (96x2048x2048x1028): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1028): 79.203

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2052.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1029x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1029x2048): 78.370
Elapsed time for attention_prob_times_values (96x2048x2048x1029): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1029): 76.700

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1947.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1030x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1030x2048): 78.260
Elapsed time for attention_prob_times_values (96x2048x2048x1030): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1030): 79.240

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1979.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1031x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1031x2048): 75.792
Elapsed time for attention_prob_times_values (96x2048x2048x1031): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1031): 77.244

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1925.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1032x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1032x2048): 78.045
Elapsed time for attention_prob_times_values (96x2048x2048x1032): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1032): 84.680

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2045.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1033x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1033x2048): 75.510
Elapsed time for attention_prob_times_values (96x2048x2048x1033): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1033): 73.726

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1880.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1034x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1034x2048): 77.047
Elapsed time for attention_prob_times_values (96x2048x2048x1034): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1034): 79.713

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1977.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1035x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1035x2048): 79.896
Elapsed time for attention_prob_times_values (96x2048x2048x1035): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1035): 77.666

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1989.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1036x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1036x2048): 81.920
Elapsed time for attention_prob_times_values (96x2048x2048x1036): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1036): 80.066

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2047.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1037x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1037x2048): 76.704
Elapsed time for attention_prob_times_values (96x2048x2048x1037): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1037): 76.596

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1939.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1038x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1038x2048): 80.977
Elapsed time for attention_prob_times_values (96x2048x2048x1038): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1038): 80.133

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2040.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1039x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1039x2048): 80.263
Elapsed time for attention_prob_times_values (96x2048x2048x1039): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1039): 78.003

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2005.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1040x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1040x2048): 82.518
Elapsed time for attention_prob_times_values (96x2048x2048x1040): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1040): 84.167

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2114.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 24984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1041x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1041x2048): 79.462
Elapsed time for attention_prob_times_values (96x2048x2048x1041): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1041): 78.235

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2002.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1042x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1042x2048): 80.539
Elapsed time for attention_prob_times_values (96x2048x2048x1042): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1042): 80.442

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2046.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1043x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1043x2048): 79.893
Elapsed time for attention_prob_times_values (96x2048x2048x1043): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1043): 78.319

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2012.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1044x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1044x2048): 80.605
Elapsed time for attention_prob_times_values (96x2048x2048x1044): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1044): 80.648

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2053.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1045x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1045x2048): 80.259
Elapsed time for attention_prob_times_values (96x2048x2048x1045): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1045): 78.506

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2023.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1046x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1046x2048): 80.839
Elapsed time for attention_prob_times_values (96x2048x2048x1046): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1046): 80.682

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2060.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1047x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1047x2048): 80.173
Elapsed time for attention_prob_times_values (96x2048x2048x1047): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1047): 77.675

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2015.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1048x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1048x2048): 81.848
Elapsed time for attention_prob_times_values (96x2048x2048x1048): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1048): 85.764

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2141.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1049x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1049x2048): 79.705
Elapsed time for attention_prob_times_values (96x2048x2048x1049): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1049): 78.579

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2024.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1050x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1050x2048): 80.454
Elapsed time for attention_prob_times_values (96x2048x2048x1050): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1050): 80.981

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2067.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1051x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1051x2048): 78.433
Elapsed time for attention_prob_times_values (96x2048x2048x1051): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1051): 78.946

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2017.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1052x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1052x2048): 81.281
Elapsed time for attention_prob_times_values (96x2048x2048x1052): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1052): 81.164

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2083.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1053x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1053x2048): 78.671
Elapsed time for attention_prob_times_values (96x2048x2048x1053): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1053): 79.054

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2025.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1054x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1054x2048): 79.458
Elapsed time for attention_prob_times_values (96x2048x2048x1054): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1054): 81.284

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2065.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1055x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1055x2048): 78.499
Elapsed time for attention_prob_times_values (96x2048x2048x1055): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1055): 79.225

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2028.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1056x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1056x2048): 90.396
Elapsed time for attention_prob_times_values (96x2048x2048x1056): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1056): 87.356

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2287.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1057x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1057x2048): 78.562
Elapsed time for attention_prob_times_values (96x2048x2048x1057): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1057): 79.368

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2035.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1058x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1058x2048): 82.472
Elapsed time for attention_prob_times_values (96x2048x2048x1058): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1058): 81.563

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2115.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1059x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1059x2048): 81.484
Elapsed time for attention_prob_times_values (96x2048x2048x1059): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1059): 79.434

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2077.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1060x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1060x2048): 83.206
Elapsed time for attention_prob_times_values (96x2048x2048x1060): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1060): 79.370

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2099.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1061x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1061x2048): 80.735
Elapsed time for attention_prob_times_values (96x2048x2048x1061): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1061): 79.309

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2069.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1062x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1062x2048): 82.217
Elapsed time for attention_prob_times_values (96x2048x2048x1062): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1062): 81.696

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2121.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1063x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1063x2048): 81.229
Elapsed time for attention_prob_times_values (96x2048x2048x1063): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1063): 79.624

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2083.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1064x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1064x2048): 83.105
Elapsed time for attention_prob_times_values (96x2048x2048x1064): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1064): 86.991

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2204.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1065x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1065x2048): 80.643
Elapsed time for attention_prob_times_values (96x2048x2048x1065): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1065): 79.735

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2081.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1066x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1066x2048): 81.363
Elapsed time for attention_prob_times_values (96x2048x2048x1066): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1066): 81.634

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2117.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1067x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1067x2048): 80.873
Elapsed time for attention_prob_times_values (96x2048x2048x1067): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1067): 79.800

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2089.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1068x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1068x2048): 82.430
Elapsed time for attention_prob_times_values (96x2048x2048x1068): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1068): 82.225

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2143.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1069x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1069x2048): 80.493
Elapsed time for attention_prob_times_values (96x2048x2048x1069): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1069): 79.928

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2089.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1070x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1070x2048): 80.434
Elapsed time for attention_prob_times_values (96x2048x2048x1070): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1070): 75.342

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2028.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1071x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1071x2048): 75.093
Elapsed time for attention_prob_times_values (96x2048x2048x1071): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1071): 79.988

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2021.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1072x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1072x2048): 82.885
Elapsed time for attention_prob_times_values (96x2048x2048x1072): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1072): 88.078

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2231.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1073x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1073x2048): 80.561
Elapsed time for attention_prob_times_values (96x2048x2048x1073): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1073): 80.281

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2102.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1074x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1074x2048): 79.543
Elapsed time for attention_prob_times_values (96x2048x2048x1074): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1074): 82.559

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2120.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1075x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1075x2048): 80.698
Elapsed time for attention_prob_times_values (96x2048x2048x1075): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1075): 80.375

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2109.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1076x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1076x2048): 82.396
Elapsed time for attention_prob_times_values (96x2048x2048x1076): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1076): 82.621

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2163.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1077x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1077x2048): 80.969
Elapsed time for attention_prob_times_values (96x2048x2048x1077): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1077): 79.530

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2105.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1078x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1078x2048): 81.651
Elapsed time for attention_prob_times_values (96x2048x2048x1078): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1078): 82.789

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2159.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1079x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1079x2048): 80.936
Elapsed time for attention_prob_times_values (96x2048x2048x1079): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1079): 78.917

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2100.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1080x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1080x2048): 82.853
Elapsed time for attention_prob_times_values (96x2048x2048x1080): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1080): 88.238

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2248.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1081x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1081x2048): 79.780
Elapsed time for attention_prob_times_values (96x2048x2048x1081): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1081): 80.724

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2113.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1082x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1082x2048): 80.310
Elapsed time for attention_prob_times_values (96x2048x2048x1082): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1082): 83.076

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2152.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 25992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1083x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1083x2048): 78.252
Elapsed time for attention_prob_times_values (96x2048x2048x1083): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1083): 80.861

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2098.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1084x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1084x2048): 80.150
Elapsed time for attention_prob_times_values (96x2048x2048x1084): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1084): 83.246

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2156.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1085x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1085x2048): 80.710
Elapsed time for attention_prob_times_values (96x2048x2048x1085): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1085): 80.942

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2136.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1086x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1086x2048): 81.425
Elapsed time for attention_prob_times_values (96x2048x2048x1086): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1086): 83.213

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2177.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1087x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1087x2048): 80.823
Elapsed time for attention_prob_times_values (96x2048x2048x1087): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1087): 80.891

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2140.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1088x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1088x2048): 93.230
Elapsed time for attention_prob_times_values (96x2048x2048x1088): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1088): 87.945

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2398.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1089x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1089x2048): 82.133
Elapsed time for attention_prob_times_values (96x2048x2048x1089): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1089): 81.115

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2164.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1090x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1090x2048): 82.970
Elapsed time for attention_prob_times_values (96x2048x2048x1090): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1090): 83.662

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2211.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1091x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1091x2048): 81.078
Elapsed time for attention_prob_times_values (96x2048x2048x1091): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1091): 81.330

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2157.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1092x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1092x2048): 83.394
Elapsed time for attention_prob_times_values (96x2048x2048x1092): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1092): 83.786

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2222.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1093x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1093x2048): 81.388
Elapsed time for attention_prob_times_values (96x2048x2048x1093): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1093): 81.391

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2166.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1094x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1094x2048): 82.602
Elapsed time for attention_prob_times_values (96x2048x2048x1094): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1094): 83.700

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2215.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1095x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1095x2048): 81.682
Elapsed time for attention_prob_times_values (96x2048x2048x1095): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1095): 81.459

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2175.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1096x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1096x2048): 83.777
Elapsed time for attention_prob_times_values (96x2048x2048x1096): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1096): 89.335

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2307.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1097x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1097x2048): 81.113
Elapsed time for attention_prob_times_values (96x2048x2048x1097): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1097): 78.033

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2124.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1098x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1098x2048): 81.881
Elapsed time for attention_prob_times_values (96x2048x2048x1098): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1098): 84.100

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2218.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1099x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1099x2048): 81.318
Elapsed time for attention_prob_times_values (96x2048x2048x1099): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1099): 81.849

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2182.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1100x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1100x2048): 82.916
Elapsed time for attention_prob_times_values (96x2048x2048x1100): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1100): 84.293

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2238.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1101x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1101x2048): 80.787
Elapsed time for attention_prob_times_values (96x2048x2048x1101): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1101): 82.034

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2182.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1102x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1102x2048): 77.622
Elapsed time for attention_prob_times_values (96x2048x2048x1102): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1102): 84.333

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2168.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1103x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1103x2048): 81.297
Elapsed time for attention_prob_times_values (96x2048x2048x1103): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1103): 82.268

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2195.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1104x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1104x2048): 83.538
Elapsed time for attention_prob_times_values (96x2048x2048x1104): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1104): 90.035

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2329.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1105x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1105x2048): 78.892
Elapsed time for attention_prob_times_values (96x2048x2048x1105): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1105): 81.940

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2162.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1106x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1106x2048): 81.818
Elapsed time for attention_prob_times_values (96x2048x2048x1106): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1106): 84.338

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2236.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1107x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1107x2048): 80.974
Elapsed time for attention_prob_times_values (96x2048x2048x1107): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1107): 78.120

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2142.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1108x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1108x2048): 82.692
Elapsed time for attention_prob_times_values (96x2048x2048x1108): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1108): 84.612

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2255.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1109x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1109x2048): 81.130
Elapsed time for attention_prob_times_values (96x2048x2048x1109): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1109): 82.665

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2210.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1110x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1110x2048): 81.904
Elapsed time for attention_prob_times_values (96x2048x2048x1110): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1110): 84.850

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2251.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1111x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1111x2048): 76.999
Elapsed time for attention_prob_times_values (96x2048x2048x1111): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1111): 82.704

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2156.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1112x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1112x2048): 83.398
Elapsed time for attention_prob_times_values (96x2048x2048x1112): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1112): 90.940

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2354.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1113x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1113x2048): 80.757
Elapsed time for attention_prob_times_values (96x2048x2048x1113): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1113): 82.844

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2215.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1114x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1114x2048): 81.597
Elapsed time for attention_prob_times_values (96x2048x2048x1114): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1114): 84.827

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2254.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1115x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1115x2048): 80.884
Elapsed time for attention_prob_times_values (96x2048x2048x1115): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1115): 82.955

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2222.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1116x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1116x2048): 82.699
Elapsed time for attention_prob_times_values (96x2048x2048x1116): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1116): 85.135

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2278.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1117x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1117x2048): 80.757
Elapsed time for attention_prob_times_values (96x2048x2048x1117): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1117): 83.120

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2226.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1118x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1118x2048): 81.780
Elapsed time for attention_prob_times_values (96x2048x2048x1118): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1118): 85.068

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2268.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1119x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1119x2048): 78.998
Elapsed time for attention_prob_times_values (96x2048x2048x1119): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1119): 83.289

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2207.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1120x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1120x2048): 88.477
Elapsed time for attention_prob_times_values (96x2048x2048x1120): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1120): 92.307

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2462.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1121x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1121x2048): 82.345
Elapsed time for attention_prob_times_values (96x2048x2048x1121): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1121): 83.190

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2257.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1122x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1122x2048): 83.200
Elapsed time for attention_prob_times_values (96x2048x2048x1122): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1122): 85.492

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2301.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1123x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1123x2048): 82.178
Elapsed time for attention_prob_times_values (96x2048x2048x1123): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1123): 83.392

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2261.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1124x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1124x2048): 83.766
Elapsed time for attention_prob_times_values (96x2048x2048x1124): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1124): 85.448

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2313.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1125x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1125x2048): 80.760
Elapsed time for attention_prob_times_values (96x2048x2048x1125): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1125): 82.370

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2231.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1126x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1126x2048): 82.713
Elapsed time for attention_prob_times_values (96x2048x2048x1126): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1126): 85.650

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2305.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1127x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1127x2048): 81.630
Elapsed time for attention_prob_times_values (96x2048x2048x1127): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1127): 83.472

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2262.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1128x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1128x2048): 83.559
Elapsed time for attention_prob_times_values (96x2048x2048x1128): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1128): 92.130

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2404.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1129x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1129x2048): 76.826
Elapsed time for attention_prob_times_values (96x2048x2048x1129): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1129): 82.599

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2186.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1130x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1130x2048): 81.918
Elapsed time for attention_prob_times_values (96x2048x2048x1130): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1130): 83.889

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2278.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1131x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1131x2048): 81.369
Elapsed time for attention_prob_times_values (96x2048x2048x1131): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1131): 80.820

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2230.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1132x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1132x2048): 82.579
Elapsed time for attention_prob_times_values (96x2048x2048x1132): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1132): 86.099

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2320.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1133x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1133x2048): 81.174
Elapsed time for attention_prob_times_values (96x2048x2048x1133): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1133): 83.779

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2272.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1134x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1134x2048): 82.328
Elapsed time for attention_prob_times_values (96x2048x2048x1134): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1134): 86.171

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2322.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1135x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1135x2048): 77.286
Elapsed time for attention_prob_times_values (96x2048x2048x1135): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1135): 83.839

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2219.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1136x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1136x2048): 83.841
Elapsed time for attention_prob_times_values (96x2048x2048x1136): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1136): 92.945

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2435.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1137x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1137x2048): 80.914
Elapsed time for attention_prob_times_values (96x2048x2048x1137): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1137): 83.966

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2278.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1138x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1138x2048): 81.860
Elapsed time for attention_prob_times_values (96x2048x2048x1138): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1138): 86.128

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2322.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1139x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1139x2048): 80.989
Elapsed time for attention_prob_times_values (96x2048x2048x1139): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1139): 84.141

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2285.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1140x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1140x2048): 82.468
Elapsed time for attention_prob_times_values (96x2048x2048x1140): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1140): 86.594

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2341.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1141x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1141x2048): 80.757
Elapsed time for attention_prob_times_values (96x2048x2048x1141): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1141): 84.182

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2286.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1142x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1142x2048): 79.336
Elapsed time for attention_prob_times_values (96x2048x2048x1142): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1142): 86.617

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2299.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1143x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1143x2048): 81.220
Elapsed time for attention_prob_times_values (96x2048x2048x1143): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1143): 84.293

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2298.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1144x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1144x2048): 83.133
Elapsed time for attention_prob_times_values (96x2048x2048x1144): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1144): 89.558

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2398.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1145x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1145x2048): 80.842
Elapsed time for attention_prob_times_values (96x2048x2048x1145): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1145): 84.428

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2299.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1146x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1146x2048): 81.569
Elapsed time for attention_prob_times_values (96x2048x2048x1146): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1146): 86.950

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2345.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1147x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1147x2048): 77.213
Elapsed time for attention_prob_times_values (96x2048x2048x1147): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1147): 84.608

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2251.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1148x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1148x2048): 82.215
Elapsed time for attention_prob_times_values (96x2048x2048x1148): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1148): 87.123

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2360.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1149x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1149x2048): 80.380
Elapsed time for attention_prob_times_values (96x2048x2048x1149): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1149): 84.645

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2303.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1150x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1150x2048): 81.484
Elapsed time for attention_prob_times_values (96x2048x2048x1150): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1150): 83.651

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2307.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1151x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1151x2048): 80.694
Elapsed time for attention_prob_times_values (96x2048x2048x1151): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1151): 84.800

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2313.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1152x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1152x2048): 88.610
Elapsed time for attention_prob_times_values (96x2048x2048x1152): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1152): 93.334

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2545.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1153x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1153x2048): 79.808
Elapsed time for attention_prob_times_values (96x2048x2048x1153): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1153): 77.971

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2210.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1154x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1154x2048): 83.179
Elapsed time for attention_prob_times_values (96x2048x2048x1154): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1154): 80.506

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2294.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1155x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1155x2048): 81.748
Elapsed time for attention_prob_times_values (96x2048x2048x1155): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1155): 76.651

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2220.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1156x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1156x2048): 83.857
Elapsed time for attention_prob_times_values (96x2048x2048x1156): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1156): 80.614

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2309.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1157x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1157x2048): 82.263
Elapsed time for attention_prob_times_values (96x2048x2048x1157): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1157): 75.851

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2219.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1158x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1158x2048): 83.009
Elapsed time for attention_prob_times_values (96x2048x2048x1158): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1158): 80.637

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2302.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1159x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1159x2048): 81.111
Elapsed time for attention_prob_times_values (96x2048x2048x1159): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1159): 78.272

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2243.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1160x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1160x2048): 81.368
Elapsed time for attention_prob_times_values (96x2048x2048x1160): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1160): 85.637

Attention duration (in seconds): 0.0224
========================================================================================================================[2023-11-24 14:33:52,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
num_attention_heads: 24, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1160x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1160x2048): 83.985
Elapsed time for attention_prob_times_values (96x2048x2048x1160): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1160): 88.481

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2429.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1161x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1161x2048): 84.057
Elapsed time for attention_prob_times_values (96x2048x2048x1161): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1161): 81.560

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2335.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1162x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1162x2048): 84.923
Elapsed time for attention_prob_times_values (96x2048x2048x1162): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1162): 83.668

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2379.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1163x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1163x2048): 84.477
Elapsed time for attention_prob_times_values (96x2048x2048x1163): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1163): 81.665

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2346.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1164x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1164x2048): 85.816
Elapsed time for attention_prob_times_values (96x2048x2048x1164): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1164): 83.412

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2392.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1165x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1165x2048): 84.040
Elapsed time for attention_prob_times_values (96x2048x2048x1165): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1165): 81.692

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2345.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1166x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1166x2048): 79.659
Elapsed time for attention_prob_times_values (96x2048x2048x1166): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1166): 84.165

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2318.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1167x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1167x2048): 83.978
Elapsed time for attention_prob_times_values (96x2048x2048x1167): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1167): 80.834

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2335.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1168x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1168x2048): 83.476
Elapsed time for attention_prob_times_values (96x2048x2048x1168): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1168): 89.421

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2450.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1169x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1169x2048): 83.959
Elapsed time for attention_prob_times_values (96x2048x2048x1169): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1169): 80.620

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2335.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1170x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1170x2048): 81.318
Elapsed time for attention_prob_times_values (96x2048x2048x1170): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1170): 81.478

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2313.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1171x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1171x2048): 84.073
Elapsed time for attention_prob_times_values (96x2048x2048x1171): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1171): 80.033

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2332.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1172x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1172x2048): 84.403
Elapsed time for attention_prob_times_values (96x2048x2048x1172): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1172): 83.339

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2387.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1173x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1173x2048): 81.673
Elapsed time for attention_prob_times_values (96x2048x2048x1173): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1173): 82.173

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2334.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1174x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1174x2048): 82.229
Elapsed time for attention_prob_times_values (96x2048x2048x1174): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1174): 84.256

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2373.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1175x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1175x2048): 84.657
Elapsed time for attention_prob_times_values (96x2048x2048x1175): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1175): 81.866

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2375.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1176x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1176x2048): 87.466
Elapsed time for attention_prob_times_values (96x2048x2048x1176): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1176): 87.249

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2495.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1177x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1177x2048): 82.487
Elapsed time for attention_prob_times_values (96x2048x2048x1177): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1177): 82.434

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2357.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1178x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1178x2048): 82.540
Elapsed time for attention_prob_times_values (96x2048x2048x1178): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1178): 84.752

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2392.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1179x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1179x2048): 79.915
Elapsed time for attention_prob_times_values (96x2048x2048x1179): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1179): 79.130

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2276.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1180x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1180x2048): 82.818
Elapsed time for attention_prob_times_values (96x2048x2048x1180): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1180): 82.875

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2374.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1181x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1181x2048): 83.639
Elapsed time for attention_prob_times_values (96x2048x2048x1181): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1181): 81.409

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2366.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1182x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1182x2048): 84.302
Elapsed time for attention_prob_times_values (96x2048x2048x1182): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1182): 84.195

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2418.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1183x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1183x2048): 80.774
Elapsed time for attention_prob_times_values (96x2048x2048x1183): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1183): 81.087

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2324.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1184x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1184x2048): 94.777
Elapsed time for attention_prob_times_values (96x2048x2048x1184): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1184): 91.067

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2670.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1185x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1185x2048): 82.346
Elapsed time for attention_prob_times_values (96x2048x2048x1185): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1185): 82.919

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2377.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1186x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1186x2048): 86.358
Elapsed time for attention_prob_times_values (96x2048x2048x1186): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1186): 83.607

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2446.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1187x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1187x2048): 83.309
Elapsed time for attention_prob_times_values (96x2048x2048x1187): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1187): 82.987

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2396.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1188x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1188x2048): 86.981
Elapsed time for attention_prob_times_values (96x2048x2048x1188): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1188): 84.332

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2470.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1189x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1189x2048): 84.514
Elapsed time for attention_prob_times_values (96x2048x2048x1189): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1189): 82.943

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2416.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1190x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1190x2048): 84.292
Elapsed time for attention_prob_times_values (96x2048x2048x1190): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1190): 84.052

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2431.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1191x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1191x2048): 84.818
Elapsed time for attention_prob_times_values (96x2048x2048x1191): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1191): 82.995

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2425.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1192x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1192x2048): 85.668
Elapsed time for attention_prob_times_values (96x2048x2048x1192): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1192): 87.847

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2510.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1193x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1193x2048): 79.992
Elapsed time for attention_prob_times_values (96x2048x2048x1193): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1193): 83.206

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2362.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1194x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1194x2048): 84.124
Elapsed time for attention_prob_times_values (96x2048x2048x1194): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1194): 85.695

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2460.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1195x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1195x2048): 84.557
Elapsed time for attention_prob_times_values (96x2048x2048x1195): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1195): 80.121

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2386.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1196x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1196x2048): 86.212
Elapsed time for attention_prob_times_values (96x2048x2048x1196): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1196): 82.734

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2451.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1197x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1197x2048): 83.981
Elapsed time for attention_prob_times_values (96x2048x2048x1197): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1197): 78.490

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2357.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1198x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1198x2048): 85.392
Elapsed time for attention_prob_times_values (96x2048x2048x1198): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1198): 84.441

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2469.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1199x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1199x2048): 83.888
Elapsed time for attention_prob_times_values (96x2048x2048x1199): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1199): 81.429

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2404.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1200x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1200x2048): 84.522
Elapsed time for attention_prob_times_values (96x2048x2048x1200): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1200): 91.357

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2557.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1201x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1201x2048): 82.194
Elapsed time for attention_prob_times_values (96x2048x2048x1201): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1201): 83.189

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2410.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1202x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1202x2048): 85.040
Elapsed time for attention_prob_times_values (96x2048x2048x1202): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1202): 84.857

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2478.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1203x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1203x2048): 82.803
Elapsed time for attention_prob_times_values (96x2048x2048x1203): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1203): 83.502

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2427.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1204x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1204x2048): 84.180
Elapsed time for attention_prob_times_values (96x2048x2048x1204): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1204): 86.365

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2491.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1205x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1205x2048): 83.456
Elapsed time for attention_prob_times_values (96x2048x2048x1205): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1205): 83.880

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2446.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1206x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1206x2048): 85.280
Elapsed time for attention_prob_times_values (96x2048x2048x1206): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1206): 85.101

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2493.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1207x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1207x2048): 84.569
Elapsed time for attention_prob_times_values (96x2048x2048x1207): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1207): 83.905

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2467.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1208x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1208x2048): 86.659
Elapsed time for attention_prob_times_values (96x2048x2048x1208): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1208): 91.706

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2612.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1209x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1209x2048): 82.378
Elapsed time for attention_prob_times_values (96x2048x2048x1209): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1209): 84.220

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2443.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1210x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1210x2048): 84.754
Elapsed time for attention_prob_times_values (96x2048x2048x1210): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1210): 86.222

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2509.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1211x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1211x2048): 83.871
Elapsed time for attention_prob_times_values (96x2048x2048x1211): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1211): 84.227

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2469.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1212x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1212x2048): 85.917
Elapsed time for attention_prob_times_values (96x2048x2048x1212): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1212): 85.791

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2524.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1213x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1213x2048): 81.404
Elapsed time for attention_prob_times_values (96x2048x2048x1213): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1213): 82.587

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2412.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1214x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1214x2048): 84.575
Elapsed time for attention_prob_times_values (96x2048x2048x1214): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1214): 87.043

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2526.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1215x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1215x2048): 84.267
Elapsed time for attention_prob_times_values (96x2048x2048x1215): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1215): 84.570

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2488.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1216x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1216x2048): 97.028
Elapsed time for attention_prob_times_values (96x2048x2048x1216): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1216): 94.098

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2818.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1217x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1217x2048): 85.853
Elapsed time for attention_prob_times_values (96x2048x2048x1217): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1217): 83.049

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2492.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1218x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1218x2048): 84.908
Elapsed time for attention_prob_times_values (96x2048x2048x1218): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1218): 85.251

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2513.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1219x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1219x2048): 84.309
Elapsed time for attention_prob_times_values (96x2048x2048x1219): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1219): 84.913

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2501.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1220x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1220x2048): 86.466
Elapsed time for attention_prob_times_values (96x2048x2048x1220): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1220): 85.654

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2546.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1221x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1221x2048): 85.010
Elapsed time for attention_prob_times_values (96x2048x2048x1221): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1221): 83.689

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2498.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1222x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1222x2048): 85.547
Elapsed time for attention_prob_times_values (96x2048x2048x1222): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1222): 87.506

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2564.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1223x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1223x2048): 84.420
Elapsed time for attention_prob_times_values (96x2048x2048x1223): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1223): 85.114

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2514.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1224x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1224x2048): 87.968
Elapsed time for attention_prob_times_values (96x2048x2048x1224): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1224): 92.554

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2677.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1225x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1225x2048): 82.740
Elapsed time for attention_prob_times_values (96x2048x2048x1225): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1225): 84.601

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2485.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1226x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1226x2048): 84.389
Elapsed time for attention_prob_times_values (96x2048x2048x1226): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1226): 87.553

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2555.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1227x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1227x2048): 83.631
Elapsed time for attention_prob_times_values (96x2048x2048x1227): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1227): 83.391

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2485.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1228x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1228x2048): 84.953
Elapsed time for attention_prob_times_values (96x2048x2048x1228): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1228): 88.010

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2574.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1229x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1229x2048): 82.557
Elapsed time for attention_prob_times_values (96x2048x2048x1229): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1229): 85.555

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2504.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1230x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1230x2048): 86.107
Elapsed time for attention_prob_times_values (96x2048x2048x1230): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1230): 84.934

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2550.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1231x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1231x2048): 84.993
Elapsed time for attention_prob_times_values (96x2048x2048x1231): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1231): 78.166

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2431.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1232x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1232x2048): 85.081
Elapsed time for attention_prob_times_values (96x2048x2048x1232): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1232): 94.118

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2669.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1233x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1233x2048): 80.618
Elapsed time for attention_prob_times_values (96x2048x2048x1233): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1233): 84.212

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2462.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1234x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1234x2048): 82.800
Elapsed time for attention_prob_times_values (96x2048x2048x1234): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1234): 85.536

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2517.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1235x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1235x2048): 83.441
Elapsed time for attention_prob_times_values (96x2048x2048x1235): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1235): 80.652

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2456.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1236x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1236x2048): 84.241
Elapsed time for attention_prob_times_values (96x2048x2048x1236): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1236): 85.545

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2543.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1237x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1237x2048): 81.717
Elapsed time for attention_prob_times_values (96x2048x2048x1237): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1237): 73.672

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2323.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1238x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1238x2048): 85.660
Elapsed time for attention_prob_times_values (96x2048x2048x1238): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1238): 79.340

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2472.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1239x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1239x2048): 84.945
Elapsed time for attention_prob_times_values (96x2048x2048x1239): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1239): 73.379

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2365.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1240x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1240x2048): 87.020
Elapsed time for attention_prob_times_values (96x2048x2048x1240): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1240): 94.196

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2719.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1241x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1241x2048): 80.132
Elapsed time for attention_prob_times_values (96x2048x2048x1241): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1241): 75.752

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2343.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1242x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1242x2048): 85.187
Elapsed time for attention_prob_times_values (96x2048x2048x1242): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1242): 81.657

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2510.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1243x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1243x2048): 84.390
Elapsed time for attention_prob_times_values (96x2048x2048x1243): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1243): 70.815

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2320.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1244x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1244x2048): 86.086
Elapsed time for attention_prob_times_values (96x2048x2048x1244): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1244): 79.140

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2486.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1245x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1245x2048): 84.314
Elapsed time for attention_prob_times_values (96x2048x2048x1245): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1245): 76.243

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2416.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1246x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1246x2048): 85.637
Elapsed time for attention_prob_times_values (96x2048x2048x1246): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1246): 81.800

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2527.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1247x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1247x2048): 84.588
Elapsed time for attention_prob_times_values (96x2048x2048x1247): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1247): 72.988

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2368.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1248x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1248x2048): 98.162
Elapsed time for attention_prob_times_values (96x2048x2048x1248): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1248): 93.304

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2894.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1249x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1249x2048): 86.282
Elapsed time for attention_prob_times_values (96x2048x2048x1249): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1249): 76.551

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2455.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1250x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1250x2048): 87.310
Elapsed time for attention_prob_times_values (96x2048x2048x1250): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1250): 82.173

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2565.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1251x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1251x2048): 83.708
Elapsed time for attention_prob_times_values (96x2048x2048x1251): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1251): 76.157

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2418.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1252x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1252x2048): 87.402
Elapsed time for attention_prob_times_values (96x2048x2048x1252): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1252): 79.846

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2532.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1253x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1253x2048): 84.347
Elapsed time for attention_prob_times_values (96x2048x2048x1253): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1253): 71.747

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2354.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1254x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1254x2048): 86.601
Elapsed time for attention_prob_times_values (96x2048x2048x1254): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1254): 81.761

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2556.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1255x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1255x2048): 82.306
Elapsed time for attention_prob_times_values (96x2048x2048x1255): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1255): 74.955

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2386.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1256x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1256x2048): 87.638
Elapsed time for attention_prob_times_values (96x2048x2048x1256): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1256): 94.906

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2773.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1257x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1257x2048): 82.802
Elapsed time for attention_prob_times_values (96x2048x2048x1257): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1257): 73.209

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2367.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1258x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1258x2048): 83.562
Elapsed time for attention_prob_times_values (96x2048x2048x1258): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1258): 82.488

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2530.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1259x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1259x2048): 84.930
Elapsed time for attention_prob_times_values (96x2048x2048x1259): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1259): 76.545

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2456.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1260x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1260x2048): 86.664
Elapsed time for attention_prob_times_values (96x2048x2048x1260): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1260): 81.750

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2568.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1261x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1261x2048): 84.990
Elapsed time for attention_prob_times_values (96x2048x2048x1261): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1261): 74.723

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2429.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1262x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1262x2048): 86.332
Elapsed time for attention_prob_times_values (96x2048x2048x1262): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1262): 82.667

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2582.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1263x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1263x2048): 84.339
Elapsed time for attention_prob_times_values (96x2048x2048x1263): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1263): 73.137

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2397.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1264x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1264x2048): 85.704
Elapsed time for attention_prob_times_values (96x2048x2048x1264): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1264): 95.142

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2761.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1265x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1265x2048): 84.279
Elapsed time for attention_prob_times_values (96x2048x2048x1265): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1265): 75.159

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2435.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1266x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1266x2048): 84.334
Elapsed time for attention_prob_times_values (96x2048x2048x1266): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1266): 81.243

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2538.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1267x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1267x2048): 82.263
Elapsed time for attention_prob_times_values (96x2048x2048x1267): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1267): 76.112

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2427.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1268x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1268x2048): 85.940
Elapsed time for attention_prob_times_values (96x2048x2048x1268): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1268): 83.072

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2595.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1269x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1269x2048): 84.413
Elapsed time for attention_prob_times_values (96x2048x2048x1269): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1269): 75.915

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2457.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1270x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1270x2048): 85.849
Elapsed time for attention_prob_times_values (96x2048x2048x1270): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1270): 82.661

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2591.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1271x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1271x2048): 82.758
Elapsed time for attention_prob_times_values (96x2048x2048x1271): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1271): 75.931

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2438.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1272x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1272x2048): 86.818
Elapsed time for attention_prob_times_values (96x2048x2048x1272): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1272): 96.261

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2813.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1273x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1273x2048): 84.041
Elapsed time for attention_prob_times_values (96x2048x2048x1273): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1273): 74.243

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2431.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1274x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1274x2048): 84.165
Elapsed time for attention_prob_times_values (96x2048x2048x1274): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1274): 81.403

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2553.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1275x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1275x2048): 83.936
Elapsed time for attention_prob_times_values (96x2048x2048x1275): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1275): 75.739

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2459.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1276x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1276x2048): 83.988
Elapsed time for attention_prob_times_values (96x2048x2048x1276): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1276): 82.794

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2577.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1277x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1277x2048): 79.740
Elapsed time for attention_prob_times_values (96x2048x2048x1277): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1277): 75.863

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2404.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1278x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1278x2048): 84.401
Elapsed time for attention_prob_times_values (96x2048x2048x1278): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1278): 82.425

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2581.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1279x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1279x2048): 83.539
Elapsed time for attention_prob_times_values (96x2048x2048x1279): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1279): 75.915

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2464.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1280x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1280x2048): 91.366
Elapsed time for attention_prob_times_values (96x2048x2048x1280): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1280): 97.366

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2922.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1281x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1281x2048): 82.247
Elapsed time for attention_prob_times_values (96x2048x2048x1281): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1281): 71.279

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2369.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1282x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1282x2048): 86.362
Elapsed time for attention_prob_times_values (96x2048x2048x1282): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1282): 77.555

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2537.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1283x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1283x2048): 82.726
Elapsed time for attention_prob_times_values (96x2048x2048x1283): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1283): 71.288

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2379.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1284x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1284x2048): 87.358
Elapsed time for attention_prob_times_values (96x2048x2048x1284): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1284): 75.817

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2524.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1285x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1285x2048): 84.980
Elapsed time for attention_prob_times_values (96x2048x2048x1285): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1285): 71.702

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2420.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1286x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1286x2048): 86.594
Elapsed time for attention_prob_times_values (96x2048x2048x1286): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1286): 77.682

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2550.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1287x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1287x2048): 85.200
Elapsed time for attention_prob_times_values (96x2048x2048x1287): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1287): 70.985

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2413.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1288x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1288x2048): 87.794
Elapsed time for attention_prob_times_values (96x2048x2048x1288): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1288): 86.024

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2710.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1289x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1289x2048): 82.779
Elapsed time for attention_prob_times_values (96x2048x2048x1289): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1289): 72.161

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2406.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1290x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1290x2048): 85.740
Elapsed time for attention_prob_times_values (96x2048x2048x1290): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1290): 76.534

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2526.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1291x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1291x2048): 83.246
Elapsed time for attention_prob_times_values (96x2048x2048x1291): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1291): 72.375

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2420.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1292x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1292x2048): 86.308
Elapsed time for attention_prob_times_values (96x2048x2048x1292): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1292): 77.388

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2552.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1293x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1293x2048): 84.270
Elapsed time for attention_prob_times_values (96x2048x2048x1293): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1293): 68.395

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2363.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1294x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1294x2048): 85.191
Elapsed time for attention_prob_times_values (96x2048x2048x1294): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1294): 77.757

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2547.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1295x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1295x2048): 85.221
Elapsed time for attention_prob_times_values (96x2048x2048x1295): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1295): 72.660

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2459.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1296x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1296x2048): 88.156
Elapsed time for attention_prob_times_values (96x2048x2048x1296): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1296): 89.135

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2781.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1297x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1297x2048): 84.799
Elapsed time for attention_prob_times_values (96x2048x2048x1297): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1297): 71.184

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2430.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1298x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1298x2048): 83.706
Elapsed time for attention_prob_times_values (96x2048x2048x1298): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1298): 76.240

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2507.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1299x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1299x2048): 85.101
Elapsed time for attention_prob_times_values (96x2048x2048x1299): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1299): 71.449

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2442.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1300x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1300x2048): 86.367
Elapsed time for attention_prob_times_values (96x2048x2048x1300): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1300): 77.375

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2568.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1301x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1301x2048): 84.373
Elapsed time for attention_prob_times_values (96x2048x2048x1301): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1301): 73.071

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2466.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1302x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1302x2048): 84.988
Elapsed time for attention_prob_times_values (96x2048x2048x1302): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1302): 78.443

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2571.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1303x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1303x2048): 85.204
Elapsed time for attention_prob_times_values (96x2048x2048x1303): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1303): 71.703

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2456.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1304x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1304x2048): 85.980
Elapsed time for attention_prob_times_values (96x2048x2048x1304): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1304): 87.409

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2736.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1305x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1305x2048): 84.631
Elapsed time for attention_prob_times_values (96x2048x2048x1305): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1305): 71.885

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2455.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1306x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1306x2048): 85.736
Elapsed time for attention_prob_times_values (96x2048x2048x1306): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1306): 77.696

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2576.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1307x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1307x2048): 85.103
Elapsed time for attention_prob_times_values (96x2048x2048x1307): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1307): 72.166

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2470.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1308x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1308x2048): 86.381
Elapsed time for attention_prob_times_values (96x2048x2048x1308): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1308): 78.491

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2603.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1309x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1309x2048): 83.535
Elapsed time for attention_prob_times_values (96x2048x2048x1309): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1309): 73.780

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2482.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1310x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1310x2048): 86.176
Elapsed time for attention_prob_times_values (96x2048x2048x1310): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1310): 74.619

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2535.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1311x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1311x2048): 85.370
Elapsed time for attention_prob_times_values (96x2048x2048x1311): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1311): 71.996

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2478.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1312x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1312x2048): 98.363
Elapsed time for attention_prob_times_values (96x2048x2048x1312): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1312): 91.359

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 3007.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1313x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1313x2048): 86.330
Elapsed time for attention_prob_times_values (96x2048x2048x1313): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1313): 74.194

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2535.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1314x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1314x2048): 87.135
Elapsed time for attention_prob_times_values (96x2048x2048x1314): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1314): 78.973

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2634.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1315x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1315x2048): 86.286
Elapsed time for attention_prob_times_values (96x2048x2048x1315): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1315): 73.876

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2532.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1316x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1316x2048): 86.756
Elapsed time for attention_prob_times_values (96x2048x2048x1316): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1316): 79.268

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2638.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1317x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1317x2048): 86.056
Elapsed time for attention_prob_times_values (96x2048x2048x1317): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1317): 73.927

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2534.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1318x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1318x2048): 87.039
Elapsed time for attention_prob_times_values (96x2048x2048x1318): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1318): 79.111

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2643.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1319x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1319x2048): 86.130
Elapsed time for attention_prob_times_values (96x2048x2048x1319): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1319): 73.173

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2525.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1320x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1320x2048): 83.444
Elapsed time for attention_prob_times_values (96x2048x2048x1320): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1320): 89.637

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2760.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1321x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1321x2048): 85.650
Elapsed time for attention_prob_times_values (96x2048x2048x1321): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1321): 72.534

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2510.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1322x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1322x2048): 86.551
Elapsed time for attention_prob_times_values (96x2048x2048x1322): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1322): 79.371

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2648.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1323x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1323x2048): 85.512
Elapsed time for attention_prob_times_values (96x2048x2048x1323): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1323): 73.928

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2538.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1324x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1324x2048): 86.877
Elapsed time for attention_prob_times_values (96x2048x2048x1324): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1324): 78.934

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2649.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1325x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1325x2048): 85.858
Elapsed time for attention_prob_times_values (96x2048x2048x1325): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1325): 72.617

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2522.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1326x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1326x2048): 84.415
Elapsed time for attention_prob_times_values (96x2048x2048x1326): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1326): 78.774

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2614.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1327x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1327x2048): 84.736
Elapsed time for attention_prob_times_values (96x2048x2048x1327): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1327): 73.185

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2521.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1328x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1328x2048): 86.891
Elapsed time for attention_prob_times_values (96x2048x2048x1328): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1328): 91.777

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2867.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1329x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1329x2048): 85.329
Elapsed time for attention_prob_times_values (96x2048x2048x1329): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1329): 71.278

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2497.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1330x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1330x2048): 84.958
Elapsed time for attention_prob_times_values (96x2048x2048x1330): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1330): 79.834

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2648.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1331x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1331x2048): 83.460
Elapsed time for attention_prob_times_values (96x2048x2048x1331): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1331): 74.099

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2527.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1332x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1332x2048): 87.263
Elapsed time for attention_prob_times_values (96x2048x2048x1332): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1332): 80.125

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2691.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1333x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1333x2048): 85.576
Elapsed time for attention_prob_times_values (96x2048x2048x1333): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1333): 74.029

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2559.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1334x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1334x2048): 84.798
Elapsed time for attention_prob_times_values (96x2048x2048x1334): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1334): 79.960

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2655.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1335x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1335x2048): 83.348
Elapsed time for attention_prob_times_values (96x2048x2048x1335): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1335): 72.858

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2510.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1336x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1336x2048): 87.308
Elapsed time for attention_prob_times_values (96x2048x2048x1336): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1336): 90.642

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2873.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1337x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1337x2048): 85.125
Elapsed time for attention_prob_times_values (96x2048x2048x1337): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1337): 73.940

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2559.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1338x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1338x2048): 86.292
Elapsed time for attention_prob_times_values (96x2048x2048x1338): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1338): 80.176

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2689.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1339x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1339x2048): 83.685
Elapsed time for attention_prob_times_values (96x2048x2048x1339): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1339): 73.594

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2536.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1340x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1340x2048): 84.923
Elapsed time for attention_prob_times_values (96x2048x2048x1340): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1340): 77.383

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2624.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1341x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1341x2048): 84.725
Elapsed time for attention_prob_times_values (96x2048x2048x1341): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1341): 73.393

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2550.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1342x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1342x2048): 80.816
Elapsed time for attention_prob_times_values (96x2048x2048x1342): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1342): 79.931

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2608.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1343x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1343x2048): 85.609
Elapsed time for attention_prob_times_values (96x2048x2048x1343): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1343): 73.361

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2566.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1344x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1344x2048): 96.872
Elapsed time for attention_prob_times_values (96x2048x2048x1344): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1344): 86.154

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2963.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1345x2048): 0.0254
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1345x2048): 42.650
Elapsed time for attention_prob_times_values (96x2048x2048x1345): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1345): 70.266

Attention duration (in seconds): 0.0408
Attention throughput (in TFLOP/s): 1726.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0408
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1346x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1346x2048): 87.229
Elapsed time for attention_prob_times_values (96x2048x2048x1346): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1346): 80.635

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2727.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1347x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1347x2048): 87.645
Elapsed time for attention_prob_times_values (96x2048x2048x1347): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1347): 73.427

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2602.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1348x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1348x2048): 88.164
Elapsed time for attention_prob_times_values (96x2048x2048x1348): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1348): 80.950

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2751.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1349x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1349x2048): 86.730
Elapsed time for attention_prob_times_values (96x2048x2048x1349): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1349): 72.643

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2578.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1350x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1350x2048): 85.358
Elapsed time for attention_prob_times_values (96x2048x2048x1350): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1350): 80.528

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2705.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1351x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1351x2048): 86.282
Elapsed time for attention_prob_times_values (96x2048x2048x1351): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1351): 73.523

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2593.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1352x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1352x2048): 87.868
Elapsed time for attention_prob_times_values (96x2048x2048x1352): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1352): 92.194

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2941.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1353x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1353x2048): 85.875
Elapsed time for attention_prob_times_values (96x2048x2048x1353): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1353): 73.360

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2588.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1354x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1354x2048): 86.962
Elapsed time for attention_prob_times_values (96x2048x2048x1354): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1354): 79.621

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2721.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1355x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1355x2048): 85.888
Elapsed time for attention_prob_times_values (96x2048x2048x1355): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1355): 74.511

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2613.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1356x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1356x2048): 91.457
Elapsed time for attention_prob_times_values (96x2048x2048x1356): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1356): 81.025

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2816.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1357x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1357x2048): 89.940
Elapsed time for attention_prob_times_values (96x2048x2048x1357): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1357): 74.487

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2673.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1358x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1358x2048): 89.061
Elapsed time for attention_prob_times_values (96x2048x2048x1358): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1358): 80.476

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2775.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1359x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1359x2048): 88.854
Elapsed time for attention_prob_times_values (96x2048x2048x1359): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1359): 72.661

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2626.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1360x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1360x2048): 91.329
Elapsed time for attention_prob_times_values (96x2048x2048x1360): 0.0207
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1360): 52.899

Attention duration (in seconds): 0.0327
Attention throughput (in TFLOP/s): 2202.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0327
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1361x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1361x2048): 87.990
Elapsed time for attention_prob_times_values (96x2048x2048x1361): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1361): 73.461

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2634.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1362x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1362x2048): 89.344
Elapsed time for attention_prob_times_values (96x2048x2048x1362): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1362): 78.491

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2751.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1363x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1363x2048): 86.460
Elapsed time for attention_prob_times_values (96x2048x2048x1363): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1363): 73.296

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2613.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1364x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1364x2048): 91.336
Elapsed time for attention_prob_times_values (96x2048x2048x1364): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1364): 81.363

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2837.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1365x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1365x2048): 81.668
Elapsed time for attention_prob_times_values (96x2048x2048x1365): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1365): 75.004

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2579.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1x2048): 1.012
Elapsed time for attention_prob_times_values (128x2048x2048x1): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1): 0.210

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 0.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 64, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x2x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x2x2048): 1.874
Elapsed time for attention_prob_times_values (128x2048x2048x2): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x2): 1.771

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 1.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x3x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x3x2048): 2.625
Elapsed time for attention_prob_times_values (128x2048x2048x3): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x3): 2.456

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 2.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x4x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x4x2048): 3.367
Elapsed time for attention_prob_times_values (128x2048x2048x4): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x4): 3.098

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 3.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x5x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x5x2048): 4.172
Elapsed time for attention_prob_times_values (128x2048x2048x5): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x5): 3.731

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 4.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x6x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x6x2048): 4.972
Elapsed time for attention_prob_times_values (128x2048x2048x6): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x6): 4.517

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 5.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x7x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x7x2048): 5.796
Elapsed time for attention_prob_times_values (128x2048x2048x7): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x7): 4.672

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 6.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x8x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x8x2048): 6.602
Elapsed time for attention_prob_times_values (128x2048x2048x8): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x8): 7.069

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 8.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x9x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x9x2048): 7.115
Elapsed time for attention_prob_times_values (128x2048x2048x9): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x9): 7.537

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 9.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x10x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x10x2048): 7.867
Elapsed time for attention_prob_times_values (128x2048x2048x10): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x10): 8.682

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 10.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x11x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x11x2048): 8.634
Elapsed time for attention_prob_times_values (128x2048x2048x11): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x11): 9.234

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 11.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x12x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x12x2048): 9.378
Elapsed time for attention_prob_times_values (128x2048x2048x12): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x12): 10.381

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 13.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x13x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x13x2048): 10.145
Elapsed time for attention_prob_times_values (128x2048x2048x13): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x13): 10.828

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 14.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x14x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x14x2048): 10.914
Elapsed time for attention_prob_times_values (128x2048x2048x14): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x14): 12.014

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 16.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x15x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x15x2048): 10.509
Elapsed time for attention_prob_times_values (128x2048x2048x15): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x15): 12.386

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 16.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x16x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x16x2048): 12.512
Elapsed time for attention_prob_times_values (128x2048x2048x16): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x16): 13.637

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 19.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x17x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x17x2048): 12.716
Elapsed time for attention_prob_times_values (128x2048x2048x17): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x17): 13.885

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 20.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x18x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x18x2048): 12.730
Elapsed time for attention_prob_times_values (128x2048x2048x18): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x18): 15.141

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 21.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x19x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x19x2048): 14.238
Elapsed time for attention_prob_times_values (128x2048x2048x19): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x19): 15.513

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 23.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x20x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x20x2048): 14.052
Elapsed time for attention_prob_times_values (128x2048x2048x20): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x20): 16.739

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 24.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x21x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x21x2048): 15.702
Elapsed time for attention_prob_times_values (128x2048x2048x21): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x21): 17.019

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 27.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x22x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x22x2048): 16.431
Elapsed time for attention_prob_times_values (128x2048x2048x22): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x22): 18.060

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 29.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x23x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x23x2048): 17.192
Elapsed time for attention_prob_times_values (128x2048x2048x23): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x23): 18.510

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 30.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x24x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x24x2048): 17.925
Elapsed time for attention_prob_times_values (128x2048x2048x24): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x24): 16.729

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 30.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x25x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x25x2048): 15.734
Elapsed time for attention_prob_times_values (128x2048x2048x25): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x25): 19.728

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 31.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x26x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x26x2048): 18.917
Elapsed time for attention_prob_times_values (128x2048x2048x26): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x26): 21.132

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 36.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x27x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x27x2048): 19.600
Elapsed time for attention_prob_times_values (128x2048x2048x27): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x27): 21.518

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 37.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x28x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x28x2048): 20.264
Elapsed time for attention_prob_times_values (128x2048x2048x28): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x28): 22.680

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 40.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x29x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x29x2048): 20.923
Elapsed time for attention_prob_times_values (128x2048x2048x29): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x29): 22.945

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 41.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x30x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x30x2048): 21.476
Elapsed time for attention_prob_times_values (128x2048x2048x30): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x30): 24.148

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 44.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x31x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x31x2048): 22.334
Elapsed time for attention_prob_times_values (128x2048x2048x31): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x31): 24.310

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 45.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x32x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x32x2048): 39.866
Elapsed time for attention_prob_times_values (128x2048x2048x32): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x32): 26.141

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 63.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x33x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x33x2048): 26.370
Elapsed time for attention_prob_times_values (128x2048x2048x33): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x33): 25.960

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 53.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x34x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x34x2048): 26.667
Elapsed time for attention_prob_times_values (128x2048x2048x34): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x34): 26.939

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 55.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x35x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x35x2048): 25.701
Elapsed time for attention_prob_times_values (128x2048x2048x35): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x35): 27.437

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 55.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x36x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x36x2048): 26.522
Elapsed time for attention_prob_times_values (128x2048x2048x36): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x36): 23.895

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 53.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x37x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x37x2048): 26.479
Elapsed time for attention_prob_times_values (128x2048x2048x37): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x37): 26.725

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 57.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x38x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x38x2048): 27.303
Elapsed time for attention_prob_times_values (128x2048x2048x38): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x38): 29.940

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 62.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x39x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x39x2048): 27.431
Elapsed time for attention_prob_times_values (128x2048x2048x39): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x39): 30.288

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 63.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x40x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x40x2048): 28.365
Elapsed time for attention_prob_times_values (128x2048x2048x40): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x40): 32.174

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 67.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x41x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x41x2048): 28.019
Elapsed time for attention_prob_times_values (128x2048x2048x41): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x41): 31.580

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 67.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x42x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x42x2048): 28.816
Elapsed time for attention_prob_times_values (128x2048x2048x42): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x42): 32.483

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 70.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x43x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x43x2048): 29.275
Elapsed time for attention_prob_times_values (128x2048x2048x43): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x43): 32.629

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 72.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x44x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x44x2048): 30.143
Elapsed time for attention_prob_times_values (128x2048x2048x44): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x44): 34.083

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 75.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x45x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x45x2048): 30.577
Elapsed time for attention_prob_times_values (128x2048x2048x45): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x45): 33.935

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 77.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x46x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x46x2048): 31.498
Elapsed time for attention_prob_times_values (128x2048x2048x46): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x46): 35.346

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 81.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x47x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x47x2048): 31.907
Elapsed time for attention_prob_times_values (128x2048x2048x47): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x47): 33.545

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 80.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x48x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x48x2048): 32.645
Elapsed time for attention_prob_times_values (128x2048x2048x48): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x48): 37.998

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 87.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x49x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x49x2048): 32.902
Elapsed time for attention_prob_times_values (128x2048x2048x49): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x49): 36.669

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 87.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x50x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x50x2048): 33.740
Elapsed time for attention_prob_times_values (128x2048x2048x50): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x50): 37.674

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 91.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x51x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x51x2048): 34.230
Elapsed time for attention_prob_times_values (128x2048x2048x51): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x51): 37.864

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 93.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x52x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x52x2048): 35.063
Elapsed time for attention_prob_times_values (128x2048x2048x52): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x52): 39.619

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 97.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x53x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x53x2048): 35.663
Elapsed time for attention_prob_times_values (128x2048x2048x53): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x53): 37.162

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 96.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x54x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x54x2048): 35.032
Elapsed time for attention_prob_times_values (128x2048x2048x54): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x54): 41.122

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 101.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x55x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x55x2048): 32.562
Elapsed time for attention_prob_times_values (128x2048x2048x55): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x55): 40.527

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 98.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x56x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x56x2048): 37.755
Elapsed time for attention_prob_times_values (128x2048x2048x56): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x56): 43.854

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 111.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x57x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x57x2048): 37.296
Elapsed time for attention_prob_times_values (128x2048x2048x57): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x57): 42.445

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 110.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x58x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x58x2048): 37.878
Elapsed time for attention_prob_times_values (128x2048x2048x58): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x58): 43.578

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 113.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x59x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x59x2048): 34.779
Elapsed time for attention_prob_times_values (128x2048x2048x59): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x59): 43.226

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 109.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x60x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x60x2048): 39.122
Elapsed time for attention_prob_times_values (128x2048x2048x60): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x60): 42.201

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 116.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x61x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x61x2048): 39.454
Elapsed time for attention_prob_times_values (128x2048x2048x61): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x61): 44.403

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 121.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x62x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x62x2048): 40.245
Elapsed time for attention_prob_times_values (128x2048x2048x62): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x62): 5.224

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 27.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x63x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x63x2048): 40.417
Elapsed time for attention_prob_times_values (128x2048x2048x63): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x63): 39.599

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 118.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x64x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x64x2048): 56.772
Elapsed time for attention_prob_times_values (128x2048x2048x64): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x64): 49.867

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 159.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x65x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x65x2048): 43.575
Elapsed time for attention_prob_times_values (128x2048x2048x65): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x65): 30.207

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 108.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x66x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x66x2048): 44.291
Elapsed time for attention_prob_times_values (128x2048x2048x66): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x66): 35.168

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 120.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x67x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x67x2048): 43.934
Elapsed time for attention_prob_times_values (128x2048x2048x67): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x67): 34.544

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 119.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x68x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x68x2048): 44.924
Elapsed time for attention_prob_times_values (128x2048x2048x68): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x68): 33.521

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 119.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x69x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x69x2048): 44.571
Elapsed time for attention_prob_times_values (128x2048x2048x69): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x69): 32.655

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 118.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x70x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x70x2048): 45.543
Elapsed time for attention_prob_times_values (128x2048x2048x70): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x70): 35.005

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 126.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x71x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x71x2048): 45.202
Elapsed time for attention_prob_times_values (128x2048x2048x71): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x71): 34.316

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 125.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x72x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x72x2048): 46.629
Elapsed time for attention_prob_times_values (128x2048x2048x72): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x72): 36.323

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 132.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x73x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x73x2048): 45.290
Elapsed time for attention_prob_times_values (128x2048x2048x73): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x73): 37.010

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 133.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x74x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x74x2048): 46.468
Elapsed time for attention_prob_times_values (128x2048x2048x74): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x74): 38.892

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 140.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x75x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x75x2048): 46.087
Elapsed time for attention_prob_times_values (128x2048x2048x75): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x75): 38.231

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 139.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x76x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x76x2048): 47.413
Elapsed time for attention_prob_times_values (128x2048x2048x76): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x76): 39.855

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 146.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x77x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x77x2048): 45.605
Elapsed time for attention_prob_times_values (128x2048x2048x77): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x77): 36.089

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 137.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x78x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x78x2048): 47.917
Elapsed time for attention_prob_times_values (128x2048x2048x78): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x78): 40.396

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 150.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x79x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x79x2048): 48.085
Elapsed time for attention_prob_times_values (128x2048x2048x79): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x79): 39.628

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 150.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x80x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x80x2048): 49.936
Elapsed time for attention_prob_times_values (128x2048x2048x80): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x80): 38.658

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 152.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x81x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x81x2048): 46.894
Elapsed time for attention_prob_times_values (128x2048x2048x81): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x81): 39.322

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 151.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x82x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x82x2048): 47.676
Elapsed time for attention_prob_times_values (128x2048x2048x82): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x82): 42.380

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 159.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x83x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x83x2048): 47.666
Elapsed time for attention_prob_times_values (128x2048x2048x83): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x83): 41.395

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 159.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x84x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x84x2048): 48.488
Elapsed time for attention_prob_times_values (128x2048x2048x84): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x84): 43.398

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 166.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x85x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x85x2048): 48.316
Elapsed time for attention_prob_times_values (128x2048x2048x85): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x85): 42.108

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 164.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x86x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x86x2048): 49.263
Elapsed time for attention_prob_times_values (128x2048x2048x86): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x86): 44.073

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 171.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x87x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x87x2048): 49.353
Elapsed time for attention_prob_times_values (128x2048x2048x87): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x87): 43.175

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 171.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x88x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x88x2048): 50.377
Elapsed time for attention_prob_times_values (128x2048x2048x88): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x88): 43.247

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 174.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x89x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x89x2048): 48.970
Elapsed time for attention_prob_times_values (128x2048x2048x89): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x89): 43.959

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 175.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x90x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x90x2048): 49.831
Elapsed time for attention_prob_times_values (128x2048x2048x90): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x90): 42.987

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 175.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x91x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x91x2048): 49.383
Elapsed time for attention_prob_times_values (128x2048x2048x91): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x91): 44.795

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 180.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x92x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x92x2048): 50.516
Elapsed time for attention_prob_times_values (128x2048x2048x92): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x92): 46.570

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 187.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x93x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x93x2048): 50.682
Elapsed time for attention_prob_times_values (128x2048x2048x93): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x93): 45.511

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 187.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x94x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x94x2048): 46.996
Elapsed time for attention_prob_times_values (128x2048x2048x94): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x94): 47.374

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 185.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x95x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x95x2048): 51.547
Elapsed time for attention_prob_times_values (128x2048x2048x95): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x95): 46.178

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 193.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x96x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x96x2048): 66.916
Elapsed time for attention_prob_times_values (128x2048x2048x96): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x96): 48.124

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 223.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x97x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x97x2048): 52.493
Elapsed time for attention_prob_times_values (128x2048x2048x97): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x97): 47.227

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 200.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x98x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x98x2048): 53.699
Elapsed time for attention_prob_times_values (128x2048x2048x98): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x98): 49.072

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 208.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x99x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x99x2048): 53.218
Elapsed time for attention_prob_times_values (128x2048x2048x99): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x99): 48.143

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 206.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x100x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x100x2048): 50.079
Elapsed time for attention_prob_times_values (128x2048x2048x100): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x100): 49.962

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 206.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x101x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x101x2048): 50.868
Elapsed time for attention_prob_times_values (128x2048x2048x101): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x101): 44.520

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 197.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x102x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x102x2048): 54.563
Elapsed time for attention_prob_times_values (128x2048x2048x102): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x102): 44.921

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 206.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x103x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x103x2048): 54.001
Elapsed time for attention_prob_times_values (128x2048x2048x103): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x103): 44.069

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 204.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x104x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x104x2048): 51.800
Elapsed time for attention_prob_times_values (128x2048x2048x104): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x104): 46.749

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 208.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x105x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x105x2048): 53.816
Elapsed time for attention_prob_times_values (128x2048x2048x105): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x105): 50.328

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 222.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x106x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x106x2048): 54.790
Elapsed time for attention_prob_times_values (128x2048x2048x106): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x106): 52.442

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 231.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x107x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x107x2048): 54.132
Elapsed time for attention_prob_times_values (128x2048x2048x107): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x107): 51.042

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 228.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x108x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x108x2048): 55.467
Elapsed time for attention_prob_times_values (128x2048x2048x108): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x108): 51.678

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 234.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x109x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x109x2048): 54.218
Elapsed time for attention_prob_times_values (128x2048x2048x109): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x109): 51.673

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 233.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x110x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x110x2048): 56.244
Elapsed time for attention_prob_times_values (128x2048x2048x110): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x110): 53.900

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 244.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x111x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x111x2048): 55.886
Elapsed time for attention_prob_times_values (128x2048x2048x111): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x111): 52.422

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 241.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x112x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x112x2048): 57.851
Elapsed time for attention_prob_times_values (128x2048x2048x112): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x112): 54.266

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 252.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x113x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x113x2048): 55.503
Elapsed time for attention_prob_times_values (128x2048x2048x113): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x113): 52.682

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 244.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x114x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x114x2048): 56.894
Elapsed time for attention_prob_times_values (128x2048x2048x114): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x114): 55.265

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 255.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x115x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x115x2048): 56.381
Elapsed time for attention_prob_times_values (128x2048x2048x115): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x115): 49.833

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 243.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x116x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x116x2048): 57.649
Elapsed time for attention_prob_times_values (128x2048x2048x116): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x116): 56.089

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 262.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x117x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x117x2048): 57.051
Elapsed time for attention_prob_times_values (128x2048x2048x117): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x117): 53.571

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 257.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x118x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x118x2048): 58.016
Elapsed time for attention_prob_times_values (128x2048x2048x118): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x118): 56.462

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 268.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x119x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x119x2048): 56.881
Elapsed time for attention_prob_times_values (128x2048x2048x119): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x119): 55.046

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 264.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x120x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x120x2048): 59.401
Elapsed time for attention_prob_times_values (128x2048x2048x120): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x120): 56.822

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 275.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x121x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x121x2048): 57.556
Elapsed time for attention_prob_times_values (128x2048x2048x121): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x121): 43.536

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 237.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x122x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x122x2048): 58.613
Elapsed time for attention_prob_times_values (128x2048x2048x122): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x122): 58.179

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 281.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x123x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x123x2048): 57.214
Elapsed time for attention_prob_times_values (128x2048x2048x123): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x123): 44.883

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 243.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x124x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x124x2048): 59.447
Elapsed time for attention_prob_times_values (128x2048x2048x124): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x124): 58.833

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 288.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x125x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x125x2048): 58.519
Elapsed time for attention_prob_times_values (128x2048x2048x125): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x125): 43.929

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 246.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x126x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x126x2048): 59.202
Elapsed time for attention_prob_times_values (128x2048x2048x126): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x126): 59.638

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 293.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x127x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x127x2048): 55.551
Elapsed time for attention_prob_times_values (128x2048x2048x127): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x127): 44.010

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 244.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x128x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x128x2048): 69.045
Elapsed time for attention_prob_times_values (128x2048x2048x128): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x128): 58.709

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 317.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x129x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x129x2048): 57.980
Elapsed time for attention_prob_times_values (128x2048x2048x129): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x129): 40.508

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 239.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x130x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x130x2048): 59.138
Elapsed time for attention_prob_times_values (128x2048x2048x130): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x130): 47.922

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 268.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x131x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x131x2048): 56.338
Elapsed time for attention_prob_times_values (128x2048x2048x131): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x131): 42.860

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 247.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x132x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x132x2048): 60.383
Elapsed time for attention_prob_times_values (128x2048x2048x132): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x132): 48.676

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 276.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x133x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x133x2048): 56.401
Elapsed time for attention_prob_times_values (128x2048x2048x133): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x133): 46.868

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 263.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x134x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x134x2048): 60.451
Elapsed time for attention_prob_times_values (128x2048x2048x134): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x134): 49.224

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 281.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x135x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x135x2048): 57.217
Elapsed time for attention_prob_times_values (128x2048x2048x135): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x135): 45.730

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 265.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x136x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x136x2048): 61.631
Elapsed time for attention_prob_times_values (128x2048x2048x136): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x136): 45.732

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 275.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x137x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x137x2048): 59.653
Elapsed time for attention_prob_times_values (128x2048x2048x137): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x137): 47.688

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 279.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x138x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x138x2048): 60.628
Elapsed time for attention_prob_times_values (128x2048x2048x138): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x138): 50.475

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 292.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x139x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x139x2048): 60.094
Elapsed time for attention_prob_times_values (128x2048x2048x139): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x139): 48.273

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 286.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x140x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x140x2048): 61.507
Elapsed time for attention_prob_times_values (128x2048x2048x140): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x140): 51.386

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 300.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x141x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x141x2048): 60.918
Elapsed time for attention_prob_times_values (128x2048x2048x141): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x141): 49.077

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 293.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x142x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x142x2048): 62.082
Elapsed time for attention_prob_times_values (128x2048x2048x142): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x142): 51.412

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 305.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x143x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x143x2048): 61.499
Elapsed time for attention_prob_times_values (128x2048x2048x143): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x143): 49.479

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 299.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x144x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x144x2048): 63.518
Elapsed time for attention_prob_times_values (128x2048x2048x144): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x144): 49.898

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 307.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x145x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x145x2048): 61.236
Elapsed time for attention_prob_times_values (128x2048x2048x145): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x145): 50.380

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 305.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x146x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x146x2048): 59.873
Elapsed time for attention_prob_times_values (128x2048x2048x146): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x146): 50.676

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 305.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x147x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x147x2048): 61.756
Elapsed time for attention_prob_times_values (128x2048x2048x147): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x147): 50.781

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 311.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x148x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x148x2048): 63.274
Elapsed time for attention_prob_times_values (128x2048x2048x148): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x148): 53.593

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 326.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x149x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x149x2048): 62.463
Elapsed time for attention_prob_times_values (128x2048x2048x149): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x149): 51.405

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 318.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x150x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x150x2048): 63.670
Elapsed time for attention_prob_times_values (128x2048x2048x150): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x150): 54.065

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 332.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x151x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x151x2048): 63.107
Elapsed time for attention_prob_times_values (128x2048x2048x151): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x151): 52.035

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 326.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x152x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x152x2048): 64.733
Elapsed time for attention_prob_times_values (128x2048x2048x152): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x152): 48.150

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 317.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x153x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x153x2048): 62.641
Elapsed time for attention_prob_times_values (128x2048x2048x153): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x153): 52.328

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 329.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x154x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x154x2048): 60.383
Elapsed time for attention_prob_times_values (128x2048x2048x154): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x154): 53.490

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 329.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x155x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x155x2048): 63.339
Elapsed time for attention_prob_times_values (128x2048x2048x155): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x155): 53.254

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 338.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x156x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x156x2048): 64.758
Elapsed time for attention_prob_times_values (128x2048x2048x156): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x156): 56.138

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 353.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x157x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x157x2048): 60.949
Elapsed time for attention_prob_times_values (128x2048x2048x157): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x157): 53.737

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 337.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x158x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x158x2048): 65.359
Elapsed time for attention_prob_times_values (128x2048x2048x158): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x158): 56.331

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 359.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x159x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x159x2048): 64.407
Elapsed time for attention_prob_times_values (128x2048x2048x159): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x159): 52.510

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 345.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x160x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x160x2048): 78.907
Elapsed time for attention_prob_times_values (128x2048x2048x160): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x160): 56.247

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 394.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x161x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x161x2048): 63.514
Elapsed time for attention_prob_times_values (128x2048x2048x161): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x161): 55.409

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 356.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x162x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x162x2048): 64.938
Elapsed time for attention_prob_times_values (128x2048x2048x162): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x162): 57.506

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 369.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x163x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x163x2048): 60.698
Elapsed time for attention_prob_times_values (128x2048x2048x163): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x163): 53.353

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 346.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x164x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x164x2048): 65.957
Elapsed time for attention_prob_times_values (128x2048x2048x164): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x164): 58.345

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 379.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x165x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x165x2048): 64.667
Elapsed time for attention_prob_times_values (128x2048x2048x165): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x165): 56.660

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 371.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x166x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x166x2048): 65.955
Elapsed time for attention_prob_times_values (128x2048x2048x166): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x166): 58.644

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 384.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x167x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x167x2048): 65.253
Elapsed time for attention_prob_times_values (128x2048x2048x167): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x167): 57.264

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 379.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x168x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x168x2048): 67.254
Elapsed time for attention_prob_times_values (128x2048x2048x168): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x168): 54.803

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 377.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x169x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x169x2048): 64.875
Elapsed time for attention_prob_times_values (128x2048x2048x169): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x169): 55.285

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 374.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x170x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x170x2048): 63.956
Elapsed time for attention_prob_times_values (128x2048x2048x170): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x170): 59.813

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 390.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x171x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x171x2048): 63.646
Elapsed time for attention_prob_times_values (128x2048x2048x171): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x171): 57.864

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 384.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x172x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x172x2048): 67.026
Elapsed time for attention_prob_times_values (128x2048x2048x172): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x172): 60.484

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 405.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x173x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x173x2048): 65.938
Elapsed time for attention_prob_times_values (128x2048x2048x173): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x173): 58.414

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 396.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x174x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x174x2048): 67.253
Elapsed time for attention_prob_times_values (128x2048x2048x174): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x174): 61.073

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 412.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x175x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x175x2048): 66.554
Elapsed time for attention_prob_times_values (128x2048x2048x175): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x175): 59.058

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 404.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x176x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x176x2048): 69.676
Elapsed time for attention_prob_times_values (128x2048x2048x176): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x176): 60.075

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 419.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x177x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x177x2048): 63.150
Elapsed time for attention_prob_times_values (128x2048x2048x177): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x177): 55.650

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 386.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x178x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x178x2048): 67.626
Elapsed time for attention_prob_times_values (128x2048x2048x178): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x178): 62.210

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 425.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x179x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x179x2048): 66.800
Elapsed time for attention_prob_times_values (128x2048x2048x179): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x179): 60.345

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 418.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x180x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x180x2048): 68.455
Elapsed time for attention_prob_times_values (128x2048x2048x180): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x180): 56.436

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 409.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x181x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x181x2048): 67.307
Elapsed time for attention_prob_times_values (128x2048x2048x181): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x181): 60.814

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 425.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x182x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x182x2048): 68.665
Elapsed time for attention_prob_times_values (128x2048x2048x182): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x182): 59.348

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 425.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x183x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x183x2048): 67.838
Elapsed time for attention_prob_times_values (128x2048x2048x183): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x183): 61.330

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 432.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x184x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x184x2048): 68.575
Elapsed time for attention_prob_times_values (128x2048x2048x184): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x184): 61.310

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 436.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x185x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x185x2048): 67.633
Elapsed time for attention_prob_times_values (128x2048x2048x185): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x185): 59.764

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 430.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x186x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x186x2048): 68.842
Elapsed time for attention_prob_times_values (128x2048x2048x186): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x186): 64.332

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 453.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x187x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x187x2048): 68.054
Elapsed time for attention_prob_times_values (128x2048x2048x187): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x187): 62.629

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 446.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x188x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x188x2048): 69.662
Elapsed time for attention_prob_times_values (128x2048x2048x188): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x188): 64.583

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 460.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x189x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x189x2048): 68.362
Elapsed time for attention_prob_times_values (128x2048x2048x189): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x189): 63.060

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 453.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x190x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x190x2048): 69.829
Elapsed time for attention_prob_times_values (128x2048x2048x190): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x190): 65.255

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 468.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x191x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x191x2048): 69.064
Elapsed time for attention_prob_times_values (128x2048x2048x191): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x191): 63.310

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 460.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x192x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x192x2048): 80.257
Elapsed time for attention_prob_times_values (128x2048x2048x192): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x192): 61.535

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 487.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x193x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x193x2048): 67.911
Elapsed time for attention_prob_times_values (128x2048x2048x193): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x193): 53.568

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 421.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x194x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x194x2048): 69.341
Elapsed time for attention_prob_times_values (128x2048x2048x194): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x194): 56.375

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 439.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x195x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x195x2048): 68.295
Elapsed time for attention_prob_times_values (128x2048x2048x195): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x195): 53.144

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 424.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x196x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x196x2048): 68.730
Elapsed time for attention_prob_times_values (128x2048x2048x196): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x196): 54.888

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 434.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x197x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x197x2048): 68.606
Elapsed time for attention_prob_times_values (128x2048x2048x197): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x197): 53.537

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 430.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x198x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x198x2048): 70.142
Elapsed time for attention_prob_times_values (128x2048x2048x198): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x198): 56.153

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 448.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x199x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x199x2048): 68.966
Elapsed time for attention_prob_times_values (128x2048x2048x199): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x199): 53.372

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 434.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x200x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x200x2048): 71.639
Elapsed time for attention_prob_times_values (128x2048x2048x200): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x200): 52.416

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 438.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x201x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x201x2048): 68.844
Elapsed time for attention_prob_times_values (128x2048x2048x201): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x201): 52.817

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 435.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x202x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x202x2048): 69.979
Elapsed time for attention_prob_times_values (128x2048x2048x202): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x202): 56.271

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 456.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x203x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x203x2048): 69.255
Elapsed time for attention_prob_times_values (128x2048x2048x203): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x203): 53.299

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 442.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x204x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x204x2048): 71.038
Elapsed time for attention_prob_times_values (128x2048x2048x204): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x204): 56.775

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 465.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x205x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x205x2048): 69.741
Elapsed time for attention_prob_times_values (128x2048x2048x205): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x205): 51.251

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 437.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x206x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x206x2048): 71.059
Elapsed time for attention_prob_times_values (128x2048x2048x206): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x206): 53.901

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 455.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x207x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x207x2048): 67.373
Elapsed time for attention_prob_times_values (128x2048x2048x207): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x207): 54.122

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 448.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x208x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x208x2048): 73.401
Elapsed time for attention_prob_times_values (128x2048x2048x208): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x208): 52.858

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 460.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x209x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x209x2048): 66.482
Elapsed time for attention_prob_times_values (128x2048x2048x209): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x209): 52.228

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 440.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x210x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x210x2048): 68.109
Elapsed time for attention_prob_times_values (128x2048x2048x210): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x210): 57.552

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 471.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x211x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x211x2048): 70.390
Elapsed time for attention_prob_times_values (128x2048x2048x211): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x211): 55.027

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 469.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x212x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x212x2048): 69.640
Elapsed time for attention_prob_times_values (128x2048x2048x212): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x212): 58.189

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 483.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x213x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x213x2048): 70.727
Elapsed time for attention_prob_times_values (128x2048x2048x213): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x213): 55.149

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 474.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x214x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x214x2048): 72.160
Elapsed time for attention_prob_times_values (128x2048x2048x214): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x214): 58.510

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 496.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x215x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x215x2048): 68.270
Elapsed time for attention_prob_times_values (128x2048x2048x215): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x215): 55.744

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 473.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x216x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x216x2048): 68.879
Elapsed time for attention_prob_times_values (128x2048x2048x216): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x216): 56.324

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 480.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x217x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x217x2048): 70.951
Elapsed time for attention_prob_times_values (128x2048x2048x217): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x217): 52.852

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 471.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x218x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x218x2048): 72.190
Elapsed time for attention_prob_times_values (128x2048x2048x218): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x218): 59.258

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 508.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x219x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x219x2048): 71.418
Elapsed time for attention_prob_times_values (128x2048x2048x219): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x219): 54.350

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 484.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x220x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x220x2048): 72.954
Elapsed time for attention_prob_times_values (128x2048x2048x220): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x220): 58.142

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 509.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x221x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x221x2048): 71.702
Elapsed time for attention_prob_times_values (128x2048x2048x221): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x221): 57.021

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 502.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x222x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x222x2048): 73.133
Elapsed time for attention_prob_times_values (128x2048x2048x222): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x222): 58.138

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 514.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x223x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x223x2048): 72.294
Elapsed time for attention_prob_times_values (128x2048x2048x223): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x223): 57.366

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 509.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x224x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x224x2048): 82.900
Elapsed time for attention_prob_times_values (128x2048x2048x224): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x224): 57.302

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 542.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x225x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x225x2048): 70.594
Elapsed time for attention_prob_times_values (128x2048x2048x225): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x225): 58.347

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 513.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x226x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x226x2048): 72.160
Elapsed time for attention_prob_times_values (128x2048x2048x226): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x226): 60.900

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 532.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x227x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x227x2048): 70.906
Elapsed time for attention_prob_times_values (128x2048x2048x227): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x227): 58.589

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 519.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x228x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x228x2048): 72.670
Elapsed time for attention_prob_times_values (128x2048x2048x228): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x228): 61.135

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 539.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x229x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x229x2048): 71.209
Elapsed time for attention_prob_times_values (128x2048x2048x229): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x229): 58.943

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 526.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x230x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x230x2048): 70.335
Elapsed time for attention_prob_times_values (128x2048x2048x230): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x230): 59.424

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 527.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x231x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x231x2048): 71.836
Elapsed time for attention_prob_times_values (128x2048x2048x231): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x231): 59.376

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 534.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x232x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x232x2048): 73.746
Elapsed time for attention_prob_times_values (128x2048x2048x232): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x232): 59.855

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 545.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x233x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x233x2048): 70.873
Elapsed time for attention_prob_times_values (128x2048x2048x233): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x233): 56.307

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 519.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x234x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x234x2048): 72.634
Elapsed time for attention_prob_times_values (128x2048x2048x234): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x234): 62.302

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 557.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x235x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x235x2048): 71.236
Elapsed time for attention_prob_times_values (128x2048x2048x235): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x235): 59.807

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 542.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x236x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x236x2048): 70.290
Elapsed time for attention_prob_times_values (128x2048x2048x236): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x236): 60.866

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 546.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x237x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x237x2048): 71.943
Elapsed time for attention_prob_times_values (128x2048x2048x237): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x237): 59.799

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 549.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x238x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x238x2048): 73.405
Elapsed time for attention_prob_times_values (128x2048x2048x238): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x238): 62.950

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 571.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x239x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x239x2048): 72.210
Elapsed time for attention_prob_times_values (128x2048x2048x239): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x239): 60.419

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 557.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x240x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x240x2048): 56.625
Elapsed time for attention_prob_times_values (128x2048x2048x240): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x240): 62.987

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 506.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x241x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x241x2048): 54.093
Elapsed time for attention_prob_times_values (128x2048x2048x241): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x241): 58.275

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 478.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x242x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x242x2048): 55.155
Elapsed time for attention_prob_times_values (128x2048x2048x242): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x242): 63.713

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 506.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x243x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x243x2048): 54.392
Elapsed time for attention_prob_times_values (128x2048x2048x243): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x243): 61.258

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 495.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x244x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x244x2048): 55.544
Elapsed time for attention_prob_times_values (128x2048x2048x244): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x244): 64.484

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 514.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x245x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x245x2048): 54.242
Elapsed time for attention_prob_times_values (128x2048x2048x245): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x245): 61.766

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 499.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x246x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x246x2048): 55.218
Elapsed time for attention_prob_times_values (128x2048x2048x246): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x246): 61.253

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 504.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x247x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x247x2048): 55.227
Elapsed time for attention_prob_times_values (128x2048x2048x247): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x247): 62.120

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 509.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x248x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x248x2048): 56.517
Elapsed time for attention_prob_times_values (128x2048x2048x248): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x248): 63.309

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 522.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x249x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x249x2048): 54.405
Elapsed time for attention_prob_times_values (128x2048x2048x249): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x249): 62.606

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 511.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x250x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x250x2048): 54.941
Elapsed time for attention_prob_times_values (128x2048x2048x250): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x250): 65.285

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 525.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x251x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x251x2048): 54.696
Elapsed time for attention_prob_times_values (128x2048x2048x251): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x251): 60.362

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 507.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x252x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x252x2048): 55.668
Elapsed time for attention_prob_times_values (128x2048x2048x252): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x252): 65.674

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 534.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x253x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x253x2048): 54.630
Elapsed time for attention_prob_times_values (128x2048x2048x253): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x253): 60.137

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 509.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x254x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x254x2048): 55.460
Elapsed time for attention_prob_times_values (128x2048x2048x254): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x254): 66.329

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 539.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x255x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x255x2048): 51.175
Elapsed time for attention_prob_times_values (128x2048x2048x255): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x255): 65.680

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 515.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x256x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x256x2048): 78.323
Elapsed time for attention_prob_times_values (128x2048x2048x256): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x256): 71.080

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 670.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x257x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x257x2048): 57.819
Elapsed time for attention_prob_times_values (128x2048x2048x257): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x257): 52.675

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 497.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x258x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x258x2048): 58.665
Elapsed time for attention_prob_times_values (128x2048x2048x258): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x258): 57.905

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 528.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x259x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x259x2048): 55.581
Elapsed time for attention_prob_times_values (128x2048x2048x259): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x259): 53.854

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 497.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x260x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x260x2048): 59.168
Elapsed time for attention_prob_times_values (128x2048x2048x260): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x260): 58.600

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 537.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x261x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x261x2048): 57.878
Elapsed time for attention_prob_times_values (128x2048x2048x261): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x261): 54.469

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 513.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x262x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x262x2048): 58.451
Elapsed time for attention_prob_times_values (128x2048x2048x262): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x262): 58.759

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 538.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x263x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x263x2048): 56.172
Elapsed time for attention_prob_times_values (128x2048x2048x263): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x263): 55.014

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 512.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x264x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x264x2048): 58.935
Elapsed time for attention_prob_times_values (128x2048x2048x264): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x264): 55.372

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 528.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x265x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x265x2048): 50.729
Elapsed time for attention_prob_times_values (128x2048x2048x265): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x265): 54.211

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 486.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x266x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x266x2048): 55.104
Elapsed time for attention_prob_times_values (128x2048x2048x266): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x266): 56.499

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 519.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x267x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x267x2048): 55.664
Elapsed time for attention_prob_times_values (128x2048x2048x267): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x267): 57.189

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 527.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x268x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x268x2048): 58.131
Elapsed time for attention_prob_times_values (128x2048x2048x268): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x268): 60.384

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 555.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x269x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x269x2048): 57.304
Elapsed time for attention_prob_times_values (128x2048x2048x269): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x269): 57.642

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 540.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x270x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x270x2048): 56.113
Elapsed time for attention_prob_times_values (128x2048x2048x270): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x270): 59.154

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 543.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x271x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x271x2048): 57.548
Elapsed time for attention_prob_times_values (128x2048x2048x271): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x271): 55.722

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 536.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x272x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x272x2048): 56.820
Elapsed time for attention_prob_times_values (128x2048x2048x272): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x272): 73.273

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 608.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x273x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x273x2048): 54.731
Elapsed time for attention_prob_times_values (128x2048x2048x273): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x273): 58.112

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 537.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x274x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x274x2048): 57.795
Elapsed time for attention_prob_times_values (128x2048x2048x274): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x274): 61.137

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 568.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x275x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x275x2048): 56.989
Elapsed time for attention_prob_times_values (128x2048x2048x275): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x275): 57.806

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 550.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x276x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x276x2048): 58.310
Elapsed time for attention_prob_times_values (128x2048x2048x276): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x276): 59.900

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 568.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x277x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x277x2048): 57.685
Elapsed time for attention_prob_times_values (128x2048x2048x277): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x277): 58.072

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 558.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x278x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x278x2048): 56.554
Elapsed time for attention_prob_times_values (128x2048x2048x278): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x278): 61.079

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 568.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x279x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x279x2048): 57.965
Elapsed time for attention_prob_times_values (128x2048x2048x279): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x279): 59.488

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 570.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x280x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x280x2048): 57.352
Elapsed time for attention_prob_times_values (128x2048x2048x280): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x280): 76.268

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 638.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x281x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x281x2048): 57.216
Elapsed time for attention_prob_times_values (128x2048x2048x281): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x281): 59.844

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 572.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x282x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x282x2048): 57.527
Elapsed time for attention_prob_times_values (128x2048x2048x282): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x282): 62.884

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 589.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x283x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x283x2048): 57.470
Elapsed time for attention_prob_times_values (128x2048x2048x283): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x283): 60.401

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 579.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x284x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x284x2048): 58.655
Elapsed time for attention_prob_times_values (128x2048x2048x284): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x284): 63.554

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 602.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x285x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x285x2048): 57.801
Elapsed time for attention_prob_times_values (128x2048x2048x285): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x285): 60.736

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 586.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x286x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x286x2048): 58.321
Elapsed time for attention_prob_times_values (128x2048x2048x286): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x286): 63.049

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 602.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x287x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x287x2048): 55.314
Elapsed time for attention_prob_times_values (128x2048x2048x287): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x287): 61.234

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 579.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x288x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x288x2048): 79.612
Elapsed time for attention_prob_times_values (128x2048x2048x288): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x288): 78.565

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 790.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x289x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x289x2048): 61.731
Elapsed time for attention_prob_times_values (128x2048x2048x289): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x289): 61.655

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 618.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x290x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x290x2048): 62.538
Elapsed time for attention_prob_times_values (128x2048x2048x290): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x290): 64.078

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 636.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x291x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x291x2048): 61.690
Elapsed time for attention_prob_times_values (128x2048x2048x291): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x291): 62.129

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 624.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x292x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x292x2048): 63.529
Elapsed time for attention_prob_times_values (128x2048x2048x292): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x292): 64.947

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 650.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x293x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x293x2048): 59.514
Elapsed time for attention_prob_times_values (128x2048x2048x293): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x293): 62.571

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 619.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x294x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x294x2048): 62.288
Elapsed time for attention_prob_times_values (128x2048x2048x294): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x294): 62.523

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 635.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x295x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x295x2048): 58.350
Elapsed time for attention_prob_times_values (128x2048x2048x295): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x295): 63.061

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 619.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x296x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x296x2048): 63.163
Elapsed time for attention_prob_times_values (128x2048x2048x296): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x296): 82.090

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 731.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x297x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x297x2048): 57.886
Elapsed time for attention_prob_times_values (128x2048x2048x297): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x297): 62.983

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 620.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x298x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x298x2048): 60.771
Elapsed time for attention_prob_times_values (128x2048x2048x298): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x298): 65.937

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 652.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x299x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x299x2048): 60.512
Elapsed time for attention_prob_times_values (128x2048x2048x299): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x299): 63.458

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 640.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x300x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x300x2048): 62.222
Elapsed time for attention_prob_times_values (128x2048x2048x300): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x300): 66.750

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 668.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x301x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x301x2048): 60.936
Elapsed time for attention_prob_times_values (128x2048x2048x301): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x301): 64.404

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 651.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x302x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x302x2048): 61.699
Elapsed time for attention_prob_times_values (128x2048x2048x302): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x302): 67.029

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 670.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x303x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x303x2048): 57.795
Elapsed time for attention_prob_times_values (128x2048x2048x303): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x303): 65.656

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 643.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x304x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x304x2048): 62.533
Elapsed time for attention_prob_times_values (128x2048x2048x304): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x304): 79.552

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 735.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x305x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x305x2048): 59.954
Elapsed time for attention_prob_times_values (128x2048x2048x305): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x305): 65.584

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 659.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x306x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x306x2048): 61.094
Elapsed time for attention_prob_times_values (128x2048x2048x306): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x306): 67.703

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 678.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x307x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x307x2048): 60.665
Elapsed time for attention_prob_times_values (128x2048x2048x307): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x307): 66.475

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 672.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x308x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x308x2048): 62.280
Elapsed time for attention_prob_times_values (128x2048x2048x308): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x308): 66.132

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 681.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x309x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x309x2048): 57.040
Elapsed time for attention_prob_times_values (128x2048x2048x309): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x309): 66.757

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 655.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x310x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x310x2048): 61.652
Elapsed time for attention_prob_times_values (128x2048x2048x310): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x310): 68.303

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 692.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x311x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x311x2048): 61.016
Elapsed time for attention_prob_times_values (128x2048x2048x311): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x311): 66.884

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 684.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x312x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x312x2048): 62.999
Elapsed time for attention_prob_times_values (128x2048x2048x312): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x312): 86.501

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 783.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x313x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x313x2048): 26.234
Elapsed time for attention_prob_times_values (128x2048x2048x313): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x313): 66.966

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 406.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x314x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x314x2048): 61.093
Elapsed time for attention_prob_times_values (128x2048x2048x314): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x314): 69.337

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 702.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x315x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x315x2048): 60.893
Elapsed time for attention_prob_times_values (128x2048x2048x315): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x315): 65.958

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 686.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x316x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x316x2048): 59.595
Elapsed time for attention_prob_times_values (128x2048x2048x316): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x316): 69.472

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 697.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x317x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x317x2048): 61.125
Elapsed time for attention_prob_times_values (128x2048x2048x317): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x317): 67.625

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 700.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x318x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x318x2048): 61.571
Elapsed time for attention_prob_times_values (128x2048x2048x318): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x318): 70.121

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 717.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x319x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x319x2048): 61.059
Elapsed time for attention_prob_times_values (128x2048x2048x319): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x319): 68.139

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 706.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x320x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x320x2048): 80.381
Elapsed time for attention_prob_times_values (128x2048x2048x320): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x320): 84.421

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 905.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x321x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x321x2048): 64.146
Elapsed time for attention_prob_times_values (128x2048x2048x321): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x321): 59.921

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 683.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x322x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x322x2048): 65.258
Elapsed time for attention_prob_times_values (128x2048x2048x322): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x322): 62.851

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 708.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x323x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x323x2048): 63.972
Elapsed time for attention_prob_times_values (128x2048x2048x323): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x323): 60.070

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 687.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x324x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x324x2048): 64.648
Elapsed time for attention_prob_times_values (128x2048x2048x324): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x324): 63.255

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 711.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x325x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x325x2048): 64.061
Elapsed time for attention_prob_times_values (128x2048x2048x325): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x325): 60.345

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 693.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x326x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x326x2048): 64.757
Elapsed time for attention_prob_times_values (128x2048x2048x326): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x326): 60.940

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 702.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x327x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x327x2048): 61.142
Elapsed time for attention_prob_times_values (128x2048x2048x327): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x327): 60.905

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 684.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x328x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x328x2048): 65.758
Elapsed time for attention_prob_times_values (128x2048x2048x328): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x328): 76.198

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 794.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x329x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x329x2048): 62.666
Elapsed time for attention_prob_times_values (128x2048x2048x329): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x329): 59.258

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 687.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x330x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x330x2048): 63.507
Elapsed time for attention_prob_times_values (128x2048x2048x330): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x330): 63.006

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 715.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x331x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x331x2048): 61.477
Elapsed time for attention_prob_times_values (128x2048x2048x331): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x331): 59.190

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 684.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x332x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x332x2048): 64.755
Elapsed time for attention_prob_times_values (128x2048x2048x332): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x332): 64.626

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 735.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x333x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x333x2048): 63.324
Elapsed time for attention_prob_times_values (128x2048x2048x333): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x333): 59.446

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 699.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x334x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x334x2048): 64.126
Elapsed time for attention_prob_times_values (128x2048x2048x334): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x334): 64.750

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 736.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x335x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x335x2048): 61.447
Elapsed time for attention_prob_times_values (128x2048x2048x335): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x335): 59.924

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 695.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x336x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x336x2048): 64.092
Elapsed time for attention_prob_times_values (128x2048x2048x336): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x336): 77.916

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 808.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x337x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x337x2048): 62.553
Elapsed time for attention_prob_times_values (128x2048x2048x337): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x337): 60.191

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 707.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x338x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x338x2048): 62.948
Elapsed time for attention_prob_times_values (128x2048x2048x338): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x338): 63.333

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 730.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x339x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x339x2048): 63.067
Elapsed time for attention_prob_times_values (128x2048x2048x339): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x339): 61.223

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 720.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x340x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x340x2048): 64.014
Elapsed time for attention_prob_times_values (128x2048x2048x340): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x340): 64.584

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 747.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x341x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x341x2048): 63.426
Elapsed time for attention_prob_times_values (128x2048x2048x341): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x341): 60.840

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 723.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x342x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x342x2048): 61.720
Elapsed time for attention_prob_times_values (128x2048x2048x342): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x342): 63.894

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 733.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x343x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x343x2048): 61.465
Elapsed time for attention_prob_times_values (128x2048x2048x343): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x343): 62.232

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 724.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x344x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x344x2048): 64.455
Elapsed time for attention_prob_times_values (128x2048x2048x344): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x344): 79.624

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 837.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x345x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x345x2048): 58.466
Elapsed time for attention_prob_times_values (128x2048x2048x345): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x345): 58.930

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 691.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x346x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x346x2048): 59.062
Elapsed time for attention_prob_times_values (128x2048x2048x346): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x346): 64.380

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 727.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x347x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x347x2048): 62.761
Elapsed time for attention_prob_times_values (128x2048x2048x347): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x347): 60.057

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 726.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x348x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x348x2048): 61.516
Elapsed time for attention_prob_times_values (128x2048x2048x348): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x348): 64.808

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 749.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x349x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x349x2048): 62.941
Elapsed time for attention_prob_times_values (128x2048x2048x349): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x349): 62.923

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 749.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x350x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x350x2048): 61.136
Elapsed time for attention_prob_times_values (128x2048x2048x350): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x350): 62.829

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 739.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x351x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x351x2048): 63.172
Elapsed time for attention_prob_times_values (128x2048x2048x351): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x351): 62.105

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 749.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x352x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x352x2048): 82.756
Elapsed time for attention_prob_times_values (128x2048x2048x352): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x352): 78.127

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 964.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x353x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x353x2048): 62.691
Elapsed time for attention_prob_times_values (128x2048x2048x353): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x353): 64.822

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 766.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x354x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x354x2048): 66.936
Elapsed time for attention_prob_times_values (128x2048x2048x354): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x354): 65.430

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 798.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x355x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x355x2048): 65.945
Elapsed time for attention_prob_times_values (128x2048x2048x355): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x355): 65.033

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 791.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x356x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x356x2048): 67.535
Elapsed time for attention_prob_times_values (128x2048x2048x356): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x356): 62.453

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 786.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x357x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x357x2048): 63.159
Elapsed time for attention_prob_times_values (128x2048x2048x357): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x357): 62.024

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 760.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x358x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x358x2048): 66.396
Elapsed time for attention_prob_times_values (128x2048x2048x358): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x358): 65.983

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 806.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x359x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x359x2048): 65.354
Elapsed time for attention_prob_times_values (128x2048x2048x359): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x359): 64.690

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 794.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x360x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x360x2048): 67.140
Elapsed time for attention_prob_times_values (128x2048x2048x360): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x360): 82.990

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 909.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x361x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x361x2048): 64.472
Elapsed time for attention_prob_times_values (128x2048x2048x361): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x361): 63.500

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 785.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x362x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x362x2048): 63.357
Elapsed time for attention_prob_times_values (128x2048x2048x362): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x362): 66.565

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 799.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x363x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x363x2048): 64.755
Elapsed time for attention_prob_times_values (128x2048x2048x363): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x363): 64.007

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 794.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x364x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x364x2048): 65.979
Elapsed time for attention_prob_times_values (128x2048x2048x364): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x364): 66.706

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 820.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x365x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x365x2048): 64.996
Elapsed time for attention_prob_times_values (128x2048x2048x365): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x365): 64.471

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 803.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x366x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x366x2048): 65.746
Elapsed time for attention_prob_times_values (128x2048x2048x366): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x366): 67.449

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 828.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x367x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x367x2048): 65.141
Elapsed time for attention_prob_times_values (128x2048x2048x367): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x367): 65.113

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 812.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x368x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x368x2048): 66.995
Elapsed time for attention_prob_times_values (128x2048x2048x368): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x368): 84.917

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 936.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x369x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x369x2048): 64.424
Elapsed time for attention_prob_times_values (128x2048x2048x369): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x369): 65.142

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 811.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x370x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x370x2048): 65.341
Elapsed time for attention_prob_times_values (128x2048x2048x370): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x370): 67.944

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 836.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x371x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x371x2048): 63.887
Elapsed time for attention_prob_times_values (128x2048x2048x371): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x371): 65.356

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 813.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x372x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x372x2048): 65.594
Elapsed time for attention_prob_times_values (128x2048x2048x372): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x372): 68.566

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 846.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x373x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x373x2048): 64.382
Elapsed time for attention_prob_times_values (128x2048x2048x373): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x373): 65.817

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 823.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x374x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x374x2048): 65.467
Elapsed time for attention_prob_times_values (128x2048x2048x374): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x374): 68.455

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 849.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x375x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x375x2048): 64.921
Elapsed time for attention_prob_times_values (128x2048x2048x375): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x375): 66.038

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 832.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x376x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x376x2048): 66.602
Elapsed time for attention_prob_times_values (128x2048x2048x376): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x376): 84.820

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 951.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x377x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x377x2048): 62.844
Elapsed time for attention_prob_times_values (128x2048x2048x377): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x377): 66.307

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 824.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x378x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x378x2048): 64.998
Elapsed time for attention_prob_times_values (128x2048x2048x378): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x378): 66.291

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 840.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x379x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x379x2048): 64.529
Elapsed time for attention_prob_times_values (128x2048x2048x379): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x379): 66.581

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 841.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x380x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x380x2048): 62.336
Elapsed time for attention_prob_times_values (128x2048x2048x380): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x380): 69.166

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 844.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x381x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x381x2048): 64.344
Elapsed time for attention_prob_times_values (128x2048x2048x381): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x381): 66.843

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 846.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x382x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x382x2048): 65.297
Elapsed time for attention_prob_times_values (128x2048x2048x382): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x382): 69.479

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 870.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x383x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x383x2048): 64.193
Elapsed time for attention_prob_times_values (128x2048x2048x383): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x383): 66.457

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 846.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x384x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x384x2048): 79.912
Elapsed time for attention_prob_times_values (128x2048x2048x384): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x384): 85.246

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1072.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x385x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x385x2048): 66.798
Elapsed time for attention_prob_times_values (128x2048x2048x385): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x385): 59.639

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 821.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x386x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x386x2048): 68.191
Elapsed time for attention_prob_times_values (128x2048x2048x386): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x386): 63.498

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 859.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x387x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x387x2048): 67.019
Elapsed time for attention_prob_times_values (128x2048x2048x387): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x387): 60.235

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 830.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x388x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x388x2048): 66.199
Elapsed time for attention_prob_times_values (128x2048x2048x388): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x388): 61.822

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 839.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x389x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x389x2048): 67.373
Elapsed time for attention_prob_times_values (128x2048x2048x389): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x389): 60.653

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 839.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x390x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x390x2048): 68.054
Elapsed time for attention_prob_times_values (128x2048x2048x390): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x390): 64.135

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 870.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x391x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x391x2048): 67.151
Elapsed time for attention_prob_times_values (128x2048x2048x391): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x391): 60.986

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 844.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x392x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x392x2048): 68.714
Elapsed time for attention_prob_times_values (128x2048x2048x392): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x392): 77.553

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 965.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x393x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x393x2048): 66.200
Elapsed time for attention_prob_times_values (128x2048x2048x393): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x393): 60.914

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 842.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x394x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x394x2048): 66.894
Elapsed time for attention_prob_times_values (128x2048x2048x394): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x394): 64.588

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 874.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x395x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x395x2048): 66.365
Elapsed time for attention_prob_times_values (128x2048x2048x395): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x395): 58.965

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 833.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x396x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x396x2048): 67.682
Elapsed time for attention_prob_times_values (128x2048x2048x396): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x396): 63.352

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 875.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x397x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x397x2048): 66.746
Elapsed time for attention_prob_times_values (128x2048x2048x397): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x397): 60.046

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 847.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x398x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x398x2048): 65.506
Elapsed time for attention_prob_times_values (128x2048x2048x398): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x398): 65.056

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 877.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x399x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x399x2048): 66.893
Elapsed time for attention_prob_times_values (128x2048x2048x399): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x399): 61.759

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 865.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x400x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x400x2048): 68.828
Elapsed time for attention_prob_times_values (128x2048x2048x400): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x400): 76.942

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 980.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x401x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x401x2048): 63.594
Elapsed time for attention_prob_times_values (128x2048x2048x401): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x401): 62.024

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 849.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x402x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x402x2048): 67.031
Elapsed time for attention_prob_times_values (128x2048x2048x402): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x402): 64.314

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 890.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x403x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x403x2048): 64.868
Elapsed time for attention_prob_times_values (128x2048x2048x403): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x403): 60.289

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 849.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x404x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x404x2048): 67.595
Elapsed time for attention_prob_times_values (128x2048x2048x404): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x404): 66.155

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 911.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x405x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x405x2048): 66.733
Elapsed time for attention_prob_times_values (128x2048x2048x405): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x405): 60.831

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 869.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x406x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x406x2048): 65.207
Elapsed time for attention_prob_times_values (128x2048x2048x406): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x406): 66.169

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 899.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x407x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x407x2048): 66.967
Elapsed time for attention_prob_times_values (128x2048x2048x407): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x407): 62.441

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 886.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x408x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x408x2048): 68.272
Elapsed time for attention_prob_times_values (128x2048x2048x408): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x408): 80.019

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1013.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x409x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x409x2048): 65.339
Elapsed time for attention_prob_times_values (128x2048x2048x409): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x409): 62.860

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 883.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x410x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x410x2048): 66.994
Elapsed time for attention_prob_times_values (128x2048x2048x410): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x410): 64.116

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 905.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x411x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x411x2048): 63.286
Elapsed time for attention_prob_times_values (128x2048x2048x411): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x411): 63.160

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 875.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x412x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x412x2048): 67.835
Elapsed time for attention_prob_times_values (128x2048x2048x412): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x412): 67.169

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 936.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x413x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x413x2048): 66.542
Elapsed time for attention_prob_times_values (128x2048x2048x413): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x413): 63.403

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 902.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x414x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x414x2048): 67.257
Elapsed time for attention_prob_times_values (128x2048x2048x414): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x414): 67.063

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 936.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x415x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x415x2048): 64.738
Elapsed time for attention_prob_times_values (128x2048x2048x415): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x415): 63.762

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 897.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x416x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x416x2048): 85.497
Elapsed time for attention_prob_times_values (128x2048x2048x416): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x416): 78.931

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1149.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x417x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x417x2048): 68.473
Elapsed time for attention_prob_times_values (128x2048x2048x417): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x417): 64.320

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 930.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x418x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x418x2048): 70.742
Elapsed time for attention_prob_times_values (128x2048x2048x418): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x418): 67.792

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 973.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x419x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x419x2048): 69.827
Elapsed time for attention_prob_times_values (128x2048x2048x419): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x419): 64.522

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 945.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x420x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x420x2048): 71.443
Elapsed time for attention_prob_times_values (128x2048x2048x420): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x420): 68.293

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 986.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x421x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x421x2048): 69.640
Elapsed time for attention_prob_times_values (128x2048x2048x421): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x421): 64.532

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 948.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x422x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x422x2048): 70.353
Elapsed time for attention_prob_times_values (128x2048x2048x422): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x422): 68.222

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 982.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x423x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x423x2048): 69.276
Elapsed time for attention_prob_times_values (128x2048x2048x423): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x423): 65.263

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 955.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x424x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x424x2048): 71.318
Elapsed time for attention_prob_times_values (128x2048x2048x424): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x424): 83.502

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1096.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x425x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x425x2048): 68.341
Elapsed time for attention_prob_times_values (128x2048x2048x425): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x425): 65.174

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 952.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x426x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x426x2048): 68.664
Elapsed time for attention_prob_times_values (128x2048x2048x426): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x426): 68.720

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 983.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x427x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x427x2048): 67.267
Elapsed time for attention_prob_times_values (128x2048x2048x427): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x427): 65.141

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 949.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x428x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x428x2048): 68.063
Elapsed time for attention_prob_times_values (128x2048x2048x428): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x428): 69.341

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 987.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x429x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x429x2048): 68.906
Elapsed time for attention_prob_times_values (128x2048x2048x429): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x429): 63.427

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 951.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x430x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x430x2048): 67.567
Elapsed time for attention_prob_times_values (128x2048x2048x430): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x430): 69.358

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 988.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x431x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x431x2048): 68.854
Elapsed time for attention_prob_times_values (128x2048x2048x431): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x431): 65.892

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 974.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x432x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x432x2048): 70.632
Elapsed time for attention_prob_times_values (128x2048x2048x432): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x432): 84.726

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1117.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x433x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x433x2048): 68.122
Elapsed time for attention_prob_times_values (128x2048x2048x433): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x433): 66.365

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 976.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x434x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x434x2048): 68.959
Elapsed time for attention_prob_times_values (128x2048x2048x434): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x434): 69.744

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1009.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x435x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x435x2048): 68.446
Elapsed time for attention_prob_times_values (128x2048x2048x435): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x435): 66.646

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 985.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x436x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x436x2048): 70.082
Elapsed time for attention_prob_times_values (128x2048x2048x436): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x436): 70.354

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1026.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x437x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x437x2048): 68.708
Elapsed time for attention_prob_times_values (128x2048x2048x437): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x437): 67.014

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 994.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x438x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x438x2048): 69.320
Elapsed time for attention_prob_times_values (128x2048x2048x438): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x438): 66.892

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 999.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x439x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x439x2048): 68.893
Elapsed time for attention_prob_times_values (128x2048x2048x439): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x439): 67.371

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1002.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x440x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x440x2048): 68.541
Elapsed time for attention_prob_times_values (128x2048x2048x440): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x440): 86.302

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1126.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x441x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x441x2048): 68.007
Elapsed time for attention_prob_times_values (128x2048x2048x441): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x441): 67.465

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1001.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x442x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x442x2048): 68.660
Elapsed time for attention_prob_times_values (128x2048x2048x442): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x442): 70.920

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1033.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x443x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x443x2048): 68.271
Elapsed time for attention_prob_times_values (128x2048x2048x443): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x443): 67.793

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1009.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x444x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x444x2048): 69.733
Elapsed time for attention_prob_times_values (128x2048x2048x444): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x444): 71.051

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1046.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x445x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x445x2048): 68.509
Elapsed time for attention_prob_times_values (128x2048x2048x445): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x445): 68.106

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1018.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x446x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x446x2048): 69.188
Elapsed time for attention_prob_times_values (128x2048x2048x446): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x446): 71.519

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1050.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x447x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x447x2048): 65.986
Elapsed time for attention_prob_times_values (128x2048x2048x447): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x447): 68.361

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1005.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x448x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x448x2048): 85.977
Elapsed time for attention_prob_times_values (128x2048x2048x448): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x448): 85.128

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1283.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x449x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x449x2048): 71.465
Elapsed time for attention_prob_times_values (128x2048x2048x449): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x449): 62.374

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1001.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x450x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x450x2048): 72.530
Elapsed time for attention_prob_times_values (128x2048x2048x450): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x450): 65.387

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1035.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x451x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x451x2048): 71.445
Elapsed time for attention_prob_times_values (128x2048x2048x451): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x451): 62.502

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1006.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x452x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x452x2048): 73.250
Elapsed time for attention_prob_times_values (128x2048x2048x452): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x452): 65.691

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1047.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x453x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x453x2048): 71.283
Elapsed time for attention_prob_times_values (128x2048x2048x453): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x453): 62.376

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1008.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x454x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x454x2048): 71.858
Elapsed time for attention_prob_times_values (128x2048x2048x454): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x454): 65.607

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1041.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x455x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x455x2048): 71.099
Elapsed time for attention_prob_times_values (128x2048x2048x455): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x455): 62.738

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1014.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x456x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x456x2048): 73.329
Elapsed time for attention_prob_times_values (128x2048x2048x456): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x456): 82.671

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1185.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x457x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x457x2048): 67.832
Elapsed time for attention_prob_times_values (128x2048x2048x457): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x457): 62.487

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 994.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x458x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x458x2048): 70.811
Elapsed time for attention_prob_times_values (128x2048x2048x458): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x458): 66.280

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1048.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x459x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x459x2048): 70.206
Elapsed time for attention_prob_times_values (128x2048x2048x459): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x459): 63.194

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1020.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x460x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x460x2048): 69.638
Elapsed time for attention_prob_times_values (128x2048x2048x460): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x460): 66.630

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1047.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x461x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x461x2048): 70.293
Elapsed time for attention_prob_times_values (128x2048x2048x461): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x461): 63.598

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1028.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x462x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x462x2048): 71.161
Elapsed time for attention_prob_times_values (128x2048x2048x462): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x462): 66.926

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1064.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x463x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x463x2048): 68.112
Elapsed time for attention_prob_times_values (128x2048x2048x463): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x463): 61.726

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1001.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x464x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x464x2048): 69.600
Elapsed time for attention_prob_times_values (128x2048x2048x464): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x464): 86.907

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1198.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x465x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x465x2048): 69.558
Elapsed time for attention_prob_times_values (128x2048x2048x465): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x465): 63.909

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1034.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x466x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x466x2048): 70.501
Elapsed time for attention_prob_times_values (128x2048x2048x466): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x466): 67.440

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1072.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x467x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x467x2048): 69.859
Elapsed time for attention_prob_times_values (128x2048x2048x467): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x467): 64.213

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1043.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x468x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x468x2048): 71.428
Elapsed time for attention_prob_times_values (128x2048x2048x468): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x468): 67.915

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1087.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x469x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x469x2048): 69.887
Elapsed time for attention_prob_times_values (128x2048x2048x469): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x469): 64.423

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1049.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x470x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x470x2048): 68.243
Elapsed time for attention_prob_times_values (128x2048x2048x470): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x470): 67.928

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1068.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x471x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x471x2048): 70.341
Elapsed time for attention_prob_times_values (128x2048x2048x471): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x471): 64.501

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1057.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x472x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x472x2048): 69.275
Elapsed time for attention_prob_times_values (128x2048x2048x472): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x472): 83.006

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1189.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x473x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x473x2048): 66.863
Elapsed time for attention_prob_times_values (128x2048x2048x473): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x473): 64.736

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1038.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x474x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x474x2048): 70.214
Elapsed time for attention_prob_times_values (128x2048x2048x474): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x474): 68.319

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1095.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x475x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x475x2048): 69.681
Elapsed time for attention_prob_times_values (128x2048x2048x475): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x475): 65.012

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1065.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x476x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x476x2048): 71.043
Elapsed time for attention_prob_times_values (128x2048x2048x476): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x476): 66.953

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1094.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x477x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x477x2048): 69.266
Elapsed time for attention_prob_times_values (128x2048x2048x477): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x477): 65.256

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1068.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x478x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x478x2048): 70.459
Elapsed time for attention_prob_times_values (128x2048x2048x478): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x478): 66.716

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1092.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x479x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x479x2048): 67.386
Elapsed time for attention_prob_times_values (128x2048x2048x479): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x479): 63.256

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1042.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x480x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x480x2048): 84.058
Elapsed time for attention_prob_times_values (128x2048x2048x480): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x480): 90.718

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1396.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x481x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x481x2048): 72.205
Elapsed time for attention_prob_times_values (128x2048x2048x481): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x481): 64.700

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1094.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x482x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x482x2048): 73.544
Elapsed time for attention_prob_times_values (128x2048x2048x482): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x482): 68.331

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1137.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x483x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x483x2048): 72.426
Elapsed time for attention_prob_times_values (128x2048x2048x483): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x483): 66.020

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1111.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x484x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x484x2048): 74.050
Elapsed time for attention_prob_times_values (128x2048x2048x484): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x484): 68.504

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1147.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x485x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x485x2048): 72.087
Elapsed time for attention_prob_times_values (128x2048x2048x485): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x485): 66.149

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1114.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x486x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x486x2048): 72.810
Elapsed time for attention_prob_times_values (128x2048x2048x486): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x486): 68.356

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1141.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x487x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x487x2048): 69.784
Elapsed time for attention_prob_times_values (128x2048x2048x487): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x487): 66.519

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1104.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x488x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x488x2048): 73.595
Elapsed time for attention_prob_times_values (128x2048x2048x488): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x488): 91.038

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1322.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x489x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x489x2048): 68.566
Elapsed time for attention_prob_times_values (128x2048x2048x489): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x489): 64.008

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1077.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x490x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x490x2048): 69.885
Elapsed time for attention_prob_times_values (128x2048x2048x490): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x490): 68.644

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1129.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x491x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x491x2048): 71.175
Elapsed time for attention_prob_times_values (128x2048x2048x491): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x491): 66.753

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1125.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x492x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x492x2048): 72.371
Elapsed time for attention_prob_times_values (128x2048x2048x492): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x492): 69.120

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1157.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x493x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x493x2048): 71.265
Elapsed time for attention_prob_times_values (128x2048x2048x493): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x493): 66.916

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1132.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x494x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x494x2048): 72.039
Elapsed time for attention_prob_times_values (128x2048x2048x494): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x494): 70.233

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1169.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x495x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x495x2048): 71.342
Elapsed time for attention_prob_times_values (128x2048x2048x495): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x495): 66.978

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1137.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x496x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x496x2048): 73.429
Elapsed time for attention_prob_times_values (128x2048x2048x496): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x496): 90.418

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1337.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x497x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x497x2048): 70.604
Elapsed time for attention_prob_times_values (128x2048x2048x497): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x497): 67.325

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1139.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x498x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x498x2048): 71.675
Elapsed time for attention_prob_times_values (128x2048x2048x498): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x498): 69.669

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1170.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x499x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x499x2048): 70.797
Elapsed time for attention_prob_times_values (128x2048x2048x499): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x499): 67.521

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1146.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x500x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x500x2048): 72.044
Elapsed time for attention_prob_times_values (128x2048x2048x500): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x500): 70.049

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1180.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x501x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x501x2048): 70.955
Elapsed time for attention_prob_times_values (128x2048x2048x501): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x501): 66.117

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1140.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x502x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x502x2048): 71.867
Elapsed time for attention_prob_times_values (128x2048x2048x502): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x502): 69.761

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1181.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x503x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x503x2048): 71.180
Elapsed time for attention_prob_times_values (128x2048x2048x503): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x503): 66.590

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1150.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x504x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x504x2048): 70.332
Elapsed time for attention_prob_times_values (128x2048x2048x504): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x504): 91.029

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1329.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x505x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x505x2048): 70.349
Elapsed time for attention_prob_times_values (128x2048x2048x505): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x505): 61.332

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1099.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x506x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x506x2048): 71.104
Elapsed time for attention_prob_times_values (128x2048x2048x506): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x506): 68.803

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1175.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x507x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x507x2048): 70.410
Elapsed time for attention_prob_times_values (128x2048x2048x507): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x507): 61.750

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1108.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x508x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x508x2048): 71.886
Elapsed time for attention_prob_times_values (128x2048x2048x508): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x508): 68.884

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1187.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x509x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x509x2048): 70.300
Elapsed time for attention_prob_times_values (128x2048x2048x509): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x509): 61.486

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1109.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x510x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x510x2048): 70.395
Elapsed time for attention_prob_times_values (128x2048x2048x510): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x510): 69.301

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1182.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x511x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x511x2048): 68.378
Elapsed time for attention_prob_times_values (128x2048x2048x511): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x511): 65.418

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1134.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x512x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x512x2048): 84.346
Elapsed time for attention_prob_times_values (128x2048x2048x512): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x512): 96.209

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1528.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x513x2048): 0.0185
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x513x2048): 29.705
Elapsed time for attention_prob_times_values (128x2048x2048x513): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x513): 61.080

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 680.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x514x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x514x2048): 74.769
Elapsed time for attention_prob_times_values (128x2048x2048x514): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x514): 66.634

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1202.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x515x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x515x2048): 72.582
Elapsed time for attention_prob_times_values (128x2048x2048x515): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x515): 63.144

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1154.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x516x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x516x2048): 73.361
Elapsed time for attention_prob_times_values (128x2048x2048x516): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x516): 65.962

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1189.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x517x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x517x2048): 71.819
Elapsed time for attention_prob_times_values (128x2048x2048x517): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x517): 63.756

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1158.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x518x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x518x2048): 73.734
Elapsed time for attention_prob_times_values (128x2048x2048x518): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x518): 66.839

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1205.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x519x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x519x2048): 72.655
Elapsed time for attention_prob_times_values (128x2048x2048x519): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x519): 63.868

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1170.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x520x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x520x2048): 74.265
Elapsed time for attention_prob_times_values (128x2048x2048x520): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x520): 77.751

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1310.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x521x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x521x2048): 71.711
Elapsed time for attention_prob_times_values (128x2048x2048x521): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x521): 62.744

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1156.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x522x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x522x2048): 72.645
Elapsed time for attention_prob_times_values (128x2048x2048x522): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x522): 66.112

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1198.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x523x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x523x2048): 70.594
Elapsed time for attention_prob_times_values (128x2048x2048x523): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x523): 62.608

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1150.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x524x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x524x2048): 71.354
Elapsed time for attention_prob_times_values (128x2048x2048x524): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x524): 67.819

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1208.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x525x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x525x2048): 72.188
Elapsed time for attention_prob_times_values (128x2048x2048x525): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x525): 63.439

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1175.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x526x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x526x2048): 73.051
Elapsed time for attention_prob_times_values (128x2048x2048x526): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x526): 67.824

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1226.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x527x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x527x2048): 72.263
Elapsed time for attention_prob_times_values (128x2048x2048x527): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x527): 64.437

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1190.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x528x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x528x2048): 72.191
Elapsed time for attention_prob_times_values (128x2048x2048x528): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x528): 79.079

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1320.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x529x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x529x2048): 70.496
Elapsed time for attention_prob_times_values (128x2048x2048x529): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x529): 64.261

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1178.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x530x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x530x2048): 72.445
Elapsed time for attention_prob_times_values (128x2048x2048x530): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x530): 66.228

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1215.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x531x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x531x2048): 71.366
Elapsed time for attention_prob_times_values (128x2048x2048x531): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x531): 64.334

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1190.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x532x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x532x2048): 72.933
Elapsed time for attention_prob_times_values (128x2048x2048x532): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x532): 68.696

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1246.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x533x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x533x2048): 72.087
Elapsed time for attention_prob_times_values (128x2048x2048x533): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x533): 64.215

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1199.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x534x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x534x2048): 72.854
Elapsed time for attention_prob_times_values (128x2048x2048x534): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x534): 68.571

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1249.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x535x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x535x2048): 72.394
Elapsed time for attention_prob_times_values (128x2048x2048x535): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x535): 63.851

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1202.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x536x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x536x2048): 72.114
Elapsed time for attention_prob_times_values (128x2048x2048x536): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x536): 79.939

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1345.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x537x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x537x2048): 69.929
Elapsed time for attention_prob_times_values (128x2048x2048x537): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x537): 64.961

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1197.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x538x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x538x2048): 72.251
Elapsed time for attention_prob_times_values (128x2048x2048x538): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x538): 67.501

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1243.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x539x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x539x2048): 69.180
Elapsed time for attention_prob_times_values (128x2048x2048x539): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x539): 63.366

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1180.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x540x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x540x2048): 71.301
Elapsed time for attention_prob_times_values (128x2048x2048x540): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x540): 69.282

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1256.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x541x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x541x2048): 71.838
Elapsed time for attention_prob_times_values (128x2048x2048x541): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x541): 64.607

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1218.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x542x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x542x2048): 72.643
Elapsed time for attention_prob_times_values (128x2048x2048x542): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x542): 67.454

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1254.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x543x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x543x2048): 72.012
Elapsed time for attention_prob_times_values (128x2048x2048x543): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x543): 64.458

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1222.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x544x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x544x2048): 89.232
Elapsed time for attention_prob_times_values (128x2048x2048x544): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x544): 82.180

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1540.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x545x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x545x2048): 74.792
Elapsed time for attention_prob_times_values (128x2048x2048x545): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x545): 65.909

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1263.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x546x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x546x2048): 74.654
Elapsed time for attention_prob_times_values (128x2048x2048x546): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x546): 67.768

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1283.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x547x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x547x2048): 73.211
Elapsed time for attention_prob_times_values (128x2048x2048x547): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x547): 65.624

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1252.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x548x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x548x2048): 76.261
Elapsed time for attention_prob_times_values (128x2048x2048x548): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x548): 68.408

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1307.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x549x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x549x2048): 74.546
Elapsed time for attention_prob_times_values (128x2048x2048x549): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x549): 66.744

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1278.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x550x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x550x2048): 75.135
Elapsed time for attention_prob_times_values (128x2048x2048x550): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x550): 69.122

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1309.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x551x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x551x2048): 74.200
Elapsed time for attention_prob_times_values (128x2048x2048x551): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x551): 66.467

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1277.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x552x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x552x2048): 76.045
Elapsed time for attention_prob_times_values (128x2048x2048x552): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x552): 82.209

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1441.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x553x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x553x2048): 72.356
Elapsed time for attention_prob_times_values (128x2048x2048x553): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x553): 65.782

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1259.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x554x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x554x2048): 72.834
Elapsed time for attention_prob_times_values (128x2048x2048x554): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x554): 69.739

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1304.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x555x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x555x2048): 73.299
Elapsed time for attention_prob_times_values (128x2048x2048x555): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x555): 65.762

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1271.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x556x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x556x2048): 74.959
Elapsed time for attention_prob_times_values (128x2048x2048x556): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x556): 70.981

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1339.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x557x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x557x2048): 72.421
Elapsed time for attention_prob_times_values (128x2048x2048x557): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x557): 66.421

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1275.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x558x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x558x2048): 72.725
Elapsed time for attention_prob_times_values (128x2048x2048x558): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x558): 71.164

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1326.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x559x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x559x2048): 73.741
Elapsed time for attention_prob_times_values (128x2048x2048x559): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x559): 67.761

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1304.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x560x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x560x2048): 75.668
Elapsed time for attention_prob_times_values (128x2048x2048x560): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x560): 80.951

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1447.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x561x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x561x2048): 71.294
Elapsed time for attention_prob_times_values (128x2048x2048x561): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x561): 65.481

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1265.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x562x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x562x2048): 71.585
Elapsed time for attention_prob_times_values (128x2048x2048x562): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x562): 71.526

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1328.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x563x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x563x2048): 73.453
Elapsed time for attention_prob_times_values (128x2048x2048x563): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x563): 67.411

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1307.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x564x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x564x2048): 74.918
Elapsed time for attention_prob_times_values (128x2048x2048x564): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x564): 72.007

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1367.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x565x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x565x2048): 73.567
Elapsed time for attention_prob_times_values (128x2048x2048x565): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x565): 68.429

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1322.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x566x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x566x2048): 72.109
Elapsed time for attention_prob_times_values (128x2048x2048x566): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x566): 72.104

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1347.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x567x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x567x2048): 73.778
Elapsed time for attention_prob_times_values (128x2048x2048x567): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x567): 68.583

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1330.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x568x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x568x2048): 75.361
Elapsed time for attention_prob_times_values (128x2048x2048x568): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x568): 84.502

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1493.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x569x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x569x2048): 70.444
Elapsed time for attention_prob_times_values (128x2048x2048x569): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x569): 68.344

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1303.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x570x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x570x2048): 70.915
Elapsed time for attention_prob_times_values (128x2048x2048x570): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x570): 71.854

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1342.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x571x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x571x2048): 73.155
Elapsed time for attention_prob_times_values (128x2048x2048x571): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x571): 66.794

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1315.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x572x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x572x2048): 71.363
Elapsed time for attention_prob_times_values (128x2048x2048x572): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x572): 69.073

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1325.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x573x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x573x2048): 71.575
Elapsed time for attention_prob_times_values (128x2048x2048x573): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x573): 68.182

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1320.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x574x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x574x2048): 70.938
Elapsed time for attention_prob_times_values (128x2048x2048x574): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x574): 72.694

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1359.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x575x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x575x2048): 73.104
Elapsed time for attention_prob_times_values (128x2048x2048x575): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x575): 68.417

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1340.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x576x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x576x2048): 89.371
Elapsed time for attention_prob_times_values (128x2048x2048x576): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x576): 86.985

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1675.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x577x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x577x2048): 75.558
Elapsed time for attention_prob_times_values (128x2048x2048x577): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x577): 63.558

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1313.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x578x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x578x2048): 76.794
Elapsed time for attention_prob_times_values (128x2048x2048x578): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x578): 66.480

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1358.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x579x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x579x2048): 75.822
Elapsed time for attention_prob_times_values (128x2048x2048x579): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x579): 63.017

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1314.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x580x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x580x2048): 77.515
Elapsed time for attention_prob_times_values (128x2048x2048x580): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x580): 65.173

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1354.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x581x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x581x2048): 73.512
Elapsed time for attention_prob_times_values (128x2048x2048x581): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x581): 62.077

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1289.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x582x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x582x2048): 76.223
Elapsed time for attention_prob_times_values (128x2048x2048x582): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x582): 66.906

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1367.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x583x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x583x2048): 77.540
Elapsed time for attention_prob_times_values (128x2048x2048x583): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x583): 64.070

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1348.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x584x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x584x2048): 77.660
Elapsed time for attention_prob_times_values (128x2048x2048x584): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x584): 83.445

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1548.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x585x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x585x2048): 71.891
Elapsed time for attention_prob_times_values (128x2048x2048x585): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x585): 60.524

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1267.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x586x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x586x2048): 72.396
Elapsed time for attention_prob_times_values (128x2048x2048x586): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x586): 65.258

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1325.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x587x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x587x2048): 71.662
Elapsed time for attention_prob_times_values (128x2048x2048x587): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x587): 64.176

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1309.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x588x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x588x2048): 77.920
Elapsed time for attention_prob_times_values (128x2048x2048x588): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x588): 67.630

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1402.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x589x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x589x2048): 74.827
Elapsed time for attention_prob_times_values (128x2048x2048x589): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x589): 63.521

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1333.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x590x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x590x2048): 74.064
Elapsed time for attention_prob_times_values (128x2048x2048x590): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x590): 64.197

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1336.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x591x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x591x2048): 71.578
Elapsed time for attention_prob_times_values (128x2048x2048x591): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x591): 64.596

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1322.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x592x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x592x2048): 73.676
Elapsed time for attention_prob_times_values (128x2048x2048x592): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x592): 88.776

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1570.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x593x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x593x2048): 74.049
Elapsed time for attention_prob_times_values (128x2048x2048x593): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x593): 64.796

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1349.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x594x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x594x2048): 72.564
Elapsed time for attention_prob_times_values (128x2048x2048x594): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x594): 67.813

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1371.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x595x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x595x2048): 73.995
Elapsed time for attention_prob_times_values (128x2048x2048x595): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x595): 65.132

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1357.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x596x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x596x2048): 71.723
Elapsed time for attention_prob_times_values (128x2048x2048x596): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x596): 68.515

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1375.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x597x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x597x2048): 74.424
Elapsed time for attention_prob_times_values (128x2048x2048x597): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x597): 65.319

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1367.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x598x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x598x2048): 75.100
Elapsed time for attention_prob_times_values (128x2048x2048x598): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x598): 68.425

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1409.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x599x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x599x2048): 72.772
Elapsed time for attention_prob_times_values (128x2048x2048x599): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x599): 65.183

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1356.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x600x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x600x2048): 76.416
Elapsed time for attention_prob_times_values (128x2048x2048x600): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x600): 89.499

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1628.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x601x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x601x2048): 73.997
Elapsed time for attention_prob_times_values (128x2048x2048x601): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x601): 65.513

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1374.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x602x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x602x2048): 74.524
Elapsed time for attention_prob_times_values (128x2048x2048x602): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x602): 67.752

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1406.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x603x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x603x2048): 70.585
Elapsed time for attention_prob_times_values (128x2048x2048x603): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x603): 61.441

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1303.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x604x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x604x2048): 73.546
Elapsed time for attention_prob_times_values (128x2048x2048x604): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x604): 66.818

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1391.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x605x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x605x2048): 71.480
Elapsed time for attention_prob_times_values (128x2048x2048x605): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x605): 66.054

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1366.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x606x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x606x2048): 75.089
Elapsed time for attention_prob_times_values (128x2048x2048x606): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x606): 69.267

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1436.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x607x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x607x2048): 74.449
Elapsed time for attention_prob_times_values (128x2048x2048x607): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x607): 64.118

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1375.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x608x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x608x2048): 87.912
Elapsed time for attention_prob_times_values (128x2048x2048x608): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x608): 92.024

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1798.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x609x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x609x2048): 74.079
Elapsed time for attention_prob_times_values (128x2048x2048x609): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x609): 66.538

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1404.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x610x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x610x2048): 77.670
Elapsed time for attention_prob_times_values (128x2048x2048x610): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x610): 67.985

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1454.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x611x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x611x2048): 76.330
Elapsed time for attention_prob_times_values (128x2048x2048x611): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x611): 66.627

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1429.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x612x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x612x2048): 78.088
Elapsed time for attention_prob_times_values (128x2048x2048x612): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x612): 69.924

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1484.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x613x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x613x2048): 76.151
Elapsed time for attention_prob_times_values (128x2048x2048x613): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x613): 66.907

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1435.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x614x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x614x2048): 76.937
Elapsed time for attention_prob_times_values (128x2048x2048x614): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x614): 69.621

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1475.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x615x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x615x2048): 75.878
Elapsed time for attention_prob_times_values (128x2048x2048x615): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x615): 67.251

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1441.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x616x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x616x2048): 77.827
Elapsed time for attention_prob_times_values (128x2048x2048x616): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x616): 91.737

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1705.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x617x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x617x2048): 74.989
Elapsed time for attention_prob_times_values (128x2048x2048x617): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x617): 67.116

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1436.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x618x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x618x2048): 75.798
Elapsed time for attention_prob_times_values (128x2048x2048x618): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x618): 69.228

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1469.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x619x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x619x2048): 73.777
Elapsed time for attention_prob_times_values (128x2048x2048x619): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x619): 67.359

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1432.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x620x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x620x2048): 76.506
Elapsed time for attention_prob_times_values (128x2048x2048x620): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x620): 70.623

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1496.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x621x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x621x2048): 75.294
Elapsed time for attention_prob_times_values (128x2048x2048x621): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x621): 67.590

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1453.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x622x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x622x2048): 76.243
Elapsed time for attention_prob_times_values (128x2048x2048x622): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x622): 70.401

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1496.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x623x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x623x2048): 74.792
Elapsed time for attention_prob_times_values (128x2048x2048x623): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x623): 66.613

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1442.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x624x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x624x2048): 77.343
Elapsed time for attention_prob_times_values (128x2048x2048x624): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x624): 93.385

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1734.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x625x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x625x2048): 74.692
Elapsed time for attention_prob_times_values (128x2048x2048x625): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x625): 67.853

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1459.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x626x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x626x2048): 75.512
Elapsed time for attention_prob_times_values (128x2048x2048x626): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x626): 70.946

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1504.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x627x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x627x2048): 74.774
Elapsed time for attention_prob_times_values (128x2048x2048x627): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x627): 68.111

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1468.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x628x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x628x2048): 76.135
Elapsed time for attention_prob_times_values (128x2048x2048x628): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x628): 70.960

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1515.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x629x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x629x2048): 74.905
Elapsed time for attention_prob_times_values (128x2048x2048x629): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x629): 68.342

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1476.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x630x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x630x2048): 75.851
Elapsed time for attention_prob_times_values (128x2048x2048x630): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x630): 70.988

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1517.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x631x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x631x2048): 75.149
Elapsed time for attention_prob_times_values (128x2048x2048x631): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x631): 68.330

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1482.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x632x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x632x2048): 76.435
Elapsed time for attention_prob_times_values (128x2048x2048x632): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x632): 94.061

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1749.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x633x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x633x2048): 74.377
Elapsed time for attention_prob_times_values (128x2048x2048x633): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x633): 68.394

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1480.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x634x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x634x2048): 75.175
Elapsed time for attention_prob_times_values (128x2048x2048x634): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x634): 70.485

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1514.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x635x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x635x2048): 74.439
Elapsed time for attention_prob_times_values (128x2048x2048x635): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x635): 68.459

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1486.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x636x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x636x2048): 75.440
Elapsed time for attention_prob_times_values (128x2048x2048x636): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x636): 70.428

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1520.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x637x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x637x2048): 73.868
Elapsed time for attention_prob_times_values (128x2048x2048x637): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x637): 69.119

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1493.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x638x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x638x2048): 75.152
Elapsed time for attention_prob_times_values (128x2048x2048x638): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x638): 71.511

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1534.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x639x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x639x2048): 74.233
Elapsed time for attention_prob_times_values (128x2048x2048x639): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x639): 67.587

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1483.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x640x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x640x2048): 86.473
Elapsed time for attention_prob_times_values (128x2048x2048x640): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x640): 97.385

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1923.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x641x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x641x2048): 76.296
Elapsed time for attention_prob_times_values (128x2048x2048x641): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x641): 62.779

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1448.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x642x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x642x2048): 77.351
Elapsed time for attention_prob_times_values (128x2048x2048x642): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x642): 66.470

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1505.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x643x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x643x2048): 75.916
Elapsed time for attention_prob_times_values (128x2048x2048x643): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x643): 63.836

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1462.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x644x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x644x2048): 77.907
Elapsed time for attention_prob_times_values (128x2048x2048x644): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x644): 66.235

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1512.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x645x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x645x2048): 74.422
Elapsed time for attention_prob_times_values (128x2048x2048x645): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x645): 64.090

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1457.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x646x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x646x2048): 77.183
Elapsed time for attention_prob_times_values (128x2048x2048x646): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x646): 66.722

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1516.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x647x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x647x2048): 76.220
Elapsed time for attention_prob_times_values (128x2048x2048x647): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x647): 64.088

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1477.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x648x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x648x2048): 73.508
Elapsed time for attention_prob_times_values (128x2048x2048x648): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x648): 77.225

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1600.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x649x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x649x2048): 71.754
Elapsed time for attention_prob_times_values (128x2048x2048x649): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x649): 62.436

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1420.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x650x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x650x2048): 75.404
Elapsed time for attention_prob_times_values (128x2048x2048x650): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x650): 67.198

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1514.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x651x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x651x2048): 75.303
Elapsed time for attention_prob_times_values (128x2048x2048x651): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x651): 64.460

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1482.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x652x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x652x2048): 76.781
Elapsed time for attention_prob_times_values (128x2048x2048x652): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x652): 67.247

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1532.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x653x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x653x2048): 75.762
Elapsed time for attention_prob_times_values (128x2048x2048x653): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x653): 64.427

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1490.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x654x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x654x2048): 76.515
Elapsed time for attention_prob_times_values (128x2048x2048x654): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x654): 67.161

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1533.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x655x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x655x2048): 75.612
Elapsed time for attention_prob_times_values (128x2048x2048x655): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x655): 64.686

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1496.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x656x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x656x2048): 77.886
Elapsed time for attention_prob_times_values (128x2048x2048x656): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x656): 81.943

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1717.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x657x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x657x2048): 75.239
Elapsed time for attention_prob_times_values (128x2048x2048x657): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x657): 64.984

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1501.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x658x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x658x2048): 76.158
Elapsed time for attention_prob_times_values (128x2048x2048x658): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x658): 67.925

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1548.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x659x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x659x2048): 75.378
Elapsed time for attention_prob_times_values (128x2048x2048x659): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x659): 65.041

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1507.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x660x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x660x2048): 76.750
Elapsed time for attention_prob_times_values (128x2048x2048x660): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x660): 68.166

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1561.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x661x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x661x2048): 75.473
Elapsed time for attention_prob_times_values (128x2048x2048x661): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x661): 65.294

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1516.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x662x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x662x2048): 76.352
Elapsed time for attention_prob_times_values (128x2048x2048x662): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x662): 67.936

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1559.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x663x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x663x2048): 75.728
Elapsed time for attention_prob_times_values (128x2048x2048x663): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x663): 65.444

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1524.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x664x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x664x2048): 77.604
Elapsed time for attention_prob_times_values (128x2048x2048x664): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x664): 82.382

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1738.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x665x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x665x2048): 74.820
Elapsed time for attention_prob_times_values (128x2048x2048x665): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x665): 64.538

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1509.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x666x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x666x2048): 75.096
Elapsed time for attention_prob_times_values (128x2048x2048x666): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x666): 68.199

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1559.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x667x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x667x2048): 75.265
Elapsed time for attention_prob_times_values (128x2048x2048x667): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x667): 65.564

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1530.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x668x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x668x2048): 75.101
Elapsed time for attention_prob_times_values (128x2048x2048x668): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x668): 68.555

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1567.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x669x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x669x2048): 75.334
Elapsed time for attention_prob_times_values (128x2048x2048x669): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x669): 65.895

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1539.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x670x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x670x2048): 76.224
Elapsed time for attention_prob_times_values (128x2048x2048x670): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x670): 68.584

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1583.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x671x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x671x2048): 75.652
Elapsed time for attention_prob_times_values (128x2048x2048x671): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x671): 65.992

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1548.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x672x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x672x2048): 91.807
Elapsed time for attention_prob_times_values (128x2048x2048x672): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x672): 84.990

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1941.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x673x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x673x2048): 75.639
Elapsed time for attention_prob_times_values (128x2048x2048x673): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x673): 64.291

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1531.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x674x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x674x2048): 76.734
Elapsed time for attention_prob_times_values (128x2048x2048x674): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x674): 68.697

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1599.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x675x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x675x2048): 74.724
Elapsed time for attention_prob_times_values (128x2048x2048x675): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x675): 66.175

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1550.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x676x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x676x2048): 79.499
Elapsed time for attention_prob_times_values (128x2048x2048x676): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x676): 68.975

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1634.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x677x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x677x2048): 77.731
Elapsed time for attention_prob_times_values (128x2048x2048x677): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x677): 64.717

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1564.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x678x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x678x2048): 78.509
Elapsed time for attention_prob_times_values (128x2048x2048x678): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x678): 68.424

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1622.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x679x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x679x2048): 75.121
Elapsed time for attention_prob_times_values (128x2048x2048x679): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x679): 65.472

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1554.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x680x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x680x2048): 81.984
Elapsed time for attention_prob_times_values (128x2048x2048x680): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x680): 84.882

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1855.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x681x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x681x2048): 75.604
Elapsed time for attention_prob_times_values (128x2048x2048x681): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x681): 66.194

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1572.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x682x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x682x2048): 77.733
Elapsed time for attention_prob_times_values (128x2048x2048x682): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x682): 68.986

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1631.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x683x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x683x2048): 77.018
Elapsed time for attention_prob_times_values (128x2048x2048x683): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x683): 63.432

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1554.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x684x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x684x2048): 78.913
Elapsed time for attention_prob_times_values (128x2048x2048x684): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x684): 68.116

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1636.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x685x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x685x2048): 73.764
Elapsed time for attention_prob_times_values (128x2048x2048x685): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x685): 62.775

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1519.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x686x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x686x2048): 76.026
Elapsed time for attention_prob_times_values (128x2048x2048x686): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x686): 64.694

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1568.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x687x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x687x2048): 76.165
Elapsed time for attention_prob_times_values (128x2048x2048x687): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x687): 66.994

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1601.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x688x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x688x2048): 72.637
Elapsed time for attention_prob_times_values (128x2048x2048x688): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x688): 86.554

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1777.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x689x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x689x2048): 76.415
Elapsed time for attention_prob_times_values (128x2048x2048x689): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x689): 66.984

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1608.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x690x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x690x2048): 77.385
Elapsed time for attention_prob_times_values (128x2048x2048x690): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x690): 70.037

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1658.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x691x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x691x2048): 76.769
Elapsed time for attention_prob_times_values (128x2048x2048x691): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x691): 66.619

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1611.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x692x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x692x2048): 76.861
Elapsed time for attention_prob_times_values (128x2048x2048x692): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x692): 70.063

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1658.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x693x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x693x2048): 75.796
Elapsed time for attention_prob_times_values (128x2048x2048x693): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x693): 67.305

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1615.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x694x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x694x2048): 77.583
Elapsed time for attention_prob_times_values (128x2048x2048x694): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x694): 70.312

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1673.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x695x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x695x2048): 75.044
Elapsed time for attention_prob_times_values (128x2048x2048x695): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x695): 67.883

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1619.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x696x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x696x2048): 77.545
Elapsed time for attention_prob_times_values (128x2048x2048x696): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x696): 86.814

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1863.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x697x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x697x2048): 76.354
Elapsed time for attention_prob_times_values (128x2048x2048x697): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x697): 68.087

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1639.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x698x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x698x2048): 77.045
Elapsed time for attention_prob_times_values (128x2048x2048x698): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x698): 70.790

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1683.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x699x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x699x2048): 76.490
Elapsed time for attention_prob_times_values (128x2048x2048x699): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x699): 68.081

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1645.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x700x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x700x2048): 78.207
Elapsed time for attention_prob_times_values (128x2048x2048x700): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x700): 70.504

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1696.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x701x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x701x2048): 76.599
Elapsed time for attention_prob_times_values (128x2048x2048x701): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x701): 67.965

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1649.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x702x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x702x2048): 77.473
Elapsed time for attention_prob_times_values (128x2048x2048x702): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x702): 70.910

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1698.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x703x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x703x2048): 76.519
Elapsed time for attention_prob_times_values (128x2048x2048x703): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x703): 68.323

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1658.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x704x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x704x2048): 92.279
Elapsed time for attention_prob_times_values (128x2048x2048x704): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x704): 86.648

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2055.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x705x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x705x2048): 78.873
Elapsed time for attention_prob_times_values (128x2048x2048x705): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x705): 65.190

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1644.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x706x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x706x2048): 79.882
Elapsed time for attention_prob_times_values (128x2048x2048x706): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x706): 68.121

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1695.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x707x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x707x2048): 78.716
Elapsed time for attention_prob_times_values (128x2048x2048x707): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x707): 65.185

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1646.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x708x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x708x2048): 80.515
Elapsed time for attention_prob_times_values (128x2048x2048x708): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x708): 66.312

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1681.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x709x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x709x2048): 78.554
Elapsed time for attention_prob_times_values (128x2048x2048x709): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x709): 65.312

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1651.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x710x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x710x2048): 79.273
Elapsed time for attention_prob_times_values (128x2048x2048x710): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x710): 68.466

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1703.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x711x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x711x2048): 78.301
Elapsed time for attention_prob_times_values (128x2048x2048x711): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x711): 65.547

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1656.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x712x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x712x2048): 80.240
Elapsed time for attention_prob_times_values (128x2048x2048x712): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x712): 88.335

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1955.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x713x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x713x2048): 77.532
Elapsed time for attention_prob_times_values (128x2048x2048x713): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x713): 64.690

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1642.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x714x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x714x2048): 78.306
Elapsed time for attention_prob_times_values (128x2048x2048x714): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x714): 68.757

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1706.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x715x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x715x2048): 77.793
Elapsed time for attention_prob_times_values (128x2048x2048x715): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x715): 65.593

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1661.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x716x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x716x2048): 79.232
Elapsed time for attention_prob_times_values (128x2048x2048x716): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x716): 69.111

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1725.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x717x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x717x2048): 77.843
Elapsed time for attention_prob_times_values (128x2048x2048x717): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x717): 63.825

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1641.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x718x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x718x2048): 78.643
Elapsed time for attention_prob_times_values (128x2048x2048x718): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x718): 69.029

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1723.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x719x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x719x2048): 77.779
Elapsed time for attention_prob_times_values (128x2048x2048x719): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x719): 65.196

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1664.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x720x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x720x2048): 79.822
Elapsed time for attention_prob_times_values (128x2048x2048x720): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x720): 89.708

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1985.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x721x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x721x2048): 77.235
Elapsed time for attention_prob_times_values (128x2048x2048x721): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x721): 65.664

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1670.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x722x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x722x2048): 78.055
Elapsed time for attention_prob_times_values (128x2048x2048x722): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x722): 69.295

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1729.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x723x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x723x2048): 77.357
Elapsed time for attention_prob_times_values (128x2048x2048x723): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x723): 65.882

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1678.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x724x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x724x2048): 78.879
Elapsed time for attention_prob_times_values (128x2048x2048x724): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x724): 67.968

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1725.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x725x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x725x2048): 77.400
Elapsed time for attention_prob_times_values (128x2048x2048x725): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x725): 65.168

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1673.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x726x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x726x2048): 78.168
Elapsed time for attention_prob_times_values (128x2048x2048x726): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x726): 68.754

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1732.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x727x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x727x2048): 76.534
Elapsed time for attention_prob_times_values (128x2048x2048x727): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x727): 66.092

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1682.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x728x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x728x2048): 78.101
Elapsed time for attention_prob_times_values (128x2048x2048x728): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x728): 90.134

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1987.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x729x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x729x2048): 76.818
Elapsed time for attention_prob_times_values (128x2048x2048x729): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x729): 66.203

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1691.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x730x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x730x2048): 77.641
Elapsed time for attention_prob_times_values (128x2048x2048x730): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x730): 68.080

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1727.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x731x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x731x2048): 77.031
Elapsed time for attention_prob_times_values (128x2048x2048x731): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x731): 66.364

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1700.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x732x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x732x2048): 78.555
Elapsed time for attention_prob_times_values (128x2048x2048x732): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x732): 68.435

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1746.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x733x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x733x2048): 77.101
Elapsed time for attention_prob_times_values (128x2048x2048x733): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x733): 66.308

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1704.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x734x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x734x2048): 77.923
Elapsed time for attention_prob_times_values (128x2048x2048x734): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x734): 68.148

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1740.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x735x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x735x2048): 77.188
Elapsed time for attention_prob_times_values (128x2048x2048x735): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x735): 66.042

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1706.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x736x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x736x2048): 92.907
Elapsed time for attention_prob_times_values (128x2048x2048x736): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x736): 92.547

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2225.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x737x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x737x2048): 79.203
Elapsed time for attention_prob_times_values (128x2048x2048x737): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x737): 66.883

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1742.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x738x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x738x2048): 80.129
Elapsed time for attention_prob_times_values (128x2048x2048x738): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x738): 68.508

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1777.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x739x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x739x2048): 79.148
Elapsed time for attention_prob_times_values (128x2048x2048x739): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x739): 65.984

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1733.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x740x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x740x2048): 80.717
Elapsed time for attention_prob_times_values (128x2048x2048x740): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x740): 68.727

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1791.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x741x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x741x2048): 79.022
Elapsed time for attention_prob_times_values (128x2048x2048x741): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x741): 67.050

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1752.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x742x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x742x2048): 79.870
Elapsed time for attention_prob_times_values (128x2048x2048x742): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x742): 68.876

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1789.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x743x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x743x2048): 78.685
Elapsed time for attention_prob_times_values (128x2048x2048x743): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x743): 67.292

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1756.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x744x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x744x2048): 80.864
Elapsed time for attention_prob_times_values (128x2048x2048x744): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x744): 92.310

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2090.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x745x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x745x2048): 77.879
Elapsed time for attention_prob_times_values (128x2048x2048x745): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x745): 67.029

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1749.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x746x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x746x2048): 78.689
Elapsed time for attention_prob_times_values (128x2048x2048x746): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x746): 69.006

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1787.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x747x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x747x2048): 77.949
Elapsed time for attention_prob_times_values (128x2048x2048x747): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x747): 67.272

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1758.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x748x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x748x2048): 79.527
Elapsed time for attention_prob_times_values (128x2048x2048x748): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x748): 69.468

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1807.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x749x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x749x2048): 78.011
Elapsed time for attention_prob_times_values (128x2048x2048x749): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x749): 67.262

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1763.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x750x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x750x2048): 78.962
Elapsed time for attention_prob_times_values (128x2048x2048x750): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x750): 69.170

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1802.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x751x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x751x2048): 78.087
Elapsed time for attention_prob_times_values (128x2048x2048x751): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x751): 67.061

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1765.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x752x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x752x2048): 80.507
Elapsed time for attention_prob_times_values (128x2048x2048x752): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x752): 92.336

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2107.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x753x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x753x2048): 76.348
Elapsed time for attention_prob_times_values (128x2048x2048x753): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x753): 64.337

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1713.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x754x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x754x2048): 75.183
Elapsed time for attention_prob_times_values (128x2048x2048x754): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x754): 67.610

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1748.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x755x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x755x2048): 75.163
Elapsed time for attention_prob_times_values (128x2048x2048x755): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x755): 65.674

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1723.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x756x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x756x2048): 73.761
Elapsed time for attention_prob_times_values (128x2048x2048x756): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x756): 67.490

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1735.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x757x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x757x2048): 74.532
Elapsed time for attention_prob_times_values (128x2048x2048x757): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x757): 64.333

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1702.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x758x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x758x2048): 75.477
Elapsed time for attention_prob_times_values (128x2048x2048x758): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x758): 69.868

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1791.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x759x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x759x2048): 74.935
Elapsed time for attention_prob_times_values (128x2048x2048x759): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x759): 67.112

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1750.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x760x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x760x2048): 76.332
Elapsed time for attention_prob_times_values (128x2048x2048x760): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x760): 94.337

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2088.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x761x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x761x2048): 73.658
Elapsed time for attention_prob_times_values (128x2048x2048x761): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x761): 67.152

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1741.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x761x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x761x2048): 75.685
Elapsed time for attention_prob_times_values (128x2048x2048x761): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x761): 65.981

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1747.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x762x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x762x2048): 76.584
Elapsed time for attention_prob_times_values (128x2048x2048x762): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x762): 68.643

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1796.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x763x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x763x2048): 75.934
Elapsed time for attention_prob_times_values (128x2048x2048x763): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x763): 66.124

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1756.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x764x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x764x2048): 77.265
Elapsed time for attention_prob_times_values (128x2048x2048x764): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x764): 68.741

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1809.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x765x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x765x2048): 75.838
Elapsed time for attention_prob_times_values (128x2048x2048x765): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x765): 66.259

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1761.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x766x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x766x2048): 76.449
Elapsed time for attention_prob_times_values (128x2048x2048x766): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x766): 68.946

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1808.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x767x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x767x2048): 75.796
Elapsed time for attention_prob_times_values (128x2048x2048x767): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x767): 65.472

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1754.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x768x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x768x2048): 86.825
Elapsed time for attention_prob_times_values (128x2048x2048x768): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x768): 96.732

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2287.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x769x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x769x2048): 77.608
Elapsed time for attention_prob_times_values (128x2048x2048x769): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x769): 61.683

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1720.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x770x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x770x2048): 78.645
Elapsed time for attention_prob_times_values (128x2048x2048x770): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x770): 65.178

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1786.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x771x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x771x2048): 77.332
Elapsed time for attention_prob_times_values (128x2048x2048x771): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x771): 62.188

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1729.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x772x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x772x2048): 79.104
Elapsed time for attention_prob_times_values (128x2048x2048x772): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x772): 65.337

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1798.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x773x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x773x2048): 77.614
Elapsed time for attention_prob_times_values (128x2048x2048x773): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x773): 62.362

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1739.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x774x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x774x2048): 78.436
Elapsed time for attention_prob_times_values (128x2048x2048x774): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x774): 65.294

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1794.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x775x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x775x2048): 75.531
Elapsed time for attention_prob_times_values (128x2048x2048x775): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x775): 62.463

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1724.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x776x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x776x2048): 78.827
Elapsed time for attention_prob_times_values (128x2048x2048x776): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x776): 82.886

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2040.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x777x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x777x2048): 76.376
Elapsed time for attention_prob_times_values (128x2048x2048x777): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x777): 62.346

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1735.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x778x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x778x2048): 77.324
Elapsed time for attention_prob_times_values (128x2048x2048x778): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x778): 65.272

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1791.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x779x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x779x2048): 76.537
Elapsed time for attention_prob_times_values (128x2048x2048x779): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x779): 62.392

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1742.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x780x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x780x2048): 77.957
Elapsed time for attention_prob_times_values (128x2048x2048x780): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x780): 65.689

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1809.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x781x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x781x2048): 74.505
Elapsed time for attention_prob_times_values (128x2048x2048x781): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x781): 59.655

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1683.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x782x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x782x2048): 77.691
Elapsed time for attention_prob_times_values (128x2048x2048x782): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x782): 64.098

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1786.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x783x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x783x2048): 74.634
Elapsed time for attention_prob_times_values (128x2048x2048x783): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x783): 60.462

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1701.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x784x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x784x2048): 79.025
Elapsed time for attention_prob_times_values (128x2048x2048x784): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x784): 84.107

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2077.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x785x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x785x2048): 76.400
Elapsed time for attention_prob_times_values (128x2048x2048x785): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x785): 62.855

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1760.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x786x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x786x2048): 77.230
Elapsed time for attention_prob_times_values (128x2048x2048x786): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x786): 65.997

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1819.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x787x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x787x2048): 76.614
Elapsed time for attention_prob_times_values (128x2048x2048x787): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x787): 62.843

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1767.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x788x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x788x2048): 77.865
Elapsed time for attention_prob_times_values (128x2048x2048x788): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x788): 66.377

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1836.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x789x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x789x2048): 76.831
Elapsed time for attention_prob_times_values (128x2048x2048x789): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x789): 63.105

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1777.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x790x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x790x2048): 77.520
Elapsed time for attention_prob_times_values (128x2048x2048x790): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x790): 66.268

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1835.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x791x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x791x2048): 76.936
Elapsed time for attention_prob_times_values (128x2048x2048x791): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x791): 63.233

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1785.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x792x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x792x2048): 76.501
Elapsed time for attention_prob_times_values (128x2048x2048x792): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x792): 84.931

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2072.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x793x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x793x2048): 76.447
Elapsed time for attention_prob_times_values (128x2048x2048x793): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x793): 63.439

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1787.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x794x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x794x2048): 77.026
Elapsed time for attention_prob_times_values (128x2048x2048x794): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x794): 66.546

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1843.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x795x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x795x2048): 76.556
Elapsed time for attention_prob_times_values (128x2048x2048x795): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x795): 63.551

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1794.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x796x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x796x2048): 78.010
Elapsed time for attention_prob_times_values (128x2048x2048x796): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x796): 67.044

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1865.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x797x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x797x2048): 76.531
Elapsed time for attention_prob_times_values (128x2048x2048x797): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x797): 63.677

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1800.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x798x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x798x2048): 77.299
Elapsed time for attention_prob_times_values (128x2048x2048x798): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x798): 66.884

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1860.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x799x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x799x2048): 76.605
Elapsed time for attention_prob_times_values (128x2048x2048x799): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x799): 63.824

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1808.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x800x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x800x2048): 91.231
Elapsed time for attention_prob_times_values (128x2048x2048x800): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x800): 86.574

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2309.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x801x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x801x2048): 79.023
Elapsed time for attention_prob_times_values (128x2048x2048x801): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x801): 64.261

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1845.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x802x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x802x2048): 79.777
Elapsed time for attention_prob_times_values (128x2048x2048x802): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x802): 67.188

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1901.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x803x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x803x2048): 78.821
Elapsed time for attention_prob_times_values (128x2048x2048x803): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x803): 64.261

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1847.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x804x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x804x2048): 80.379
Elapsed time for attention_prob_times_values (128x2048x2048x804): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x804): 67.607

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1918.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x805x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x805x2048): 78.622
Elapsed time for attention_prob_times_values (128x2048x2048x805): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x805): 64.301

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1850.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x806x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x806x2048): 79.460
Elapsed time for attention_prob_times_values (128x2048x2048x806): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x806): 67.373

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1909.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x807x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x807x2048): 78.471
Elapsed time for attention_prob_times_values (128x2048x2048x807): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x807): 64.526

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1856.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x808x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x808x2048): 80.413
Elapsed time for attention_prob_times_values (128x2048x2048x808): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x808): 86.711

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2190.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x809x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x809x2048): 77.718
Elapsed time for attention_prob_times_values (128x2048x2048x809): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x809): 63.494

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1836.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x810x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x810x2048): 77.301
Elapsed time for attention_prob_times_values (128x2048x2048x810): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x810): 67.564

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1897.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x811x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x811x2048): 77.844
Elapsed time for attention_prob_times_values (128x2048x2048x811): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x811): 63.791

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1847.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x812x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x812x2048): 78.335
Elapsed time for attention_prob_times_values (128x2048x2048x812): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x812): 66.636

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1899.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x813x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x813x2048): 77.541
Elapsed time for attention_prob_times_values (128x2048x2048x813): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x813): 63.715

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 1847.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x814x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x814x2048): 77.599
Elapsed time for attention_prob_times_values (128x2048x2048x814): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x814): 79.299

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2073.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x815x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x815x2048): 77.199
Elapsed time for attention_prob_times_values (128x2048x2048x815): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x815): 78.648

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2062.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x816x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x816x2048): 83.392
Elapsed time for attention_prob_times_values (128x2048x2048x816): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x816): 87.455

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2262.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x817x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x817x2048): 76.645
Elapsed time for attention_prob_times_values (128x2048x2048x817): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x817): 78.787

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2061.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x818x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x818x2048): 78.646
Elapsed time for attention_prob_times_values (128x2048x2048x818): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x818): 79.827

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2104.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x819x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x819x2048): 76.758
Elapsed time for attention_prob_times_values (128x2048x2048x819): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x819): 77.940

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2056.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x820x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x820x2048): 78.304
Elapsed time for attention_prob_times_values (128x2048x2048x820): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x820): 81.457

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2125.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x821x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x821x2048): 80.308
Elapsed time for attention_prob_times_values (128x2048x2048x821): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x821): 78.960

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2122.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x822x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x822x2048): 78.807
Elapsed time for attention_prob_times_values (128x2048x2048x822): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x822): 81.466

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2138.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x823x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x823x2048): 78.314
Elapsed time for attention_prob_times_values (128x2048x2048x823): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x823): 79.179

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2103.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x824x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x824x2048): 80.007
Elapsed time for attention_prob_times_values (128x2048x2048x824): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x824): 88.144

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2243.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x825x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x825x2048): 77.290
Elapsed time for attention_prob_times_values (128x2048x2048x825): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x825): 79.380

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2097.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x826x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x826x2048): 78.069
Elapsed time for attention_prob_times_values (128x2048x2048x826): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x826): 81.848

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2142.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x827x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x827x2048): 76.164
Elapsed time for attention_prob_times_values (128x2048x2048x827): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x827): 79.554

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2089.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x828x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x828x2048): 79.029
Elapsed time for attention_prob_times_values (128x2048x2048x828): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x828): 80.297

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2140.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x829x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x829x2048): 76.023
Elapsed time for attention_prob_times_values (128x2048x2048x829): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x829): 78.477

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2077.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x830x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x830x2048): 78.237
Elapsed time for attention_prob_times_values (128x2048x2048x830): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x830): 82.237

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2160.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x831x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x831x2048): 77.534
Elapsed time for attention_prob_times_values (128x2048x2048x831): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x831): 79.981

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2123.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x832x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x832x2048): 91.395
Elapsed time for attention_prob_times_values (128x2048x2048x832): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x832): 90.053

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2449.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x833x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x833x2048): 79.351
Elapsed time for attention_prob_times_values (128x2048x2048x833): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x833): 80.208

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2156.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x834x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x834x2048): 80.251
Elapsed time for attention_prob_times_values (128x2048x2048x834): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x834): 82.715

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2204.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x835x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x835x2048): 79.110
Elapsed time for attention_prob_times_values (128x2048x2048x835): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x835): 80.428

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2161.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x836x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x836x2048): 81.095
Elapsed time for attention_prob_times_values (128x2048x2048x836): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x836): 82.933

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2224.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x837x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x837x2048): 79.007
Elapsed time for attention_prob_times_values (128x2048x2048x837): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x837): 80.595

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2166.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x838x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x838x2048): 79.951
Elapsed time for attention_prob_times_values (128x2048x2048x838): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x838): 81.111

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2189.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x839x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x839x2048): 78.600
Elapsed time for attention_prob_times_values (128x2048x2048x839): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x839): 80.829

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2169.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x840x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x840x2048): 80.971
Elapsed time for attention_prob_times_values (128x2048x2048x840): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x840): 89.867

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2321.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x841x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x841x2048): 78.166
Elapsed time for attention_prob_times_values (128x2048x2048x841): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x841): 80.920

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2169.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x842x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x842x2048): 78.827
Elapsed time for attention_prob_times_values (128x2048x2048x842): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x842): 83.437

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2214.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x843x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x843x2048): 78.483
Elapsed time for attention_prob_times_values (128x2048x2048x843): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x843): 81.161

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2182.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x844x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x844x2048): 79.918
Elapsed time for attention_prob_times_values (128x2048x2048x844): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x844): 83.768

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2239.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x845x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x845x2048): 78.413
Elapsed time for attention_prob_times_values (128x2048x2048x845): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x845): 81.353

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2188.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x846x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x846x2048): 79.363
Elapsed time for attention_prob_times_values (128x2048x2048x846): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x846): 82.531

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2220.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x847x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x847x2048): 76.744
Elapsed time for attention_prob_times_values (128x2048x2048x847): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x847): 81.750

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2174.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x848x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x848x2048): 80.675
Elapsed time for attention_prob_times_values (128x2048x2048x848): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x848): 88.597

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2322.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x849x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x849x2048): 76.491
Elapsed time for attention_prob_times_values (128x2048x2048x849): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x849): 81.950

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2178.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x850x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x850x2048): 78.882
Elapsed time for attention_prob_times_values (128x2048x2048x850): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x850): 84.192

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2244.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x851x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x851x2048): 78.094
Elapsed time for attention_prob_times_values (128x2048x2048x851): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x851): 81.286

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2198.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x852x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x852x2048): 78.853
Elapsed time for attention_prob_times_values (128x2048x2048x852): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x852): 84.454

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2253.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x853x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x853x2048): 78.260
Elapsed time for attention_prob_times_values (128x2048x2048x853): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x853): 82.057

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2215.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x854x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x854x2048): 78.011
Elapsed time for attention_prob_times_values (128x2048x2048x854): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x854): 84.527

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2246.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x855x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x855x2048): 78.409
Elapsed time for attention_prob_times_values (128x2048x2048x855): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x855): 77.158

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2155.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x856x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x856x2048): 76.684
Elapsed time for attention_prob_times_values (128x2048x2048x856): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x856): 91.639

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2317.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x857x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x857x2048): 77.758
Elapsed time for attention_prob_times_values (128x2048x2048x857): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x857): 82.632

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2225.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x858x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x858x2048): 76.095
Elapsed time for attention_prob_times_values (128x2048x2048x858): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x858): 79.223

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2159.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x859x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x859x2048): 73.665
Elapsed time for attention_prob_times_values (128x2048x2048x859): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x859): 82.821

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2171.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x860x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x860x2048): 79.654
Elapsed time for attention_prob_times_values (128x2048x2048x860): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x860): 82.596

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2260.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x861x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x861x2048): 75.416
Elapsed time for attention_prob_times_values (128x2048x2048x861): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x861): 82.938

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2204.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x862x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x862x2048): 78.799
Elapsed time for attention_prob_times_values (128x2048x2048x862): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x862): 85.171

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2286.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x863x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x863x2048): 73.226
Elapsed time for attention_prob_times_values (128x2048x2048x863): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x863): 83.150

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2178.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x864x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x864x2048): 92.340
Elapsed time for attention_prob_times_values (128x2048x2048x864): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x864): 90.126

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2554.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x865x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x865x2048): 75.192
Elapsed time for attention_prob_times_values (128x2048x2048x865): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x865): 78.291

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2150.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x866x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x866x2048): 77.586
Elapsed time for attention_prob_times_values (128x2048x2048x866): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x866): 85.540

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2283.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x867x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x867x2048): 78.784
Elapsed time for attention_prob_times_values (128x2048x2048x867): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x867): 82.592

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2265.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x868x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x868x2048): 79.259
Elapsed time for attention_prob_times_values (128x2048x2048x868): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x868): 85.826

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2317.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x869x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x869x2048): 79.329
Elapsed time for attention_prob_times_values (128x2048x2048x869): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x869): 80.122

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2244.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x870x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x870x2048): 78.381
Elapsed time for attention_prob_times_values (128x2048x2048x870): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x870): 85.839

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2309.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x871x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x871x2048): 79.140
Elapsed time for attention_prob_times_values (128x2048x2048x871): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x871): 81.802

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2270.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x872x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x872x2048): 79.275
Elapsed time for attention_prob_times_values (128x2048x2048x872): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x872): 93.151

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2419.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x873x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x873x2048): 78.390
Elapsed time for attention_prob_times_values (128x2048x2048x873): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x873): 82.106

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2268.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x874x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x874x2048): 77.762
Elapsed time for attention_prob_times_values (128x2048x2048x874): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x874): 86.168

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2314.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x875x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x875x2048): 78.667
Elapsed time for attention_prob_times_values (128x2048x2048x875): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x875): 81.370

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2267.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x876x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x876x2048): 78.057
Elapsed time for attention_prob_times_values (128x2048x2048x876): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x876): 86.426

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2327.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x877x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x877x2048): 78.691
Elapsed time for attention_prob_times_values (128x2048x2048x877): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x877): 83.630

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2303.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x878x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x878x2048): 77.601
Elapsed time for attention_prob_times_values (128x2048x2048x878): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x878): 86.426

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2325.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x879x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x879x2048): 78.753
Elapsed time for attention_prob_times_values (128x2048x2048x879): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x879): 82.937

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2300.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x880x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x880x2048): 80.516
Elapsed time for attention_prob_times_values (128x2048x2048x880): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x880): 94.327

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2475.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x881x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x881x2048): 78.212
Elapsed time for attention_prob_times_values (128x2048x2048x881): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x881): 80.988

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2270.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x882x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x882x2048): 75.504
Elapsed time for attention_prob_times_values (128x2048x2048x882): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x882): 83.655

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2267.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x883x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x883x2048): 74.705
Elapsed time for attention_prob_times_values (128x2048x2048x883): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x883): 84.443

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2266.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x884x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x884x2048): 77.194
Elapsed time for attention_prob_times_values (128x2048x2048x884): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x884): 81.828

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2274.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x885x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x885x2048): 74.668
Elapsed time for attention_prob_times_values (128x2048x2048x885): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x885): 84.559

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2272.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x886x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x886x2048): 76.216
Elapsed time for attention_prob_times_values (128x2048x2048x886): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x886): 83.845

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2290.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x887x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x887x2048): 73.623
Elapsed time for attention_prob_times_values (128x2048x2048x887): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x887): 84.658

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2261.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x888x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x888x2048): 75.289
Elapsed time for attention_prob_times_values (128x2048x2048x888): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x888): 89.560

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2351.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x889x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x889x2048): 77.615
Elapsed time for attention_prob_times_values (128x2048x2048x889): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x889): 84.824

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2332.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x890x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x890x2048): 78.683
Elapsed time for attention_prob_times_values (128x2048x2048x890): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x890): 86.701

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2376.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x891x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x891x2048): 77.294
Elapsed time for attention_prob_times_values (128x2048x2048x891): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x891): 84.979

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2335.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x892x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x892x2048): 79.429
Elapsed time for attention_prob_times_values (128x2048x2048x892): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x892): 86.414

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2390.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x893x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x893x2048): 77.728
Elapsed time for attention_prob_times_values (128x2048x2048x893): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x893): 85.132

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2348.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x894x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x894x2048): 78.734
Elapsed time for attention_prob_times_values (128x2048x2048x894): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x894): 87.025

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2392.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x895x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x895x2048): 77.028
Elapsed time for attention_prob_times_values (128x2048x2048x895): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x895): 85.263

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2344.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x896x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x896x2048): 88.472
Elapsed time for attention_prob_times_values (128x2048x2048x896): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x896): 97.066

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2684.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x897x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x897x2048): 78.105
Elapsed time for attention_prob_times_values (128x2048x2048x897): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x897): 74.145

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2208.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x898x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x898x2048): 80.436
Elapsed time for attention_prob_times_values (128x2048x2048x898): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x898): 74.540

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2248.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x899x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x899x2048): 72.881
Elapsed time for attention_prob_times_values (128x2048x2048x899): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x899): 72.426

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2113.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x900x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x900x2048): 81.157
Elapsed time for attention_prob_times_values (128x2048x2048x900): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x900): 74.866

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2268.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x901x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x901x2048): 75.441
Elapsed time for attention_prob_times_values (128x2048x2048x901): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x901): 76.969

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2221.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x902x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x902x2048): 80.358
Elapsed time for attention_prob_times_values (128x2048x2048x902): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x902): 75.488

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2272.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x903x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x903x2048): 74.978
Elapsed time for attention_prob_times_values (128x2048x2048x903): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x903): 77.128

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2221.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x904x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x904x2048): 81.060
Elapsed time for attention_prob_times_values (128x2048x2048x904): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x904): 82.916

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2397.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x905x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x905x2048): 78.416
Elapsed time for attention_prob_times_values (128x2048x2048x905): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x905): 77.356

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2280.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x906x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x906x2048): 79.972
Elapsed time for attention_prob_times_values (128x2048x2048x906): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x906): 78.897

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2328.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x907x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x907x2048): 75.112
Elapsed time for attention_prob_times_values (128x2048x2048x907): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x907): 77.539

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2239.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x908x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x908x2048): 80.566
Elapsed time for attention_prob_times_values (128x2048x2048x908): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x908): 74.747

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2277.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x909x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x909x2048): 76.317
Elapsed time for attention_prob_times_values (128x2048x2048x909): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x909): 77.651

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2263.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x910x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x910x2048): 80.024
Elapsed time for attention_prob_times_values (128x2048x2048x910): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x910): 75.924

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2293.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x911x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x911x2048): 75.466
Elapsed time for attention_prob_times_values (128x2048x2048x911): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x911): 77.816

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2257.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x912x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x912x2048): 81.426
Elapsed time for attention_prob_times_values (128x2048x2048x912): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x912): 84.597

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2447.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x913x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x913x2048): 78.770
Elapsed time for attention_prob_times_values (128x2048x2048x913): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x913): 78.088

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2316.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x914x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x914x2048): 80.023
Elapsed time for attention_prob_times_values (128x2048x2048x914): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x914): 77.058

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2321.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x915x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x915x2048): 75.019
Elapsed time for attention_prob_times_values (128x2048x2048x915): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x915): 78.220

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2266.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x916x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x916x2048): 80.203
Elapsed time for attention_prob_times_values (128x2048x2048x916): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x916): 77.052

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2328.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x917x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x917x2048): 76.782
Elapsed time for attention_prob_times_values (128x2048x2048x917): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x917): 78.306

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2299.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x918x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x918x2048): 79.833
Elapsed time for attention_prob_times_values (128x2048x2048x918): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x918): 78.941

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2356.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x919x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x919x2048): 78.488
Elapsed time for attention_prob_times_values (128x2048x2048x919): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x919): 78.502

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2332.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x920x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x920x2048): 80.197
Elapsed time for attention_prob_times_values (128x2048x2048x920): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x920): 85.616

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2463.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x921x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x921x2048): 76.875
Elapsed time for attention_prob_times_values (128x2048x2048x921): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x921): 78.708

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2316.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x922x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x922x2048): 77.913
Elapsed time for attention_prob_times_values (128x2048x2048x922): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x922): 79.728

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2349.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x923x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x923x2048): 76.669
Elapsed time for attention_prob_times_values (128x2048x2048x923): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x923): 78.867

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2320.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x924x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x924x2048): 80.093
Elapsed time for attention_prob_times_values (128x2048x2048x924): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x924): 81.095

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2407.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x925x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x925x2048): 78.874
Elapsed time for attention_prob_times_values (128x2048x2048x925): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x925): 78.715

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2356.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x926x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x926x2048): 79.479
Elapsed time for attention_prob_times_values (128x2048x2048x926): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x926): 81.122

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2403.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x927x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x927x2048): 79.135
Elapsed time for attention_prob_times_values (128x2048x2048x927): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x927): 79.006

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2369.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x928x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x928x2048): 92.437
Elapsed time for attention_prob_times_values (128x2048x2048x928): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x928): 87.328

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2694.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x929x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x929x2048): 80.949
Elapsed time for attention_prob_times_values (128x2048x2048x929): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x929): 79.173

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2404.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x930x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x930x2048): 81.471
Elapsed time for attention_prob_times_values (128x2048x2048x930): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x930): 81.427

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2448.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x931x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x931x2048): 80.719
Elapsed time for attention_prob_times_values (128x2048x2048x931): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x931): 79.269

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2407.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x932x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x932x2048): 82.235
Elapsed time for attention_prob_times_values (128x2048x2048x932): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x932): 81.621

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2468.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x933x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x933x2048): 80.619
Elapsed time for attention_prob_times_values (128x2048x2048x933): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x933): 79.325

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2411.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x934x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x934x2048): 81.218
Elapsed time for attention_prob_times_values (128x2048x2048x934): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x934): 81.644

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2458.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x935x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x935x2048): 80.305
Elapsed time for attention_prob_times_values (128x2048x2048x935): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x935): 79.204

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2409.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x936x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x936x2048): 82.080
Elapsed time for attention_prob_times_values (128x2048x2048x936): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x936): 87.501

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2562.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x937x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x937x2048): 79.729
Elapsed time for attention_prob_times_values (128x2048x2048x937): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x937): 79.188

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2406.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x938x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x938x2048): 80.062
Elapsed time for attention_prob_times_values (128x2048x2048x938): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x938): 81.984

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2455.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x939x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x939x2048): 79.909
Elapsed time for attention_prob_times_values (128x2048x2048x939): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x939): 79.277

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2415.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x940x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x940x2048): 81.865
Elapsed time for attention_prob_times_values (128x2048x2048x940): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x940): 82.229

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2492.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x941x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x941x2048): 80.126
Elapsed time for attention_prob_times_values (128x2048x2048x941): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x941): 77.858

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2401.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x942x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x942x2048): 78.919
Elapsed time for attention_prob_times_values (128x2048x2048x942): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x942): 82.215

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2451.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x943x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x943x2048): 80.088
Elapsed time for attention_prob_times_values (128x2048x2048x943): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x943): 76.662

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2386.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x944x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x944x2048): 77.367
Elapsed time for attention_prob_times_values (128x2048x2048x944): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x944): 88.527

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2518.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x945x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x945x2048): 79.676
Elapsed time for attention_prob_times_values (128x2048x2048x945): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x945): 75.768

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2371.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x946x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x946x2048): 77.300
Elapsed time for attention_prob_times_values (128x2048x2048x946): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x946): 82.531

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2439.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x947x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x947x2048): 79.833
Elapsed time for attention_prob_times_values (128x2048x2048x947): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x947): 76.894

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2396.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x948x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x948x2048): 77.195
Elapsed time for attention_prob_times_values (128x2048x2048x948): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x948): 82.776

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2446.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x949x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x949x2048): 80.066
Elapsed time for attention_prob_times_values (128x2048x2048x949): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x949): 77.467

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2414.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x950x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x950x2048): 76.662
Elapsed time for attention_prob_times_values (128x2048x2048x950): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x950): 82.808

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2443.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x951x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x951x2048): 79.977
Elapsed time for attention_prob_times_values (128x2048x2048x951): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x951): 78.084

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2427.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x952x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x952x2048): 79.685
Elapsed time for attention_prob_times_values (128x2048x2048x952): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x952): 88.894

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2584.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x953x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x953x2048): 79.637
Elapsed time for attention_prob_times_values (128x2048x2048x953): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x953): 79.253

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2445.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x954x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x954x2048): 77.289
Elapsed time for attention_prob_times_values (128x2048x2048x954): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x954): 83.002

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2466.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x955x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x955x2048): 77.137
Elapsed time for attention_prob_times_values (128x2048x2048x955): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x955): 78.073

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2393.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x956x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x956x2048): 81.123
Elapsed time for attention_prob_times_values (128x2048x2048x956): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x956): 79.976

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2486.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x957x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x957x2048): 75.600
Elapsed time for attention_prob_times_values (128x2048x2048x957): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x957): 81.153

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2419.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x958x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x958x2048): 80.617
Elapsed time for attention_prob_times_values (128x2048x2048x958): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x958): 81.086

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2501.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x959x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x959x2048): 78.380
Elapsed time for attention_prob_times_values (128x2048x2048x959): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x959): 81.192

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2470.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x960x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x960x2048): 93.074
Elapsed time for attention_prob_times_values (128x2048x2048x960): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x960): 90.347

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2842.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x961x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x961x2048): 80.853
Elapsed time for attention_prob_times_values (128x2048x2048x961): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x961): 81.299

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2515.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x962x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x962x2048): 82.300
Elapsed time for attention_prob_times_values (128x2048x2048x962): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x962): 78.695

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2499.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x963x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x963x2048): 80.197
Elapsed time for attention_prob_times_values (128x2048x2048x963): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x963): 81.403

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2512.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x964x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x964x2048): 82.979
Elapsed time for attention_prob_times_values (128x2048x2048x964): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x964): 82.848

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2580.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x965x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x965x2048): 80.280
Elapsed time for attention_prob_times_values (128x2048x2048x965): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x965): 81.702

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2523.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x966x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x966x2048): 81.919
Elapsed time for attention_prob_times_values (128x2048x2048x966): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x966): 83.365

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2577.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x967x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x967x2048): 80.328
Elapsed time for attention_prob_times_values (128x2048x2048x967): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x967): 81.921

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2532.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x968x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x968x2048): 82.900
Elapsed time for attention_prob_times_values (128x2048x2048x968): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x968): 85.413

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2629.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x969x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x969x2048): 75.735
Elapsed time for attention_prob_times_values (128x2048x2048x969): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x969): 82.082

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2464.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x970x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x970x2048): 81.155
Elapsed time for attention_prob_times_values (128x2048x2048x970): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x970): 83.715

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2580.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x971x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x971x2048): 80.011
Elapsed time for attention_prob_times_values (128x2048x2048x971): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x971): 82.224

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2542.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x972x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x972x2048): 82.043
Elapsed time for attention_prob_times_values (128x2048x2048x972): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x972): 79.912

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2540.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x973x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x973x2048): 79.964
Elapsed time for attention_prob_times_values (128x2048x2048x973): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x973): 82.395

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2548.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x974x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x974x2048): 81.537
Elapsed time for attention_prob_times_values (128x2048x2048x974): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x974): 83.545

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2594.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x975x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x975x2048): 80.068
Elapsed time for attention_prob_times_values (128x2048x2048x975): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x975): 82.511

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2557.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x976x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x976x2048): 82.622
Elapsed time for attention_prob_times_values (128x2048x2048x976): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x976): 91.307

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2732.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x977x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x977x2048): 79.411
Elapsed time for attention_prob_times_values (128x2048x2048x977): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x977): 82.800

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2556.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x978x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x978x2048): 80.879
Elapsed time for attention_prob_times_values (128x2048x2048x978): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x978): 82.352

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2575.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x979x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x979x2048): 79.511
Elapsed time for attention_prob_times_values (128x2048x2048x979): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x979): 82.926

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2564.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x980x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x980x2048): 81.802
Elapsed time for attention_prob_times_values (128x2048x2048x980): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x980): 84.987

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2636.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x981x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x981x2048): 80.088
Elapsed time for attention_prob_times_values (128x2048x2048x981): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x981): 82.923

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2579.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x982x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x982x2048): 81.082
Elapsed time for attention_prob_times_values (128x2048x2048x982): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x982): 84.869

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2627.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x983x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x983x2048): 79.820
Elapsed time for attention_prob_times_values (128x2048x2048x983): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x983): 82.997

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2581.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x984x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x984x2048): 82.349
Elapsed time for attention_prob_times_values (128x2048x2048x984): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x984): 91.233

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2748.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x985x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x985x2048): 79.279
Elapsed time for attention_prob_times_values (128x2048x2048x985): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x985): 83.247

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2581.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x986x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x986x2048): 80.528
Elapsed time for attention_prob_times_values (128x2048x2048x986): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x986): 84.706

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2626.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x987x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x987x2048): 79.216
Elapsed time for attention_prob_times_values (128x2048x2048x987): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x987): 83.406

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2587.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x988x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x988x2048): 80.065
Elapsed time for attention_prob_times_values (128x2048x2048x988): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x988): 85.089

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2629.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x989x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x989x2048): 77.104
Elapsed time for attention_prob_times_values (128x2048x2048x989): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x989): 83.417

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2556.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x990x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x990x2048): 80.012
Elapsed time for attention_prob_times_values (128x2048x2048x990): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x990): 81.599

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2580.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x991x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x991x2048): 76.245
Elapsed time for attention_prob_times_values (128x2048x2048x991): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x991): 83.514

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2548.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x992x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x992x2048): 90.552
Elapsed time for attention_prob_times_values (128x2048x2048x992): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x992): 89.604

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2882.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x993x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x993x2048): 77.514
Elapsed time for attention_prob_times_values (128x2048x2048x993): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x993): 83.830

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2580.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x994x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x994x2048): 78.842
Elapsed time for attention_prob_times_values (128x2048x2048x994): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x994): 86.010

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2637.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x995x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x995x2048): 81.282
Elapsed time for attention_prob_times_values (128x2048x2048x995): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x995): 83.729

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2647.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x996x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x996x2048): 82.881
Elapsed time for attention_prob_times_values (128x2048x2048x996): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x996): 85.858

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2709.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x997x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x997x2048): 81.263
Elapsed time for attention_prob_times_values (128x2048x2048x997): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x997): 83.867

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2654.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x998x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x998x2048): 81.872
Elapsed time for attention_prob_times_values (128x2048x2048x998): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x998): 85.335

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2689.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x999x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x999x2048): 80.958
Elapsed time for attention_prob_times_values (128x2048x2048x999): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x999): 84.061

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2657.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1000x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1000x2048): 78.288
Elapsed time for attention_prob_times_values (128x2048x2048x1000): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1000): 89.964

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2700.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1001x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1001x2048): 77.406
Elapsed time for attention_prob_times_values (128x2048x2048x1001): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1001): 80.526

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2548.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1002x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1002x2048): 79.099
Elapsed time for attention_prob_times_values (128x2048x2048x1002): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1002): 82.003

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2601.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1003x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1003x2048): 80.588
Elapsed time for attention_prob_times_values (128x2048x2048x1003): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1003): 80.945

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2612.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1004x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1004x2048): 78.670
Elapsed time for attention_prob_times_values (128x2048x2048x1004): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1004): 86.716

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2670.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1005x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1005x2048): 80.760
Elapsed time for attention_prob_times_values (128x2048x2048x1005): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1005): 80.769

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2617.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1006x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1006x2048): 77.409
Elapsed time for attention_prob_times_values (128x2048x2048x1006): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1006): 86.739

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2653.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1007x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1007x2048): 80.544
Elapsed time for attention_prob_times_values (128x2048x2048x1007): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1007): 83.946

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2669.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1008x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1008x2048): 82.914
Elapsed time for attention_prob_times_values (128x2048x2048x1008): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1008): 94.173

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2866.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1009x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1009x2048): 79.956
Elapsed time for attention_prob_times_values (128x2048x2048x1009): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1009): 84.025

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2665.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1010x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1010x2048): 80.704
Elapsed time for attention_prob_times_values (128x2048x2048x1010): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1010): 87.009

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2726.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1011x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1011x2048): 80.206
Elapsed time for attention_prob_times_values (128x2048x2048x1011): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1011): 83.870

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2672.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1012x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1012x2048): 81.535
Elapsed time for attention_prob_times_values (128x2048x2048x1012): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1012): 87.171

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2748.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1013x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1013x2048): 80.464
Elapsed time for attention_prob_times_values (128x2048x2048x1013): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1013): 84.531

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2692.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1014x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1014x2048): 80.717
Elapsed time for attention_prob_times_values (128x2048x2048x1014): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1014): 87.213

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2740.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1015x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1015x2048): 80.266
Elapsed time for attention_prob_times_values (128x2048x2048x1015): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1015): 84.008

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2686.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1016x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1016x2048): 81.123
Elapsed time for attention_prob_times_values (128x2048x2048x1016): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1016): 91.992

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2823.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1017x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1017x2048): 79.939
Elapsed time for attention_prob_times_values (128x2048x2048x1017): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1017): 84.425

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2692.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1018x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1018x2048): 80.558
Elapsed time for attention_prob_times_values (128x2048x2048x1018): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1018): 87.424

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2751.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1019x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1019x2048): 80.612
Elapsed time for attention_prob_times_values (128x2048x2048x1019): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1019): 84.846

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2715.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1020x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1020x2048): 84.620
Elapsed time for attention_prob_times_values (128x2048x2048x1020): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1020): 86.555

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2813.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1021x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1021x2048): 80.242
Elapsed time for attention_prob_times_values (128x2048x2048x1021): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1021): 83.922

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2699.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1022x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1022x2048): 79.331
Elapsed time for attention_prob_times_values (128x2048x2048x1022): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1022): 87.068

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2734.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1023x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1023x2048): 80.171
Elapsed time for attention_prob_times_values (128x2048x2048x1023): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1023): 83.612

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2698.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
