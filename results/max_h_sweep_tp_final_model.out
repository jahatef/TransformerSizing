1.13.1 

[2023-09-27 00:08:42,471] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-09-27 00:08:43,197] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.146.194, master_port=6000
[2023-09-27 00:08:43,197] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-09-27 00:08:46,312] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0388
Attention throughput (in TFLOP/s): 127.357
MLP duration (in seconds): 0.0367
MLP throughput (in TFLOP/s): 239.830
Transformer duration (in seconds): 0.0786
Transformer throughput (in TFLOP/s): 174.765
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0533
Attention throughput (in TFLOP/s): 95.514
MLP duration (in seconds): 0.0382
MLP throughput (in TFLOP/s): 237.644
Transformer duration (in seconds): 0.0956
Transformer throughput (in TFLOP/s): 148.248
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0546
Attention throughput (in TFLOP/s): 96.132
MLP duration (in seconds): 0.0394
MLP throughput (in TFLOP/s): 237.158
Transformer duration (in seconds): 0.0980
Transformer throughput (in TFLOP/s): 148.901
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0551
Attention throughput (in TFLOP/s): 97.836
MLP duration (in seconds): 0.0409
MLP throughput (in TFLOP/s): 235.913
Transformer duration (in seconds): 0.1007
Transformer throughput (in TFLOP/s): 149.357
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0570
Attention throughput (in TFLOP/s): 97.406
MLP duration (in seconds): 0.0418
MLP throughput (in TFLOP/s): 237.724
Transformer duration (in seconds): 0.1027
Transformer throughput (in TFLOP/s): 150.684
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0576
Attention throughput (in TFLOP/s): 99.071
MLP duration (in seconds): 0.0431
MLP throughput (in TFLOP/s): 237.349
Transformer duration (in seconds): 0.1047
Transformer throughput (in TFLOP/s): 152.090
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0596
Attention throughput (in TFLOP/s): 98.347
MLP duration (in seconds): 0.0432
MLP throughput (in TFLOP/s): 243.422
Transformer duration (in seconds): 0.1057
Transformer throughput (in TFLOP/s): 155.036
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 99.073
MLP duration (in seconds): 0.0446
MLP throughput (in TFLOP/s): 242.653
Transformer duration (in seconds): 0.1075
Transformer throughput (in TFLOP/s): 156.691
Transformer - MLP - Attention (in seconds): 0.0021
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0454
Attention throughput (in TFLOP/s): 136.295
MLP duration (in seconds): 0.0468
MLP throughput (in TFLOP/s): 237.722
Transformer duration (in seconds): 0.0955
Transformer throughput (in TFLOP/s): 181.365
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0607
Attention throughput (in TFLOP/s): 104.572
MLP duration (in seconds): 0.0479
MLP throughput (in TFLOP/s): 239.074
Transformer duration (in seconds): 0.1128
Transformer throughput (in TFLOP/s): 157.748
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0490
Attention throughput (in TFLOP/s): 132.995
MLP duration (in seconds): 0.0496
MLP throughput (in TFLOP/s): 237.087
Transformer duration (in seconds): 0.1020
Transformer throughput (in TFLOP/s): 179.088
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0622
Attention throughput (in TFLOP/s): 107.531
MLP duration (in seconds): 0.0510
MLP throughput (in TFLOP/s): 237.037
Transformer duration (in seconds): 0.1179
Transformer throughput (in TFLOP/s): 159.144
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0502
Attention throughput (in TFLOP/s): 136.652
MLP duration (in seconds): 0.0510
MLP throughput (in TFLOP/s): 243.043
Transformer duration (in seconds): 0.1040
Transformer throughput (in TFLOP/s): 185.134
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0638
Attention throughput (in TFLOP/s): 110.152
MLP duration (in seconds): 0.0540
MLP throughput (in TFLOP/s): 235.675
Transformer duration (in seconds): 0.1218
Transformer throughput (in TFLOP/s): 162.273
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0518
Attention throughput (in TFLOP/s): 139.017
MLP duration (in seconds): 0.0540
MLP throughput (in TFLOP/s): 241.832
Transformer duration (in seconds): 0.1087
Transformer throughput (in TFLOP/s): 186.479
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0657
Attention throughput (in TFLOP/s): 112.254
MLP duration (in seconds): 0.0572
MLP throughput (in TFLOP/s): 234.205
Transformer duration (in seconds): 0.1274
Transformer throughput (in TFLOP/s): 163.074
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0512
Attention throughput (in TFLOP/s): 147.607
MLP duration (in seconds): 0.0570
MLP throughput (in TFLOP/s): 241.095
Transformer duration (in seconds): 0.1107
Transformer throughput (in TFLOP/s): 192.473
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0672
Attention throughput (in TFLOP/s): 115.255
MLP duration (in seconds): 0.0600
MLP throughput (in TFLOP/s): 234.702
Transformer duration (in seconds): 0.1317
Transformer throughput (in TFLOP/s): 165.816
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0551
Attention throughput (in TFLOP/s): 143.942
MLP duration (in seconds): 0.0616
MLP throughput (in TFLOP/s): 234.476
Transformer duration (in seconds): 0.1204
Transformer throughput (in TFLOP/s): 185.774
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 117.979
MLP duration (in seconds): 0.0626
MLP throughput (in TFLOP/s): 236.146
Transformer duration (in seconds): 0.1352
Transformer throughput (in TFLOP/s): 169.412
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0561
Attention throughput (in TFLOP/s): 147.970
MLP duration (in seconds): 0.0621
MLP throughput (in TFLOP/s): 243.954
Transformer duration (in seconds): 0.1211
Transformer throughput (in TFLOP/s): 193.589
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0704
Attention throughput (in TFLOP/s): 120.629
MLP duration (in seconds): 0.0656
MLP throughput (in TFLOP/s): 236.520
Transformer duration (in seconds): 0.1403
Transformer throughput (in TFLOP/s): 171.062
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0575
Attention throughput (in TFLOP/s): 150.828
MLP duration (in seconds): 0.0654
MLP throughput (in TFLOP/s): 242.785
Transformer duration (in seconds): 0.1262
Transformer throughput (in TFLOP/s): 194.678
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0717
Attention throughput (in TFLOP/s): 123.848
MLP duration (in seconds): 0.0686
MLP throughput (in TFLOP/s): 236.772
Transformer duration (in seconds): 0.1443
Transformer throughput (in TFLOP/s): 174.166
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0572
Attention throughput (in TFLOP/s): 158.612
MLP duration (in seconds): 0.0683
MLP throughput (in TFLOP/s): 243.462
Transformer duration (in seconds): 0.1282
Transformer throughput (in TFLOP/s): 200.524
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0734
Attention throughput (in TFLOP/s): 126.223
MLP duration (in seconds): 0.0702
MLP throughput (in TFLOP/s): 242.374
Transformer duration (in seconds): 0.1467
Transformer throughput (in TFLOP/s): 179.173
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 155.829
MLP duration (in seconds): 0.0735
MLP throughput (in TFLOP/s): 236.613
Transformer duration (in seconds): 0.1387
Transformer throughput (in TFLOP/s): 193.708
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0750
Attention throughput (in TFLOP/s): 129.048
MLP duration (in seconds): 0.0753
MLP throughput (in TFLOP/s): 236.284
Transformer duration (in seconds): 0.1549
Transformer throughput (in TFLOP/s): 177.239
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 157.708
MLP duration (in seconds): 0.0756
MLP throughput (in TFLOP/s): 240.541
Transformer duration (in seconds): 0.1418
Transformer throughput (in TFLOP/s): 197.871
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 130.544
MLP duration (in seconds): 0.0791
MLP throughput (in TFLOP/s): 234.716
Transformer duration (in seconds): 0.1609
Transformer throughput (in TFLOP/s): 178.148
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0641
Attention throughput (in TFLOP/s): 160.654
MLP duration (in seconds): 0.0776
MLP throughput (in TFLOP/s): 244.472
Transformer duration (in seconds): 0.1452
Transformer throughput (in TFLOP/s): 201.538
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0790
Attention throughput (in TFLOP/s): 132.946
MLP duration (in seconds): 0.0810
MLP throughput (in TFLOP/s): 239.125
Transformer duration (in seconds): 0.1645
Transformer throughput (in TFLOP/s): 181.708
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0633
Attention throughput (in TFLOP/s): 169.467
MLP duration (in seconds): 0.0808
MLP throughput (in TFLOP/s): 244.825
Transformer duration (in seconds): 0.1481
Transformer throughput (in TFLOP/s): 206.023
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0824
Attention throughput (in TFLOP/s): 132.739
MLP duration (in seconds): 0.0847
MLP throughput (in TFLOP/s): 238.532
Transformer duration (in seconds): 0.1715
Transformer throughput (in TFLOP/s): 181.628
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 162.283
MLP duration (in seconds): 0.0839
MLP throughput (in TFLOP/s): 245.734
Transformer duration (in seconds): 0.1564
Transformer throughput (in TFLOP/s): 203.142
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0848
Attention throughput (in TFLOP/s): 134.102
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 238.595
Transformer duration (in seconds): 0.1780
Transformer throughput (in TFLOP/s): 182.167
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 165.655
MLP duration (in seconds): 0.0873
MLP throughput (in TFLOP/s): 245.929
Transformer duration (in seconds): 0.1613
Transformer throughput (in TFLOP/s): 205.024
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0870
Attention throughput (in TFLOP/s): 135.932
MLP duration (in seconds): 0.0891
MLP throughput (in TFLOP/s): 245.806
Transformer duration (in seconds): 0.1812
Transformer throughput (in TFLOP/s): 186.180
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0719
Attention throughput (in TFLOP/s): 167.568
MLP duration (in seconds): 0.0907
MLP throughput (in TFLOP/s): 246.287
Transformer duration (in seconds): 0.1667
Transformer throughput (in TFLOP/s): 206.316
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0896
Attention throughput (in TFLOP/s): 137.001
MLP duration (in seconds): 0.0931
MLP throughput (in TFLOP/s): 244.659
Transformer duration (in seconds): 0.1870
Transformer throughput (in TFLOP/s): 187.486
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 173.249
MLP duration (in seconds): 0.0949
MLP throughput (in TFLOP/s): 244.724
Transformer duration (in seconds): 0.1712
Transformer throughput (in TFLOP/s): 208.674
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0927
Attention throughput (in TFLOP/s): 137.444
MLP duration (in seconds): 0.0968
MLP throughput (in TFLOP/s): 244.646
Transformer duration (in seconds): 0.1936
Transformer throughput (in TFLOP/s): 188.118
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 169.916
MLP duration (in seconds): 0.0984
MLP throughput (in TFLOP/s): 245.188
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 207.048
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0944
Attention throughput (in TFLOP/s): 139.983
MLP duration (in seconds): 0.1002
MLP throughput (in TFLOP/s): 245.272
Transformer duration (in seconds): 0.1977
Transformer throughput (in TFLOP/s): 191.199
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 174.113
MLP duration (in seconds): 0.1008
MLP throughput (in TFLOP/s): 248.455
Transformer duration (in seconds): 0.1830
Transformer throughput (in TFLOP/s): 210.333
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0956
Attention throughput (in TFLOP/s): 143.175
MLP duration (in seconds): 0.1037
MLP throughput (in TFLOP/s): 246.051
Transformer duration (in seconds): 0.2037
Transformer throughput (in TFLOP/s): 192.493
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0794
Attention throughput (in TFLOP/s): 175.533
MLP duration (in seconds): 0.1056
MLP throughput (in TFLOP/s): 246.141
Transformer duration (in seconds): 0.1900
Transformer throughput (in TFLOP/s): 210.076
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0982
Attention throughput (in TFLOP/s): 144.412
MLP duration (in seconds): 0.1078
MLP throughput (in TFLOP/s): 245.528
Transformer duration (in seconds): 0.2092
Transformer throughput (in TFLOP/s): 194.305
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0795
Attention throughput (in TFLOP/s): 181.512
MLP duration (in seconds): 0.1090
MLP throughput (in TFLOP/s): 247.227
Transformer duration (in seconds): 0.1931
Transformer throughput (in TFLOP/s): 214.213
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1005
Attention throughput (in TFLOP/s): 146.064
MLP duration (in seconds): 0.1115
MLP throughput (in TFLOP/s): 245.822
Transformer duration (in seconds): 0.2153
Transformer throughput (in TFLOP/s): 195.556
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0836
Attention throughput (in TFLOP/s): 178.662
MLP duration (in seconds): 0.1138
MLP throughput (in TFLOP/s): 245.279
Transformer duration (in seconds): 0.2021
Transformer throughput (in TFLOP/s): 211.950
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1024
Attention throughput (in TFLOP/s): 148.315
MLP duration (in seconds): 0.1164
MLP throughput (in TFLOP/s): 243.930
Transformer duration (in seconds): 0.2230
Transformer throughput (in TFLOP/s): 195.432
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0859
Attention throughput (in TFLOP/s): 179.725
MLP duration (in seconds): 0.1180
MLP throughput (in TFLOP/s): 244.886
Transformer duration (in seconds): 0.2080
Transformer throughput (in TFLOP/s): 213.201
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1041
Attention throughput (in TFLOP/s): 150.889
MLP duration (in seconds): 0.1191
MLP throughput (in TFLOP/s): 246.722
Transformer duration (in seconds): 0.2271
Transformer throughput (in TFLOP/s): 198.611
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0874
Attention throughput (in TFLOP/s): 182.682
MLP duration (in seconds): 0.1212
MLP throughput (in TFLOP/s): 246.748
Transformer duration (in seconds): 0.2129
Transformer throughput (in TFLOP/s): 215.456
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1061
Attention throughput (in TFLOP/s): 152.933
MLP duration (in seconds): 0.1231
MLP throughput (in TFLOP/s): 246.971
Transformer duration (in seconds): 0.2341
Transformer throughput (in TFLOP/s): 199.214
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0872
Attention throughput (in TFLOP/s): 189.179
MLP duration (in seconds): 0.1256
MLP throughput (in TFLOP/s): 246.166
Transformer duration (in seconds): 0.2180
Transformer throughput (in TFLOP/s): 217.504
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1082
Attention throughput (in TFLOP/s): 154.934
MLP duration (in seconds): 0.1285
MLP throughput (in TFLOP/s): 244.753
Transformer duration (in seconds): 0.2415
Transformer throughput (in TFLOP/s): 199.607
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0926
Attention throughput (in TFLOP/s): 183.976
MLP duration (in seconds): 0.1304
MLP throughput (in TFLOP/s): 245.157
Transformer duration (in seconds): 0.2276
Transformer throughput (in TFLOP/s): 215.218
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1106
Attention throughput (in TFLOP/s): 156.446
MLP duration (in seconds): 0.1326
MLP throughput (in TFLOP/s): 244.972
Transformer duration (in seconds): 0.2484
Transformer throughput (in TFLOP/s): 200.431
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0941
Attention throughput (in TFLOP/s): 186.791
MLP duration (in seconds): 0.1346
MLP throughput (in TFLOP/s): 245.404
Transformer duration (in seconds): 0.2331
Transformer throughput (in TFLOP/s): 217.014
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1137
Attention throughput (in TFLOP/s): 157.037
MLP duration (in seconds): 0.1359
MLP throughput (in TFLOP/s): 246.986
Transformer duration (in seconds): 0.2526
Transformer throughput (in TFLOP/s): 203.508
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0947
Attention throughput (in TFLOP/s): 191.522
MLP duration (in seconds): 0.1384
MLP throughput (in TFLOP/s): 246.295
Transformer duration (in seconds): 0.2386
Transformer throughput (in TFLOP/s): 218.837
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1155
Attention throughput (in TFLOP/s): 159.414
MLP duration (in seconds): 0.1409
MLP throughput (in TFLOP/s): 245.765
Transformer duration (in seconds): 0.2606
Transformer throughput (in TFLOP/s): 203.556
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0958
Attention throughput (in TFLOP/s): 195.086
MLP duration (in seconds): 0.1435
MLP throughput (in TFLOP/s): 245.133
Transformer duration (in seconds): 0.2459
Transformer throughput (in TFLOP/s): 219.072
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1229
Attention throughput (in TFLOP/s): 154.415
MLP duration (in seconds): 0.1455
MLP throughput (in TFLOP/s): 245.565
Transformer duration (in seconds): 0.2722
Transformer throughput (in TFLOP/s): 201.003
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1031
Attention throughput (in TFLOP/s): 186.821
MLP duration (in seconds): 0.1480
MLP throughput (in TFLOP/s): 245.232
Transformer duration (in seconds): 0.2564
Transformer throughput (in TFLOP/s): 216.699
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1259
Attention throughput (in TFLOP/s): 155.348
MLP duration (in seconds): 0.1503
MLP throughput (in TFLOP/s): 245.159
Transformer duration (in seconds): 0.2799
Transformer throughput (in TFLOP/s): 201.540
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1055
Attention throughput (in TFLOP/s): 188.149
MLP duration (in seconds): 0.1532
MLP throughput (in TFLOP/s): 244.254
Transformer duration (in seconds): 0.2632
Transformer throughput (in TFLOP/s): 217.592
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1272
Attention throughput (in TFLOP/s): 158.327
MLP duration (in seconds): 0.1539
MLP throughput (in TFLOP/s): 246.759
Transformer duration (in seconds): 0.2864
Transformer throughput (in TFLOP/s): 202.954
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1075
Attention throughput (in TFLOP/s): 190.007
MLP duration (in seconds): 0.1567
MLP throughput (in TFLOP/s): 246.135
Transformer duration (in seconds): 0.2690
Transformer throughput (in TFLOP/s): 219.299
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1312
Attention throughput (in TFLOP/s): 158.007
MLP duration (in seconds): 0.1596
MLP throughput (in TFLOP/s): 245.172
Transformer duration (in seconds): 0.2970
Transformer throughput (in TFLOP/s): 201.578
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1083
Attention throughput (in TFLOP/s): 194.115
MLP duration (in seconds): 0.1618
MLP throughput (in TFLOP/s): 245.473
Transformer duration (in seconds): 0.2744
Transformer throughput (in TFLOP/s): 221.380
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1346
Attention throughput (in TFLOP/s): 158.467
MLP duration (in seconds): 0.1647
MLP throughput (in TFLOP/s): 244.704
Transformer duration (in seconds): 0.3047
Transformer throughput (in TFLOP/s): 202.271
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1139
Attention throughput (in TFLOP/s): 189.962
MLP duration (in seconds): 0.1673
MLP throughput (in TFLOP/s): 244.392
Transformer duration (in seconds): 0.2860
Transformer throughput (in TFLOP/s): 218.657
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1379
Attention throughput (in TFLOP/s): 159.139
MLP duration (in seconds): 0.1691
MLP throughput (in TFLOP/s): 245.315
Transformer duration (in seconds): 0.3118
Transformer throughput (in TFLOP/s): 203.405
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1157
Attention throughput (in TFLOP/s): 192.285
MLP duration (in seconds): 0.1723
MLP throughput (in TFLOP/s): 244.292
Transformer duration (in seconds): 0.2923
Transformer throughput (in TFLOP/s): 220.143
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1400
Attention throughput (in TFLOP/s): 161.118
MLP duration (in seconds): 0.1742
MLP throughput (in TFLOP/s): 245.037
Transformer duration (in seconds): 0.3196
Transformer throughput (in TFLOP/s): 204.182
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1184
Attention throughput (in TFLOP/s): 193.169
MLP duration (in seconds): 0.1766
MLP throughput (in TFLOP/s): 245.139
Transformer duration (in seconds): 0.3003
Transformer throughput (in TFLOP/s): 220.378
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1420
Attention throughput (in TFLOP/s): 163.254
MLP duration (in seconds): 0.1799
MLP throughput (in TFLOP/s): 244.069
Transformer duration (in seconds): 0.3253
Transformer throughput (in TFLOP/s): 206.282
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1181
Attention throughput (in TFLOP/s): 198.990
MLP duration (in seconds): 0.1816
MLP throughput (in TFLOP/s): 245.224
Transformer duration (in seconds): 0.3030
Transformer throughput (in TFLOP/s): 224.538
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1317
Attention throughput (in TFLOP/s): 180.836
MLP duration (in seconds): 0.1834
MLP throughput (in TFLOP/s): 246.215
Transformer duration (in seconds): 0.3198
Transformer throughput (in TFLOP/s): 215.657
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1224
Attention throughput (in TFLOP/s): 197.243
MLP duration (in seconds): 0.1870
MLP throughput (in TFLOP/s): 244.775
Transformer duration (in seconds): 0.3124
Transformer throughput (in TFLOP/s): 223.796
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1343
Attention throughput (in TFLOP/s): 182.147
MLP duration (in seconds): 0.1892
MLP throughput (in TFLOP/s): 245.207
Transformer duration (in seconds): 0.3277
Transformer throughput (in TFLOP/s): 216.262
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1239
Attention throughput (in TFLOP/s): 200.075
MLP duration (in seconds): 0.1913
MLP throughput (in TFLOP/s): 245.838
Transformer duration (in seconds): 0.3190
Transformer throughput (in TFLOP/s): 225.153
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1366
Attention throughput (in TFLOP/s): 183.829
MLP duration (in seconds): 0.1936
MLP throughput (in TFLOP/s): 246.250
Transformer duration (in seconds): 0.3341
Transformer throughput (in TFLOP/s): 217.907
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1270
Attention throughput (in TFLOP/s): 200.435
MLP duration (in seconds): 0.1959
MLP throughput (in TFLOP/s): 246.613
Transformer duration (in seconds): 0.3278
Transformer throughput (in TFLOP/s): 225.033
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1405
Attention throughput (in TFLOP/s): 183.449
MLP duration (in seconds): 0.1995
MLP throughput (in TFLOP/s): 245.459
Transformer duration (in seconds): 0.3442
Transformer throughput (in TFLOP/s): 217.136
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1288
Attention throughput (in TFLOP/s): 202.790
MLP duration (in seconds): 0.2026
MLP throughput (in TFLOP/s): 244.929
Transformer duration (in seconds): 0.3348
Transformer throughput (in TFLOP/s): 226.191
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1419
Attention throughput (in TFLOP/s): 186.335
MLP duration (in seconds): 0.2045
MLP throughput (in TFLOP/s): 245.784
Transformer duration (in seconds): 0.3498
Transformer throughput (in TFLOP/s): 219.303
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1314
Attention throughput (in TFLOP/s): 203.822
MLP duration (in seconds): 0.2065
MLP throughput (in TFLOP/s): 246.623
Transformer duration (in seconds): 0.3421
Transformer throughput (in TFLOP/s): 227.181
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1433
Attention throughput (in TFLOP/s): 189.322
MLP duration (in seconds): 0.2092
MLP throughput (in TFLOP/s): 246.579
Transformer duration (in seconds): 0.3565
Transformer throughput (in TFLOP/s): 220.825
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1339
Attention throughput (in TFLOP/s): 205.178
MLP duration (in seconds): 0.2127
MLP throughput (in TFLOP/s): 245.761
Transformer duration (in seconds): 0.3514
Transformer throughput (in TFLOP/s): 226.908
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1464
Attention throughput (in TFLOP/s): 189.968
MLP duration (in seconds): 0.2159
MLP throughput (in TFLOP/s): 245.197
Transformer duration (in seconds): 0.3672
Transformer throughput (in TFLOP/s): 219.901
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1380
Attention throughput (in TFLOP/s): 204.014
MLP duration (in seconds): 0.2187
MLP throughput (in TFLOP/s): 245.133
Transformer duration (in seconds): 0.3613
Transformer throughput (in TFLOP/s): 226.308
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1487
Attention throughput (in TFLOP/s): 191.798
MLP duration (in seconds): 0.2206
MLP throughput (in TFLOP/s): 246.067
Transformer duration (in seconds): 0.3740
Transformer throughput (in TFLOP/s): 221.411
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1375
Attention throughput (in TFLOP/s): 209.973
MLP duration (in seconds): 0.2237
MLP throughput (in TFLOP/s): 245.802
Transformer duration (in seconds): 0.3667
Transformer throughput (in TFLOP/s): 228.612
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1537
Attention throughput (in TFLOP/s): 190.132
MLP duration (in seconds): 0.2879
MLP throughput (in TFLOP/s): 193.350
Transformer duration (in seconds): 0.4482
Transformer throughput (in TFLOP/s): 189.392
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1429
Attention throughput (in TFLOP/s): 206.978
MLP duration (in seconds): 0.2824
MLP throughput (in TFLOP/s): 199.582
Transformer duration (in seconds): 0.4349
Transformer throughput (in TFLOP/s): 197.584
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1555
Attention throughput (in TFLOP/s): 192.418
MLP duration (in seconds): 0.2948
MLP throughput (in TFLOP/s): 193.537
Transformer duration (in seconds): 0.4580
Transformer throughput (in TFLOP/s): 189.903
Transformer - MLP - Attention (in seconds): 0.0077
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1467
Attention throughput (in TFLOP/s): 206.463
MLP duration (in seconds): 0.2995
MLP throughput (in TFLOP/s): 192.878
Transformer duration (in seconds): 0.4559
Transformer throughput (in TFLOP/s): 193.132
Transformer - MLP - Attention (in seconds): 0.0097
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1599
Attention throughput (in TFLOP/s): 191.721
MLP duration (in seconds): 0.3106
MLP throughput (in TFLOP/s): 188.258
Transformer duration (in seconds): 0.4745
Transformer throughput (in TFLOP/s): 187.793
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1495
Attention throughput (in TFLOP/s): 207.388
MLP duration (in seconds): 0.3089
MLP throughput (in TFLOP/s): 191.548
Transformer duration (in seconds): 0.4625
Transformer throughput (in TFLOP/s): 195.013
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1619
Attention throughput (in TFLOP/s): 193.771
MLP duration (in seconds): 0.3092
MLP throughput (in TFLOP/s): 193.698
Transformer duration (in seconds): 0.4817
Transformer throughput (in TFLOP/s): 189.493
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1516
Attention throughput (in TFLOP/s): 209.400
MLP duration (in seconds): 0.3135
MLP throughput (in TFLOP/s): 193.322
Transformer duration (in seconds): 0.4717
Transformer throughput (in TFLOP/s): 195.800
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1654
Attention throughput (in TFLOP/s): 194.223
MLP duration (in seconds): 0.3189
MLP throughput (in TFLOP/s): 192.312
Transformer duration (in seconds): 0.4878
Transformer throughput (in TFLOP/s): 191.561
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1551
Attention throughput (in TFLOP/s): 209.437
MLP duration (in seconds): 0.3173
MLP throughput (in TFLOP/s): 195.597
Transformer duration (in seconds): 0.4772
Transformer throughput (in TFLOP/s): 198.143
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1680
Attention throughput (in TFLOP/s): 195.652
MLP duration (in seconds): 0.3265
MLP throughput (in TFLOP/s): 192.329
Transformer duration (in seconds): 0.5023
Transformer throughput (in TFLOP/s): 190.435
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1578
Attention throughput (in TFLOP/s): 210.696
MLP duration (in seconds): 0.3340
MLP throughput (in TFLOP/s): 190.210
Transformer duration (in seconds): 0.4954
Transformer throughput (in TFLOP/s): 195.351
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1706
Attention throughput (in TFLOP/s): 197.076
MLP duration (in seconds): 0.3341
MLP throughput (in TFLOP/s): 192.345
Transformer duration (in seconds): 0.5143
Transformer throughput (in TFLOP/s): 190.329
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1617
Attention throughput (in TFLOP/s): 210.233
MLP duration (in seconds): 0.3372
MLP throughput (in TFLOP/s): 192.796
Transformer duration (in seconds): 0.5053
Transformer throughput (in TFLOP/s): 195.960
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1747
Attention throughput (in TFLOP/s): 196.820
MLP duration (in seconds): 0.2740
MLP throughput (in TFLOP/s): 240.002
Transformer duration (in seconds): 0.4528
Transformer throughput (in TFLOP/s): 221.173
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1632
Attention throughput (in TFLOP/s): 213.074
MLP duration (in seconds): 0.2768
MLP throughput (in TFLOP/s): 240.297
Transformer duration (in seconds): 0.4441
Transformer throughput (in TFLOP/s): 228.060
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1778
Attention throughput (in TFLOP/s): 197.776
MLP duration (in seconds): 0.2791
MLP throughput (in TFLOP/s): 241.094
Transformer duration (in seconds): 0.4619
Transformer throughput (in TFLOP/s): 221.752
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1682
Attention throughput (in TFLOP/s): 211.377
MLP duration (in seconds): 0.2826
MLP throughput (in TFLOP/s): 240.763
Transformer duration (in seconds): 0.4544
Transformer throughput (in TFLOP/s): 227.972
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1802
Attention throughput (in TFLOP/s): 199.447
MLP duration (in seconds): 0.2828
MLP throughput (in TFLOP/s): 243.325
Transformer duration (in seconds): 0.4709
Transformer throughput (in TFLOP/s): 222.425
Transformer - MLP - Attention (in seconds): 0.0080
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1735
Attention throughput (in TFLOP/s): 209.441
MLP duration (in seconds): 0.2869
MLP throughput (in TFLOP/s): 242.511
Transformer duration (in seconds): 0.4648
Transformer throughput (in TFLOP/s): 227.849
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1859
Attention throughput (in TFLOP/s): 197.548
MLP duration (in seconds): 0.2900
MLP throughput (in TFLOP/s): 242.605
Transformer duration (in seconds): 0.4810
Transformer throughput (in TFLOP/s): 222.629
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================

1.13.1 

[2023-09-28 22:10:54,686] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-09-28 22:10:55,433] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.150.175, master_port=6000
[2023-09-28 22:10:55,434] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-09-28 22:10:58,543] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0392
Attention throughput (in TFLOP/s): 63.040
MLP duration (in seconds): 0.0371
MLP throughput (in TFLOP/s): 118.563
Transformer duration (in seconds): 0.0794
Transformer throughput (in TFLOP/s): 86.560
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0536
Attention throughput (in TFLOP/s): 47.539
MLP duration (in seconds): 0.0385
MLP throughput (in TFLOP/s): 117.884
Transformer duration (in seconds): 0.0961
Transformer throughput (in TFLOP/s): 73.691
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0548
Attention throughput (in TFLOP/s): 47.884
MLP duration (in seconds): 0.0398
MLP throughput (in TFLOP/s): 117.490
Transformer duration (in seconds): 0.0986
Transformer throughput (in TFLOP/s): 74.014
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0553
Attention throughput (in TFLOP/s): 48.791
MLP duration (in seconds): 0.0410
MLP throughput (in TFLOP/s): 117.641
Transformer duration (in seconds): 0.1008
Transformer throughput (in TFLOP/s): 74.613
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0571
Attention throughput (in TFLOP/s): 48.594
MLP duration (in seconds): 0.0419
MLP throughput (in TFLOP/s): 118.599
Transformer duration (in seconds): 0.1026
Transformer throughput (in TFLOP/s): 75.442
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0577
Attention throughput (in TFLOP/s): 49.462
MLP duration (in seconds): 0.0431
MLP throughput (in TFLOP/s): 118.561
Transformer duration (in seconds): 0.1045
Transformer throughput (in TFLOP/s): 76.200
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0597
Attention throughput (in TFLOP/s): 49.089
MLP duration (in seconds): 0.0429
MLP throughput (in TFLOP/s): 122.729
Transformer duration (in seconds): 0.1058
Transformer throughput (in TFLOP/s): 77.426
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0609
Attention throughput (in TFLOP/s): 49.470
MLP duration (in seconds): 0.0446
MLP throughput (in TFLOP/s): 121.366
Transformer duration (in seconds): 0.1079
Transformer throughput (in TFLOP/s): 78.077
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0453
Attention throughput (in TFLOP/s): 68.211
MLP duration (in seconds): 0.0469
MLP throughput (in TFLOP/s): 118.666
Transformer duration (in seconds): 0.0956
Transformer throughput (in TFLOP/s): 90.598
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0610
Attention throughput (in TFLOP/s): 52.044
MLP duration (in seconds): 0.0484
MLP throughput (in TFLOP/s): 118.217
Transformer duration (in seconds): 0.1138
Transformer throughput (in TFLOP/s): 78.176
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 65.899
MLP duration (in seconds): 0.0504
MLP throughput (in TFLOP/s): 116.680
Transformer duration (in seconds): 0.1031
Transformer throughput (in TFLOP/s): 88.655
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 53.370
MLP duration (in seconds): 0.0519
MLP throughput (in TFLOP/s): 116.454
Transformer duration (in seconds): 0.1190
Transformer throughput (in TFLOP/s): 78.846
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0504
Attention throughput (in TFLOP/s): 68.005
MLP duration (in seconds): 0.0516
MLP throughput (in TFLOP/s): 120.289
Transformer duration (in seconds): 0.1051
Transformer throughput (in TFLOP/s): 91.626
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0643
Attention throughput (in TFLOP/s): 54.673
MLP duration (in seconds): 0.0547
MLP throughput (in TFLOP/s): 116.477
Transformer duration (in seconds): 0.1228
Transformer throughput (in TFLOP/s): 80.442
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0521
Attention throughput (in TFLOP/s): 69.153
MLP duration (in seconds): 0.0544
MLP throughput (in TFLOP/s): 120.173
Transformer duration (in seconds): 0.1093
Transformer throughput (in TFLOP/s): 92.735
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0659
Attention throughput (in TFLOP/s): 55.978
MLP duration (in seconds): 0.0575
MLP throughput (in TFLOP/s): 116.511
Transformer duration (in seconds): 0.1275
Transformer throughput (in TFLOP/s): 81.481
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0512
Attention throughput (in TFLOP/s): 73.834
MLP duration (in seconds): 0.0569
MLP throughput (in TFLOP/s): 120.723
Transformer duration (in seconds): 0.1112
Transformer throughput (in TFLOP/s): 95.801
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0674
Attention throughput (in TFLOP/s): 57.425
MLP duration (in seconds): 0.0604
MLP throughput (in TFLOP/s): 116.542
Transformer duration (in seconds): 0.1320
Transformer throughput (in TFLOP/s): 82.699
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0552
Attention throughput (in TFLOP/s): 71.797
MLP duration (in seconds): 0.0617
MLP throughput (in TFLOP/s): 116.929
Transformer duration (in seconds): 0.1207
Transformer throughput (in TFLOP/s): 92.654
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0689
Attention throughput (in TFLOP/s): 58.843
MLP duration (in seconds): 0.0633
MLP throughput (in TFLOP/s): 116.829
Transformer duration (in seconds): 0.1366
Transformer throughput (in TFLOP/s): 83.837
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0565
Attention throughput (in TFLOP/s): 73.470
MLP duration (in seconds): 0.0628
MLP throughput (in TFLOP/s): 120.638
Transformer duration (in seconds): 0.1226
Transformer throughput (in TFLOP/s): 95.673
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0709
Attention throughput (in TFLOP/s): 59.883
MLP duration (in seconds): 0.0664
MLP throughput (in TFLOP/s): 116.790
Transformer duration (in seconds): 0.1415
Transformer throughput (in TFLOP/s): 84.810
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0580
Attention throughput (in TFLOP/s): 74.880
MLP duration (in seconds): 0.0658
MLP throughput (in TFLOP/s): 120.648
Transformer duration (in seconds): 0.1273
Transformer throughput (in TFLOP/s): 96.511
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 61.470
MLP duration (in seconds): 0.0695
MLP throughput (in TFLOP/s): 116.911
Transformer duration (in seconds): 0.1460
Transformer throughput (in TFLOP/s): 86.061
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0577
Attention throughput (in TFLOP/s): 78.671
MLP duration (in seconds): 0.0689
MLP throughput (in TFLOP/s): 120.614
Transformer duration (in seconds): 0.1296
Transformer throughput (in TFLOP/s): 99.124
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0740
Attention throughput (in TFLOP/s): 62.620
MLP duration (in seconds): 0.0706
MLP throughput (in TFLOP/s): 120.388
Transformer duration (in seconds): 0.1480
Transformer throughput (in TFLOP/s): 88.758
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0613
Attention throughput (in TFLOP/s): 77.257
MLP duration (in seconds): 0.0742
MLP throughput (in TFLOP/s): 117.140
Transformer duration (in seconds): 0.1396
Transformer throughput (in TFLOP/s): 96.203
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0754
Attention throughput (in TFLOP/s): 64.109
MLP duration (in seconds): 0.0760
MLP throughput (in TFLOP/s): 117.068
Transformer duration (in seconds): 0.1557
Transformer throughput (in TFLOP/s): 88.156
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0627
Attention throughput (in TFLOP/s): 78.715
MLP duration (in seconds): 0.0756
MLP throughput (in TFLOP/s): 120.285
Transformer duration (in seconds): 0.1413
Transformer throughput (in TFLOP/s): 99.291
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 65.255
MLP duration (in seconds): 0.0794
MLP throughput (in TFLOP/s): 117.008
Transformer duration (in seconds): 0.1608
Transformer throughput (in TFLOP/s): 89.131
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0639
Attention throughput (in TFLOP/s): 80.585
MLP duration (in seconds): 0.0775
MLP throughput (in TFLOP/s): 122.390
Transformer duration (in seconds): 0.1449
Transformer throughput (in TFLOP/s): 100.987
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0789
Attention throughput (in TFLOP/s): 66.555
MLP duration (in seconds): 0.0815
MLP throughput (in TFLOP/s): 118.889
Transformer duration (in seconds): 0.1645
Transformer throughput (in TFLOP/s): 90.825
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0632
Attention throughput (in TFLOP/s): 84.826
MLP duration (in seconds): 0.0810
MLP throughput (in TFLOP/s): 122.191
Transformer duration (in seconds): 0.1476
Transformer throughput (in TFLOP/s): 103.325
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0822
Attention throughput (in TFLOP/s): 66.494
MLP duration (in seconds): 0.0849
MLP throughput (in TFLOP/s): 118.987
Transformer duration (in seconds): 0.1717
Transformer throughput (in TFLOP/s): 90.675
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 81.156
MLP duration (in seconds): 0.0843
MLP throughput (in TFLOP/s): 122.265
Transformer duration (in seconds): 0.1558
Transformer throughput (in TFLOP/s): 101.970
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0845
Attention throughput (in TFLOP/s): 67.271
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 118.831
Transformer duration (in seconds): 0.1778
Transformer throughput (in TFLOP/s): 91.198
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0699
Attention throughput (in TFLOP/s): 82.904
MLP duration (in seconds): 0.0875
MLP throughput (in TFLOP/s): 122.658
Transformer duration (in seconds): 0.1609
Transformer throughput (in TFLOP/s): 102.775
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0873
Attention throughput (in TFLOP/s): 67.683
MLP duration (in seconds): 0.0895
MLP throughput (in TFLOP/s): 122.403
Transformer duration (in seconds): 0.1803
Transformer throughput (in TFLOP/s): 93.537
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0719
Attention throughput (in TFLOP/s): 83.811
MLP duration (in seconds): 0.0911
MLP throughput (in TFLOP/s): 122.683
Transformer duration (in seconds): 0.1664
Transformer throughput (in TFLOP/s): 103.310
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0899
Attention throughput (in TFLOP/s): 68.295
MLP duration (in seconds): 0.0931
MLP throughput (in TFLOP/s): 122.327
Transformer duration (in seconds): 0.1866
Transformer throughput (in TFLOP/s): 93.959
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 86.637
MLP duration (in seconds): 0.0950
MLP throughput (in TFLOP/s): 122.218
Transformer duration (in seconds): 0.1711
Transformer throughput (in TFLOP/s): 104.437
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0928
Attention throughput (in TFLOP/s): 68.620
MLP duration (in seconds): 0.0971
MLP throughput (in TFLOP/s): 121.936
Transformer duration (in seconds): 0.1932
Transformer throughput (in TFLOP/s): 94.243
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 84.959
MLP duration (in seconds): 0.0986
MLP throughput (in TFLOP/s): 122.361
Transformer duration (in seconds): 0.1783
Transformer throughput (in TFLOP/s): 104.066
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0939
Attention throughput (in TFLOP/s): 70.324
MLP duration (in seconds): 0.1005
MLP throughput (in TFLOP/s): 122.270
Transformer duration (in seconds): 0.1985
Transformer throughput (in TFLOP/s): 95.217
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0776
Attention throughput (in TFLOP/s): 86.693
MLP duration (in seconds): 0.1021
MLP throughput (in TFLOP/s): 122.697
Transformer duration (in seconds): 0.1831
Transformer throughput (in TFLOP/s): 105.121
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0959
Attention throughput (in TFLOP/s): 71.414
MLP duration (in seconds): 0.1043
MLP throughput (in TFLOP/s): 122.358
Transformer duration (in seconds): 0.2036
Transformer throughput (in TFLOP/s): 96.276
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0796
Attention throughput (in TFLOP/s): 87.591
MLP duration (in seconds): 0.1062
MLP throughput (in TFLOP/s): 122.306
Transformer duration (in seconds): 0.1895
Transformer throughput (in TFLOP/s): 105.344
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0978
Attention throughput (in TFLOP/s): 72.522
MLP duration (in seconds): 0.1080
MLP throughput (in TFLOP/s): 122.492
Transformer duration (in seconds): 0.2102
Transformer throughput (in TFLOP/s): 96.690
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0799
Attention throughput (in TFLOP/s): 90.271
MLP duration (in seconds): 0.1100
MLP throughput (in TFLOP/s): 122.412
Transformer duration (in seconds): 0.1940
Transformer throughput (in TFLOP/s): 106.599
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0999
Attention throughput (in TFLOP/s): 73.507
MLP duration (in seconds): 0.1123
MLP throughput (in TFLOP/s): 122.118
Transformer duration (in seconds): 0.2155
Transformer throughput (in TFLOP/s): 97.698
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0835
Attention throughput (in TFLOP/s): 89.387
MLP duration (in seconds): 0.1139
MLP throughput (in TFLOP/s): 122.535
Transformer duration (in seconds): 0.2009
Transformer throughput (in TFLOP/s): 106.600
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1015
Attention throughput (in TFLOP/s): 74.837
MLP duration (in seconds): 0.1162
MLP throughput (in TFLOP/s): 122.204
Transformer duration (in seconds): 0.2218
Transformer throughput (in TFLOP/s): 98.254
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0854
Attention throughput (in TFLOP/s): 90.434
MLP duration (in seconds): 0.1179
MLP throughput (in TFLOP/s): 122.544
Transformer duration (in seconds): 0.2067
Transformer throughput (in TFLOP/s): 107.258
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1034
Attention throughput (in TFLOP/s): 75.933
MLP duration (in seconds): 0.1202
MLP throughput (in TFLOP/s): 122.317
Transformer duration (in seconds): 0.2279
Transformer throughput (in TFLOP/s): 98.935
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0870
Attention throughput (in TFLOP/s): 91.792
MLP duration (in seconds): 0.1223
MLP throughput (in TFLOP/s): 122.227
Transformer duration (in seconds): 0.2127
Transformer throughput (in TFLOP/s): 107.823
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1057
Attention throughput (in TFLOP/s): 76.762
MLP duration (in seconds): 0.1245
MLP throughput (in TFLOP/s): 122.143
Transformer duration (in seconds): 0.2338
Transformer throughput (in TFLOP/s): 99.730
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0878
Attention throughput (in TFLOP/s): 93.943
MLP duration (in seconds): 0.1263
MLP throughput (in TFLOP/s): 122.462
Transformer duration (in seconds): 0.2179
Transformer throughput (in TFLOP/s): 108.815
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1081
Attention throughput (in TFLOP/s): 77.513
MLP duration (in seconds): 0.1290
MLP throughput (in TFLOP/s): 121.870
Transformer duration (in seconds): 0.2411
Transformer throughput (in TFLOP/s): 99.971
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0921
Attention throughput (in TFLOP/s): 92.490
MLP duration (in seconds): 0.1310
MLP throughput (in TFLOP/s): 122.006
Transformer duration (in seconds): 0.2264
Transformer throughput (in TFLOP/s): 108.208
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1103
Attention throughput (in TFLOP/s): 78.443
MLP duration (in seconds): 0.1332
MLP throughput (in TFLOP/s): 121.944
Transformer duration (in seconds): 0.2472
Transformer throughput (in TFLOP/s): 100.703
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0938
Attention throughput (in TFLOP/s): 93.681
MLP duration (in seconds): 0.1352
MLP throughput (in TFLOP/s): 122.108
Transformer duration (in seconds): 0.2331
Transformer throughput (in TFLOP/s): 108.540
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1126
Attention throughput (in TFLOP/s): 79.279
MLP duration (in seconds): 0.1371
MLP throughput (in TFLOP/s): 122.338
Transformer duration (in seconds): 0.2538
Transformer throughput (in TFLOP/s): 101.278
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0957
Attention throughput (in TFLOP/s): 94.671
MLP duration (in seconds): 0.1395
MLP throughput (in TFLOP/s): 122.191
Transformer duration (in seconds): 0.2392
Transformer throughput (in TFLOP/s): 109.152
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1146
Attention throughput (in TFLOP/s): 80.307
MLP duration (in seconds): 0.1418
MLP throughput (in TFLOP/s): 122.097
Transformer duration (in seconds): 0.2613
Transformer throughput (in TFLOP/s): 101.488
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0965
Attention throughput (in TFLOP/s): 96.882
MLP duration (in seconds): 0.1446
MLP throughput (in TFLOP/s): 121.665
Transformer duration (in seconds): 0.2444
Transformer throughput (in TFLOP/s): 110.221
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1223
Attention throughput (in TFLOP/s): 77.554
MLP duration (in seconds): 0.1461
MLP throughput (in TFLOP/s): 122.292
Transformer duration (in seconds): 0.2723
Transformer throughput (in TFLOP/s): 100.469
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1031
Attention throughput (in TFLOP/s): 93.451
MLP duration (in seconds): 0.1488
MLP throughput (in TFLOP/s): 121.982
Transformer duration (in seconds): 0.2558
Transformer throughput (in TFLOP/s): 108.585
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1256
Attention throughput (in TFLOP/s): 77.804
MLP duration (in seconds): 0.1516
MLP throughput (in TFLOP/s): 121.576
Transformer duration (in seconds): 0.2804
Transformer throughput (in TFLOP/s): 100.595
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1057
Attention throughput (in TFLOP/s): 93.904
MLP duration (in seconds): 0.1540
MLP throughput (in TFLOP/s): 121.495
Transformer duration (in seconds): 0.2633
Transformer throughput (in TFLOP/s): 108.735
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1288
Attention throughput (in TFLOP/s): 78.156
MLP duration (in seconds): 0.1561
MLP throughput (in TFLOP/s): 121.659
Transformer duration (in seconds): 0.2891
Transformer throughput (in TFLOP/s): 100.524
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1086
Attention throughput (in TFLOP/s): 94.091
MLP duration (in seconds): 0.1584
MLP throughput (in TFLOP/s): 121.719
Transformer duration (in seconds): 0.2702
Transformer throughput (in TFLOP/s): 109.179
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1323
Attention throughput (in TFLOP/s): 78.351
MLP duration (in seconds): 0.1606
MLP throughput (in TFLOP/s): 121.871
Transformer duration (in seconds): 0.2963
Transformer throughput (in TFLOP/s): 101.007
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1089
Attention throughput (in TFLOP/s): 96.536
MLP duration (in seconds): 0.1631
MLP throughput (in TFLOP/s): 121.740
Transformer duration (in seconds): 0.2752
Transformer throughput (in TFLOP/s): 110.353
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1348
Attention throughput (in TFLOP/s): 79.090
MLP duration (in seconds): 0.1657
MLP throughput (in TFLOP/s): 121.614
Transformer duration (in seconds): 0.3040
Transformer throughput (in TFLOP/s): 101.380
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1141
Attention throughput (in TFLOP/s): 94.827
MLP duration (in seconds): 0.1684
MLP throughput (in TFLOP/s): 121.423
Transformer duration (in seconds): 0.2857
Transformer throughput (in TFLOP/s): 109.449
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1375
Attention throughput (in TFLOP/s): 79.804
MLP duration (in seconds): 0.1703
MLP throughput (in TFLOP/s): 121.840
Transformer duration (in seconds): 0.3120
Transformer throughput (in TFLOP/s): 101.657
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1152
Attention throughput (in TFLOP/s): 96.557
MLP duration (in seconds): 0.1724
MLP throughput (in TFLOP/s): 122.077
Transformer duration (in seconds): 0.2913
Transformer throughput (in TFLOP/s): 110.425
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1396
Attention throughput (in TFLOP/s): 80.768
MLP duration (in seconds): 0.1746
MLP throughput (in TFLOP/s): 122.236
Transformer duration (in seconds): 0.3170
Transformer throughput (in TFLOP/s): 102.908
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1180
Attention throughput (in TFLOP/s): 96.883
MLP duration (in seconds): 0.1779
MLP throughput (in TFLOP/s): 121.691
Transformer duration (in seconds): 0.2996
Transformer throughput (in TFLOP/s): 110.442
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1428
Attention throughput (in TFLOP/s): 81.189
MLP duration (in seconds): 0.1805
MLP throughput (in TFLOP/s): 121.622
Transformer duration (in seconds): 0.3262
Transformer throughput (in TFLOP/s): 102.863
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1177
Attention throughput (in TFLOP/s): 99.839
MLP duration (in seconds): 0.1834
MLP throughput (in TFLOP/s): 121.375
Transformer duration (in seconds): 0.3048
Transformer throughput (in TFLOP/s): 111.592
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1323
Attention throughput (in TFLOP/s): 90.025
MLP duration (in seconds): 0.1855
MLP throughput (in TFLOP/s): 121.669
Transformer duration (in seconds): 0.3210
Transformer throughput (in TFLOP/s): 107.419
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1233
Attention throughput (in TFLOP/s): 97.918
MLP duration (in seconds): 0.1875
MLP throughput (in TFLOP/s): 122.053
Transformer duration (in seconds): 0.3150
Transformer throughput (in TFLOP/s): 110.971
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1352
Attention throughput (in TFLOP/s): 90.482
MLP duration (in seconds): 0.1912
MLP throughput (in TFLOP/s): 121.371
Transformer duration (in seconds): 0.3288
Transformer throughput (in TFLOP/s): 107.774
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1260
Attention throughput (in TFLOP/s): 98.390
MLP duration (in seconds): 0.1927
MLP throughput (in TFLOP/s): 122.032
Transformer duration (in seconds): 0.3214
Transformer throughput (in TFLOP/s): 111.760
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1375
Attention throughput (in TFLOP/s): 91.365
MLP duration (in seconds): 0.1959
MLP throughput (in TFLOP/s): 121.659
Transformer duration (in seconds): 0.3365
Transformer throughput (in TFLOP/s): 108.161
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1287
Attention throughput (in TFLOP/s): 98.879
MLP duration (in seconds): 0.1977
MLP throughput (in TFLOP/s): 122.170
Transformer duration (in seconds): 0.3294
Transformer throughput (in TFLOP/s): 111.986
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1398
Attention throughput (in TFLOP/s): 92.227
MLP duration (in seconds): 0.2003
MLP throughput (in TFLOP/s): 122.240
Transformer duration (in seconds): 0.3444
Transformer throughput (in TFLOP/s): 108.529
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1289
Attention throughput (in TFLOP/s): 101.260
MLP duration (in seconds): 0.2029
MLP throughput (in TFLOP/s): 122.257
Transformer duration (in seconds): 0.3351
Transformer throughput (in TFLOP/s): 113.005
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1420
Attention throughput (in TFLOP/s): 93.135
MLP duration (in seconds): 0.2058
MLP throughput (in TFLOP/s): 122.158
Transformer duration (in seconds): 0.3509
Transformer throughput (in TFLOP/s): 109.317
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1332
Attention throughput (in TFLOP/s): 100.582
MLP duration (in seconds): 0.2079
MLP throughput (in TFLOP/s): 122.458
Transformer duration (in seconds): 0.3457
Transformer throughput (in TFLOP/s): 112.396
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1449
Attention throughput (in TFLOP/s): 93.622
MLP duration (in seconds): 0.2110
MLP throughput (in TFLOP/s): 122.262
Transformer duration (in seconds): 0.3594
Transformer throughput (in TFLOP/s): 109.531
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1359
Attention throughput (in TFLOP/s): 101.094
MLP duration (in seconds): 0.2143
MLP throughput (in TFLOP/s): 121.954
Transformer duration (in seconds): 0.3522
Transformer throughput (in TFLOP/s): 113.182
Transformer - MLP - Attention (in seconds): 0.0021
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1477
Attention throughput (in TFLOP/s): 94.185
MLP duration (in seconds): 0.2160
MLP throughput (in TFLOP/s): 122.530
Transformer duration (in seconds): 0.3676
Transformer throughput (in TFLOP/s): 109.846
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1383
Attention throughput (in TFLOP/s): 101.787
MLP duration (in seconds): 0.2186
MLP throughput (in TFLOP/s): 122.613
Transformer duration (in seconds): 0.3612
Transformer throughput (in TFLOP/s): 113.185
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1507
Attention throughput (in TFLOP/s): 94.614
MLP duration (in seconds): 0.2225
MLP throughput (in TFLOP/s): 122.004
Transformer duration (in seconds): 0.3755
Transformer throughput (in TFLOP/s): 110.268
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1387
Attention throughput (in TFLOP/s): 104.061
MLP duration (in seconds): 0.2255
MLP throughput (in TFLOP/s): 121.912
Transformer duration (in seconds): 0.3665
Transformer throughput (in TFLOP/s): 114.390
Transformer - MLP - Attention (in seconds): 0.0023
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1540
Attention throughput (in TFLOP/s): 94.865
MLP duration (in seconds): 0.2865
MLP throughput (in TFLOP/s): 97.143
Transformer duration (in seconds): 0.4473
Transformer throughput (in TFLOP/s): 94.881
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1444
Attention throughput (in TFLOP/s): 102.427
MLP duration (in seconds): 0.2824
MLP throughput (in TFLOP/s): 99.792
Transformer duration (in seconds): 0.4355
Transformer throughput (in TFLOP/s): 98.663
Transformer - MLP - Attention (in seconds): 0.0087
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1570
Attention throughput (in TFLOP/s): 95.300
MLP duration (in seconds): 0.2936
MLP throughput (in TFLOP/s): 97.162
Transformer duration (in seconds): 0.4568
Transformer throughput (in TFLOP/s): 95.205
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1470
Attention throughput (in TFLOP/s): 102.999
MLP duration (in seconds): 0.2994
MLP throughput (in TFLOP/s): 96.468
Transformer duration (in seconds): 0.4548
Transformer throughput (in TFLOP/s): 96.798
Transformer - MLP - Attention (in seconds): 0.0084
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1599
Attention throughput (in TFLOP/s): 95.856
MLP duration (in seconds): 0.3088
MLP throughput (in TFLOP/s): 94.662
Transformer duration (in seconds): 0.4729
Transformer throughput (in TFLOP/s): 94.221
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1503
Attention throughput (in TFLOP/s): 103.181
MLP duration (in seconds): 0.3061
MLP throughput (in TFLOP/s): 96.672
Transformer duration (in seconds): 0.4637
Transformer throughput (in TFLOP/s): 97.251
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1629
Attention throughput (in TFLOP/s): 96.333
MLP duration (in seconds): 0.3067
MLP throughput (in TFLOP/s): 97.641
Transformer duration (in seconds): 0.4809
Transformer throughput (in TFLOP/s): 94.902
Transformer - MLP - Attention (in seconds): 0.0113
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1518
Attention throughput (in TFLOP/s): 104.541
MLP duration (in seconds): 0.3132
MLP throughput (in TFLOP/s): 96.750
Transformer duration (in seconds): 0.4712
Transformer throughput (in TFLOP/s): 97.997
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1664
Attention throughput (in TFLOP/s): 96.498
MLP duration (in seconds): 0.3169
MLP throughput (in TFLOP/s): 96.759
Transformer duration (in seconds): 0.4882
Transformer throughput (in TFLOP/s): 95.710
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1567
Attention throughput (in TFLOP/s): 103.658
MLP duration (in seconds): 0.3156
MLP throughput (in TFLOP/s): 98.336
Transformer duration (in seconds): 0.4791
Transformer throughput (in TFLOP/s): 98.669
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1689
Attention throughput (in TFLOP/s): 97.314
MLP duration (in seconds): 0.3253
MLP throughput (in TFLOP/s): 96.531
Transformer duration (in seconds): 0.5043
Transformer throughput (in TFLOP/s): 94.844
Transformer - MLP - Attention (in seconds): 0.0102
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1588
Attention throughput (in TFLOP/s): 104.661
MLP duration (in seconds): 0.3337
MLP throughput (in TFLOP/s): 95.184
Transformer duration (in seconds): 0.4983
Transformer throughput (in TFLOP/s): 97.109
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1722
Attention throughput (in TFLOP/s): 97.615
MLP duration (in seconds): 0.3330
MLP throughput (in TFLOP/s): 96.492
Transformer duration (in seconds): 0.5149
Transformer throughput (in TFLOP/s): 95.061
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1623
Attention throughput (in TFLOP/s): 104.749
MLP duration (in seconds): 0.3358
MLP throughput (in TFLOP/s): 96.817
Transformer duration (in seconds): 0.5062
Transformer throughput (in TFLOP/s): 97.802
Transformer - MLP - Attention (in seconds): 0.0081
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1754
Attention throughput (in TFLOP/s): 98.024
MLP duration (in seconds): 0.2770
MLP throughput (in TFLOP/s): 118.722
Transformer duration (in seconds): 0.4557
Transformer throughput (in TFLOP/s): 109.891
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1637
Attention throughput (in TFLOP/s): 106.202
MLP duration (in seconds): 0.2800
MLP throughput (in TFLOP/s): 118.772
Transformer duration (in seconds): 0.4481
Transformer throughput (in TFLOP/s): 113.031
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1783
Attention throughput (in TFLOP/s): 98.597
MLP duration (in seconds): 0.2819
MLP throughput (in TFLOP/s): 119.351
Transformer duration (in seconds): 0.4650
Transformer throughput (in TFLOP/s): 110.159
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1680
Attention throughput (in TFLOP/s): 105.780
MLP duration (in seconds): 0.2851
MLP throughput (in TFLOP/s): 119.342
Transformer duration (in seconds): 0.4589
Transformer throughput (in TFLOP/s): 112.869
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================

1.13.1 

[2023-09-29 23:05:00,377] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-09-29 23:05:01,131] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.144.149, master_port=6000
[2023-09-29 23:05:01,132] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-09-29 23:05:04,085] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1775
Attention throughput (in TFLOP/s): 101.246
MLP duration (in seconds): 0.2809
MLP throughput (in TFLOP/s): 122.468
Transformer duration (in seconds): 0.4674
Transformer throughput (in TFLOP/s): 112.045
Transformer - MLP - Attention (in seconds): 0.0090
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1711
Attention throughput (in TFLOP/s): 106.164
MLP duration (in seconds): 0.2849
MLP throughput (in TFLOP/s): 122.131
Transformer duration (in seconds): 0.4624
Transformer throughput (in TFLOP/s): 114.535
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 2, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1845
Attention throughput (in TFLOP/s): 99.523
MLP duration (in seconds): 0.2881
MLP throughput (in TFLOP/s): 122.091
Transformer duration (in seconds): 0.4788
Transformer throughput (in TFLOP/s): 111.830
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
